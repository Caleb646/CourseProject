https://campuswire.com/c/G984118D3/feed/933 Where do we submit our team formation? Is this done through the Microsoft CMT, or is it done through some Google Doc form?#736 links to this [Google Doc ](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit) which says: > After deciding your topics, each student should please enter their details in [this sign-up sheet](https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit?usp=sharing). Carefully enter the information used while registering in CMT into the first few columns (this information will be used for grading, so please be careful!). Then, enter your group name (could be anything, but please make it unique) and project theme / specific topic. Only the group leader needs to enter the project topic. However, every student needs to enter all other details.  tl;dr [this](https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit?usp=sharing) is the signup sheet on Google Docs
https://campuswire.com/c/G984118D3/feed/505 MP2.2 Deadline What is the deadline of MP2.2? In the description of MP2.2, I saw the deadline as Sept 18 at 11:59 PM CDT and was working according to that. But now I see that coursera has the deadline as Sept 18 at 1:59 AM CDT and my assignment is overdue. What is the actual deadline?It may be a timezone issue. My assignment shows Sept 18, 2022, at 11:59 pm (CDT) as the due date. Try clearing cookies and signing in with a new session. My assignment shows Sept 18, 2022, at 11:59 pm (CDT) as the due date as well. I think you may need to change the Timezone setting on Coursera.  By the way, the TA corrected the deadline several days ago. 9/18 11:59pm cst
https://campuswire.com/c/G984118D3/feed/839 prepare for the exam with the book Hi what chapters in the book should we read to prepare for the exam All chapter listed on the weekly overviews for weeks 1-6 on Coursera:  - Week 1: Ch. 1-6 - Week 2: Ch. 6 (section 6.3), 8 - Week 3: Ch. 9 - Week 4: Ch. 6 (section 6.4) - Week 5: Ch. 7, 10 - Week 6: Ch. 10 (section 10.4), 11 Please prioritize reviewing the **lecture videos** and your **weekly quizzes** and **practice quizzes**!   It is good that you're referring to the book, but most exam questions will be based on (and use similar notation to) lecture video material. got it thanks 
https://campuswire.com/c/G984118D3/feed/627 Struggling with MP2.3 I'm afraid I don't really know where to start here. Part of the issue is syntax (relatively new to Python).  The initial problem I'm having is when I try to modify "score_one()" and run the code, I seem to be getting hung up in the "if len(sys.argv) !=2" test and execution stops there. I'm sure this is something rudimentary, but any hints would be appreciated.I am also very new to python, I believe it's a boiler plate code. If you comment out these lines , you should be good to run the file  from the example """ if len(sys.argv) != 2:         print("Usage: {} config.toml".format(sys.argv[0]))         sys.exit(1) """ Are you passing the config.toml to the program?  How are you running the code - in a Jupyter/CoLab notebook, or locally? If it's local, you'll want both parameters. If it's a notebook, you can change that sys.argv check to "1" instead of "2" and just make sure the config.toml file is in the same directory as your notebook when you execute I don't know if your using an ide, i can help more if you are.  But basically you need to run the program like this...  python search_eval.py config.toml    config.toml is the argv   if you're trying to run script from terminal - python search_eval.py config.toml  Thanks a ton. That worked. hI William. what should i do if i'm using vscode? i tried print(len(sys.argv)) and i got a 1 when you said running local are you referring to the terminal?  I'm running in vscode so i guess sys.argv should be 1 instead of 2? Thank you
https://campuswire.com/c/G984118D3/feed/749 Question about Topic of Project Proposal  Hello,  May I know who/where can we reach out to confirm whether our project proposal is good to go? Or should we just propose something that we want to do first and wait for counterproposal?  On the other hand, it seems like the link(https://github.com/CS410Assignments/CourseProject/network/members) is not working   Thanks Hi,  You do not need approval for the proposal submission. After the deadline, the proposals will be peer-reviewed by both students and instructors.   Not sure why that link is not working - I will look into it!
https://campuswire.com/c/G984118D3/feed/607 Error when uploading to liveDataLab ranker = InL2Ranker()  TypeError: __init__() takes exactly 2 arguments (1 given)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/00048d28-aa90-48dd-ab39-2d2fc6fae55c/image.png)I get the same TypeError on MP2.4 too with only certain types of rankers. This only happens when uploading to LiveDataLab and I have no problems locally. Your `__init__` function needs to use default values so that it can be called *without any arguments* (i.e.: as `InL2Ranker()`). You might be tempted, like I was, to set the default to be the optimum value you found from the study. Unfortunately, this results in the grader failing. Instead you need to set default to match the skeleton code provided (see `some_param`).  **This information is not anywhere in the instructions; I found it by trial & error.**
https://campuswire.com/c/G984118D3/feed/224 Looking to collaborate Hi everyone, I recently joined this class so I am just getting up to date with the class structure. I live in Washington State (PST) and would love to work as a group! I am open to any time zone as well!  Please send me a DM on here or you can find me on Slack. I am excited to meet you all! Thanks, Rachel Lee You might want to join the room # project-team-search. Just found it, thank you for taking the time to respond :)
https://campuswire.com/c/G984118D3/feed/351 Activating VPN I downloaded the VPN client from the Illinois webstore, and I get to here: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/39055fc3-3e9e-4ec9-8f55-bdca70894110/image.png)  Am I on the right track? How exactly do I get access to the campus network?Yes, this is where you enter vpn.illinois.edu In case someone has difficulty setting up the VPN, there are detailed instructions (including an installation video) by Operating System in this [Wiki](https://answers.uillinois.edu/illinois/page.php?id=98773).
https://campuswire.com/c/G984118D3/feed/947 Unary encoding 0 Hi, this is more a question about curiousity, but Unary encoding is expressed as x greater than or equal to 1 = x-1 followed by a 0.  So for the value of 1, we would expect the result to simply be 0. 2, would be 10, 3 would be 110 etc...  In the case where we have a 0 value, would this result in a unary encoding NULL? Short answer: it's undefined.  Longer answer: there are two forms of unary coding: one for non-negative natural numbers (n >= 0), and the other for positive numbers (n >= 1). The second form is used in this class.  In the former, 0 => 0, 1 => 10, 2 => 110, etc In the latter, 1 => 0, 2 => 10, 3 => 110, etc  So we can represent a unary-coded 0 if we're using the first form but not the second. Notice that the same code word, ex: 110, decodes into different numbers (2 or 3) depending on which form is used so you can't mix and match and need to be consistent in which is used. Since we're using the second (ie, defined for positive integers) form in the class, 110 decodes to 3, and there is no valid encoding for 0. Thanks for your response! That definitely clarifies this.
https://campuswire.com/c/G984118D3/feed/892 Project Part 1 Question Just to confirm, for part 1 of the project (team formation 5% due October 17th), all we need to do is form a team and have each of us:  1) Create a Microsoft CMT account under the "CS410Expo2022" conference using our Illinois email. Where the leader has to fork the GitHub repo and create a new submission on CMT using our forked repo URL as our abstract and add group members as authors. 2) Fill out the the following google drive sign-up form (Where only the group leader needs to include the actual project topic).  https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit#gid=0  Am I right?i think the due date got updated, so its both due oct 23 now. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ef72084d-2cae-4a3d-8176-e45807319214/image.png) Thanks! Question solved I have the same understanding, Ali. See #736 confirming the dates and requirements: team formation on October 17, topic selection and proposal submission on October 23. I don't think this is correct. See #736  Hi, Maciej, Thanks for letting me know I had it right in my original post. Thanks for confirming!
https://campuswire.com/c/G984118D3/feed/756 Will Formulas Be Provided on Exam 1? Hello! Will any formulas be provided on the exam? For example, the ranking functions for Dirichlet prior and JM? Thanks!  Formulas of basic concepts are usually not provided. Please refer to the quizzes/practice quizzes about this. 
https://campuswire.com/c/G984118D3/feed/728 Issue about MP2.4 submission I submitted my MP this noon successfully,  but when I planned to submit it this night again. It didn't work. And it shows below: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a1c662fe-6f25-476e-a73d-7655206846bd/image.png)  I think of the token expiration issue, after I regenerate the token, and relink the GitHub account with livedatalab. But it still doesn't work. Could anyone give some hints or suggestions?Qiyang, After generating your new token in GitHub, it is necessary to go to LiveDataLab and "delete linked accounts" (top right corner of home page) then "link new account". David Thx, I will try it. I think I just relink the account but not delete the old one. I also relinked without deleting. That didn't work for me. I hope you are successful. Thx! It is fixed.
https://campuswire.com/c/G984118D3/feed/391 Lesson 3.3 - Slide 3 Hi, I wanted to ask if there is any particular reason some of these data points are blue and others are white? ![Screen%20Shot%202022-09-08%20at%203.24.18%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b7c5e150-6108-41ba-be8a-5011492bbf2a/Screen%20Shot%202022-09-08%20at%203.24.18%20PM.png)Blue points mean the new retrieved document is relevant, so the Recall increase as the Precision changes. White points mean the new document is not relevant, so the Recall will not increase as the Precision decreases. At the end of curve, we assume that the Precision of these points is 0 because these relevant documents are not retrieved. Awesome, thank you very much!
https://campuswire.com/c/G984118D3/feed/1040 Group Name for Solo Teams in Google Doc Hello, this may be a silly question, but I'm wondering if teams of one still need to pick a unique group name in the sign-up sheet for the project? Currently many folks, including myself, have just put "Working Alone" under the Group Name column, but I just wanted to be sure that this wouldn't cause any issues for the TAs when it came to assigning project reviewers. I'm happy to change this to a unique name if "working alone" will not work.Please try to make the name unique, if possible. This way, there will be no confusion in the grading.  Understood, thank you Kevin!
https://campuswire.com/c/G984118D3/feed/189 LiveDataLab Hi all, the livedatalab link should be fixed now.   To register for the course, use: http://livedatalab.centralus.cloudapp.azure.com/course/join/WZS77P00E570J74   The webhook is: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger   You should be able to see your logs and score on the leaderboard.   Thanks for your patience!
https://campuswire.com/c/G984118D3/feed/1099 build_vocabulary Hi, I am trying to understand the build_vocabulary step, do we build it from the document? is the list going to be ['willis', 'tower', 'seattle', 'chicago'...] so the lenghth would be 6?The vocabulary is the unique list of words in the entire corpus, i.e. the collection of documents. You have to process each word in each docuement of the corpus. If the word is not already in the list, then you add it to the list. The final list will look something like "['willis', 'tower', 'seattle', 'chicago'...]." thanks so the length would be 6 correct? If it helps, for me the length is 8 as '0' & '1' also got added to the list. I had a length of 8 Could you help me understand how to get from 6 to 8? I'm applying numpy unique function to the each doc in self.documents but am only getting 6 words without the 0 and 1 6 should be fine. I originally got 6, and made changes and now get 8. Is it 6 or 8, or are both fine? Me too, I wonder if a TA would bee able to answer this? Yes, 6 is correct. You may need to remove 0 and 1 as they might be the labels. I completed the project and my vocab size for text.data was 6 -> ['chicago' 'mount' 'rainier' 'seattle' 'tower' 'willis'] I passed with 8. So I guess 6 or 8 will work I had 6, as I did remove the labels. But, I suppose 8 also will not be a problem
https://campuswire.com/c/G984118D3/feed/566 Unequal length array while using scipy test ![Screen%20Shot%202022-09-21%20at%2011.45.17%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/81a75e37-2d19-4f15-a733-92a24a591bf1/Screen%20Shot%202022-09-21%20at%2011.45.17%20PM.png) Hey! I am trying to calculate the p-value using the scipy.stats.ttest_rel however I am getting an error saying that we are comparing unequal arrays. The InL2 function is working for me, Could someone help me in telling where I might be going wrong ? Are you reading in the values from a text file? If so, there could be an extra newline at the end, and if it's not stripped out, you might be comparing different length arrays. I did it in R.   bm25 = read.table('bm25.avg_p.txt')$V1 inl2 = read.table('inl2.avg_p.txt')$V1 t.test(bm25, inl2, paired=T)  I think I had to change the $V1 to something like $V5 to get the actual number out of the line of text. Other than that, it worked fine. Your first list should be all float numbers, not single digits, i.e., [0.123] not [0,1,2,3]. That's why you have length of 3216 instead of 225.
https://campuswire.com/c/G984118D3/feed/975 p(A,B|C) This should be a fundamental math question, but it bothers me.  Does p(A,B|C) mean: 1) p((A|C),(B|C)) prob of (A given C) and (B given C) or 2) p ((A), (B|C)) prob of A and (B given C)?  Thanks!It should be the probability of (A and B ) given C this is a very interesting topic (i recommend Advanced Bayesian Modeling [ABM] course as well; it is definitely tough but will go in depth into this topic if you are really interested in this)  first, as Anonymous says notation-wise it stands for the joint probability of `A and B` given C.  generally, it is NOT the joint probability between `A` and `B Given C` $$P(A, (B|C))$$. however, if B and C are independent, than $$P(B|C) = P(B)$$, so in that way it could get simplified  i refer to this post on stackoverflow: https://stats.stackexchange.com/questions/440993/does-pab-cpb-pac  the C in  $$p(A,B∣C) = p(A|B,C)p(B∣C)$$ refers to your `pior information`  this is formulating, in words: the `joint probability of A and B, Given C` equals the `joint probability of A, Given B and C` times `the probability of B, Given C`  thus, your two events are `B Given C` and then, once you discover B you can discover A by `A, Given B and C `  so let's look at $$p(A∣C,B∣C)$$  i've tried deriving the equation this way, with some insight from this stack overflow answer as well https://stats.stackexchange.com/questions/337667/pab-and-pac-known-what-is-pabc  $$\begin{aligned} P(A,B |C) &= \frac{P(A,B)P(C|B,A)}{p(C)} \\ &=\frac{P(A)P(B)P(C|B,A)}{P(C)} \\ &= \frac{P(A)P(B)\frac{P(C|B)P(C|A)}{P(C)}}{P(C)} \\ &= \frac{P(A)P(B)\frac{P(C)P(B|C)}{P(B)} \frac{P(C)P(A|C)}{P(A)}}{P(C)P(C)} \\ &= P(B|C)P(A|C)\end{aligned}$$  i get to the last step by cancelling out terms that are in both the numerators and the denominators. i believe this shows it is your 1) formula if my understanding is correct  it's been a little while since i took ABM so this is also review for me a bit. i am happy to try to discuss further!!  i also recommend looking into [Joint Probability Distributions](https://en.wikipedia.org/wiki/Joint_probability_distribution) to see if this helps p(A, B|C) means p(A and B|C). If A and B are independent given C, p(A, B|C) = p(A|C)p(B|C).  ',' has a higher precedence than '|' 
https://campuswire.com/c/G984118D3/feed/1155 multiple runtime warning Hello, I got these runtime warning, after that all the likelihood value becomes `nan`. I'm not sure what is causing that.  I did normalize the matrix `topic_word_prob` and `document_topic_prob`.  ![Screen%20Shot%202022-10-23%20at%2008.18.12.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/818aa615-13f6-49d8-a9d5-4ab0143b99a0/Screen%20Shot%202022-10-23%20at%2008.18.12.png)Did you get it work on local ? It seems you have some overflow issue. What do you mean by "work on local"? This is the result from livelab. Did you get it converged on your local machine ?  Actually, I don't know how to run the code on a local machine. What's the command? You can run it in IDE or python xxx.py in command line. I suggest you to get it work locally before submit it to the LivedataLab. If your file is `plsa.py`, you can go to this working directory in your terminal and enter `python plsa.py` to execute your code locally. Locally, it also has these errors. It's recommended to first test **on your local machine**. Based on your logs here, it seems that you miscalculated the log likelihood.  Thank you!
https://campuswire.com/c/G984118D3/feed/826 Study guide for exam Will there be any study guide provided for exam preparation? I don't see one posted in coursera, though there is a lecture video (the last one) of week 6 with a good summary of topics. Could be worth checking out Each week has an Overview page with Guiding Questions. I’d recommend going through and writing answers for these for Weeks 1-6 as a study guide. They’ve been very helpful for me at least  According to the TAs/professors, the quizzes are essentially our study guides. The questions are drawn primarily from those, either exactly or similarly. From the syllabus:  > ...exams are mostly to confirm that you have indeed mastered the materials. So, they will have similar questions to the quiz questions that you have already seen. And in fact, some questions may be exactly the same as the questions in the quizzes.  I personally did not find lesson 6.10 to be of much help. It was just a high-level overview of the course material covered through week 6 with no concrete reference to the exam content as far as I could tell. In one of the office hours, professor mentioned of sharing the list of topics. I think he wanted to ensure everyone else has finished their quiz.
https://campuswire.com/c/G984118D3/feed/691 MP2.3 what are we supposed to do with load_ranker(cfg_file)? I'm not sure what are we supposed to do with with this function, since the instructions tell us to ignore the parameter. Am i just hard code it and a return a inl2 ranker like this?  ``` def load_ranker(cfg_file):     """     Use this function to return the Ranker object to evaluate, e.g. return InL2Ranker(some_param=1.0)      The parameter to this function, cfg_file, is the path to a     configuration file used to load the index. You can ignore this for MP2.     """     #     return metapy.index.InL2Ranker(some_param=1.0) ```  that easy? really confused. why don't i just call the ranker in main if that's the caseHello, the InL2Ranker is a class you made it. That means it is not in  metapy.index. So, you can just return InL2Ranker directly. that's it? just one line?  ``` def load_ranker(cfg_file):     InL2Ranker(some_param=1.0) ```  Thank you Since it is implemented here:  ``` class InL2Ranker(metapy.index.RankingFunction):     """     Create a new ranking function in Python that can be used in MeTA.     """     def __init__(self, some_param=1.0):         self.param = some_param         # You *must* call the base class constructor here!         super(InL2Ranker, self).__init__() ```  You just need to return the class like so:  ``` def load_ranker(cfg_file):     """     Use this function to return the Ranker object to evaluate, e.g. return InL2Ranker(some_param=1.0)      The parameter to this function, cfg_file, is the path to a     configuration file used to load the index. You can ignore this for MP2.     """     #     return InL2Ranker(some_param=1.0) ```  For load_ranker function, yes. you need to add "return"  If don't assign param, it will use the default as in init function some_param=1.0.
https://campuswire.com/c/G984118D3/feed/1225 Runtime warning ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2345a52d-9ef5-47f5-89b1-b2ea79a9e995/image.png)  Could someone give advice on how to deal with this error. When testing on my local machine, these runtime errors don't come up. It's only when I run it on livedatalab does it show up.I would suggest to check: 1. If your probability matrix sum to 1 2. If your log likelihoods is a negtive value and increase monotonically. 1) Which probability matrix? There are three . Also by sum to 1 do you mean normalizing it? Cause if so, I have done that.   2) Regarding my log likelihood values, they are negative, but the first 3 to 4 values decrease(meaning go towards zero) and then from that point forwards they start increasing monotonically(in the negative direction away from zero). At around 520-570 iterations my likelihood elements start becoming (-inf) and I get the error in the photo above when the first (-inf) value shows up. After that the program runs till 4k iterations and stops showing "SUCCESS"
https://campuswire.com/c/G984118D3/feed/18 The MP1 "Open Tool" button is not working. Hello everyone, I am able to access the MP1 through Livedatalab directly but not through **Coursera**. However, the instructions suggest that "**It is important to access LiveDataLab from the assignment on Coursera atleast once for each MP to ensure your grades are uploaded to Coursera**."  Do we need to worry about the "Open Tool" button? Or can we ignore it since it is not working?The "Open Tool" link is broken. You can reference this previous post for the correct link: https://campuswire.com/c/G984118D3/feed/11 So it means we can ignore the Coursera link and our grade will be uploaded to Coursera for sure? I just want to confirm to avoid any future issues. Thanks We are working on fixing the links and updating MP setup documentation. We will let you know via a post once it is done. Thank you!
https://campuswire.com/c/G984118D3/feed/914 Question 4 Hi,   I feel a little confused the result of P(Q|D2). why the value of it is 1/18. Thanks a lot.![question.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/638b08f9-cfbb-4a5c-aeec-d29b953645d2/question.png)JM Smoothing interpolates between the ML Estimate and the Collection LM  In the question above, the parameters we are given is that: ``` Query = "online course" D1 = "online course catalog" D2 = "affordable online education" ```  For D2, the only query term in the document is "online"; "course" does not appear. So, let's go one query term at a time:  $$\begin{aligned} P("online" | D2) &= (1 - \lambda) \frac{c("online",D2)}{|D2|} + \lambda P("online" | C) \\ &=(1 - 0.5) \frac{1}{3} + (0.5) \frac{1}{3} \\ &= \frac{1}{3} \\ P("course"|D2) &= (1 - \lambda) \frac{c("course", D2)}{|D2|} + \lambda P("course" | C) \\ &= (1 - 0.5) \frac{0}{3} + 0.5 \frac{1}{3} \\ &= \frac{1}{6} \\ P("online\ course" | D2) &=  P("online" | D2) P("course" | D2) \\ &= \frac{1}{3} * \frac{1}{6} \\ &= \frac{1}{18} \end{aligned}$$  The word counts per document ($$c(w, d)$$) are just the count of that term in the document and then those are normalized by number of terms in the document. In both D1 and D2 there are just three words, so when "online" appears in D2 that's how we get `1/3` for that term. "course" does not appear in D2, so we get `0/3`. The Collection LM gives us the estimates for those terms as well, which we can use for the second term in the expressions, and gives us a non-zero answer for $$P("course"|D2)$$. The final step is that, since this is a Unigram LM model, we assume each word is independently drawn from the Language Model, allowing us to multiply the probabilities together to get the probability for the whole term. One thing to note is that "education" does not appear in the query, so we do not need to use it here  Please let me know if that made sense, I am happy to clarify further Given, Q= “online course” D1 = “online course catalog” D2 = “affordable online education”  Using unigram query likelihood model, we have to determine P(Q|D2),  P(Q|D2) = P(w1|D) * P(w2|D) (since query has two words)  We know the formula for P(w|D) is:  **P(w|D) = [(1-lambda) * c(w|D)/|D|] + [lambda \* P(w|C)]**  Now, using the above formula, we will calculate P(w1|D) and P(w2|D),  P(w1|D) = [(0.5 * 1/3) + (0.5 * 1/3)] = 1/3  P(w2|D) = (0.5 * 0/3) + (0.5 * 1/3) = 0.5/3 = 1/6  => P(Q|D2) = 1/3 * 1/6 = 1/18 Thank you for your help. It make sense. Thank you for your help. It make sense.
https://campuswire.com/c/G984118D3/feed/417 Text book section 6.3.4 (page 104) BM25 Hello, As I was reading this part of BM25 introduction, I could not make sense how BM25 term frequency transformation be helpful in preventing "spammer"? Could someone please give me a real-world scenario where BM25 helps in this matter?  Thank you. > If we set k to a very large number, on the other hand, it’s going to look more like the linear transformation function. In this sense, this transformation is very flexible since it allows us to control the shape of the TF curve quite easily. It also has the nice property of a simple upper bound. This upper bound is useful to control the influence of a particular term. **For example, we can prevent a spammer from just increasing the count of one term to spam all queries that might match this term**If the transformation function was linear, then every count of a query term within a document would contribute to the document's rank. With the BM25 transformation function, $$y = \frac{(k + 1)x}{x+k}$$ with some reasonably-sized $$k$$, the contribution of every additional count is in a somewhat logarithmic fashion, so that an excessive occurrence of a term does not artificially inflate a document's rank (see figure below from the textbook).  For a real-life example, consider a user querying the term "computer". A spammer may set up a web page that simply contains the term "computer" hundreds of times in an attempt to artificially increase the rank of that web page on naïvely implemented search engines. Perhaps the spammer received per-impression ad revenue from the web page as a motivation for doing so. By reducing and effectively limiting the contribution of repeated uses of a term, such an attempt's success is reduced and truly relevant documents are not as easily ranked below the spam.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/466c36cf-5603-4690-93f2-771f59a97b07/image.png) It makes a lot more sense now. Your example is perfect. Thank you very much Steve
https://campuswire.com/c/G984118D3/feed/185 Extending Quiz 1 Deadline I was wondering if it would be possible to extend the deadline for the orientation quiz and quiz 1 for those who joined the class after the Sunday deadline.   Thank you!Same situation here. Just have the Coursera access this morning. Me three! In Coursera Platform it said will be extended to Week 8 Yes! of course! Whenever you guys submit your Quiz 1 please post a private note here on Campuswire to remind us to remove the late penalty from your grades. this is wrong, there will be a 5% late penalty per day incurred if you don't ask for an extension. Where is this information on Coursera? it will need to be corrected.
https://campuswire.com/c/G984118D3/feed/1327 Practice Quiz 11 Q11 How is the answer that there is a tie? The closest two documents to D6 are D5 and D4, with distances of 0.1 and 0.3, respectively. They both have a label of 1. The D2-D6 distance of 0.7 is much greater than either of those.In the question, please note that these are the similarity values, the higher the similarity value, the closer the documents are to each other. If we consider this, D6 is closer to D1 and D2 for k=2 and hence, a tie of 0 or 1. The table is NOT a distance measure. It is a similarity table. Higher the similarity value between the docs, greater is their closeness.  Thus, for D6, D1 and D2 are similar. Since, they fall in different labels, the algorithm results in a tie.
https://campuswire.com/c/G984118D3/feed/38 Quiz deadlines issue fixed Some posts have raised the issue of the quiz deadlines being wrong. We have fixed the problem. The Week 1 quiz is due on Aug 28, 11:59 PM CDT. Please let us know by commenting on this post if you find any issues with the other deadlines or have any issues accessing the quiz.
https://campuswire.com/c/G984118D3/feed/1245 MP3 Runtime warning Still not able to figure out why I keep getting the runtime warning: divide by zero encountered in log. I believe all my calculations are correct, so I am not sure why I am getting this error. Post #1225  Also, my score in coursera for MP3 has not updated even though my score in livedatalab is 1.  Does that mean it will be updated later or do I need to do something else?You may need to wait for a while before your grades are updated on Coursera.  I remember this happened to me when I was submitting MP2. I just added a new commit and pushed and everything worked as expected. I do not know why it fixed the issue for me. It is past the deadline now, so if it still does not show up you can contact the instructors and they'll be able to see that you submitted before the deadline.
https://campuswire.com/c/G984118D3/feed/781 MP2.4 Issue with leaderboard For some reason whenever I click on the leaderboard link on MP2.4 (and MP2.3 when I just tried it), I am logged out and redirected to the livedatalab login page. So, I am unable to see the scores of my submissions or any of the other leaderboard info.  This is all I can see: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9f481397-9fe6-478a-8bda-4e26be945b18/image.png)  Does anyone know where I can see the evaluation info or my score?Your submission looks good, seems like Coursera also picked it up.  But how do I see the evaluation info on the leaderboard such as my score & details of performance?  Direct link might work for you  http://livedatalab.centralus.cloudapp.azure.com/project/leaderboard/539268866
https://campuswire.com/c/G984118D3/feed/1172 Will we get any stats for the first exam? Will we get any stats for the first exam like the class average and standard deviation?Coursera system doesn't seem to provide us with the statistics of grades, so we had to export the grades to compute the statistics. Based on the grades we see so far in the system, the mean of the exam 1 grades is 15.7 and the standard deviation is 2.76. The median is 16. This means that most of you have done very well. Congratulations!   Thanks!
https://campuswire.com/c/G984118D3/feed/685 For 2.3, anyone able to import numpy module? import numpy as np  livedatalab gives error,  File "search_eval.py", line 7, in <module>  import numpy as np  ImportError: No module named numpy  Build step 'Execute shell' marked build as failure  Finished: FAILUREIt seems numpy is not supported by livedatalab.  It's similar to scipy.  I recommend commenting out your numpy code as only your significance.txt p-value is needed after calculation. No need to import numpy. You can use math.log(x, 2).  Livedatalab does not support numpy. So, I think you have to comment out your numpy code after you got p-value.
https://campuswire.com/c/G984118D3/feed/774 Repo for LiveDataLab? I like the final project idea of trying to enhance LiveDataLab. Is their a code repo for that project available, or is the idea more to do a proof of concept?Currently, the repo is private. I would suggest attending Prof. Zhai's office hours to discuss this idea.
https://campuswire.com/c/G984118D3/feed/1283 When is the deadline to review the other proposals of other groups? titleIn Week 1 Project Content, it says the deadline is Oct 30th The proposal peer-grading should be done between Oct 24 (12:00 am CST)-Oct 30 (11:59pm CST). 
https://campuswire.com/c/G984118D3/feed/959 MP3, Project proposal, Tech Review Topics all due the same week  I find it really stressful to have MP3, Project proposal, Tech Review Topics singup all due the same day on October 23rd, the week right after the Midterm. Even though the tech review topics signup only requires selecting a topic, but I'd still appreciate to invest time looking into all the options, reading some papers and choosing a topic I am interested in.  Given that MP3 was released two weeks later than expected on syllabus, can we get an extention on the MP3? I have the same stress as you. We have extended MP3 deadline. Hope that helps in relieving some of the stress.
https://campuswire.com/c/G984118D3/feed/699 Where to put idx? I'm a bit lost as to where to put  ``` import metapy idx = metapy.index.make_inverted_index('config.toml') ``` Is it supposed to be in it's own file or can I just place the ``` idx = metapy.index.make_inverted_index('config.toml') ``` after the imports at the top of the search_eval.py file?It is good practice to import libraries at the top of your code.  After import has been successful, your program should have access to the library no matter where you call it. check the example code in `search_eval.py`, which evaluates the ranker returned from `load_ranker`. the inverted index is created [on this line](https://github.com/CS410Assignments/MP2.3/blob/c86039b9106d1a563e80be551f303eb22f2fb6f9/search_eval.py#L41)  you could restructure the code as you would like, but having it here in the main entry point also works. basically, you'll want to create your inverted index before you start calling your `ranker.score` function, as defined [on this line](https://github.com/CS410Assignments/MP2.3/blob/c86039b9106d1a563e80be551f303eb22f2fb6f9/search_eval.py#L63), since the score function takes in the inverted index
https://campuswire.com/c/G984118D3/feed/867 Collaborative filtering. Is it fair to say that the length of the vector for user similarity represents the user activity? Or is the activity never taken into account for the vector representation of the User Profile? So the vector length of a user who is more active will be longer than the one who is less. Hence, if we use L2 distance for the similarity between users, then user activity will make the similarity somewhat biased?  A 2-D representation might look something like below for the vectors where the L2 distance is higher but the Cosine is less. So Cosine seems to be a better measure for similarity vs. L2.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4ee0489e-1fac-4fbb-80ed-46b619e6843a/image.png)From my understanding,  >the length of the vector for user similarity represents the user activity  I think so.  >is the activity never taken into account for the vector representation of the User Profile?  The vector incorporates both user activity history like rating of items and profiles like gender.  >if we use L2 distance for the similarity between users, then user activity will make the similarity somewhat biased?  I think so.  >Cosine seems to be a better measure for similarity vs. L2.  It depends on the features you take into vectors. If magnitude does not matter, cosine similarity works better. If it does, we prefer L2.  I understand we basically prefer cosine similarity for recommender systems because we want to categorize users with similar directions rather than similar vector norms. Yeah same here with all explanation.
https://campuswire.com/c/G984118D3/feed/368 MP2.1 "my submission" only shows 10 results Hi, I don't know if this happens to other students as well. For me, the page "my submission" only shows 10 of my submissions. I want to delete some duplicates (the source PDF files cannot be opened and I submitted twice by mistake) but they never showed up on the records. How can I see all of my submissions? I am using the macOS.  Thank you!You should be able to see all of your submissions (up to 50). What is your username? I'll check the records in the backend.  my username is same as my uiuc email: ypang10@illinois.edu Thanks, and can you send me a screenshot of your submissions page that you see? I found this issue yesterday. I don't have access to my mac right now but I will check it tonight. For now, I keep this post unresolved. Thank you!  Got it - if you noticed it before this post: https://campuswire.com/c/G984118D3/feed/344 then the issue should be resolved. Yes, it has been fixed already! Thank you!
https://campuswire.com/c/G984118D3/feed/193 Additional readings Hello,  Should we be taking notes on the additional readings to prepare for the exams? Or are the additional readings to expand our personal knowledge and understanding only?  Will the exams content cover the additional readings?  Thanks!No, the additional readings will not be covered in any exam or quiz questions. You are also not required to read any of them. Of course, you will likely find those readings interesting and it would be beneficial to you if you manage to read some of them, even if it's just skimming over them.  Thank you! I have found them to be quite helpful with my understanding of the content so far, but was curious if I should be taking notes on them. That takes the pressure off while reading them though!
https://campuswire.com/c/G984118D3/feed/994 Exam 1 curve? Hello,  I was just wondering if there will be a curve for the exam since the difficulty seemed a bit high for exam 1. Thank you!Yes, I have the same question.  I have the same question. Yes I agree with the difficulty part! I hope they curve :( Same question here. Same question. same! Yes the curve will be really appreciated. Aside from some similar (calculation) questions, I don't think other questions are related to the lectures. Most are very tricky and require very deep understanding of the material. A curve is very appreciated.  I guess there would have no curve as the cutoff is loose. still no updates?
https://campuswire.com/c/G984118D3/feed/194 Submission Queue I've been on the submission queue for about 20 minutes now, was wondering if this is normal behavior?  ![](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/18017250-2869-4cda-abda-3039a323e3f4/Screen%20Shot%202022-08-30%20at%204.54.51%20PM.png)This happened to me randomly, just push again with some empty commit (like change a whitespace somewhere or something). Ideally it'll start execution with your push. That worked. Thank you!
https://campuswire.com/c/G984118D3/feed/1252 MP3 LiveDataLab issue I am unable to get any output on MP3 in livedatalab, all my runs have the below error which doesn't seem to be with my code:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9a2d6955-5076-4e84-b020-3c1a07692f8c/image.png)  It seems like there is some error in your code. The error message is before the grade posting: 'encoding' is an invalid keyword argument for this function   Have you tried running your code locally? Yes, my code runs fine locally without errors. If it is 100% correct is yet to be determined.  Maybe 'encoding' is not supported as a keyword by the Python version on the grader. I just updated my code and now my webhook is not working, any advice on getting that back working?  Nvm, got it working! It was the keyword 'encoding', seems the Python on the grader didn't recognize it. I made sure to use Python 3.5 locally so Idk what the issue could be.
https://campuswire.com/c/G984118D3/feed/98 Size of the groups for the project? Hi everyone, I was wondering what the maximum size for the group project is. That is because it says the max size is 3 in the syllabus, but it says that the max size is 5 on the Course Project Section in Coursera (week 1).  So can we assume that the max size of the group for the project is 5 people? Sorry for the confusion and thank you for pointing out the inconsistency. The max size is 5.  Is the same output expected from groups regardless of their size (e.g. group of 5 is expected to do the same amount as a group of 3)?
https://campuswire.com/c/G984118D3/feed/45 MP1 install metapy / multiple python versions issue Hi,  I'm not able to install metapy successfully on my Mac and I'm getting same error as some of previous posts:  *Collecting metapy   Using cached metapy-0.2.13.tar.gz (3.4 MB)   Preparing metadata (setup.py) ... done Requirement already satisfied: pytoml in ./opt/anaconda3/lib/python3.9/site-packages (0.1.21) Building wheels for collected packages: metapy   Building wheel for metapy (setup.py) ... error   error: subprocess-exited-with-error      × python setup.py bdist_wheel did not run successfully.   │ exit code: 1   ╰─> [3 lines of output]       running bdist_wheel       running build       error: Could not find cmake executable       [end of output]*    Not too sure if this is because the instruction says we should use python 2.7 or 3.5 but my version is 3.9 (it was installed through Anaconda)? If that's the case is there any hint on how I can specify the exact python version to be used? I've already had 2.7, 3.5, 3.7 and 3.9 installed on my machine in different places (which is kind of a mess) and it is always using 3.9 by default.   Appreciate if anyone can help! You can specify the environment to use the version of Python needed (2.7 or 3.5) for the programming assignment.   https://www.youtube.com/watch?v=GqTsFOtZiQI  It works for me. I also faced the same error, I installed the 2.7.17 Python version and it helped me successfully install metapy.  Thanks this is very helpful!! I was also able to do it successfully for python 2.7 but not 3.5. Since many IDE (Visual Studio Code for example) no longer support 2.7, I created a virtual environment using python 3.5 and then select it as interpreter in the IDE. More information found here: https://code.visualstudio.com/docs/python/environments For anyone facing the issue of installing metapy on their Mac M1/M1 Pro machines, I believe there are a couple of reasons: - Metapy does not work after python 3.7 and most of the Mac machines come with python 3.8+ installed.  - The metapy package requires "cmake" to be installed and it is not found.  The easiest way to resolve both of these issues is to install Anaconda and use it to both install cmake as well as get the right python version. Here are the steps that worked for me:  1. Download the Anaconda from their website. 2. In your terminal, enter (this will download cmake for you) ``` conda install cmake ``` 3. Next, enter the following to create an environment to use python 3.7. ``` conda create -n $PYTHON37_ENV_NAME python=3.7 anaconda conda activate python=3.7 ``` 4. Next, in your terminal, type in  ``` conda install python=3.7 ``` 5. Next activate your environment ``` conda activate python=3.7 ``` 6. You can check your python version is 3.7 here by typing "python --version" 7. Now try installing metapy again in your python 3.7 environment: ``` pip install metapy pytoml ``` metapy should successfully install now. Hope this helped!  Reference: https://stackoverflow.com/questions/43630002/conda-install-downgrade-python-version  This is one of the best summary so far, and please also refer to #28 regarding our discussion on which version of anaconda you installed (M1 vs x86) and different pathways from there.  Thanks! Was able to get python code running in the virtual environment created by conda. (Just a note that I did it even without installing cmake) I also have an M1 machine. To solve this issue, I am simply using Google Colab for MPs, because metapy doesn't work on M1. Refer to this post - https://campuswire.com/c/G984118D3/feed/153 Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153   Thank you for finding this! However, I am still having a bit of trouble.  With regards to "conda create -n $PYTHON37_ENV_NAME python=3.7 anaconda", what does $PYTHON37_ENV_NAME mean?  The "anaconda" at the end means to install all packages present in the base environment, right?  When I try to run this command, I get a lot of stuff, then finally:  ``` Preparing transaction: ...working... done Verifying transaction: ...working... done Executing transaction: ...working... done ERROR conda.core.link:_execute(730): An error occurred while installing package 'defaults::qt-5.9.7-vc14h73c81de_0'. Rolling back transaction: ...working... done  LinkError: post-link script failed for package defaults::qt-5.9.7-vc14h73c81de_0 location of failed script: C:\Users\jbao8\anaconda3\envs\python=3.7\Scripts\.qt-post-link.bat ==> script messages <== <None> ==> script output <== stdout: stderr: 'C:\Users\jbao8\anaconda3\envs\python' is not recognized as an internal or external command, operable program or batch file.  return code: 1  ()  ```  What is going on? What should I do? Hello,  $PYTHON37_ENV_NAME just mean any environment name you wish to use. See a complete explanation of Condo commands here: https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html  hope it's helpful.  Okay. I will try to do stuff in a VirtualBox VM in the future.
https://campuswire.com/c/G984118D3/feed/744 MP2.4 Score  I'm just wondering for those of you who have higher scores (Top 20), are you using the Metapy ranker or your own?I used Okapi BM25 and adjusted the parameters. Note that a small change in the parameters can lead to a large change in the final score after trying both ( metapy & and my own ranker - similar to what we created in 2.3) I settled for one from metapy after extensive tuning. mainly BM25 Thank you. let's be careful here to not give an unfair advantage over people who have been making many submissions and trying many options to optimize their score. some folks have made over 100 submissions atm the top 3 alone have submitted 69, 63, and 89 times. i imagine some level of guidance is fine, but i recommend not getting too specific here until after the submission deadline Hi,  I feel idea of this exercise is for us to evaluate the different ranking algorithms and see how they work. I would suggest that we focus on learning them first and then trying them and evaluating their performance.  It would be helpful to understand the concept behind them and analyzing the type of data they would be most effective. In the process we will eventually be able to get a ranker with tuned parameters that can put one on the leaderboard. have to point out in optimization problem, sometimes pure luck is important too - there is a difference where your randomizer roll you a "0.123" to start with versus "0.124" :-)  lol ?
https://campuswire.com/c/G984118D3/feed/574 MP 2.4 Baseline on Leaderboard The readme on GitHub stated the following, but I don't see the "baseline" on the leaderboard.     *You must beat the "baseline" on the leaderboard to get full credit, i.e. your "Overall Score" should be greater than the Overall Score of the baseline. The last column on the Leaderboard indicates whether you completed this requirement or not (1 or 0).*  I have seen from the leaderboard, all students are 0 so far. So I think the baseline maybe an absolute score. Unless TAs update the "Overall Score" later on.  The baseline is now on the leaderboard as "test-user" with score 0.4170671141624618 (see Priyanka's comment on #583)
https://campuswire.com/c/G984118D3/feed/108 Working independently for final? I thought I read somewhere that working on our own is an option for the final project, but now I can't find that info. Is that correct? Thank you. Yes, that is what the "Course Project Overview" page says.  > Group work is encouraged, but not required (i.e., you can have a one-person team). The maximum size of a team is 5 members
https://campuswire.com/c/G984118D3/feed/1124 LDA number of parameters Hi everyone,  In LDA, are there either M or K x M **beta** parameters? (where K denotes the number of topics and M denotes the vocabulary size). Thanks!  ![Screen%20Shot%202022-10-21%20at%207.34.00%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/869d8a5c-6096-4cf2-85e7-fb62662be061/Screen%20Shot%202022-10-21%20at%207.34.00%20PM.png)Topic coverage has k parameters: $$\alpha_{1}, \alpha_{2}, ... \alpha_{k}$$. The Dirichlet distribution on which the topic word distributions are based has M parameters: $$\beta_{1}, \beta_{2}, ... \beta_{M}$$. (This is also mentioned in the textbook; you might want to refer to page 381).  Thank you very much, Harita!
https://campuswire.com/c/G984118D3/feed/326 metapy analyzers will end python ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8f0986ec-7dac-422f-81ac-a3e1129c1610/image.png)solved. how did you solve it? I encountered the same issue  you can enter python in your cmd first, then run those line by line. You will see which line has the issue. For me, it was a line that related to metapy. So I found there is something wrong with the installation and also something wrong with the exact line I was trying to execute. Hope this helps!  Got it! Thanks so much! I will try it :)
https://campuswire.com/c/G984118D3/feed/13 Typo on page 6 of mp setup.docx Hi guys, I just want to point out there's a typo on page 6 of this document. The Payload URL should be http://**livedatalab**.centralus.cloudapp.azure.com/api/webhook/trigger instead of http://livelab.centralus.cloudapp.azure.com/api/webhook/trigger.
https://campuswire.com/c/G984118D3/feed/815 Coursera No Score for MP2.4 Hi, I noticed that there is no score for MP2.4 on Coursera, while I got 1 on LiveDataLab. Has the score of MP2.4 been updated?  Coursera Grade: ![Screen%20Shot%202022-10-04%20at%204.53.23%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/72118c58-5f86-4e61-a5a0-6738a79babf5/Screen%20Shot%202022-10-04%20at%204.53.23%20PM.png)  My last submission on LiveDataLab: ![Screen%20Shot%202022-10-04%20at%205.26.34%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cc6bb43c-4661-4713-ba99-d3af34ac852f/Screen%20Shot%202022-10-04%20at%205.26.34%20PM.png)Did you try to open the LiveDataLab through OpenTool link from Coursera ?  Oh no. I did not find the scroll-down to reach there this time! Now I opened the LiveDataLab through the OpenTool link and updated my github again. I received the score but only 50%. Thank you very much for your help!  ![Screen%20Shot%202022-10-04%20at%206.28.58%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9cc3d96a-3cb3-47f7-b93f-03dda82e8226/Screen%20Shot%202022-10-04%20at%206.28.58%20PM.png)  BTW, can TAs help me fix the problem? Very appreciated!  I'd try asking that question on a post directed only to Instructors & TAs so it get's their attention Thanks! I’ll try!
https://campuswire.com/c/G984118D3/feed/1038 Extra OH on Thursday (10/20) Hi All,  I will hold an extra office hour on Thursday (10/20) at 7PM-8PM. Please check Live Events for the schedule.   Update:  (10/20) Sorry that I will **start at 8PM**, one hour late than the usual time.   Thanks, Yuxiang.
https://campuswire.com/c/G984118D3/feed/807 Format of questions in Exam 1 Hello, I wanted to know if the format of questions in exam 1, would be the same as all the quizzes we have had till now,ie, mcq's and select all that apply. Or would we also have questions where we have to write answers for them. Thank you in advance. Based on the comments from this announcement of Exam 1: #805   It seems that it will be around 20 MCQ's (probably a similar style to our quizzes, if I were to guess). I'd also check the last lecture of week 6 for a review. It appears the exam will be similar to the quizzes as pointed out Note this from the instructions  Exams are mostly to confirm that you have indeed mastered the materials. So, they will have similar questions to the quiz questions that you have already seen.
https://campuswire.com/c/G984118D3/feed/161 For finding a project team... ... please use the newly-created room #project-team-search:  https://campuswire.com/c/G984118D3/rooms/CFF99205E  When posting there, it may help to not only describe your timezone, but also your general topic interests.  Thanks, Kevin 
https://campuswire.com/c/G984118D3/feed/1147 Log Likelihood not monotonically increasing Hi,  when working on MP3, I noticed that my log likelihood incrased from -50k to -35k after ~10 iterations, then gradually decrased to -45k at the end of the 50th iteration. Is this normal? Shouldn't the log likelihood keep increasing until it converges? Thanks!This seems unusual. Can you pass the livedatalab?  My code had the same issue. You might want to check if your E step and M step update relevant variables correctly.
https://campuswire.com/c/G984118D3/feed/1177 LiveData lab converging after 85 iterations My code is converging after about 85 iterations on live data lab and then is saying INCORRECT IMPLENTATION, not sure how to debug further. (I am using 3 loops, could this be the reason?)  Also, the submissions are not showing up on the leaderboard.   Does this mean the mp grade will be 0 as it is not passing live data lab or will it be manually graded and given a reduced grade ? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/aae80891-cfbc-4006-9db4-1495a211908c/image.png)The log likelihood should be a negtive value. Please recheck your probability matrix. I am printing the absolute difference; my likelihood numbers are negative, Thank you!  However, I think my mistake is in the vocabulary as I have words like '\tmount' and '\tchicago' The vocab now only has 6 words, and my abs difference is lesser than epsilon at Iteration 19, but it still says incorrect  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/eb607697-0d48-4f76-ba50-f1c5bf5c2b11/image.png) comment out any print statements. Even the print statements orginally given such as "E Step:", "M steps:" ?  I removed the ones that I added and now the it goes to 85 iterations again :/ (Does this mean the mp grade will be 0 as it is not passing live data lab or will it be manually graded and given a reduced grade)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1c15be49-3545-4a4c-af0f-252c70a4179a/image.png)
https://campuswire.com/c/G984118D3/feed/232 Gamma encoding example The example slide about gamma encoding says that floor of log(x) is 2. My calculator is saying that ln(5) = 1.60943 which would make that floor value 1. Can anyone please help me understand what I'm missing here?Ya, you just have to use log base 2 instead of ln.
https://campuswire.com/c/G984118D3/feed/289 How to make sure MP1 completed successfully? I just submitted MP1. It shows submission is successful in livedatalab. The leaderboard shows the score=1 and Coursera graded 100%. As it's my first time submitting for MP, just want to make sure if it's completed? Thanks.I think it is completed. Thanks Yu Yes. You should be good.  Yeah. It is submitted successfully I think so. Once the grade is loaded on Coursera. You should have successfully submitted your work. Thanks all for the answers. I think I’m good. Enjoy the long weekend! I think the grade on Coursera will be used to calculate your final letter grade for this course. So as long as you get 100% on Coursera, you are good. Got it.  Thanks. 
https://campuswire.com/c/G984118D3/feed/39 Uninstalled and installed 3.7python but can't change path ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/fa67179d-7fd0-4eee-b4c7-7430a928056c/image.png) Posts say installing metapy issue need python3.7 I uninstalled the 3.10 and installed tahe 3.7 but the vs isn't using the 3.7 version, how could I change the pathIm not really sure if there exists 3.7 and when I install it again this happens ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f8fca00e-8ce8-41d1-9f81-37e5630eb545/image.png) https://stackoverflow.com/questions/3701646/how-to-add-to-the-pythonpath-in-windows-so-it-finds-my-modules-packages  This is how you can update your path.  However, I highly recommend setting up a conda environment for stuff like this.  https://docs.conda.io/projects/conda/en/4.6.0/_downloads/52a95608c49671267e40c689e0bc00ca/conda-cheatsheet.pdf Could you be a bit more specific on how to use conda to solve the issue? (Im using windows and I have an anaconda navigator in my laptop https://docs.conda.io/projects/conda/en/latest/_downloads/843d9e0198f2a193a3484886fa28163c/conda-cheatsheet.pdf  This is another cheat sheet.    I created a conda virtual environment with python 3.5.    conda create --name MP_env conda activate MP_env conda install python=3.5  Then load the other packages like this and use this environment.  Apologies if you already are doing this!  so does this mean you aren't doing mp on vs code but on anaconda? Does it mean you launch jupyter notebook to do the mp on anaconda platform? I like Spyder personally.  I am still working on getting the right version to run spyder and the metapy toolkit.  I don't personally use vs code much, I know many people do.   I believe 3.7 was not in use anymore. If you are using Windows for Assignment, then installing the 2.7.17 Python version using Windows Installer (https://www.python.org/downloads/windows/) trick worked for me. Make sure you check to "Update Path" variable while doing the Install. After this step, if you run 'pip install metapy pytoml' it works smoothly. I have also tried python 3.5, been told should work but reuslt in this![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ddffe311-dcb2-4329-863b-a418c8ef6967/image.png) Do you think I should uninstall 3.5?  Also I don't see the option to update path at this step, is it normal? thanks ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dc85fc33-d397-4186-a782-de0e5a470c86/image.png) In the above snapshot, the last cross, enable it (Add Python.exe to Path) python 3.7.13 works for me to pass MP1. Solved by using Bash Thanks Ayush, that's very helpful.
https://campuswire.com/c/G984118D3/feed/865 Practice Quiz #5 Q.8 I am wondering about the meaning of the following choice.  > The number of parameters in the transition matrix is smaller than the degree of freedom of the transition matrix  Is the transition matrix M for PageRank fixed or learnable? I think it is fixed from the lecture.Hello Yutaro, Please see the answer to #794  Thank you! I read this post, but I still wonder whether the transition matrix is fixed or not. In the lecture, it seems to distribute the transition possibility equally for each outbound link. Hello Yutaro, According to the lecture, the __original__ transition matrix is fixed, but PageRank algorithm iteratively performs matrix calculations to get a result.  However, I think the meaning of the quiz answer is different. To my understanding, "number of parameters" is the  number of possible web page destinations (N). Generally, "degrees of freedom" means the number of independent parameters of a model, or the number of things that can be assigned a statistical probability. This also is the number of possible destinations (N). So parameters cannot be smaller than degrees of freedom. I hope this is helpful to you. Oh, I see. It makes sense now. Thank you so much, David! 
https://campuswire.com/c/G984118D3/feed/172 can't join zoom meeting hey i can't join the zoom meeting - do i have to use the zoom account associated with school email address... if that's the case i can't because it shows up as "claimed". Yet according to the orientation about zoom meeting in our class, seems any free zoom account should work. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c50486e2-242e-430e-b83a-d533c250f434/image.png)Jianci, I have a personal Zoom account and had the same experience. What has worked consistently for me is to join through the browser rather than the Zoom app. This seems to avoid confusion between the Illinois Zoom account and my personal Zoom account. In any case, when I join through the browser it prompts me as required to log in to illinois.edu with SSO. After I log in, it puts me directly in the meeting. thanks! now it's showing meeting not started. I haven't be able to set up for PM1 yet - really need some help  I think Yuxiang concluded the meeting early because there were only one or two people attending. I was able to complete MP1, so I will try to help if I can. Are you running Windows or Mac? I emailed her saying I need help to get into the office hour... :-( I hope you can connect. She really tries to be helpful. also I insisted in joining the meeting via app is because the instruction specified that we needed to download zoom client app. :-/ I think we need to log into Zoom with Illinois email i did. i have taken other classes and not the first time log into a zoom office hour - somehow today doesn't work. i eventually turned off chrome completely and then went straight to log into UIUC account, then it worked. Guess like David said there was a confusion between my personal account and my UIUC account. Glad it has been resolved! I was having the same issue. After select SSO then use student account log in, I can join the zoom meeting. 
https://campuswire.com/c/G984118D3/feed/549 Looking for Team for Course project Hello there! I'm a master's student looking to join or create a team for the course project. I'm taking the class online, but I have a lot of flexibility to meet and discuss next steps online during the weekdays, and some weekends.   If you would like to reach out to me, you can just reply to this post or email me here at andrer2@illinois.edu. Looking forward to hearing from you all.Hey Andre, what timezone are you in? EST Same, we can team up if you'd like? Sure thing. I'll keep you posted when I get more messages. You could also try asking in the room #project-team-search .
https://campuswire.com/c/G984118D3/feed/267 tar command not working Hi everyone, I'm having trouble running the tar command. I'm using command prompt in Windows and keep getting a Syntax Error. There's one other post about this on Campuswire, but it didn't help me solve the problem. I've tried tar xvf, tar -xvf, and putting the filename in quotes as well as not having it in quotes. Nothing seems to work, so I'm wondering if anyone else had this issue and figured out how to solve it? Thank you!I figured out the problem I was having! I was trying to run the tar command in python, but I needed to do it in the Windows Command Prompt shell. Once I did that, all the problems were fixed. If you run the code on Jupyter Notebook, I think you can put an exclamation point at the beginning of the line of code, like "!tar ...". I solved a similar issue on Mac by adding it.
https://campuswire.com/c/G984118D3/feed/568 Adjacent matrix,HIT algorithm I am confused about the example of adjacent matrix given in the lecture.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7a805472-8f35-40c2-b56e-b1b7facf9c28/image.png) Can anyone explain how the A is calculated? Thanks! It's based on the graph shown to the left: every path from a node (i) to another node (j) is represented with a 1, and if there is no path, a 0.   For example, there's a path from d1 to d3, so row 1/column 3 is a 1 in the matrix. But there's no path shown from d1 to d2, so the corresponding element in the matrix (1,2) is a 0.
https://campuswire.com/c/G984118D3/feed/1170 Week 9 Practice Quiz To calculate the probability on question two I thought I would use the formula from the following lecture slide: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6403db4a-db33-405b-8804-daae5848cd87/image.png)  But this leads me to the wrong answer of 0.0589:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bf50e61c-ded4-4172-82cf-e4069c7e9721/image.png)  Bit confused why the method shown on the slide doesn't apply here  This is because we are generating the document i.e "the technology" from a single model itself once it is chosen. Its given in question as highlighted above ("all"). So once we choose first unigram model, we multiply probabilities of the all the words in that model to form given document => leads us to the formula given in practice quiz. If "all" wasn't given then we would use the formula given in the slides #944 also has some further discussion here  there are two possible cases as Dhyey mentions; either the topic model is chosen before we select each word, or it is chosen before we select any word. in the first case, model selection occurs as many times as there are words. in the second case, model selection happens once. those cases will change the probability calculation
https://campuswire.com/c/G984118D3/feed/1 Welcome to CS410! Dear students,  Thank you for taking CS410, and welcome to CS410 Campuswire!  The course website is at https://courses.grainger.illinois.edu/cs410/fa2022/ .  Please take a look at the introduction slides of the course, available here:  [410DSO-intro-2022.pptx](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6825e761-587d-4dea-bdf2-0d8d06665a58/410DSO-intro-2022.pptx).   We will use Campuswire for all the  announcements and discussions of issues related to the class. It also serves as the major platform for question answering. So please make sure that you are regularly on Campuswire.   I am very lucky to have six excellent experienced TAs to assist me in teaching this online class. They are:  1. Assma Boughoula  (Head TA, boughou1@illinois.edu)  2. Kevin Ros (Project Coordinator, kjros2@illinois.edu)  3. Campos Daniel (New MP Designer, dcampos3@illinois.edu) 	 4. Yuxiang Liu (MP Coordinator, yuxiang@illinois.edu) 	 5. Harita Reddy (Campuswire Coordinator, haritar2@illinois.edu) 	 6. Priyanka Dey (1/2 TA, LiveDataLab Support, pdey3@illinois.edu)  I want to use this opportunity to thank all of them for serving as the TAs for this class. Special thanks to Assma for serving as the Head TA to coordinate all the TA work and to Kevin for taking on the big task of coordinating course projects.   This is our first post on Campuswire. We will post more information about the course here soon. Meanwhile, please feel free to post any questions that you may have here and we will do our best to answer your questions as soon as we can.  We believe that collaborative learning is powerful and beneficial, so we strongly encourage all of you to share your expertise and help answer questions from each other if you can.  Hope you will enjoy this online class. The TAs and I  are looking forward to working with all of you!  Thanks,  ChengXiang ("Cheng") Zhai, Instructor of CS410 Email: czhai@illinois.edu
https://campuswire.com/c/G984118D3/feed/513 Error while setting up meta for MP2.3 Hi everyone,  I am trying to set up Meta by following the steps on this document (linked from MP2.3). When I run the make command from first screenshot, I am getting the error in the second screenshot. Please let me know if you have any ideas on how to solve this. Thank you.  ![Screen%20Shot%202022-09-18%20at%209.13.47%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/72d1164a-3035-4c80-978f-39c5fb75c01f/Screen%20Shot%202022-09-18%20at%209.13.47%20PM.png)![Screen%20Shot%202022-09-18%20at%209.13.21%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/39f6673f-79e5-4af5-aa25-be20d565f92c/Screen%20Shot%202022-09-18%20at%209.13.21%20PM.png) I wouldnt suggest using brew for installing cmake. Install anaconda for mac os. and using conda as package manager to install python 3.5 , then cmake and rest of the packages like metapy   check this link to create an environment with conda for python 3.5 and then activate it. then you can run conda install cmake. It should work ( it worked for me on both mac / windows) Including a post that has details of making python 3.5 environment [https://campuswire.com/c/G984118D3/feed/478] Python 3.7 also works for me. You can also try it on google colab.
https://campuswire.com/c/G984118D3/feed/385 MP release dates Hello prof and TAs, would it be possible to update the assignment release dates in https://www.coursera.org/learn/cs-410/supplement/wBlrv/course-deadlines-late-policies-and-academic-calendar so that we can plan better?Just FYI, we have taken note of your question and will be updating it soon.
https://campuswire.com/c/G984118D3/feed/295 Min of IDF(W) Just make sure that I get the correct answer since our professor leaves this one as an exercise.  The min of IDF(W) is $$log(\frac{M+1}{M})$$, right? May I ask what the base of log it use? And do we further simplify the result?My understanding is base of log is 2 log of base 2 is used !! log((M+1) / M) =  log (1 + 1/M) When M is large, 1/M is small so this mean log(1+x) when x is small, we have log(1+x) -> x. So the answer should be 1/M, am I correct?
https://campuswire.com/c/G984118D3/feed/758 MP2.4 parameter Can I simply change the parameter for OkapiBM25? From the leaderboard, I already got full credit, even though I only change the input parameter.Yes, if you're happy with that then you don't need to do more
https://campuswire.com/c/G984118D3/feed/490 What's the deadline of MP2.3? In the description of MP 2.3(pic1), it says the due is 9/25 11.59 pm, however, the actual deadline is set as 9/25 at 1.59 am ![2.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b38194a4-caf2-44fa-ae8c-ef9d1d9abd90/2.png)![1.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f3a8fe71-0966-4158-a596-ceed04c21738/1.png)(pic2). Is this a typo? As all the other deadlines are given as 11:59 PM CDT looks like a typo. Correct deadline would be Sep 25, 2022 11:59 PM. May be TAs can confirm. Yes, this is a typo, thank you for catching this!   I've updated the Coursera deadline to be 9/25 at 11:59pm CT Thanks!
https://campuswire.com/c/G984118D3/feed/481 MP2.2 Just to confirm again,  Search Again Your submissions  Total results: 20  **Number of submitted judgments: 5**  This should be good enough. right?Yes, looks right as the number of submitted judgements should be >=5 and the relevance has to be defined for 10 or more search results for each query.   Yes, you should be good :) I ran a search query and got 9 results back. I then submitted relevance judgement (relevant / not relevant radio buttons from the search results) against all those 9 search results. And the tool accepted my relevance submission. Just wanted to let you know, in case the search result brings fewer than 10 results, submitting less than 10 relevance judgements also will work. See here #457 
https://campuswire.com/c/G984118D3/feed/22 MP1 Grader Connection Error Here's the error from the submission log:  ``` raise ConnectionError(e, request=request)  requests.exceptions.ConnectionError: HTTPConnectionPool(host='http', port=80): Max retries exceeded with url: //livedatalab.centralus.cloudapp.azure.com/api/project/leaderboard/submit?leaderboard=63030e64205b89430b59b1f4 (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f44c47bdb50>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))  Build step 'Execute shell' marked build as failure  Finished: FAILURE ```  #21 Pause on LiveDataLab submissions right now until there is an update
https://campuswire.com/c/G984118D3/feed/1280 Tech Review Approval Should we assume our Tech Review proposals have been accepted, or will there be some process for course staff to let us know if our proposal for Tech Review is acceptable or not? Thank you!It should be automatically graded via LiveDataLab oh, sorry perhaps i missed something. were we supposed to submit something to livedata lab for the tech review proposal as well? i thought we just needed to sign up on the sheet and the livedatalab was for the actual tech review report Oh no my apologies, I thought you meant the actual report. For the proposal, as long as it is somewhat related to the course, then you should be fine. So generally, there is no need to worry about the proposal being accepted! ah no worries -- thank you very much Kevin!! Is tech review for completion marks?
https://campuswire.com/c/G984118D3/feed/273 Doubt Regarding Lesson 2.6 In Lesson 2.6: System Implementation - Fast Search, Professor showed an example where he ranked documents based on TF sum. Can you share an example, where you rank documents using inverted indexing and scoring function as Okapi BM25?  Also in the general form of scoring function, what can be g function, adjustment function? Can you show in terms of Okapi BM25 what will be aggregate function, g function and adjustment function? In other words, write okapi BM25 ranking function in general formYou raised excellent questions! It would actually be very useful for you (and all the students in the class) to think about the answers to these questions, and I believe that if you compare the BM25 formula with the general form of the scoring function, you likely will be able to figure out the answer. This kind of practice would help you implement other scoring functions using a toolkit like MeTA (you'll have a chance to work on this in MP2.3). That said, let me also provide answers to your questions.   First, it's useful to think about the difference between BM25 and the simple TF sum scoring function. You'll realize that there are 3 differences: 1) The TF used in BM25 is some transformed form of the raw TF. 2) There's an IDF component in BM25. 3) There's also a doc length normalization component, which is actually integrated with the TF transformation. So once you realize the difference, you can think about how to modify the TF sum scoring process to support BM25. First, instead of using the raw TF, you can do some further computation on top of the raw TF to obtain a BM25 TF. That is, instead of adding the raw TF to the score accumulator, you can add some form of transformation of the raw TF.  When computing the transformed/normalized TF using the BM25, you'll need document length, which can be easily obtained from a document dictionary where we store the length for each document. Now, consider how to add IDF. This value can also be obtained from a term dictionary as soon as you see/process a query term since it doesn't depend on specific documents to be scored. You can keep the IDF of the current query term and multiple the TF value by this IDF value before you add to the score accumulator.  I'll leave the writing of BM25 using the general form of the scoring function as a useful exercise for you. 
https://campuswire.com/c/G984118D3/feed/900 How can we check earned Extra Credit? Hi there, I wonder whether we can check our earned extra credit or not before the deadline of dropping courses, this Friday.   Could the course staff upload the extra credit we earned so far? Thank you very much!I think extra credits get added at the end of the semester when the final credits are rolled out. There is no in between announcements. Oh, I see. Let's wait for a TA's reply. In few courses I have seen extra credit getting added during the semester.  Usually a week after the general credit is given for the assignment.   I have seen the extra credit is based on your score in campuswire. You can see there is a number by your name, and the top students get the credit The top students? Are you sure? I just saw it will be based on our reputation scores. But I am not sure of the detail. Like how to convert the reputation scores to the actual EC we get. May be TAs can help us understand how to convert the reputation scores to the actual EC we get. Our plan is to calculate all the extra credits at the end of the semester mostly based on the ranking of reputation scores and overall activities on Campuswire (but also with consideration of the nature of contributions), so I'm not sure if we can meaningfully estimate the extra credit at this point. If you have particular concerns about your grade and consider dropping the course, please post a private note to us and we'd be happy to help you analyze your current situation and provide advice to you.  I see. Thank you for your reply!
https://campuswire.com/c/G984118D3/feed/822 Lecture 8.9 Bayesian  Hello,  I am confusing about Bayesian estimation. What are the inputs and outputs? What are we getting from this estimation and how does it related to topic mining?![Bayesain.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b3072996-5138-40a7-b493-edfb17f6873a/Bayesain.png)In a probabilistic topic model, we would represent a topic as a word distribution $$\theta$$. We need to figure out a way to estimate this word distribution $$\theta$$. One way of estimating this is using the likelihood function (the probability of the data given the assumed word distribution). This would be the maximum likelihood estimation method, where we are trying to find parameter $$\theta$$ such that $$P(X|\theta)$$ (the probability of the data given the parameters) is maximized. The parameters here are nothing but the $$\theta$$ that we mentioned earlier, i.e., the word distribution of the topic.  Rather than just finding $$\theta$$ by maximizing the likelihood of the data $$X$$, we could incorporate any prior knowledge about $$\theta$$ that we may have (for instance, for a specific use case, $$p(w|\theta)$$ for a specific word should have a specific value). We can use Bayesian estimation if we want to incorporate this prior knowledge. Using Bayes rule, we try to maximise $$p(\theta|X)$$, which is basically maximizing $$p(X|\theta)P(\theta)$$. Note that we have incorporated prior knowledge here through $$P(\theta)$$.  This is a more general estimator than maximum likelihood estimator.   Reference: Page 341 in the textbook.
https://campuswire.com/c/G984118D3/feed/1242 MP3 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/88e036c1-00a6-48f2-9e57-3b2eed7a0405/image.png)![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c9e5fe21-9890-4875-9d05-e3e8d721a66a/image.png) On the server and on my local I am seeing different log value why is thatIt is because we are using a different input file for autograder.  Hi Yuxiang, could you please take a look at #1241 and let me know if I'm on the right track please? TY!
https://campuswire.com/c/G984118D3/feed/1223 Proposal Peer Rev Question Do we have to add comments for this question, or can we leave it as N/A if we have nothing to say?  ![Screen%20Shot%202022-10-25%20at%206.42.22%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7156260e-bf8d-41ad-a21f-41537bc6266d/Screen%20Shot%202022-10-25%20at%206.42.22%20PM.png) I feel its more like a feedback section. If you feel everything in the proposal looks good ( and you don't have anything to add) then may be we can just mention that.  If everything looks good, then please say it looks good.  Since the class is very large, it is infeasible for the course staff to provide detailed feedback on each proposal. So we hope that you use this section to provide at least some sort of feedback to the group. 
https://campuswire.com/c/G984118D3/feed/738 MP2.4: Scores not updated on LiveDataLab I have been trying to run search_eval.py with various rankers and different parameters.  I do see NDCG scores get updated on my local run but LiveDatalab is not updating the score after I push changes to my private repo. Anyone seeing this issue ? Any solutions ? Thanks.Fix it  - I did not update load_ranker instead I was changing the main code :-( 
https://campuswire.com/c/G984118D3/feed/19 Unable to install metapy on M1 machines Hi, is anyone facing any issues installing metapy on Apple M1 machines? It's not letting me install it, and giving an error saying: ``` pip install metapy Collecting metapy   Using cached metapy-0.2.13.tar.gz (3.4 MB)   Preparing metadata (setup.py) ... done Using legacy 'setup.py install' for metapy, since package 'wheel' is not installed. Installing collected packages: metapy   Running setup.py install for metapy ... error   error: subprocess-exited-with-error    × Running setup.py install for metapy did not run successfully.   │ exit code: 1   ╰─> [3 lines of output]       running install       running build       error: Could not find cmake executable       [end of output]    note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure  × Encountered error while trying to install package. ╰─> metapy ``` The `setup.py` script of `metapy` requires that you have the `cmake` binary available in your PATH (https://github.com/meta-toolkit/metapy/blob/master/setup.py#L63). You can read instructions for installing CMake here: https://cmake.org/install/.  Another alternative is to use Docker to create a container with the relevant dependencies. Personally, I prefer this approach to avoid messing with the `PATH` of my Mac unless it is completely indispensable. For example, the following `Dockerfile` creates a container with `cmake` and `metapy`.  ```dockerfile FROM --platform=linux/amd64 python:3.7-slim-buster  RUN apt-get update  RUN apt-get install -y build-essential libssl-dev RUN apt-get install -y cmake RUN pip install --upgrade pip RUN pip install metapy ``` Thank you! You may just install python 2.7 which would be install in /usr/local/bin/. Currently I have two python versions and it works fine for me. Tried pyenv but it is not supporting the older version of python for now.  I solved this by installing anaconda and running everything inside of it. https://www.anaconda.com/ This is the download website. After you install anaconda, go to your terminal, input "conda active base". Then, for me, I choose python 3.7 so my next command is "conda create -n py37_env python=3.7" if you want to activate the environment you just created, input "conda activate py37_env". Then "pip install metapy pytoml." If you want to exit conda environment, simply input "conda deactivate".  Wish that helps! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8b521755-a8ca-4af4-bcbc-7178f4821f9e/image.png)  Seems like for any python version after 3.8, pip install metapy does not work :) Haoran, thank you very much for your clear instructions. I also use Anaconda and appreciate your easy solution to install metapy. So far, the 3.7 environment is working very well! It is very helpful. Thanks! For anyone facing the issue of installing metapy on their Mac M1/M1 Pro machines, I believe there are a couple of reasons: - Metapy does not work after python 3.7 and most of the Mac machines come with python 3.8+ installed.  - The metapy package requires "cmake" to be installed and it is not found.  The easiest way to resolve both of these issues is to install Anaconda and use it to both install cmake as well as get the right python version. Here are the steps that worked for me:  1. Download the Anaconda from their website. 2. In your terminal, enter (this will download cmake for you) ``` conda install cmake ``` 3. Next, enter the following to create an environment to use python 3.7. ``` conda create -n $PYTHON37_ENV_NAME python=3.7 anaconda conda activate python=3.7 ``` 4. Next, in your terminal, type in  ``` conda install python=3.7 ``` 5. Next activate your environment ``` conda activate python=3.7 ``` 6. You can check your python version is 3.7 here by typing "python --version" 7. Now try installing metapy again in your python 3.7 environment: ``` pip install metapy pytoml ``` metapy should successfully install now. Hope this helped!  Reference: https://stackoverflow.com/questions/43630002/conda-install-downgrade-python-version  Hi, I'm struggling to downgrade my python to 3.7 (i currently have 3.8.9), even though i followed your every step of your guide, until step 6, where I check my python version and it still shows python 3.8.9. Do you know how to resolve this? Thank you so much for these instructions btw! Hi Kristie,  Did the conda install process complete for you? I would also advise you to check out the discussion under my comment on #28 for the anaconda version you downloaded. If you downloaded the wrong version of anaconda, you may face this issue. Hi Gautam, I downloaded anaconda from the website per your comment, and pip install metapy works successfully, but once i try to import metapy, i keep getting the same error of "module not found". After creating the environment to use python 3.7 in your instructions above, my python version is still 3.8.9 :( Thank you! I also have an M1 machine. To solve this issue, I am simply using Google Colab for MPs, because metapy doesn't work on M1. Refer to this post - https://campuswire.com/c/G984118D3/feed/153 Hi Kristie, make sure you enter the python 3.7 environment before trying the python --version. Every time you open a new terminal session, you will need to do  ``` conda activate python=3.7 ``` Hi Gautam,  I got it working, thank you so much for your help!
https://campuswire.com/c/G984118D3/feed/946 Optional Lectures Hi, Do we know if the optional lectures for Week 5 & 6 are included in the exam? ThanksThey are not, no None of the optional lectures would be covered in the exam.  
https://campuswire.com/c/G984118D3/feed/1293 Peer Review - Submit or Save Draft? I am not clear on whether to submit or save as draft the Proposal reviews?  We can obviously only score the proposal, but will submitting maintain the page visibility for future scoring of progress and final?Please submit the proposal, do not just save as draft (in the latter, it will not be visible to the authors). And yes, submitting will maintain the page for future scoring of progress and final reports. Thanks!  When I try to submit with only the answers for the proposal related questions, it doesnt let me to as it highlights other questions in red and asks me to fill in the values. What can I do?
https://campuswire.com/c/G984118D3/feed/137 Tech Review Sign-on Sheet Just curious, in the sign-on sheet, why do we need to provide github link? also, there is one column for reference - does it imply we only need to read one paper (because it's a short review for only 2 pages). Or doesn't it mean that we need put list all the reference in a single cell?? I think in the reference column you can mention links where one can find more information about the topic you chose for tech review. For example, if the topic you chose is "NLTK for information extraction" then you can put the following link in the reference column "https://www.nltk.org/" To add on to the current answer - we ask that you provide a Github link so that other students can view and learn from your technology review To my understanding, the review should be a pdf file uploaded to our GitHub. Is that correct?  Yes - and make sure you follow the pdf naming conventions. LiveDataLab will not register a grade if the file is incorrectly named.  thanks for all the answers - got it
https://campuswire.com/c/G984118D3/feed/957 Exam and ProctorU Public Service Announcement - the next available Proctor U appointment is 20 hours away! I have always previously found that if you pay the premium, you can get a spot in minutes. Not today! So if you haven't booked yet, get on it asap. Maybe they should introduce Uber surge charging... :-)Next available spots around midnight CT, and then pretty consistently from then on. I suspect there was a holiday.  In general, it seems to show results up to 85 minutes after the time you specify (ie, selecting 5:35 would show you 7:00 but not 7:10) and up to 30 minutes before the time you specify (ie, selecting 5:30 would show you 5:00 but not 4:50).  But! It's also weird! Searching for 6:00 AM showed me 6:30AM, 6:40AM, etc, but searching for 5:00 AM showed me spots for 6:00 AM, 6:10 AM and 6:20 AM. I think their Ranking function needs some work!
https://campuswire.com/c/G984118D3/feed/640 Unable to install metapy after activating conda environment I'm trying to create another conda virtual environment with python 3.5 on it since python 3.7 is having issues. I have done the steps described in other posts such as #28. When I do, I cannot install metapy.  I keep getting "legacy install failure" every time I do so. I have been trying to remove the virtual environment when it doesn't work and do the whole process over again, but I get the same error.  When I installed python 3.7 for MP 1, it worked perfectly, but when I try to install a lower version, I have issues and I'm not sure how to fix them.is there a reason you are trying to use a lower version of python? for me version 3.7.14 worked fine for both MP1 and MP2.3 metapy should work with python 3.7. What issues are you facing? Anisha, I confirm that metapy can be installed and works properly in a Python 3.7 virtual environment. Please share more information about the problems you are currently experiencing with 3.7 and maybe someone here can offer a more specific recommendation. David  I mainly asked this question because I've seen posts where people have had issues running the ranker function with 3.7, but it ended up being fine for me.
https://campuswire.com/c/G984118D3/feed/907 Calculating Log Likelihood the calculate_likelihood function in the MP asks us to calculate the log likelihood of the model using the updated probs. Is there an equation on how to do this? I tried looking in the 9.7 and 9.8 lecture slides but did not find steps for calculating the log likelihood using the probability matrices.   Can someone guide me on the process I need to follow for this? Thank you so much!  ![Screen%20Shot%202022-10-11%20at%2011.22.05%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3408c951-43f8-41e4-81c0-17fbeae43fd7/Screen%20Shot%202022-10-11%20at%2011.22.05%20PM.png)The formula is written here: ![log_plsa.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cfbcd9fb-1d1a-419a-81ac-cb3e07e73c64/log_plsa.png) Without giving away the exact code, you can think of your self.term_doc_matrix as *c(w,d)* , self.document_topic_prob as *$$\pi$$* and self.topic_word_prob as *p(w| $$\theta$$)* . Hope this helps! Do we have to check if c(w, d) does not equal zero, for each combination of word and document?
https://campuswire.com/c/G984118D3/feed/642 Clarification on Summation part of the formula for Ranker I believe I have implemented everything in the formula except for the summation. When I interpret the summation part of the formula, I think it is "for each term in the query and the document".  However, the specification for score_one states that "You need to override this function to return a score for a single term." Wouldn't this reasoning make the summation part of this formula irrelevant or is there something in the way I am interpreting this part of the formula that is the problem?No, you don't need to implement the summation You don't need to implement the summation, python code will take care of it. in the assignment, it says "The function you’re writing represents one term in the large InL2 sum."
https://campuswire.com/c/G984118D3/feed/840 groups size + work distribution how was wondering how does the work load compare to a group of 5 compared to a group of 3? There is a post, I think, that said we should do N * 20 hours of work, with N being number of people in your group. I suppose the work load of each person should be 20 hours
https://campuswire.com/c/G984118D3/feed/81 Slack account As undergraduate students, are we allowed to sign up a Slack account in MCS for this course?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e7827418-a941-4754-81e4-b3c22a88f3b2/image.png)  According to the syllabus:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/de612b31-cadd-4c17-904e-d5e158bb98b6/image.png)   UPDATE:  Sorry, the officer said I cannot join this channel since I am not in the degree program.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ca5a7c43-14ba-4a95-a425-1ff170ab243d/image.png)Yes , you should be able to use your Illinois.edu account to log on to the Slack and join the relevant channels. Most of us are on slack :) May I ask where you get the invitation link? I am an undergraduate student, I cannot join the workspace of MCS currently. I tried sending you invite on your illinois id. Looks like you are already added to workspace however your account is deactived. May be drop mail to mcs_support@illinois.edu and ask for assistance.  Thank you very much. I will send an email to see what happened.  the MCS slack is specifically setup for master's students who are pursuing their degree online. If you're not an MCS student please don't try to access that slack, it is NOT managed by the CS 410 staff but by the CS department. For CS 410 students who would like to have a chat space with other students, please use the Campuswire "Rooms" , you can create your own or join the default one  I got it. Thank you.
https://campuswire.com/c/G984118D3/feed/412 practice quiz 3 - Q6 For this question, can someone help me how to derive the parameter should be higher? I get the point that recall should weigh more. What is the equation of F measure?![Screen%20Shot%202022-09-09%20at%2010.38.47%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/868f8714-07c2-4d6d-8ac8-66de2a3dd090/Screen%20Shot%202022-09-09%20at%2010.38.47%20PM.png)I also wondered about this one. Luckily, this has been discussed a bit in #389  ; to summarize: in the F-measure, Recall is considered $$\beta$$ times as important as Precision in the F-measure. This means a higher $$\beta$$ will balance recall as more important, which for this case would likely be more valuable since we would want to ideally retrieve every possible document related to terrorists communicating about attack plans on US soil, even at the cost of getting some irrelevant documents back (also confirmed in the descriptive text you shared saying "low precision is less harmful")  Post #386 is also mentioned in that discussion which provides further clarity on F-measures  Please let me know if that helped or if I can provide any clarification! which week? To answer this question, we need to figure out two things: - What does the tweet search for possible communication between terrorists represent in terms of the user task, i.e., do we want to get only the top results or as many results as we can? In this case, it would show as many tweets as possible, so we have to reward **recall**. A higher recall value means returning most of the relevant documents. -  How do we adjust the `F-measure` to reward recall? From the formula $$F_{\beta} = \frac{(\beta^2+1)P*R}{\beta^2P+R}$$ we can appreciate that the **Recall** weight is $$\beta$$ times **Precision**. Therefore, when $$\beta = 1$$ Precision and Recall are given the same weight, and when $$\beta > 1$$ **Recall** is given more importance. There is a **great post** from the professor about this topic here: https://campuswire.com/c/G984118D3/feed/237. Also, check David Burrus' answer. week3
https://campuswire.com/c/G984118D3/feed/616 MP2.4: The purpose of the task Hi,  MP2.4 seems to be just about fine tuning the parameters for the ranker method that we choose. We aim to beat the baseline, which we have no idea about how it was generated and therefore it becomes a number-guessing game.   For me, the concept of beating the baseline sounds fun. But this is just performing some grid search and then brutally testing with an interval. The method related to parameters fine tuning is also not covered in the lectures.  I am just doubting what we are supposed to learn from this exercise?  I wonder if I understand the task correctly? I've completed MP2.4. Your description is accurate-- just parameter fine tuning. I think ostensibly you're supposed to learn how parameters influence different rankers.  Thank you for your reply. Yeah, it does feel a bit strange. The connection does feel very weak. From some other UIUC courses, you will be directed to experiment and will, for instance, generate some graphs which will illustrate this point, or even compare different methods clearly. But for MP2.4, it is just like randomly guessing and comparing.  Maybe it is also part of the daily work for data engineers? Haha. I think there is opportunity with it. And the class competition part is fun.  I've been trying to understand why one ranking method performs differently, whether there is an interaction between the parameters or if they're independent, and just using it as a way to better understand the content.  Yea I agree with Joseph. I believe it's just to really help our own knowledge and understanding to why certain parameters work better than others. If we see a performance change using different parameters, i believe the exercise wants us to think why exactly did that change and use better parameters based on that. Don't think you are supposed to just randomly guess numbers and brute force it,  but rather try to make better and better educated guesses based on what parameters and results you have seen so far.  By "random guessing", I mean under the current structure of the MP, making an educated guess is not very likely.  Take BM25 method as an example, we learnt $$k1$$, $$b$$ from the lectures and understand what they mean. But fine-tuning these parameters has a lot to do with the target queries and documents. We received one sample dataset, but have no ideas about the other two. There is not enough information presented in the task description.   Also I was expecting there should be more information about how the TAs got the baseline. I like the form of this fun competition. But without knowing how the baseline was generated, comparing our results with the baseline becomes less meaningful. Yes, we beat the baseline and got a better search engine, but why? Which part did we do better than the baseline, and which part not? We have no way to find out. Ah I see what you are saying now. Yea that makes sense I would also like to know how to properly find out the best parameters as well. Hopefully we can get some kind of guidance or clarification on that.
https://campuswire.com/c/G984118D3/feed/998 Project topic for Oct 17th deadline Do we need to have the project topic and brief description filled out in the google sheet for Team Formation which is due Oct 17th, or is the topic and brief description part due along with the proposal due date of Oct 23?  I am asking because I think it may take some time to come up with an idea for the project.It isn't strictly necessary, but please try to write down a rough idea. You can always change it between now and the proposal deadline. 
https://campuswire.com/c/G984118D3/feed/578 Values of c in InL2 ranking function. I have implemented the InL2 ranking function. What is a good range of c values to test for it?You can start with a wide grid search and then narrow it down from there. To start off, I usually like to begin with a small steps and then grow exponentially larger, e.g. $$c > 0 \rightarrow$$ c = [0.1, 0.5, 1, 2, 5, 10, 20, 50, 100, 500, 1000]. After your first run, determine which of those values seem to have the most desirable values, and test a smaller subset around them and repeat until you determine any further gains to be negligible. I did  a grid search as Steve recommended. sorry, I am confused. Do you mean the c should not be 1 as the default? Should the code have the optimal c just for the first part of "Writing InL2" . Thanks! We were supposed to investigate various c values and check which was the best, though this part was not graded.
https://campuswire.com/c/G984118D3/feed/808 Reviewing Past Quizzes Would there be a way to review the actual quizzes (or is the practice quiz the only means of reviewing for the exam)? The 'try again' button is greyed out, so I cannot retake the quizzes without seeing the answers inside of 'feedback'.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/08a36f65-4208-4bb6-976d-ab884fa8fb45/image.png)There should be a `View Feedback` button when you go to the quiz assignment. While you cannot take the quiz again, you can see what you answered and what was correct or not  Let me know if that helps; i can also upload a screenshot showing the button if that helps I agree. It would be nice if it can be enabled now. It will help to practice for the exam.    I agree that it will be of great help, but not sure if it would allow.
https://campuswire.com/c/G984118D3/feed/487 deadline for MP2.2 I have finished MP2.2, but just noticed the deadline is Sep 17 (sat) on Coursera but in the description of the assignment it says deadline is Sep 18.   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1c1d2185-35cc-4155-901f-54551eac4e01/image.png)  vs  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e9a9cb1c-9a50-417d-ae6c-1b808862579d/image.png)Good catch! I've just changed the Coursera deadline to Sept 18, 2022 at 11:59pm CDT. 
https://campuswire.com/c/G984118D3/feed/158 VS Code and Github This video helped me to configure my VSCODE for GitHub :) hope this will also helpful for some folks.  https://youtu.be/i_23KUAEtUMThanks for sharing. This is helpful.  GitHub desktop can be used with any IDE of our choice and is super easy to configure and manage https://www.youtube.com/watch?v=77W2JSL7-r8
https://campuswire.com/c/G984118D3/feed/359 Clarification for MP2.1 Hello,  I just want to make sure I am understanding correctly.   For MP2.1, I have set up the VPN, chrome extension and successfully submitted one page so far to the digital library.   To complete MP2.1, do we simply submit 15 unique webpages/resources?   Before starting this MP, I followed the MP set up instructions before I realized the coursera page had different instructions and was brought to this github page: https://github.com/CS410Assignments/MP2.1.   Is this github page still relevant? or should we be focusing only on the 15 unique resource submissions as I was seeing we do not use livedatalab?  Thanks!> To complete MP2.1, do we simply submit 15 unique webpages/resources?  Yes, this is all that is required for MP2.1.  > Is this github page still relevant? or should we be focusing only on the 15 unique resource submissions as I was seeing we do not use livedatalab?  Sorry for the confusion, the github page is not relevant to MP2.1 nor MP2.2. You should only focus on the 15 unique resource submissions, and you do not need use LiveDataLab.   Thank you Kevin! I will try to find some insightful articles to add No problem, and wonderful! Looking forward to reading what you collect.
https://campuswire.com/c/G984118D3/feed/317 Can't install metapy pytoml ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d89fbb87-c013-4a78-8807-060f6b7d35de/image.png)  I have installed anaconda, but the python version is 3.9. And the installation failed for metapy. What should I do?Try installing python 3.5! > Please note that students have had issues using metapy with specific Python versions in the past (e.g. Python 3.7 on mac). To avoid issues, please use Python 2.7 or 3.5. Your code will be tested using Python 3.5 I tried Python 3.5 and it works.   Also, just in case you want to try Jupyter, you can follow the order to install: Python -> Jupyter -> metapy and pytoml Just try to do it in python 3.5 it seems to work fine then.  Try Anaconda and create new environment in which select python3.5.  I followed the answer in this post #28 and installed 3.7, it works fine for me. Do this, but install Python 3.5 rather than 3.7
https://campuswire.com/c/G984118D3/feed/1142 Week video 9.5 question ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/605b16a9-bd4b-49c7-8d4c-0bff8c1fbc43/image.png)  Can anyone explain c(w,d) and Z=0| w',  what does the w' stand for here?Since $$p(w|\theta_{d})$$ is a probability, we need to normalize the numerator $$c(w,d)p^{n}(z=0|w)$$ by dividing with sum of $$c(w,d)p^{n}(z=0|w)$$ for all the words in the vocabulary. So in the denominator, we are summing over all words $$w^{'}$$ in the vocabulary. c(w, d) stands for the count of the given word (w)  in a given document (d), Z stands for the topic, and w also means the given word.
https://campuswire.com/c/G984118D3/feed/279 Lecture 3.5 IDCG Based on the assumption of IDCG, is it correct to claim $$IDCG = relevance_{1} + \sum_{i=2}^{n}\frac{relevances_{i}}{log_{2}i} \textbf{, where relevances is in descending order}$$  ![Screen%20Shot%202022-09-03%20at%2017.31.14.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/37c36171-9e7c-49b6-8603-82dcc08a88f1/Screen%20Shot%202022-09-03%20at%2017.31.14.png)If $$relevance_{1}$$ is the highest relevance (relevance score of the highest relevance document), and all the $$relevance_{i}s$$ are in the descending order, then yes, you are correct.  Based on the lectures I could relate to DCG and  nDCG. What is IDCG referring to here ? IDCG should related to IdealDCG.
https://campuswire.com/c/G984118D3/feed/251 MP1 - No module named metapy Hi , I followed few posts and installed anaconda and in the base did set up python 3.7 environment. Then opened the visual code and tried to run the bare code and still see the error "No module named metapy". Am i missing any link or step here ? Please suggest !   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ec1ed007-672e-4dc3-9ad3-cd9cd6c19521/image.png)I think vscode is not recognizing the conda environment. It's is active within the Anaconda prompt but is not active in vscode.  There are a couple of things you could try: 1. You could try running the script, ```example.py``` within the Anaconda prompt. 2. You could activate the environment in vscode by pressing ````Ctrl+Shift+P```` to open the command palette and press *Select Interpreter*. You should see your environment in the drop down and then select it to activate it. Thanks for another pointer there. This would probably also have solved the problem. I was able to fix it by uninstalling python and installing another lower version and it did the trick.  You could just pointed to version 3.5 using Conda for setup.  Please refer to the is post:  https://campuswire.com/c/G984118D3/feed/200  This was posted by a student Charles Stolz and helped me with installing metapy without having to uninstall Python version 3.7.  conda create -n cs410project1 python=3.5 conda activate cs410project1  pip install metapy pytoml There’s usually a shortcut via a status bar in the bottom right of the window, as well. I recommend using the Google Colab. With Colab, you don't have to worry about the environment setting. Create a new notebook with Colab and install metapy using the following command, and it's done! ``` !pip install metapy ```  Hope this helps.
https://campuswire.com/c/G984118D3/feed/532 MP 2.3 Issue I did some trouble-shooting with my code, and cannot figure out what the actual issue is with my ranker function. For context, I am using math.log(w/e, 2), and I checked to see if I had copied any of the external files incorrectly from the TA's MP2.3. Also, I managed to get the points for the significance-test aspect.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/17f94c38-d5c3-436f-a6b4-a19b1651c780/image.png)  The issue that the auto-grader gave me was quite vague. Anyone got advice on how to circumvent this issue?Hey Justin -   I'm curious, what do the "w" and "e" represent in math.log(w/e, 2)?    It's obviously difficult to suggest potential revisions without more information, but without giving too much away I think I can safely mention I got an average precision of 0.5 for Query 2 (when using "some_param" = 1.0).  If you got something different, I suspect it's simply a matter of going term by term and making sure your formula:  1. precisely matches the equation provided 2. references the correct fields from (https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html)  3. has the correct order of operations (i.e. parentheses are where they belong)  Apologies for the obvious suggestions, but I feel but that's all I can safely say without saying too much.  I verified the parenthesis, and I'm also getting 0.5 for Query 2.  How does the grader verify that the implementation is correct? Am I supposed to save the output to a file or print to the console?  For the log, I meant I used math.log(val, 2) instead of math.log2(val) because grader is using Python 2.7. the grader only verifies that the inln2 function has been implemented correctly for the first part.   check that you have implemented the two equations correctly, and that you called InL2Ranker (score_one) in load_ranker Ah maybe that's the issue...I didn't make any changes to the code that prints to console.   I believe the two text files with the average precision outputs are simply used to perform/verify the hypothesis test, but the first part probably expects the results to be printed to console. 
https://campuswire.com/c/G984118D3/feed/1233 MP 3 Grade not updated on Coursera Hello,  I submitted MP 3 and received 1 on the leaderboard in livedatala. However, my grade on Coursera is still not updated. Please help me! Am I doing something wrong?  Thank you, Heet ParikhThis post covers an similar issue for MP1: https://campuswire.com/c/G984118D3/feed/155  See if that fixes the grading issue. Got it, thanks!
https://campuswire.com/c/G984118D3/feed/455 Office hour recording Do we have recordings for OH?Yes, you can find it here: https://www.coursera.org/learn/cs-410/resources/nOxcB
https://campuswire.com/c/G984118D3/feed/270 MP1 - github intro? Hello - for those of us without experience using github, are there any introductory resources you would recommend?  Thanks!There are several resources that will walk you through the basics of forking, committing, pushing to remotes, and branching that will enable you to complete the assignments. Here are couple of good places to start:  https://docs.github.com/en/get-started/quickstart/git-and-github-learning-resources https://skills.github.com/  If you would like to further understand Git I would recommend the book Pro Git. It's freely available here: https://git-scm.com/book/en/v2. Thanks a bunch Rick! I was able to fumble through enough to submit the assignment but I also know this is an important tool that I want to be more comfortable with. https://docs.github.com/en/get-started/using-git/about-git https://docs.github.com/en/get-started/using-git  github has some resources that can help you ramp up!
https://campuswire.com/c/G984118D3/feed/1315 Backend Server Hi there, thank you all course staff for your hard work on the course project. I see the next deadline is for project progress report. May I ask when we can get the credits for renting the server on AWS or GCP. Or some instructions about using online server on campus?  Our proposal involves two main components. One is the Chrome extension, and another one is the backend server for storing research papers (a database of a collection of documents) and real-time responses (send the pre-computed ranking and get information from other online websites like Wikipedia to users).If you sign up for the Github Student Pack with your UIUC student id card, you get a $100 credit for Azure Cloud, $200 credit for Digital Ocean, and $13 / month credit for Heroku.  https://education.github.com/pack A quick cheat is to use a CSV file as your database (I know, cringe) , and from there using Github's website hosting ability. Anyone who has a Github account has a free website using Github Pages.   Here is a link that will explain more: https://pages.github.com/  You will have to have some web development knowledge, but it is certainly possible to do so free.  I don't think that we can provide credits for renting a server. But for the purposes of the course project, you can demo your project locally, or as others suggested, use free methods / student sign-up benefits. If we can demo locally,  we do not need to consider an online server then. Thank you for your reply!
https://campuswire.com/c/G984118D3/feed/126 Textbook I have a quick question. Will a free download of the textbook be provided? Thanks!Yes, you can search the library website https://www.library.illinois.edu/ for the text book, there will be pdf download option on it. The textbook name is mentioned in the syllabus I believe.  sad. I haven't got my I card approved.  Attached for everyone's convenience: [TextMiningAnalysis-zhai.pdf](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/53cfe92e-18dc-482d-80d7-866b6396e612/TextMiningAnalysis-zhai.pdf) Thanks a lot!
https://campuswire.com/c/G984118D3/feed/812 Exam 1 Slots on Next Friday Hi, this is my first time using ProctorU.  My ideal schedule for exam 1 is on next Friday, October 14, but currently there is no available slots on next Friday. Does anyone know whether more slots will be released in the future? Or is the current available schedule all we have?  Thanks in advance!I found that keep on hitting "next available times" will show more slots than just hit submit on the select time section. I think there are no more slots for Friday or Saturday. Those who waited for "additional instructions" from the TAs were pretty late in the game. Kinda messed up. It would be nice to have some more slots open those days. I booked a slot several weeks ago and there were no Friday or Saturday exam slots open then, either Seems to be a big issue with ProctorU availability - or it could be planned site maintenance or something. Either way, I'd assume there wouldn't be additional slots released...that's a ProctorU issue, probably not something CS 410 staff can provide.
https://campuswire.com/c/G984118D3/feed/202 Fail to download file by wget I tried to download file through wget in the readme file, but the following error occurs:  Invoke-WebRequest : A parameter cannot be found that matches parameter name 'nc'. At line:1 char:6 + wget -nc https://raw.githubusercontent.com/meta-toolkit/meta/master/d ... +      ~~~     + CategoryInfo          : InvalidArgument: (:) [Invoke-WebRequest], ParameterBindingExce      ption     + FullyQualifiedErrorId : NamedParameterNotFound,Microsoft.PowerShell.Commands.InvokeWeb     RequestCommand  What should I do to fix that? Thank you.It looks like you are using windows prompt. I found this link may help you   https://stackoverflow.com/questions/70687650/invoke-webrequest-a-parameter-cannot-be-found-that-matches-parameter-name-lfo I got the same error and this was my roundabout way of getting it to work: ``` pip install wget import wget url = "https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt" filename = wget.download(url)  tok = metapy.analyzers.ListFilter(tok, filename, metapy.analyzers.ListFilter.Type.Reject) ```  Here's more information about the wget python package if you're interested: https://pypi.org/project/wget/ For windows, the below worked for me from the command prompt curl  https://raw.githubusercontent.com/meta-toolkit/meta/master/data/lemur-stopwords.txt  > lemur-stopwords.txt You can always go to the website and download the file. Hi Sophia, thanks for this trick. It was really helpful. Did this also work for you when you had to run the tar command later on? I can't seem to have that work Please disregard. I figured out that the problem was that I wasn't running the command in the Command Prompt shell, but rather in python. Thanks! Hi Arda, I'm glad that was helpful. The tar command just unzips and extracts the files downloaded from wget, so I did that manually using free software like 7-Zip.
https://campuswire.com/c/G984118D3/feed/567 MP2.4 Overall Score of the baseline  Hello,  Could we find the"Overall Score of the baseline" from letterhead? Also I am wondering how is the baseline score calculated (so that we understand how to beat it).  Thanks.where can we find baseline score?
https://campuswire.com/c/G984118D3/feed/299 Submitting MP1 (Github Account Not Found) Hi,  I need some assistance in configuring GitHub with LiveDataLab.  All my code is done but I am having trouble during submission. I have tried re-linking my account in GitHub multiple times as well as re-provisioning my PAT and recreating the webhook (as suggested in other threads).  None of these have resolved the communication between git and livedatalab.   Can anyone assist?  Linked account: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7af10296-e4ff-48a3-97b6-ab0ce680f6e2/image.png)  URL: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger Webhook Settings: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3944dde8-daca-480e-9be3-ad89f7465c4d/image.png) Webhook Delivery Log: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1648daf4-f99b-48d0-be9a-a36ff39339fc/image.png)For your linked github account, the host domain should be github.com (note the lowercase 'g') not Github.com. Please delete the linked account, and then try again.  Thanks Priyanka! I can see the webhook working with LiveDataLab now.
https://campuswire.com/c/G984118D3/feed/48 Are the Quizzes Open Note? My guess is that the exams are closed note based off the fact that we are using proctoring software but what about the quizzes?Yes, the exams are closed note, but you can use whatever resources for the quizzes. So if you are uncertain about a quiz question, you could (and should) go back to the relevant parts of the lectures to (further) study it until you fully understand it, and then you can go back to the quiz question to solve it. However, the best practice is to FIRST make sure that you fully understand the lectures BEFORE attempting to work on the quizzes. This way, you can use the quizzes as a "mini exam" to check how well you've mastered the week's materials. That is, try to be fully prepared before you start working on the quizzes. This has the extra benefit that you will be very confident when it comes to those two exams  as the purpose of those exams is mainly to verify that you can indeed independently and confidently solve the questions in all those quizzes. Some exam questions may be identical to the questions you see in the quizzes. The WORST practice is to do the opposite, i.e., start with quiz questions and try to guess the correct answers and only review the lectures about questions that you didn't answer correctly. This would leave many holes in your knowledge and you'll still have to re-learn all the materials in order to do well in the exam and you probably would not be confident in being able to answer all those questions in the quizzes even when you can score well on the quizzes. So please do not do that! Always make sure that you watch all the lecture videos before working on the quizzes. You'll benefit most in this way. It's also the most efficient way of learning and would ensure that you don't leave any hole in the mastery of materials from the previous weeks.  
https://campuswire.com/c/G984118D3/feed/1314 tech review content Hi,  I saw there is a discussion about the content for Tech Review #1052   So, based on my understanding, if we choose a topic/paper (e.g., an arXiv paper as the reference), we essentially condense it to ~2 pages using our own words?  Thanks!It is a bit broader than that - if you select a topic, you may find 3-4 reference papers on that topic, and summarize the high-level points and take-aways in your tech review.  Many thanks!
https://campuswire.com/c/G984118D3/feed/133 Group Work Required? I just want to double check; the syllabus states that for the final project, we must work in groups of `at most 3`. would a group of 1 (aka, just myself) be acceptable as well, or do we need at least one partner?You can work independently (and sorry, the syllabus is out of date: the max size is 5). However, we do encourage students to work in groups! How can I find teammates? I have same question , also if I will not be able to find teammats due to time zone etc…
https://campuswire.com/c/G984118D3/feed/272 Practice 2.3: binary representation? Hi, for this question, it's easy to get the gamma code for 9 is 0001001... but I'm super confused about the rest of the question- what is the binary representation of the gamma code? Isn't the gamma code already in binary form??? are you asking about the binary form of "1001"?Hi, the gamma code 9 is not 0001001, the question asks exactly for that, at ~10min in the lecture 2.5 video, there is an explanation ha thanks- got where i was wrong. 
https://campuswire.com/c/G984118D3/feed/1102 What is the valid range for log_likelihood values? Hello,  To everyone that completed MP3, could you please give a general idea of what a good log_likelihood value is like after some iterations? I am trying to find out why my submission is not getting accepted, but I have no clue if I am moving in the right direction or not. A pass/fail result is not really helpful in this regard. Thank you in advance.It should be negative and decreasing. I saw some people reporting large negative values after multiple iterations. Convergence means that the absolute difference should be decreasing. You could run multiple times to check as the optimization might be influenced by the initialization.  I just finished MP3 and completed it and my code never hit the the absolute difference stop condition, but this is what some of my likelihoods looked like ``` [-inf, -257249.44227615878, ...   -227154.83981627066, -227154.83297881295] ``` This was for 100 iterations of the test.txt data, but 50 iterations of it had similar numbers in the -20k range My values are quite similar (-17xxxx) and it converged, but didnt get accepted by the grader.  Not going to lie, I was lazy at the very end and just decided to do this function using 3 nested for loops (one pertaining to each $$\sum$$ in the formula) and that worked for me, so could work for you. Other than that perhaps your formula interpretation is wrong not sure though. So the for loops are allowed? Interesting, I might go for that if I cant figure this out. Thanks for the insight I'm also getting values in the range of -177xxx. Does anyone know what could be the reason for this? in `main()` function, number of iterations is 100, but it's not large enough for my algorithm to converge. My case converges at roughly 450 iterations on test data. Hi! So do I! Did you figure out what issue it is?
https://campuswire.com/c/G984118D3/feed/626 MP2.3 cant parse config.toml Anyone know what to do about this error in google colab? I couldnt find any info online and I used the copy path option for the path to config.toml. I tried using both python 3.6 and 3.7 btw.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3d433124-83cc-4914-b262-3b83e1dae7bc/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c3f54acf-1a0c-409d-b1d5-e61c2c771f1e/image.png)How are you running this program. Did not run into this issue.  You can try this, in the folder where the files are present (on terminal cd to that folder) python ./search_eval.py ./config.toml  this should work  I am using google colab, so I am trying to run that individual line. Looks like a relative vs. absolute path error - in what environment are you running this? If it's on a local machine, you almost certainly don't want "/" which tells the function to look at your root directory...  The provided function looks like this: >     cfg = sys.argv[1] >     print('Building or loading index...') >     idx = metapy.index.make_inverted_index(cfg)  ...and when you run the program (`python search_eval.py config.toml`) it looks for `config.toml` in your current working directory.  Try replacing "/" with "./" and see if that fixes your issue  Saw your comment about using Colab - in that case, make sure config.toml is in the same directory as your notebook, and definitely use "./" instead of "/" before `config.toml` I believe its in the same directory. This is my files directory. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/346b4c42-9efc-4811-a821-353b2cf8add0/image.png)  Replacing "/" with "./" still gave me the same error If it is in the same directory on google colab, just remove the "/" and that should do it. Has this problem been resolved? Have not found a solution. invalid file for list filter should mean that the path to /config.toml is correct but the things inside it are not. I am having the same issue, is it resolved for you? It was not but I just modified the score_one code without testing and it worked It has not been resolved, Im still unable to run the code in colab
https://campuswire.com/c/G984118D3/feed/766 Setup instructions for MP2.4 wrong on Coursera - webhook Quick note in case anyone else is in a similar position where their webhook isn't working for MP2.4:  The PDF setup instructions linked on Coursera for MP2.4 list an incorrect webhook URL in step 13. The correct link is listed in the first setup instructions (from MP2.1) - http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger  Seems to be a few different versions of the MP setup instructions floating around and several have the wrong LiveDataLink webhook URL...hoping this can be corrected for future MPs!
https://campuswire.com/c/G984118D3/feed/1165 MP3 - build_term_doc_matrix() issue  Hi, can anybody provide any hint on how to build_term_doc_matrix()?  I use collections.Counter() to get a list of dicts, which is {key : count} for each document. Since some keys are missing from some documents, I wonder how can I turn this into the matrix we need? Thank you. your `term_doc_matrix` is a matrix with M columns and N rows, with M being the vocabulary size and N being the number of documents.  considering the "brute force" method where your just have a nested for loop, you can then do the following procedure (pseudocode):  ```python for d in documents:   for j in vocabulary:     # count of term j in doc d     term_doc_matrix[d][j] = count(j, d) ```  i did not use counter, and it seems from previous posts it may not be the "recommended" method (though it seems others have used it successfully). you should be able to perform some sort of expansion on your dictionary, but you'll have to be careful because in python dictionaries are ordered by insert order.  for me, it was much simpler to go with the more straightforward approach It's better not to import extra packages as livedatalab may not support them. You can simply use a for loop in this function. 
https://campuswire.com/c/G984118D3/feed/321 Syntax error at -wget & tar xvf I am unable to solve this error ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cad9f763-4b6b-4235-97db-ac700e9ca5bf/image.png) I downloaded the package from github to overcome wget error , then iIam getting error in calling this module ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f368bc6d-ee2a-4a27-9af6-42368e57b29a/image.png)  You are running wget CLI in the Python environment. Try call wget in the terminal instead. It shows this error ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c199ceca-ead5-4aea-991f-7485f5d3d556/image.png) Do you have wget installed on your laptop? I have solved this!  Please just forget about this wget and directly copy the url and paste it in your web browser. You should be taken to a page showing the content. Then you can save as and save it to your project location directory so that python can see it. Let me know if this helps  You don't need to install or use wget. Just copy and paste the link in that command to a new tab in your browser and you can download the documents easily  use curl in windows The `README.md` markdown specifically encodes when instructions are [bash](https://github.com/CS410Assignments/MP1/blob/main/README.md?plain=1#L209-L212), as opposed to when they are [python](https://github.com/CS410Assignments/MP1/blob/main/README.md?plain=1#L214-L218). As a shortcut, if you don't see syntax highlighting on [github](https://github.com/CS410Assignments/MP1/blob/main/README.md), it's `bash`.
https://campuswire.com/c/G984118D3/feed/681 MP2.3 Instructions I am a bit confused about what we're supposed to do in MP2.3.  The avg_p file of a ranker for the p-value calculation - will it contain the MAP of the ranker with different param values? Or will it contain per query avg_p values for a particular param of the ranker?  Say for BM25, shall we fix a k3 and print the avg_p of each query in the avg_p file? or shall we vary k3 10 times and print the MAP of these 10 settings in the avg_p file?  If we calculate the p-value using per query avg_p values at a fixed k3 of BM25 and a fixed c of INL2, where do we use the analysis with varying ranker params to find maximum MAP?Your output of new rank function and BM25 will be like this : query 1 avg_p= query 2 avg_p=  ... ... query 225 avg_p= There is nothing you need to change for the BM25.  All you need to do is to append the average precision of each query of new rank function and BM25 into list(or array), then use the code below to calculate the p value. Finally put the p-value into significance.txt   from scipy import stats   stats.ttest_rel(rvs1, rvs2)  *rvs1, rvs2 are two lists for p-value calculation in t-testing.  see the reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html   Hi, thanks for getting back. I actually did exactly that. But I am confused about where are we supposed to put the analysis of the varying parameter. For example, we're instructed to check how varying c affects INL2 performance. So, we're supposed to run INL2 with 10 values of c. But in which function are we supposed to loop over the 10 c values? Once you are done with the 70% test in the program you can create a file that holds the floats in a file. Then create a similar file using BM25 algo and compare them with the Scipy function. The final file only needs to have the p-value, a float value, for the grader to work. Make sure you check the output of the scipy package and see that the p-value going into the file. You do not need to submit the test code just the significance.txt file containing the p-value In the main you can loop with 10 different c to  test.  Thanks.
https://campuswire.com/c/G984118D3/feed/565 How to run search_eval.py? Hello,  I think I'm lost in doing MP2.3.  When I try to run search_eval.py, there is an error like following: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/99b3b8d4-330f-4d6f-9faa-1c1491e9c080/image.png)  It seems that there should be two sys.argv for running the python file, but I couldn't find guides about arguments for search_eval.py.  Could you tell me a little how I can run search_eval.py?  Thanks.Run it by doing: python search_eval.py config.toml
https://campuswire.com/c/G984118D3/feed/524 MP2.4 Not Up Yet Anyone else not seeing MP2.4?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a27a4347-fac2-47c6-87c5-5b3a673243c4/image.png)I am not seeing it either I don't have MP 2.4 yet either.  Anyone have a grade on MP 2.2 yet? We were targeting to release it on Sep 21. Yeah I also don't see a grade for MP2.2 - Assuming TAs will publish grades later this week since many of us don't have grades out yet.
https://campuswire.com/c/G984118D3/feed/1255 MP3 deadline Given the amount of recent confusion on MP3, I believe it is safe to say this final MP has proven difficult to a majority of the class. Given the already generous extension of 3 more days, would it be possible to extend the deadline to Friday?   I feel as though many students are overwhelmed and have likely accepted the fact they will be turning this in late.    Thank you for your considerationHi, we won't consider to further extend MP3 as it seems unfair to the students who have finished it, but we have reduced the late penalty to 20%.  I believe this to be a fair middle-ground and appreciate your consideration. Thank you very much.  Is this late penalty per-day?
https://campuswire.com/c/G984118D3/feed/438 MP 2.1 Grade Hello,  I submitted my work for MP 2.1 before the deadline (15 entries), but Coursera shows it is overdue.   I understand that grades for this MP will be entered later in the week, but just wanted to make sure that the "Overdue" that is shown on Coursera won't affect my grade.  ThanksI was about to ask the same question. Thank you. I think the assignment will be shown as overdue until the TA team updates our score on Coursera. The overdue is recorded by Coursera because it didn't detect a submission by the deadline. This was the case for some of my other courses too. Please see https://campuswire.com/c/G984118D3/feed/430 As already suggested in some posts below, please don't worry about the "overdue" status of MP2.1 on Coursera. It will be graded using the CS410 DL system and we'll upload your grades to Coursera later.  2.1 Grade is already released. Its showing a grade for me. Can you please try refreshing Coursera and accessing the [Grades page](https://www.coursera.org/learn/cs-410/home/assignments) 
https://campuswire.com/c/G984118D3/feed/1153 Quiz 9 question 1&2 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/80a07fb3-f965-48f5-b662-876add0313f1/image.png) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1a1b06ae-7cb4-44ea-b9ac-755e0d85ba72/image.png) For first question and second question what is the point here by "all" and "each"In the first question, "all the words" refers to how the system works. In this particular case, the entire document is generated from whatever model is chosen. Therefore the model choice is only made once in the process. So in the first case, our first decision would be to pick the model- and then generate the entire document based on that decision. That would give us   (probability of picking model 1 * probability of generating entire document with the first model) + (probability of picking model 2 * probability of generating document with the second model)  = (0.5 * (0.4 * 0.4)) + (0.5 * (0.1 * 0.1)) = 0.08 + 0.005 = 0.0085  In the second case- we need to choose a model for each word. So every time we want to generate a word, we have to go through the whole process -> 1. pick a model 2. generate a word  This would mean that every time we generate a word we need to take into account the fact that a model needs to be chosen first. This would give us 4 possibilities of how the document is generated: - pick *data* from model 1 and *software* from model 1 - pick *data* from model 2 and *software* from model 2 - pick *data* from model 1 and *software* from model 2 - pick *data* from model 2 and *software* from model 1  we simply have to now add up the probabilities of these 4 possibilities.  (0.5 * 0.4 * 0.5 * 0.4) + (0.5.* 0.1 * 0.5 * 0.1) +  (0.5 * 0.4 * 0.5 * 0.1) + (0.5 * 0.1 * 0.5 * 0.4)  = 0.5 * 0.5 * (0.16 + 0.01 + 0.04 + 0.04) =0.25 * 0.25  =0.625  Hope this helped! I would suggest to keep the quiz post private for others still working on it. 1. We can solve this with the model here for all words using difference is multiplying the background and the document model by the second word. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7f65739c-bf24-40c3-bd30-d4b0a01cdcc6/image.png) 2. Using Likelihood approach for each word ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bf5b1400-f095-4f21-b0a7-3df5b4660be1/image.png)
https://campuswire.com/c/G984118D3/feed/721 Why add 1 in log transformations? Why do so many of our log transformations require us to add 1? One example is with IDF(W) as log(M+1). Why not just use M?the reason it is common to add 1 is because what if `M` is equal to 0? `log(0)` is not well defined, so that leads to a computational error. instead, it is safe to add 1 so we know that the minimum log argument we can encounter is 1. not sure if there are particular reasons for the specific IDF case, but in general it is common practice to add one for most cases because of this.  will mark your question as unresolved in case anyone has additional reasons to use this specific transformation If M is the total number of documents in the corpus, and the word appeared in all of the documents, then M/(docs containing term) would be 1.  I would imagine this happens a lot with small words.  The base 2 log would be 0, messing up any calculation you were doing.  So, if you add 1, (M+1)/(docs containing term) could never be 1.
https://campuswire.com/c/G984118D3/feed/664 Grader Failing on Scipy My code works perfectly fine in local. I have imported scipy  - "from scipy import stats"  The Grader is complaining about it. When I remove the "from scipy import stats" , then my could wouldn't work.   Has anyone come across this issue and is there a trick to solve it .  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/033d013b-8376-4ba6-b4d8-57ddf75d721f/image.png)the grader does not support scipy. just remove the scipy part in your code and push again For the points of p-value, you don't need to include the calculation code to get those points. Just the significance.txt file is fine. Right. I have changed my code back to its original code in main (remove the test part) then it works. 
https://campuswire.com/c/G984118D3/feed/1076 Will Exam 2 be Cumulative? Hi,  Will Exam 2 be cumulative over all the lectures/material in the course (Weeks 1-12), or will it mostly just cover the material after Exam 1 (Weeks 7-12)?  Thanks!The Syllabus doesn't explicitly say that it'll be cumulative. It does however say:  " This course will have two 1-hour exams. The exams are intended to test your understanding of the material you learn in the course and will contain questions similar to those seen in the weekly quizzes."  Presumably if both exams are 1 hour, one section covering Text Retrieval / Search Engines, while the 2nd portion of the course from Week 7 to Week 12 are based on Text Mining and Analytics MOOC, the 2nd exam should only cover content from Weeks 7-12. at least from the [Course Introduction Video](https://www.coursera.org/learn/cs-410/lecture/MsDCs/course-introduction-video), it is stated: > So, the first exam will be given at the end of the first MOOC on Test Retrieval, and the second one will be given at the end of the second MOOC on Text Mining  While some knowledge from the Text Mining MOOC is built off of information from the first half of the semester, this leads me to conclude that the exam will primarily focus on the second half of the semester It will be from week 7-12 I think it is week 7-12. It will probably cover lectures from week 7- 12
https://campuswire.com/c/G984118D3/feed/1221 M-step i dont understand the m-step formula can someone please detail it for me ?Hey Gabin  for MP3 , we use PLSA so I will focus on EM within PLSA. In class, we generally used PLSA with a background model. For the assignment, there is no background model. Here is the M-Step we were shown in class:  $$\begin{aligned} \pi_{dj}^{(n + 1)} &= \frac{\sum_{w \in V}c(w, d)(1 - p(z_{dw} = B))p(z_{dw} = j)}{\sum_{j'}\sum_{w \in V} c(w,d)(1 - p(z_{dw} = B))p(z_{dw} = j')} \\ p^{(n + 1)}(w| \theta_j) &= \frac{\sum_{d \in C}c(w,d)(1 - p(z_{dw} = B))p(z_{dw} = j)}{\sum_{w' \in V}\sum_{d \in C} c(w', d)(1 - p(z_{dw'} = B))p(z_{dw'} = j)}\end{aligned}$$  so what is this saying? #1196 has a few more details as well. so does #1179 , point 3 in my answer talks a bit more about the normalization step   but at every M-Step, we are calculating two quantities (or really, two matrices): $$\pi_{dj}$$ at the next time step and $$p(w | \theta_j)$$ at the next time step. both of these leverage the E-step results $$p(z_{dw} = j)$$  the first term, $$\pi_{dj}$$, is the document topic probabilities  $$p(z | d)$$. the second term $$p(w| \theta_j)$$ is the topic word probabilities $$p(w | j)$$  luckily, we have a bit of a shortcut. we can ignore the denominator for now which allows us to see that:  $$\begin{aligned} \pi_{dj}^{(n + 1)} &\propto \sum_{w \in V} c(w, d)(1 - p(z_{dw} = B))p(z_{dw} = j) \\ p^{(n + 1)}(w | \theta_j) &\propto \sum_{d \in C} c(w,d)(1 - p(z_{dw} = B))p(z_{dw} = j) \\ where \\ \forall d \in C&; \sum_{j = 1}^k \pi_{dj} = 1 \\ \forall j\in [1,k] &; \sum_{w \in V} p(w | \theta_j) = 1 \end{aligned}$$  the constraints after the "where" inform our normalization step, thus implicitly returning our denominator to us while making some of the calculations simpler.  in other words, we can see that $$\pi_{dj}^{(n + 1)}$$ is proportional to the sum over all the words in the vocabulary of: the count of the terms in the document (given by `d` in $$\pi_{dj}$$) times (1 minus the probability of the background model given `d` and `w` -- this comes from the e-step) times the probability of topic `j` given `d` and `w` (also comes from the e-step)  $$p^{(n + 1)} (w | \theta_j)$$ is proportional to the sum over all the documents in the collection of: the count of `w` in `d` (here the `w` is from $$p(w| \theta_j)$$) times (1 minus the probability of the background model given `d` and `w`) times the probability of topic `j` given `d` and `w`. here `j` also comes from $$p(w | \theta_j)$$  we can further simplify from here to omit the background model, as it is not part of MP3. that makes the equation simpler. finally, we can normalize using the constraints above. fundamentally, the individual quantities we are calculating for both $$\pi_{dj}$$ and $$p(w|\theta_j)$$ are actually the same -- $$c(w,d)$$ and $$p(z_{dw} = j)$$ (this second one comes from the e-step). we just sum them over a different range. in $$\pi_{dj}$$ we sum over every word in the vocabulary. in $$p(w|\theta_j)$$ we sum over all documents in the collection.  please let me know if that made sense. i am happy to discuss further if that wasn't clear or if you have any other questions as well. i am also trying to think of the proper "intuition" to think of these equations more simply, and may update my answer if i can think of it awesome got it now  thanks 
https://campuswire.com/c/G984118D3/feed/636 MP2.3 Grade not visible on Coursera Hello Instructors,  I see that I am not the only one with the grade on DataLabs not appearing in Coursera but its been a day since anyone else posted. Has this issue been resolved?  Also, Just to be sure, the autograder in Datalabs looks into both the correctness of score_one function and significance.txt right? So as long as a score of 1 appears that means we got 100%?  Thanks!My Coursera grade also does not seem to be updating.    I have a score of 1 in the livedatalab MP2.3 Leaderboard but only a 70% on Coursera.  I implemented the first part of MP2.3 before I finished the rest a couple of days later. Make sure you navigate from the coursera link for the MP assignment to livedatalab for them to connect. Then I would recommending changing something small in the GitHub (like adding a space) so that you can repost it to make sure it receives it.  My grade on coursera was almost instant with the leaderboard update. See #591 , the same issue has not been resolved yet. I did that multiple times and it wasn't working. Agreed, does not work for me either Zetian. I did an empty commit and went back into livedatalab through Coursera and this time my Coursera grade updated.    I changed nothing except accomplishing an empty commit.  Try this to see if it works for you :) My grade is not reflecting either but I received a score of 1. so I assume it should be updated on coursera soon.  I still have this issue too. Doesnt work for me. Tried multiple times. click open tool in coursera and re-login in livelab. after that do another commit in github, the grade should be available in a minutue Tried multiple times :( as long as you got a score 1 in livedatalab, you dont have to worry about it. Grades can always be added in couresra by TAs manually 
https://campuswire.com/c/G984118D3/feed/360 DCG vs nDCG Hi,  while we tend to prefer the nDCG to DCG. I came across the following limitations of nDCG from wiki:  1. Normalized DCG metric does not penalize for bad documents in the result 2. Normalized DCG does not penalize for missing documents in the result 3. Normalized DCG may not be suitable to measure performance of queries that may often have several equally good results  How to overcome these limitations in practical terms?The explanation from [the Wikipedia page](https://en.wikipedia.org/wiki/Discounted_cumulative_gain#:~:text=0.785-,Limitations,-%5Bedit%5D) might answer this question.  > Normalized DCG metric does not penalize for bad documents in the result  We can penalize bad documents by setting the scores 1, 0, and -1 instead of 2,1, and 0.  > Normalized DCG does not penalize for missing documents in the result  We can penalize missing documents by enforcing a fixed set size for the result set.   > Normalized DCG may not be suitable to measure performance of queries that may often have several equally good results  This seems to be especially true when this metric is limited to only the first few results. So a possible workaround might be to increase a result set size, although this change can sometimes be unpreferable.  
https://campuswire.com/c/G984118D3/feed/339 CS 410 DL Hi,  I presume that the content so far studied in week1 , week2  and possibly week3 corresponding references over the internet are only required to be submitted in this DL right?  Please advise.There is no required content with respect to the week. For example, if you're already on week 5 and have found something useful, you can submit that for MP2.1. Moreover, you'll still be able to submit content once MP2.1 is over (we encourage you to do so throughout the semester, in the case that you come across a useful reference).  Thank you! No problem!
https://campuswire.com/c/G984118D3/feed/198 Webhook not seen Hello,   I was wondering if I should be able to see my webhook after creating it.   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/fdb64fba-7c50-4bc5-b6a4-f69efa5f48f2/image.png)  After clicking "Add webhook" multiple times, I still do not see the webhook. I've tried this a few times, but my list of webhooks is empty. Does anyone know how to go about fixing this?   Here is an image of my webhook which is where I believe I'm supposed to see if it gets successfull added:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/646bee87-a76f-4e90-a4bd-ec693f1625d6/image.png)   Hey, yes you should be able to see the webhook after creating it. Yes, you should be able to see as below ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4fb78a73-96bb-44ba-9c56-1555897b8ca9/image.png)
https://campuswire.com/c/G984118D3/feed/935 Question for Quiz 2 can anyone explain the logics behind?🥺 "If Zipf's law does not hold, will an inverted index be much faster or slower?"  The answer is slowerZipf's law tells us that highly-ranked words have low frequency. If Zipf's law does not hold, we would expect highly ranked words to have higher frequency. High-frequency words, such as stop words, take up a lot of space in an inverted index. So, if Zipf's law does not hold, we would expect an inverted index to become bigger and therefore slower. The inverted index takes advantage of the fact that most of the higher ranked words tend to occur rarely. This idea comes from Zipf's law. However, if this is not true, then inverted index will start suffering from the need for more space and time
https://campuswire.com/c/G984118D3/feed/974 Password Exam on Coursera I don't quite understand, what kind of things should I type in the answer box there.This is for your proctor to input when you take the exam. You have to schedule a proctor session via proctor U and the proctor has the password to the exam. Once the proctor inputs the password into that password quiz, the actual exam quiz will be unlocked which you can then take. So I'm supposed to ignore this before I actually start taking the test? Yup this is purely for the exam. The proctor will know what to do. 
https://campuswire.com/c/G984118D3/feed/556 question about smoothing method In the lecture, it mentioned that we can use the collection language model to do the smoothing. Not sure if I missed anything in the lecture, but in practice, when we run the retrieval engine, the only information we know is the user query. in this case, how do we know which collection language model we will use? do we need to have a separate model to identify the "collection" of each query? Thank you! Actually, you have a bit more information than the user query at retrieval time. You have your entire collection at that point. The user is writing a query to retrieve hopefully relevant documents from your collection. For example, in the case of Google, the collection is the entirety of the indexed web (give or take). Your collection may be tailored to a specific use case; for example the CS410DL search engine we are building is more specific to the Text Information field, instead of Google's more general scope. By using the statistics of our collection, we can then build the relevant metrics. There are also more general corpuses that have these unigram word probabilities, like the background model, or topic specific models (ie for NLP or for Nutrition). These are usually pre-defined by some other collection, and we can leverage those pre-calculated language models That makes a lot of sense. Thank you! May I know why in section 5.3 Feedback in LM, in the generative mixture model, we use background model instead of collection model? I am always confused about when should we use collection model or background model. Screen Shot 2022-09-21 at 3.59.06 PM![Screen%20Shot%202022-09-21%20at%203.59.06%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ff50d8e2-74ad-4ce9-ace0-4ff4d3812bda/Screen%20Shot%202022-09-21%20at%203.59.06%20PM.png) Hey Yu; I'm actually going through Week 5 right now myself, so I will amend my answer once I get to 5.3 (may not be until tomorrow, but I will update when I get there). However, in general, my belief is that it probably comes down to what your purpose is. Using the general background model has more utility for more use cases, and so is more widely applicable. Topic specific LMs are beneficial when you know you plan to limit your domain a bit more. However, with the generative mixture model I imagine there is something slightly more complex at work so I will update as I progress and think this through. Definitely an interesting question!  Also, I've amended my original answer slightly to provide a little more context for now Thanks a lot. Take your time, feel free to let me know if there is any update. Hey Yu; I think I've gotten a better sense now for how this works in the Generative Mixture Model and the reasoning here; In the Generative Mixture Model, we assume that the terms in the feedback documents are generated from one of two models. When we choose to generate a term for a document, we select which model generates that term randomly. Let's assume the case where $$\lambda = 0$$; in other words, all the terms in the feedback document are generated from the `Topic LM`. Well, this actually presents a problem.  To understand why, I want to clarify that we do not yet know the `Topic LM` probabilities. We actually build the probabilities based off of the feedback document. Thus, if $$\lambda = 0$$, we create the `Topic LM` from the term frequencies in the feedback documents (using Maximum Likelihood). The problem comes, then, when we consider what terms will be most common in the Feedback Documents. It will be stop words (`the`, `and`, etc). So, we want to mix in another model that is able to implicitly remove the stop words from the `Topic LM` for us.   In this case, we know that the `Background LM` tells us what words are common in general. And what we want to remove are the words that are most common **in general**. If we used something more specific here, we may actually end up filtering relevant terms (for example if we are interested in a computer topic and used a `Collection LM` based on computer science papers, we may end up filtering out terms like "computer" or "software", which is undesirable). Since the most common stop words are represented best by the `Background LM` in this case then, it is the best choice for removing those common terms without filtering too aggressively.  So, when $$\lambda > 0$$, we can attribute the most common words in the feedback documents that are also common in the `Background LM` to the `Background LM`. The remaining terms will get attributed to the `Topic LM`. This effectively allows us to remove stop words.  So, we use the `Background LM` here because it is very general, and contains the most common stop words which we want to remove from the `Topic LM`.  By trying different probabilities for we can maximize the log likelihood for $$p(F | \theta)$$ at a given $$\lambda$$ value (which means we are trying different probability distributions for the `Topic LM` to perform that maximization over $$\theta$$)  More generally, and to succinctly answer you question **the Background LM seems to be the best choice for stop-word removal because of its generality**, which is also basically its purpose in the Generative Mixture Model  I am happy to elaborate a bit more if I missed anything or if there are further questions. Please let me know if that all made sense. Thanks!
https://campuswire.com/c/G984118D3/feed/647 Question about tasks in MP2.3 I am confused about if I get all tasks needed to be completed in MP2.3. Firstly, we need to modify function at  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4c4eabe2-1d55-4a80-b306-7eee173c86df/image.png) according to the formula, right? But some variables in the formula is missing, ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1785be3e-35ba-4e5f-ac19-230c9d8f28d2/image.png)like c(t, Q). How should we compute them? Secondly, we need to change the ranker ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9a521f13-fe7c-482e-a60c-16bda117e1ef/image.png) Then we need to conduct the significance test, right? Hello Anonymous, We have been asked first to modify score_one to use one of the ranker functions presented in the lectures, such as Jelinek-Mercer or BM25. We need the output of this step for the significance test. Second, we have been asked to modify the return value of score_one to match the formulas provided. All of the necessary variables, including c(t,Q), are available through the sd object, although you may need to read carefully the descriptions at the link beginning with https://meta-toolkit.org.  Finally, we are asked to compare the results from the above two steps, using a paired t-test. How we run the t-test is up to us. David c(t,Q) means count of terms in query I fixed it. Thank you! Thank you Hi David:  Please correct me if i'm wrong - so step 2 is to modify load_ranker() to make it return InL2 ranker or NM25 ranker? step 3 is to modify main() to make it create those three .txt and save them to working dir?  Hello SiCheng,  Yes. As mentioned in README.md, we are asked to change the ranker at least twice. We are asked first to change it to one of the five metapy methods such as OkapiBM25 or DirichletPrior (along with our choice of parameters). Later we are asked to change load_ranker to call InL2.   Step 3 can be achieved more than one way. For example, one could modify the program to write the average precision values to a text file, as you indicated. Or, one could copy-and-paste the console output to create a text file. Key points, in order to make a paired T-test analysis, are that the two text files should contain only numbers, and the two files should contain the same quantity of numbers.   I hope this is helpful. David
https://campuswire.com/c/G984118D3/feed/930 Deadline mismatches! When is team formation due and how do we submit our team formations?  In the project overview from week 6, it says that team formation is due on 10/17, but in week 9, it says team formation is due at the same time as project proposal on 10/23. Which is correct? And how and where do we submit team formations and proposals?  ![Screen%20Shot%202022-10-13%20at%202.08.50%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0fc16c4f-db0e-4a08-8860-17550fcb8d8f/Screen%20Shot%202022-10-13%20at%202.08.50%20PM.png)  ![Screen%20Shot%202022-10-13%20at%202.08.34%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0da12b36-bc4a-4c0a-8448-4050cdb31f9a/Screen%20Shot%202022-10-13%20at%202.08.34%20PM.png) > In the project overview from week 6, it says that team formation is due on 10/17, but in week 9, it says team formation is due at the same time as project proposal on 10/23. Which is correct?   The team formation is due 10/17. I've updated the Coursera title to reflect that.   > And how and where do we submit team formations and proposals?  Instructions and reference material can be found [in this Campuswire post ](https://campuswire.com/c/G984118D3/feed/736) 
https://campuswire.com/c/G984118D3/feed/925 Memorizing Equations for Exam 1 Since no cheatsheets are allowed for the exams, do we need to memorize equations for exams? Such as the smoothing function for statistical language model, or the gamma integer compression algorithm? Cuz I saw we have questions on quizzes require those equations.In general, you should not need to make effort to memorize anything. That said, there are some simple formulas that you are expected to fully understand. Once you understand them, you should naturally be able to write it down and shouldn't need to make any effort to memorize it. For example, you are expected to fully understand the formulas of precision, recall, average precision, and F1. If you fully understand them, you will find it easy to write down the formulas yourself (based on your understand). You don't need to remember nDCG (since it's log-based discounting is a somewhat arbitrary way of doing it, and there's really nothing there to understand except for the idea of discounting based on positions), but you should also know the ideas behind nDCG. You mentioned smoothing, which is an important technique, so you are also expected to fully understand Dirichlet prior smoothing and the JM smoothing (linear interpolation). Again, once you fully understand how they work, it should be straightforward to write down the formulas on your own. And yes, you should know how gamma coding works. In sum, focus on *understanding*, rather than memorization.   In general, you can refer to the last slide of a lecture that says something like "what you should know" to see what you are expected to understand.     Thanks for the guidance professor. The last summary slide 6.10 talks about HITS and learning to Rank which are covered in the optional lectures. Should we expect questions on the optional lectures as well? I found this study guide in slack the other day... hope it helps:  https://uiuc-mcsds.slack.com/files/U59M5H83W/F7JHBRF61/cs410exam1.pdf In case you can't open that file directly (like I couldn't), you can also search for it in Slack here: ![Screen%20Shot%202022-10-14%20at%2012.48.32.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1d116df1-68e8-450b-a78c-3d09eb61ece2/Screen%20Shot%202022-10-14%20at%2012.48.32.png) Thanks for this. It is 5 years old, is it still accurate?  Thanks Maciej!  @Akshat, it's a summary of the content and it covers content/topics of the lectures, so I'd say it's still relevant I can't get into that Slack: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5335219e-2fb6-4627-a119-04a145d17104/image.png)  Can anyone upload the pdf somewhere else? Try this one: Uploaded from my PC  [CS410exam1.pdf](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a4fb3b31-9f88-49a7-b0de-6ed4ecbde668/CS410exam1.pdf) check my reply below, for some reason I couldn't upload to the thread Thanks!
https://campuswire.com/c/G984118D3/feed/630 Writing InL2 function In the readme.md file it is mentioned to go over the following link to understand the retrieval function. I am not able to get access to this citation.  "You will now implement a retrieval function called InL2. It is described in (http://dl.acm.org/citation.cfm?id=582416) paper: "  is it required to over this paper to know how to implement this function ?     The equation is shown in the instruction. You just write python code to implement the equation. I don’t think you need to read this paper just for purpose of the assignment  You only need the formula, which is given in the readme/assignment overview. The paper (and the linked GitHub repo) will help if you get stuck on how to map formula parameters to the `sd` object variables, though! You don't need to go over the paper. The readme on github includes a picture of the equation. You need to implement that equation in the space given
https://campuswire.com/c/G984118D3/feed/478 Spoiler alert: issues running metapy on python 3.7 for MP2.3 Hopefully this will save people some time.   It appears the ranker.score function will run indefinitely on python 3.7. I downgraded python to 3.5 and issue resolved. (Though it's a different battle to install correct ipykernel for visual studio code for python 3.5.... )My togo solution is to use docker with rosseta installed on if apple silicon. Python version won't make any differences. I noticed the same issue :-(. Were you able to downgrade your python using anaconda? I have M1 macbook and I cannot downgrade python to 3.5.. ![Screen%20Shot%202022-09-16%20at%202.35.44%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dba47f9d-3f5c-4a5c-bfbe-05c1cad54e1d/Screen%20Shot%202022-09-16%20at%202.35.44%20PM.png)  . . Edit: I am finally able to install python 3.5 on conda env. I just re-created my env and it worked.  ``` conda create -n $PYTHON35_ENV_NAME python=3.5 conda activate python=3.5  conda install python=3.5 conda activate python=3.5 ``` I did just the same thing with you. :) Life lesson learnt. Would have to check it out for next course. https://segmentfault.com/a/1190000015158290 This is what i been using and it's very simple. M2 Macbook
https://campuswire.com/c/G984118D3/feed/573 about the query term in KL divergence model Hi, I have a little hard time to understand exact what this term means, and how it is different from c(w,q) in the query likelihood model. In Query likelihood and KL-divergence, both terms refer to the query users use to search :![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/89e008d8-add2-4034-af7f-7eb8d6d616cc/image.png)I think that $${\hat\theta_Q} $$ is the query language model updated after feedback.  All of the various $$\theta$$s throughout the text are language models containing the probability of all the words in the model.  c(w, q) is just number of copies of word w in query q.
https://campuswire.com/c/G984118D3/feed/1011 Midterm Setup Confusion I saw before on ProctorU that you're supposed to have a mirror or something?? I don't have any way to get a mirror next to my desk.  I saw that if you don't have a mirror, you need a phone. Can someone explain the setup?  Is it just like when we take normal exams over Zoom and the proctor has to be able to see my computer screen and my paper? Or is it similar to Proctorio where you need to move your laptop around sometimes and your desk has to be clear?   I want to know what I need for the exam and if I need to clean my room or my desk or something.  Also, will we be able to use the built-in laptop calculator or anything?Please keep you phone with you. Please follow their instructions.   The proctor need something to check your environment, either your front cam of your phone or a web camera should be ok. They asked me to move my web camera 360 degrees to check all room corners. He/She also want to see where you put your phone. To add to previous answeres. No you do not have any built-in calculator. And for using mirror or phone to show your desktop and room, you only show that once before the exam. One tip would be to avoid setting close to your normal desk with additional displays/screens, even if they are not connected to your computer/laptop. There is a chance you might be asked to put the displays away. It depends on the proctor and won't always happen, but it would be a pain if it does. Use your phone, mirror not required. No calculator is allowed.  Yes, it is just like taking your exam over zoom. They want to make sure they can see all around you. They may even ask you to pick up your laptop and show them the whole room.
https://campuswire.com/c/G984118D3/feed/1318 Question Regarding Project Progress Report Hi,   I was wondering if there is any additional requirement for the project progress reports besides: - the progress made - remaining tasks - any challenges/issues being faced  Does it have any additional requirements such as format, and the minimum number of pages?  ThanksNo, there is no format or minimum number of pages requirement. However, the completion grade is based off of sufficiently answering each question, so we expect that this will require 1-2 pages total.  Thank you so much for the clarification. 
https://campuswire.com/c/G984118D3/feed/274 Leaderboard - Does not load on Livedatalab For some reason the leaderboard does not load for me. It continues to show loading.....  Any one facing the same issue ?Can you try relinking your GitHub again (removing the link from livedatalab and adding again). It seems like you passed the tests but the leaderboard is not updating. If that doesn’t work, I can manually update your coursera grade.  I just did it again. Deleted the linked account, added the account again. Made a push on the MP1 code to github. The logs shows SUCCESS ! But leaderboard is still not loadind and I also dont see the grade in Coursera. It shows as If I have not completed my assignment. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/fa6a7e2a-71ae-417a-baa8-5d52df733710/image.png) I've updated the grade on coursera thank you !! It shows like this now.   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/68e2a078-4476-4104-9b98-adb8be90e9b6/image.png) Hello. I may know why you have that issue. I was having the same problem.  When I was initially looking for the Leaderboard in "LiveDataLab", I accidentally went up to "Create" and then selected "Leaderboard", which gets stuck on loading.   If you did this, try this approach. Go to Projects -> MP1. You should see leaderboard on the left.   ![Screen%20Shot%202022-09-03%20at%206.30.10%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5f996da5-764e-4060-95eb-3e47675a673f/Screen%20Shot%202022-09-03%20at%206.30.10%20PM.png) Thank you Anshul. I was able to see it the way you mentioned. !! Hello Priyanka, Can I request a grade update on Coursera? I have the MP1 status as success (1 in the Leaderboard), but Coursera is indicating the assignment as late at this time. Thanks  
https://campuswire.com/c/G984118D3/feed/178 success on ping and fail on push deliever ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3821f7e2-5d58-4b14-a59e-17671764f0e3/image.png) I set up the webhook with the correct link start with livedatalab. I did success on the ping but once i push my code to github, it doesn't shows up and grade for me i also had the same `Github Account Not Found` error as you had. While your case may be different, for me I had to use my **exact** username in github (case sensitive) as the host name when adding the github account (with the token) to the livedatalab. What messed me up was the case sensitivity my github account are all lowercases and i type my name to livedatalab are all lowercase. But it still doesn't work ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/205808b0-5ab9-4626-b2a3-97df53d18e94/image.png) The picture is the error return by livedatalab when i push my code to github. Can you try to delete the webhook and re-add it?  Yes, i tried to delete the webhook and re-add it manytimes, it only works on the first times Can you try relinking your github account? It's likely the repository is not accessible? What is the name of your repo for MP1?   If this doesn't work, I would suggest recreating a livedatalab account and resubmitting --- you might have missed a step from before.  Yes, I had relinkede my account yesterday, and it doesn't work, I alreayd have gread and post on the coursera. If i recreat another account, will that affect my grade ? the repo for MP1 is MP1_private  No I think it should be fine. Just make sure you use your illinois email when registering for a new account. saw the same error, too. It means LiveDataLab is not connected to your GitHub account. a couple things to try - put this in browser `https://api.github.com/users/{{github_usename}}` if a JSON object shows up, `github_username` is correct and is what to use linking LiveDataLab - make sure host domain is lowercase in LiveDataLab like `github.com`. Sometimes it got auto-corrected to uppercase - if still not work, unlink it, refresh the page, and link again with a new token  Good Luck~ Thank you so much, but when i put the `https://api.github.com/users/{{github_usename}}` in browser, it reuturns  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ce701d50-fffa-4dcc-a0be-cc3fdb481e79/image.png) replace `{{github_usename}}` with the value you used in LiveDataLab. It's just a step to make sure you are using the correct username. If still error, then you are using the wrong GitHub username this api return a JSON object and it is what i linked on the livedatalab OK, I fixed thhis problem. The reason of this becasue i add my another account ad contributor. 
https://campuswire.com/c/G984118D3/feed/146 deadline for project progress report Hi I find on coursera w13 is for us to do group project, however the deadline for project progress report is Nov13, which is the end of w12, is this deadline correct？yes, the progress report deadline is Nov 13. working on the course project starts as soon as you submit your proposal, all the way through the final submission deadline. w13 just doesn't have any other material covered, that's why its description is " work on group project"
https://campuswire.com/c/G984118D3/feed/327 Grading failed? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/11a9d202-1753-4797-91fb-1e29587a51eb/image.png)  Why it is showing using python 3.7? I don't think I use python 3.7 anywhere. My computer environment use python 3.5 and I was able to execute all the instruction steps.I used 3.7 and it works. In the log it says "tok" is not defined. In your code, did you define tok as the parameter? It turns out to be a "code error", which means I am failed to execute my code in my local computer. So I definitely won't expect a success from the grading either. I have corrected my code and managed to solve the issue.   My lesson learned: try to execute it on my local and make it a success run first.
https://campuswire.com/c/G984118D3/feed/515 Mp 2.2 is Overdue I have submitted 7 queries yesterday but still  It's showing assignment is overdue.  As per the instruction , I should get full credit for submitting at-least 5 queries.   ![Screen%20Shot%202022-09-18%20at%2011.03.42%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d341d3f5-c884-40de-b799-f50e15d9cf08/Screen%20Shot%202022-09-18%20at%2011.03.42%20PM.png)  ![Screen%20Shot%202022-09-18%20at%2011.15.08%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0616d4b7-1dde-4f0a-bfd3-e95d3002f1b1/Screen%20Shot%202022-09-18%20at%2011.15.08%20PM.png) The grading for MP2.2 is similar to that of MP2.1. This is what it says in the overview document. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c79c3d5c-0bbf-4956-a90f-af1e8f01b8fa/image.png)  There was a similar clarification for this query for MP2.1 #438  coursera indicates it as overdue as no submission was received before the deadline. it's normal and you'll see your score after the TA team updates it in coursera. it's the same as MP2.1. Mine shows the same. I think it may get updated as soon as TAs update the grades. And you typically get an email once you receive new grade I believe this to be true for all who have submitted; it may take some time to populate a score and that the assignment is overdue until the TA's are finished assigning all grades. I have learned to take lightly the information displayed in Coursera
https://campuswire.com/c/G984118D3/feed/993 Directions for PDF of the proposal? From the context of the instructions, it seems like we have to make some kind of write-up with the proposal of our project. However, I can't find any instructions regarding that, I can only see the instructions for team signup and CMT submission. If we do, could you tell me where to find the instruction for the write-up? What do we need to include in our proposal PDF?  Thanks!The content to include in your proposal PDF is determined by your topic. Please see [this Google doc](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit?usp=sharing ), navigate to your topic, and see the "Requirements" subsection for what to include in the proposal. 
https://campuswire.com/c/G984118D3/feed/700 Grade still not visible in Coursera The rank and score on LiveDataLab is 1 but it is still not updating on coursera.   Just wanted to confirm if LiveDataLab scores are enough for us to receive the full gradeYou should receive the full grade when TAs verify but if you do an empty commit through VS Code or a minor commit through Github like adding a space, you should be able to log back into livedatalab and the score should update. Should be updated now Thank you, will try that next time Thank you! same here! the grade is not visible in Coursera I still dont see a grade on coursera. I see an "Overdue" for MP2.3. Just want to make sure I will get a 100% with a score of 1 on live data lab. Thank you. I don't see a grade in Coursera either. Please kindly advise. Thanks in advance! 
https://campuswire.com/c/G984118D3/feed/1114 Question about MP3 I am currently computing the E-step. I simply use np.dot(self.document_topic_prob, self.topic_word_prob) to get the numerator part of the e-step equation. The denominator is the sum of this dot product. The shape is (1000, 6) eventually, which I feel is reasonable. However, the shape of p(z|d, w) should be (1000, 2, 6). It means that my shape is incorrect. I am not sure what is the problem. Can I get some hint please? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6bb7ee51-3a3c-4b6a-9d5b-644f8403f867/image.png) Maybe think of *. I am sorry. I do not really understand. Could you explain more please? What is *. elementwise multiplication Thank you for the reply. I see your point. However, the shape of P(z | d) is (1000, 2) and P(w | z) is (2, 6). Since their shape is not the same. How does the element-wise multiplication work? unless my shape of P(z | d) and P(w | z) are not correct when I created them. Maybe you can think of reshaping the matrix to achieve that, but I'm using the for loop instead. It was useful for me to first write down for loops on paper and then convert them to numpy transformations. Instead of directly converting the formula to numpy matrix transformations, just try out  formula -> for loop -> numpy. That was very useful for me. Thank you so much. I will try it Thank you for the hint, I will try using for loop as well Resolved?  yes
https://campuswire.com/c/G984118D3/feed/1100 Final Project -Intelligent Browsing Hello,  Our group chose Interllegent browing for our project topic for now and we are interested in web extension. To what extend do professional expect our web extension to performance. If we are working on web extension, how large the dataset is expected to be? Are we going to implement a new ranking function or we can use functions from Metapy?The expectation is that you and your group (size N) will spend ~20*N hours on the project. There is no specific expectation on data set size or ranking function selection. 
https://campuswire.com/c/G984118D3/feed/428 Github Webhook not working. The request is failed after git push. I tried to do redelivery also Github Webhook not working. The request is failed after git push. I tried to do redelivery also. It says "failed to connect to host"   Can anyone help me here .  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b058a4aa-72b5-4ffa-8de2-67830f1c1704/image.png)not sure what url you are using, but can you try with: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger  this is what worked for me Using the same URL. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/45b6d3e3-5aca-4726-8de6-cd1649c139da/image.png) You are using livelab, it should be livedatalab in the URL Thanks I have updated and submission is fine. Can I see what is the error in code ? I got score 0 in leader board Looks like you have a 1 on coursera Thanks . I have corrected max in the code. will there be anyway I can see what is the error for failure? the link in the instruction is wrong
https://campuswire.com/c/G984118D3/feed/218 try with Python 3.5, cannot import metapy: DLL load failed see below - any thoughts? tried with some solutions from stackoverflow - didn't work either. now trying with conda... fingers crossed   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/08b16f49-cd69-4f60-ab95-00994b59be7f/image.png)changed to Python 3.7. Problem solved. 
https://campuswire.com/c/G984118D3/feed/884 Okapi BM25 Model - question about denominator In the denominator (c(w,d) + k), should it not be in brackets instead of just k being multiplied to document length normalizer?  Reference from book (Chapter 6 Retrieval Models, page 108) ![Screen%20Shot%202022-10-10%20at%202.10.06%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3a094dda-fe25-442f-9012-a0ec1c029a70/Screen%20Shot%202022-10-10%20at%202.10.06%20PM.png)I think the formula mentioned in the image is correct as BM25. I was thinking since “c(w, d) + k” being part of bm25 transformation denominator so the whole term should have been multiplied by the document length normalizer. Not sure if I missed something. Oh, I see, but it makes the meaning of each term totally different.  $$\frac{(k+1)x}{x+k}$$ is the basic sub-linear transformation for $$x$$ with the upper limit $$k+1$$.   $$\frac{(k+1)x}{norm*(x+k)}$$ changes the upper limit with respect to $$norm$$.  On the other hand, $$\frac{(k+1)x}{x+norm*k}$$ does not change the upper limit of $$k+1$$.  I think you can comprehend this difference with this toy. https://www.desmos.com/calculator  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/afa58ffb-86d6-4de0-bc62-fe3b89592e31/image.png)   Thanks! Thank you for the great visualization and analysis! 
https://campuswire.com/c/G984118D3/feed/1093 need more help with EM algorithm I have a hard time to figure the dimensions of the terms and the matrix operations among them.  if I use d,w,t to represent the dimensions of document, words,and topics and use the term notation in the MP3 code,  I think P(z | d) is matrix of dimension (d, t), #d of rows and #t of columns.  P(w | z) is a matrix of dimension (t, w), #t of rows and # w of columns.  1) what is the dimension of P(z | d, w)?  in the lecture note as below, ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/20f1058d-76f1-4b12-84d4-bebd26255be5/image.png)  the ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/458609c7-e28a-4a5f-b057-6c8ac485f56d/image.png) can be mapped to P(z | d), and the ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6b8170ca-b998-45a6-87a2-7637e5052e7f/image.png) is P(w | z),  but how to do the matrix multiplication between P(z | d) and P(w | z)  to satisfy the equation above?  it is not a dot product between P(z | d) and P(w | z).  And I don't think the element wise multiplication between P(z | d) and P(w | z) can work because of the dimension mismatch. Please give some hints. Thanks!If you are not familiar with numpy matrix operation you can think about using the for loop. no, I am not going to use the for loop. It is more troublesome.  is P(z | d, w) still 2D matrix????? 3D I would suggest to check how the numpy matrix operation works, think about a smaller doc set and try some smaller matrix first. is there any right topic thread I can read outside the campuswire? Thanks! The dimension of p(z|d, w) is d, t, w (3-dimensional).  If you feel dot product or matrix multiplication cannot calculate in one-step, you may consider to use for loop. The formula shows how to calculate for each individual d, t, w. I can suggest you using matrix multiplication to get the denominators (or directly use normalize function) and using for loop to get topic_prob.  I did it, not as bad as it seemed..
https://campuswire.com/c/G984118D3/feed/563 MP 2.3 Question What does $$|D|$$ mean in the tfn function? I could not find any matching term in the score_data.h file.that will correspond to the "magnitude" of the document -- aka its length. see the properties defined [in this reference](https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html) to find what property corresponds to that term It's asking for the length of the document Thanks! Thanks!
https://campuswire.com/c/G984118D3/feed/350 No access to Google sign up sheet It says "No access. Request sent You’ll get an email letting you know if your request was approved"  But I checked my email box, no such approval emails there.  Please help.  Regards,  Mingqing TengYou will first need to activate Google app in your illinois account: https://www.nuestraverdad.com/post/activating-google-apps-uiuc  Then you can log into google using the illinois edu email address to view the document. Hey Mingqing - please make sure to log in using your illinois.edu email for access. Hi, Cao, thanks. I did all you suggested. I can acess all the google documents right now but the "Sign up sheet". I dont know what happened. Can you provide the link of the sign up sheet? I have no access to the signup sheet link as well from the github [README](https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVxixU5vvMuUaM/edit?usp=sharing). I did switch to my google app illinois account, any help? [](url) https://docs.google.com/spreadsheets/d/198HqeztqhCHbCbcLeuOmoynnA3Z68cVxixU5vvMuUaM/edit?usp=sharing I dont have the access either. We need help from the TA. Hey sorry for the confusion - you do not need to access the sign-up sheet for MP2.1. For both MP2.1 and MP2.2, please do not follow the LiveDataLab instructions, as everything will be done via the the Digital Library extension / website.  Tried the steps mentioned in this step .  https://www.nuestraverdad.com/post/activating-google-apps-uiuc . Then I tried requesting access after switching to Ilinois ID still getting this , Couldn't send request error in google sheet Can TA please help on this? I also connected VPN, but still no acess to the sign up sheet..... Do you have access to the sign up sheet now?
https://campuswire.com/c/G984118D3/feed/1097 DBLP Topics + Labels I completed MP3 without issue, but was interested in extending the learning experience by experimenting with the DBLP dataset.  - Is there an optimal number of topics for this dataset?   - Are there any training labels you could provide?  Initial experiments with DBLP indicate it would be advantageous to use a tokenizer/vectorizer to build the term doc matrix due to the size of the corpus/vocabulary.    However, subsequent matrix multiplication operations are difficult even with only 2 topics because of high memory requirements.   - In practice, should we use stop words and/or max features size to limit the size of the term doc matrix? - Additionally is there a straightforward way to implement the PLSA algorithm without the need for the 3d topic prob matrix?  My intuition is that the 3d topic prob matrix could be replaced by (2) 2d matrices, which would be very helpful  for matrix operations.  This would also allow sparse matrix functionality, which only supports 2d.  
https://campuswire.com/c/G984118D3/feed/389 Practice question: 3.6 Hey, just wondering, for the context of this question:" an automatic system that filters tweets in the search for possible communication between terrorists about attack plans on US soil" - due to the extremely high stake if any terrorist communication somehow is missed, I would guess we are looking for a very high recall, which means we would rather sacrificing the accuracy, but to make sure we keep as many relevant tweets as possible - as we have such bias, doesn't it mean we would prefer lower F-score, because F-score is trying to balance between accuracy and recall. Or one step further, is there any context in which we would prefer a low F-score?Yes, this example seems to be looking for a high recall so that all the communication tweets are listed even if that means the ranking of such tweets is not very precise. Having higher beta value supports such usecases. If it helps, can refer to https://campuswire.com/c/G984118D3/feed/386 In my opinion, F1-score, Recall and Precision are all important metrics when we evaluate the result. In this case, we pay more attention to Recall, so it will reduce F1-score to a certain extent. However, we cannot just prefer the lowest F1-score. We can give Recall a larger weight and weight the three metrics to evaluate the result. As many have said, F score is a balance between accuracy and recall (not an average). I believe we always prefer higher for accuracy, recall as well as F score,  but we cannot have them all. In this specific terrorist case, I believe we should prefer high recall and F score, while lower accuracy is acceptable.  I havent seen a case where lower F score (which means both accuracy and recall are low) is desired though. According to [Wikipedia](https://en.wikipedia.org/wiki/F-score#F%CE%B2_score), recall is considered $$\beta$$ times as important as precision. Therefore, if recall is more important (as it is here), we want a high value of $$\beta$$.
https://campuswire.com/c/G984118D3/feed/769 Question on paradigmatic and syntagmatic relations Hello,  In the book, Chapter 13.2, page 281, and also in the slides it is mentioned that  > This also shows some interesting connections between the discovery of syntagmatic relations and paradigmatic relations. Specifically, words that are paradigmatically related tend to have a syntagmatic relation with **the same word**.   I am quite confused on comprehending the relationship between paradigmatic and syntagmatic. Especially in the last sentence. What exactly do the words **"the same word"** reference to?  Would any one have a good example that helps explaining this relationship please? Thank youExample: 'Car' and 'Bus' have a paradigmatic relationship because they belong to the same class (they are both vehicles) and can occur at the same locations in a sentence. 'Car' has a syntagmatic relationship with 'Drive' as they are co-occurring elements.  'Bus' has a syntagmatic relationship with 'Drive' as they are also co-occurring elements. So two words that have a paradigmatic relationship ('Car' and 'Bus') have a syntagmatic relationship with the same word ('Drive'). Oh I see it now, what a great example. Thank you!
https://campuswire.com/c/G984118D3/feed/555 Question about section 5.2 In section 5.2 slides page 7, it mentioned that Rocchio method can be used for relevance feedback and pseudo feedback. May I know why this method cannot be used in implicit feedback? Screen Shot 2022-09-21 at 9.07.51 ![Screen%20Shot%202022-09-21%20at%209.07.51%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/44c411a5-63d4-43f3-afc9-763bdccfcb1d/Screen%20Shot%202022-09-21%20at%209.07.51%20PM.png) My assumption is that, the tags coming from implicit feedback is not very trustworthy. For example, some non-relevant docs still get clicked just because it is one of those top ranked result (maybe this can be solved by using long click through rate instead of click through rate); while some relevant docs doesn't get clicked just because they are ranked to the bottom of the result and user already find what they need. But the pseudo feedback also has the same problem of inaccuracy of tag. So I am quite curious about why Rocchio is not suitable for implicit feedback.  By the way, I feel that to improve the performance of retrieval engine depends on not only the algorithm to incorporating the feedback information, but also the signal quality of feedback collection mechanism, given that the judgement might not be reliable in pseudo/ implicit feedback but relevance feedback is a luxury to have in most case.  So in practice, is there any way to tell us which part is the major blocker of the retrieval engine performance? Thus we can decide what should be our next step, we need to try some new algorithm? or we actually need to pay some money to the human raters in order to improve the quality of feedback data? Thanks.I think your intuition is correct. In the case of relevance feedback, we obviously have the user judgements. In the case of pseudorelevance feedback, we only take into account the top $$k$$ ranked results, with the assumption that the top-ranked results are likely relevant (and if not, perhaps they have some degree of similarity at the very least) - our relevance analysis here is bound to a subset of the top-ranked results. In the case of implicit feedback, there are no such "reasonable" constraints to make such an assumption - perhaps the user decided to click on only the lowest ranked results for some reason, in which case the feedback would be incorrectly interpreted.  Also, recall that from the Rocchio discussion in section 5.2, non-relevant documents tend to be positionally scattered (when considering the documents in a 2D space) as opposed to relevant documents which tend to be positionally clustered, meaning that a few negative examples do not have an overly significant weight in influencing the updated query vector away from the desired position. thanks for your reply. I noticed that in quiz 5, it mentioned the least reliable method is pseudo feedback.. so I am not sure if it is really just because of implicit feedback is not reliable.... Screen Shot 2022-09-21 at 9.03.32 PM![Screen%20Shot%202022-09-21%20at%209.03.32%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/32fcb5ab-31c3-44df-90cf-217eba26dabb/Screen%20Shot%202022-09-21%20at%209.03.32%20PM.png) Hello Yu, I think the feedback mechanisms can be understood this way: - Relevance feedback is the most reliable, because users passed judgment on the documents themselves. In other words, this method has the greatest input from actual users. - Implicit feedback is the next most reliable, because user click-throughs indicate which documents "look" reliable in the search results. So there is some user input, but only on the headline or snippet presented by the search engine. - Pseudo-feedback is the least reliable, because there is no user interaction at all. It's a kind of circular logic: the search engine says the top k documents are the most relevant, therefore those documents must be the most relevant.  Implicit feedback is actually more reliable than pseudo.
https://campuswire.com/c/G984118D3/feed/1037 What type of demo is expected to be given for final project? Hi TA and Prof,  We have a question regarding the requirement of the final project. The [doc](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit#) says we are required to provide a "demo". What is the types of the demo is expected? Does it need to be an interface like an interactable webpage, or just a demonstration that the code can run (like training codes or data generation codes)?  Thanks!The demo requirements depend on the project topic. For example, if you propose a new ML model, then the demo might be some basic inference on a small test set. If you propose a Chrome extension, then the demo may be you screen-recording how a user would walk through its use. 
https://campuswire.com/c/G984118D3/feed/1126 Proposal Length How long is our proposal supposed to be? A single page? Single or double spaced?There is no length or format requirement - just take as much space as necessary to answer the required questions. But a good rule of thumb is that it should probably be longer than a single page, as that probably means you have included enough information to answer everything.  The requirement lets us to write roughly one page and it give us some questions to answer.
https://campuswire.com/c/G984118D3/feed/1311 Dropped Status in Quiz 3 and Quiz 4 Hello,  Does anyone know why the status of Quiz 3 and 4 is "Dropped"? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6d91351f-fc3b-48ea-80bc-68b55baa3ab5/image.png)  I remember they were in "Graded" before, am I the only one or do you have the same status?  Thanks!Only the top 10 quiz scores are graded, so your lowest two scores are dropped. For me it shows as graded. Oh I see, thanks for explanation!
https://campuswire.com/c/G984118D3/feed/969 Quiz 4 Problem can someone help to explain this question ? From my understanding, the purpose of smoothing is ensure a unseen word will have a non-zero probability. ![%E6%88%AA%E5%B1%8F2022-10-15%20%E4%B8%8B%E5%8D%881.00.35.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a289443b-7324-412a-a75a-5265f14c00d1/%E6%88%AA%E5%B1%8F2022-10-15%20%E4%B8%8B%E5%8D%881.00.35.png)You are correct: smoothing prevents unseen words from having zero probability. Because if p(w|d) = 0, then log p(w|d) is undefined and therefore likelihood is undefined. (It's not clear how "undefined" and "minus infinity" are equivalent, but all answer choices say the same. So we only need to consider the "if a term" piece of the answer choices.) Thanks! The asymptotic behavior is that is tends to negative infinity as the probability tends to 0, but they just saved some words, at the expense of being less mathematically precise. I don't think this is a course where that type of pedantry is of much value.
https://campuswire.com/c/G984118D3/feed/963 May I ask correct answer of Q2 #2? Unfortunately, I ended up solving problem #2 of quiz 2 wrong.  May I ask correct answer of Q2 #2?  The problem is,  Let w1, w2, and w3 represent three words in the dictionary of an inverted index. Suppose we have the following document frequency distribution: w1 : 1000 w2 : 100 w3 : 10  Question 2 If we enter the query Q= “w1 w2” then the minimum possible number of accumulators needed to score all the matching documents is:  Is correct answer 1000? (Because we need to check at least 1000 document for the w1 in query)  If not, could you explain a little bit about this problem?  Thank you!The video for [Lesson 2.6: System Implementation - Fast Search](https://www.coursera.org/learn/cs-410/lecture/QKK7y/lesson-2-6-system-implementation-fast-search), starting at 5:45 gives very good example of this. It's a two-term query, "info security", where "info" matches 5 different documents and "security" matches 3 different documents.  The example starts by allocating 5 accumulators (each accumulator is the running total of count of relevant terms for a given document), and then walking through the postings, updating a given accumulator if that document contains the term per the posting table. After the first term is fully accounted for, the example walks through the postings for the term (starting at timestamp 8:00). Notice that when both terms appear in the same document, the same accumulator is updated.  The worst case is if w1 and w2 don't appear in any of the same documents, because we would need to instantiate 1100 accumulators (one per document). Since we use one accumulator per document, if both terms appear in the same document, we can reuse the accumulator.  I hope this helps. There must be a separate accumulator for each document containing one or more of the query words.  Suppose each document contains only one of the query words. In other words, 1000 documents contain only w1 and do not contain w2, and 100 other documents contain only w2 and do not contain w1. Then we need the __maximum__ possible number of accumulators = 1000 + 100 = 1100. Now suppose that every document containing w2 also contains w1. In other words, 100 documents contain w2 and w1, and 900 more documents contain w1 only. Then we need the __minimum__ number of accumulators = max (1000,100) = 1000.  I hope this is helpful.
https://campuswire.com/c/G984118D3/feed/87 liveData Lab webhook link is not working http://livedatalab.centralus.cloudapp.azure.com/api/webhook/triggeIt should be http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger  This is not working for me. Maybe the endpoint is down? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bd71c60a-c120-4cf4-8ca3-bad018696b4e/image.png) Try deleting the webhook and add again. Can you share what error (hover over red triangle) you are getting and what your config is? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b5775758-fe55-40c7-b549-d4319ca2302b/image.png) This works for me! Thank you! Solved by deleting the existing one and re-adding it.
https://campuswire.com/c/G984118D3/feed/167 EducationalWeb The project topic I was considering seemed like it would a good extension to the EducationalWeb platform described in the "CS 410 Project Topics" document, but it appears to be down. Both http://educationalweb.web.illinois.edu/ (Error 403) and http://timan102.cs.illinois.edu/explanation/ (blank page) do not load anything useful.  Is this a platform that we are still able to extend or has it run its course and we should focus on smartmoocs instead?Thank you for your interest in the EducationalWeb! To access the webpage(s), you must be using the UIUC VPN, which can be downloaded using the instructions [here.](https://help.uillinois.edu/TDClient/42/UIUC/Requests/ServiceDet?ID=167)  The project was created and maintained by Bhavya (bhavya2@illinois.edu), who used to be a TA for CS410. If you (or anyone) is interested in learning more, please let me know and I will put you in contact with her! The timan102 server supporting this system is no longer in use. The EducationalWeb system is now available  at http://timan103.cs.illinois.edu/explanation/ Please **make sure to use Chrome browser to access it** (I believe that it only works with Chrome). You can also use the following link, which would be re-directed to timan103.cs...... .   https://educationalweb.web.illinois.edu  Bhavya is still interested in supervising any course project related to EducationalWeb. Please feel free to contact her at bhavya2@illinois.edu. 
https://campuswire.com/c/G984118D3/feed/461 Question about Reduce function How many times is the reduce funtion called? According to the first slide, it seems like there is only one reduce function to process all the KV pairs. But later it says each reduce function handles a unique key.   ![1.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5dc8e26c-97e8-4102-967a-5dd5f2f1a0ba/1.png)![2.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1c25e6b5-a333-4b38-99fd-e91025538823/2.png)A reducer is **usually** called once for each unique key. In the second slide, 3 different reducers are called and each is only called once. Depends on the resource manager, one reducer can also take more than one tasks. You can search more about MapReduce. Hope it helps! As already explained in the following post, the Reduce function is called once for each unique key. In general, there will be multiple unique key values, thus in our case, there will be many many instances of the Reduce function being executed simultaneously in parallel and on different machines. This is why we could speed up the indexing process.  Thanks for the detailed explanation. My question has been perfectly solved.
https://campuswire.com/c/G984118D3/feed/945 practice quiz9- Q10 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4e95ee36-1c68-4dff-8d5b-0dcbaff0664e/image.png) by checking the lecture note, I still have a hard time understanding the two options in the question.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e0505383-0da9-410f-a76a-fa2d19b9ac9a/image.png)The math in this slide obscures the question a bit, but the part that's relevant is really just the first bullet: "two unigram language models, the topic of d (and) the background topic"  This suggests that the mixture model refers to individual language models that are mixed between the two given **topics** - it says nothing about individual documents. I think the first choice (selected) should be correct. could anyone comment on the second option "topic are a mixture of words where the mixing weight depends not only on the topics but also the documents"? Why is wrong? I believe "mixing weight" means parameters such as λB (percentage of background words) and (1 - λB) that we see in the E-M equations. So documents have no direct influence on the mixing weight.
https://campuswire.com/c/G984118D3/feed/973 Practicing Quizzes for Exam Hi All,  I have created google forms for practicing the quizzes. I know it is too late for many people but I just created today for my practice. Thought it could help you practice without looking at the answers:  **Week1** - https://forms.gle/L16oskEGv29zqn3Q7 **Week2** https://forms.gle/u6dzxMKWyRWCH85cA **Week3** https://forms.gle/n5qcMR38QZSopv8a8 **Week4** https://forms.gle/re3avn4Vi8ZKffWc6 **Week5** https://forms.gle/SQ5Y4Sc4vnuaMpC6A **Week6** https://forms.gle/55UjA19ZYDQAqRtVA  **Disclaimer**: This is purely for helping you. If you are concerned about your answer are being collected etc...please do not use them. I do not intend to use your answer feedback for any of my purposes.  ThanksThank you for doing this.  Thanks for making the effort to create those useful practice questions! Thank you professor!
https://campuswire.com/c/G984118D3/feed/1331 still didn't see the score for proposal submission anything do i need to do? are we still waiting for the scores to be input onto coursera?I'm still waiting for my grade/meta-review as well. The TAs said they would give an update when the process is complete, so I suppose they're still working. In the meantime, I'm just moving forward with the proposed project since the peer reviews were positive - after all, the progress report is due in under two weeks... Same here. We need the TA's reviews before we can start the project. We are still working on grading all projects. In the interest of time, please move forward with your proposal. 
https://campuswire.com/c/G984118D3/feed/182 Livedatalab says course has no projects The course projects have disappeared from livedatalab  They were showing earlier but not sure what happened. How do I get them to reappear?   Anyone else experiencing this?   ![datalab1.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dd76f3c7-c387-40b7-8871-88fa1bc84290/datalab1.png)![datalab2.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/336218a9-a4f0-4a72-9e22-a83c24ef6995/datalab2.png)I am experiencing this as well. I do not currently see any projects under the CS 410 course on LiveDataLab. Same problem i'm experiencing the same problem. i was able to commit and push, with the submission appearing in LiveDataLab, but i could not see my score. same problem. I also couldn't see any projects under the page. I am experiencing the same problem. I am able to see it now. Please try again
https://campuswire.com/c/G984118D3/feed/23 How to get access to tech review sign up sheet with uiuc email？ Hi I'm trying to log in to the Google sheet for topic proposal of tech review but it always ask me to sign in with a Google email which doesn't have access to it, how can I log in with my uiuc's outlook email？It seems to open when I am logged in with my Illinois email. Can you try again? For those have the same issues( maybe just a problem for new students) I have figure it out  by doing these  https://cloud-dashboard.illinois.edu/   turn on your GoogleApp service, then open  g.illinois.edu   and select gmail, it'll ask you to verify your Illinois email and save your profile to Chrome. After doing this you'll be directly log in to Google sheet when you click those links Thanks! This instruction is really helpful. It also addresses my problem that I cannot log in my uiuc email through Outlook recently.
https://campuswire.com/c/G984118D3/feed/96 How to Compute IDF? Hi,   In lecture 2.1, we have been provided the formula to compute IDF as: $$ IDF\left ( W \right ) = log\left ( \frac{M + 1}{k} \right ) $$  In the example inside of the slide, it shows: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3b00f5f1-989d-4066-99c2-bec3bfc6adea/image.png)  Given we have a corpus: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f86e1698-09b9-4e8d-9fff-2a98be2ab953/image.png)  Should the IDF for the word `news` be: $$ IDF\left ( \mathrm{"news"} \right ) = log\left ( \frac{M + 1}{k} \right ) = log\left ( \frac{5 + 1}{5} \right ) = 0.263 $$ instead of `1.5` ?  Sorry if I misunderstand something.   ThanksFrom what I understand, the values given in the slides are just indicative examples. They're not actual calculations done on the documents. As far as your understanding of the function and calculation is concerned, I think that is on point. I see... thank you so much.
https://campuswire.com/c/G984118D3/feed/469 Lesson 4.2: Total N-1 Parameters in Unigram LM? Hi, in lesson 4.2, when professor Zhai talking about the number of the parameters of unigram LM, he said: "So now the model has precisely N parameters, where N is vocabulary size. We have one probability for each word, and all these probabilities must sum to 1. So strictly speaking, we actually have N-1 parameters."  I'm a bit confused about why we actually have N-1 parameters. Could someone explain it to me? Thanks!![1.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c2f27845-0a35-4c3f-949f-c197d22df3c0/1.png)when you know the probabilities for N-1 words, the probability for last word can be calculated by 1 minus the total probability for the first N-1 words. So the probability for last word would be known. thus there are N-1 pramaters. This is my understanding. In this model, there are N parameters and one constraint - sum of probability of all words is 1. You only need to determine N-1 parameters, and can calculate the Nth parameter. So the model and all parameters of it can be determined. In this case, we can say this model just have N-1 parameters. What others have answered is correct. You only need to find out N-1 probabilities. The Nth probability can be obtained by subtracting the sum of other probabilities from 1.
https://campuswire.com/c/G984118D3/feed/1059 SciPy for MP3 The readme suggests to use SciPy in MP3 but it seems the autograder doesn't have the SciPy module. Is it supposed to be this way? I didn't use the SciPy and passed the MP3, matrix operation should be enough. Same here, I too didn't use Scipy and was able to complete MP3  It may be easy to use it, but numpy matrix operations are also helpful. Agree, NumPy is enough. I didnt use Scipy and passed the grader...
https://campuswire.com/c/G984118D3/feed/688 MP2.3 Question Hello,  I'm not familiar with python, the instruction asks us to modify score_one() function, but the code calls for  ranker.score(), is score() function overwritten by score_one() ? What's the logic befind? Thanks.Hello, I think modify score_one function is your first step. Then, you need to modify the load_ranker function to call your InL2Ranker Class and function. Step 1 is to implement the algorithm by modifying this function:  ``` def score_one(self, sd):         """         You need to override this function to return a score for a single term.         For fields available in the score_data sd object,         @see https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html         """         return (self.param + sd.doc_term_count) / (self.param * sd.doc_unique_terms + sd.doc_size) ```  Step 2 is to replace the current metapy.index.JelinekMercer() with 'InL2Ranker(some_param=1.0)' as identified in the python comment:  ``` def load_ranker(cfg_file):     """     Use this function to return the Ranker object to evaluate, e.g. return InL2Ranker(some_param=1.0)      The parameter to this function, cfg_file, is the path to a     configuration file used to load the index. You can ignore this for MP2.     """     return metapy.index.JelinekMercer() ```  Hope this helps! I finished the assignment,  I just want to know why the program calls ranker.score() in main instead of ranker.score_one() ? It seems we call ranker.score() because score is an attribute of the ranker variable assigned when created using the metapy_index where score is a function defined under the ranker class (https://github.com/meta-toolkit/metapy/blob/master/src/metapy_index.cpp):  ``` #C++ of metapy     py::class_<index::ranker> rank_base{m_idx, "Ranker"};     rank_base         .def("score",              [](index::ranker& ranker, index::inverted_index& idx,                 const corpus::document& query, uint64_t num_results,                 const index::ranker::filter_function_type& filter) {                  return ranker.score(idx, query, num_results, filter);              },              "Scores the documents in the inverted index with respect to the "              "query using this ranker",              py::arg("idx"), py::arg("query"), py::arg("num_results") = 10,              py::arg("filter")              = std::function<bool(doc_id)>([](doc_id) { return true; }))         .def("score",              [](index::ranker& ranker, index::inverted_index& idx,                 std::unordered_map<std::string, double>& query,                 uint64_t num_results,                 const index::ranker::filter_function_type& filter) {                  return ranker.score(idx, query.begin(), query.end(),                                      num_results, filter); ________________________________________________________________ #Our Python use class InL2Ranker(metapy.index.RankingFunction):     """     Create a new ranking function in Python that can be used in MeTA.     """     def __init__(self, some_param=1.0):         self.param = some_param         # You *must* call the base class constructor here!         super(InL2Ranker, self).__init__()  def load_ranker(cfg_file):     """     Use this function to return the Ranker object to evaluate, e.g. return InL2Ranker(some_param=1.0)      The parameter to this function, cfg_file, is the path to a     configuration file used to load the index. You can ignore this for MP2.     """     return InL2Ranker(some_param=1.0)   ranker = load_ranker(cfg)  results = ranker.score(idx, query, top_k) ```  I wish I was a C/C++ wiz to provide a better answer but hope this helps!  My understanding is that class InL2Ranker is subclass of metapy.index.RankingFunction. The score_one( ) is override the super class'. And the score( ) is super class' function which calls score_one( ).
https://campuswire.com/c/G984118D3/feed/196 Problem With Webhook For context, I have already set-up the personal key to link the LiveDataLab with my Github account.   I get this: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5be6257e-7f82-4e7f-a1b5-c612bd27bc76/image.png) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0557744c-e3cf-45ea-8ca9-06daf5683cea/image.png)  I get a response 500, and deleting the webhook and reapplying it with the link the professor provided does not work (in other words, pinging back successfully with the green checkmark).  Here is what I tried to do with the webhook: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/87894cef-18e4-4b99-967d-bc56f6a03eb9/image.png)  Please help resolve this issue of mine. Thanks.I am having the same issue just now trying to make the webhook work. I tried all the tips I found on here but no luck so far.  I am having the same issue! I am now seeing an internal error when trying to deliver the payload too. It looks like none of the submission history on livedatalab loads anymore either. I am also having the same issue, I tried everything on campuswire - still not working. Same issue here as well. I tried deleting and adding the webhook but no luck. I could ping/push now at a less rush time (6:30am CST). Looks like Livedatalab issue is resolved either by itself or some TAs.
https://campuswire.com/c/G984118D3/feed/179 Cant Import metapy Edit: Follow #28 would solve the problem even for windows10.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1b6e212e-176e-4b9b-ab32-768a8b279686/image.png)  **pip install metapy pytoml** seems only produce **Collecting metapy**.   I am trying to install metapy using windows cmd.It looks to me that **metapy** package is not correctly installed in your machine. Can you simply type *metapy* inside your python window and see if it returns you the path where your package is installed (Ideally, it should be inside Python 3.5.2 directory for you)?   Snapshot result would look something like this: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0bfd5aa0-d3af-4315-8516-77bda411b53a/image.png) I tried, metapy is simply not installed. I gave up the windows cmd. I tried anaconda and followed the pinned instruction #28 and the instruction for mac also works for windows10. Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153 Did you just copy paste the .py file into a new .pynb file? Yes I just copied it into a cell of the ipynb file
https://campuswire.com/c/G984118D3/feed/1161 MP3 deadline Can someone confirm if the MP3 deadline has been extended to Oct 26? On coursera it still says Oct 23 (today). Thanks!Please refer #1067 Thank you! The deadline has been extended to Oct 26 Thanks!
https://campuswire.com/c/G984118D3/feed/498 I only see MP1 and MP2.3 on LiveDataLab? As the title says. I cannot find MP2.1 or MP2.2.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/15f3c2dd-fa7f-4c22-b7d3-7c78da0cd625/image.png)MP2.1 and 2.2 should be done outside LiveDataLab. It should be done through http://timan.cs.illinois.edu:4000/ under Illinois.edu VPN. Files are attached for your reference.  [CS410%20Digital%20Library%20Instructions.pdf](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e4c4af66-fbf9-44bd-b788-3149611b1829/CS410%20Digital%20Library%20Instructions.pdf) [Overview%20of%20CS410%20MP2%20and%20Instructions%20for%20MP2.1.pdf](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b8620f40-26f5-40d6-b9c1-20ccc49f5373/Overview%20of%20CS410%20MP2%20and%20Instructions%20for%20MP2.1.pdf) Thank you!!
https://campuswire.com/c/G984118D3/feed/358 Is there a way to play the coursera video at a timestamp Hi,  While tagging the coursera videos, I was wondering if there is a way to have a URL which can play the video starting at the particular given timestamp, similar to youtube?   Does coursera support it?I think the suggested way would be to simply highlight the text from the transcript You can highlight the text from the video script and click on Save. You will be able to add notes and save notes at this point along with the video at that point in time. The next time when you click on see "all notes" you should be able to view them in the order you saved. And will also be able to play the video from that time stamp. Hope it helps.
https://campuswire.com/c/G984118D3/feed/109 Not sure whats wrong with the livedatalab I have pushed my code to GitHub; however, on livedatalab I just see this... I am really confused about what went wrong... I don't really know how to test my code either. I would really appreciate someone's help here. I have attached the screenshots below ![Screen%20Shot%202022-08-26%20at%202.54.45%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e78f5bc1-d064-4ea9-bef8-400667fff9dc/Screen%20Shot%202022-08-26%20at%202.54.45%20PM.png)![Screen%20Shot%202022-08-26%20at%202.54.49%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5d8e8ea1-c172-4e48-bb22-554ec254304e/Screen%20Shot%202022-08-26%20at%202.54.49%20PM.png)Have you set up the webhook for your github account and linked your github with on livedatalab? It seems the grader is not able to access your code. I did add the webhook in my github but it still says FAILURE Are you sure you have set up your github account with the right access token? Can you try relinking your github and then resubmitting? Do you see a checkmark similar to the screenshot for your webhook set-up?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e645201e-8d37-490e-86bd-92f5b296e0d0/image.png) yes, I do see the checkmark. It still says "FAILURE". Same message as seen in the screenshot in the original post. did you commit all files you changed in MP1 folder? example.py , config.toml, etc ? maybe it can't find one of the files? I made sure that everything was added correctly... I still can't figure this out. I made multiple attempts trying to relink the GitHub on the livedatalab and tried the webhook steps again... still... nothing works. It still seems like the grader cannot access your code. Are you sure you have used the correct PAT key when linking the github?  I got it to work. Just had to restart everything and regenerate a new PAT key. Thank you all! wait... do we need a config.toml to finish this MP? I did not use but the code still ran without error on my vscode. Hi im having this same issue. Did you press delete linked accounts and then add a new account on livedatalab?
https://campuswire.com/c/G984118D3/feed/656 livedatalab issue I am trying to submit my code in github.com (with code commit) but somehow it doesnot show any changes in livedatalab. So I regenerated github token and added it in. As I tried submitted again and it didn't work I deleted existing links and recreated the link. It still seems not working. And, I don't know if it will impact my MP1 submission (hope not). Any suggestion? Thanks.MP1 is already graded, and it would only use your highest grade so it won't be impacted. Is the problem solved? I have the same problem here It's resolved. To make sure, I also updated MP1 code and committed it. So it triggered the run in livedatalab again and it shows success in MP1.
https://campuswire.com/c/G984118D3/feed/475 MP2.3- Stuck at Running queries Hello all,   I am trying to run my code and it seems like it got stuck at 'ranker.score(idx, query, top_k)'. Any help would be appreciated!   ![Screen%20Shot%202022-09-15%20at%209.28.15%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bf928364-a226-4a30-b63e-0e9b25bbcb04/Screen%20Shot%202022-09-15%20at%209.28.15%20PM.png)  Thank you.See #478 Thanks
https://campuswire.com/c/G984118D3/feed/157 Projects tab from LivedataLab  is missing HI, I am trying to access livedatalab from coursera, it does not show me "projects" (or any other tabs) to select MP1.  Not sure what is wrong in my setup. Please let me know how to fix this issue ?  I get an error indicating user already enrolled if I click on following link. I am not sure if  this is an issue.   http://livedatalab.centralus.cloudapp.azure.com/course/join/WZS77P00E570J74 I don't see project tab when I login to livedatalab.  ![Screenshot%20%2822%29.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4aac55a6-0b5e-4f47-a473-1ef09eb1b704/Screenshot%20%2822%29.png) Thanks.Since you are already enrolled, the above link will give you an "already enrolled" message. Can you try using this link and see if you see the projects in the menu choices (http://livedatalab.centralus.cloudapp.azure.com/)? Make sure you are logged in using your Illinois Id and see a Log out option on the top right corner when using the above link.  Also, sometimes deleting browser cookies or using another browser solves the problem when the page doesn't open correctly, worth giving a try.  Hope this helps. Thanks for the hints.   I expanded my browser window and now  I am able to see logout and all other navigation buttons (including projects) !  Hey Ayush, I am having the same problem here. After clicking the link you provided, here is my screen shot. Plus it appears my coursera account is not able to receive the auto grader result from livedatalab yet. Many thanks for the kind help.  ![A8FCFFF8-4BAF-4C55-B31D-C237FAFA09BC.jpeg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/36bf4d9e-9d98-415f-8337-4d02f60e2e74/A8FCFFF8-4BAF-4C55-B31D-C237FAFA09BC.jpeg) Can you please try resubmitting it after opening the Livdatalab page using Coursera 'Open Tool' option? If you directly opened the Livdatalab page and did the Git commit/push, it could be that your connection is not made to Coursera and that's why you don't see the grade there. More details are provided in #155. Hope this helps.
https://campuswire.com/c/G984118D3/feed/542 LM smoothing Hello, I am trying to understand the smoothing process of a LM.  First we have **(slide 1)**: ![3.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/618596b7-3fbb-45e4-82aa-751001c0c8ea/3.PNG) To smooth this LM we can do this **(slide 2)**: ![1.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e3d201f2-f496-43d9-98a5-4af4f757ff5d/1.PNG) As a result, we have the ranking function. And now we also need to smooth the P_seen(w|d) and for example below is using the JM smoothing **(slide 3)**: ![2.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cf7a18de-60d7-48a9-9bdd-fead15574c32/2.PNG)  Questions:  1. Is the smoothing in slide 2 also JM smoothing? 2. What is "**Discounted**" ML estimate in slide 2? 3. Why there are two smoothing in this process? Why can't we just smooth only once? 4. How do we get this equation below: ![Capture.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c8ad47e4-c3ef-4cdf-b833-676fe09a4ea3/Capture.PNG)  I was revisiting the slides for the midterm exam and I found these really confused me. Please advise, thank you very much! (1) The smoothing in slide 2 is the general intuition/idea behind smoothing, not a specific smoothing method. This idea is used in both the smoothing methods that we discuss, where $$\alpha_{d}$$ is the smoothing parameter. In case of JM smoothing, the value of $$\alpha_{d}$$ will be $$\lambda$$. (2) It is called "Discounted" MLE, because we are not going to use the actual MLE, rather, we are going to deduct some amount from the MLE in order to give some weight to $$p(w|C)$$. (3) I did not understand this question - could you please clarify about where you see smoothing being done twice? (4) The numerator -  $$p_{seen}(w|d)$$, which is $$(1-\lambda)p_{ML}(w|d) + \lambda p(w|C)$$ (p_{ML}(w|d) is nothing but the maximum likelihood estimate). The denominator $$\alpha_{d}p(w|C)$$ for JM smoothing is $$\lambda p(w|C)$$ because $$\alpha_{d} = \lambda$$ for JM smoothing. Thank you so mush! Your answers clear all my confusion. For question (3) now I understand: only one smoothing has been done. What we are doing in slide 3 is just rewrite the P_seen(w|d).
https://campuswire.com/c/G984118D3/feed/881 can't have the exam on coursera hey i'v scheduled the exam using the link from the TA, and followed through the registration process, but when the proctor input the pass word on the "proctorU password" session, the page always responded by saying "0%" is correct, and then i can't proceed to get my exam started...Angela, I'm really sorry to hear that. The same thing happened to me this morning. Assma very kindly confirmed that ProctorU has been given the correct password for this exam. I suspect, based on my experience with ProctorU over the last five hours, that an incorrect copy of that password was loaded into ProctorU's customer service system, as only one person at ProctorU was able to get it working. David hey I managed to solve the issue - please ask for the help of Douglas.G - seems he is the guy can crack this issue. hey I hope you finished the exam ... it's really annoying. i actually think it's because when the password is correct, system didn't assign a valid number other than 0 to the credit/score, so the next page reject the validation as it's looking for the confirmation from the previous page. Seems like it worked for me so it might be them giving incorrect passwords. It's definitely an issue with the proctors either having a bad password or typing a good password incorrectly. I'd like to think that the "adventures" of this morning would inspire the ProctorU folks to figure this out. Although, for both me and Angela, Douglas G has been the only person who can get it working.  Thank you both very much. I just finished my exam and the initial password still wasn't fixed, but the fix was relatively easy since I knew what to ask. You saved me (and my proctor) so much time! I am facing the same issue for half an hour. My proctor is still struggling with this. I hope someone can help me... Just tell her/him to ask Douglas.G - that’s their coworker. I asked for his name in case other ppl need his help  Thanks,I did ask, but it is still not solved. I will ask the proctor again perhaps he is off the work? just a guess. Do the TAs recommend taking this exam on weekdays? Considering that the ProctorU folks have to reach out to others for potential issues. Just asking since this is the first time I am using this service and I am scheduled to take the test on Sunday. New slots for the weekend opened apparently! Checkout proctorU again  i have used procu service before, this is the first time i ran into an issue. I don't think there is a difference.
https://campuswire.com/c/G984118D3/feed/354 What should we do on LiveDataLab about MP 2.1 I see there is a project here on LiveDataLab about MP 2.1, I would like to confirm if we don't have to do anything about it there? Feeling strange because I saw 3 students started on that one.   I understand we just have to use the extension to collect 15 entries which can be checked on this website: http://timan.cs.illinois.edu:4000/.  Please correct me if I am wrong.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d5bb4bc3-975b-427e-ac4c-980114d1d537/image.png)You are correct - there is no need to use LiveDataLab for MP2.1 or MP2.2. If you can see the 15 entries on the linked website, then you have completed MP2.1. 
https://campuswire.com/c/G984118D3/feed/188 Cannot open the link for final project Hello,  I am trying to access this link:https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit?usp=sharing and it always require authorization. How to access to this link?Logging using illinois id to access the doc works for me How did you login Google docs with Illinois id?  Here's the instructions for setting up google apps via UIUC email  https://help.uillinois.edu/TDClient/42/UIUC/Requests/ServiceDet?ID=135 Thank you! works right now.
https://campuswire.com/c/G984118D3/feed/988 MP3 Clarifications I have some questions about this MP.  1. Do we have to deal with synonyms, stemming, capitalization, or stopword/punctuation removal in any way?  2. Should we sort the stored vocabulary alphabetically? Can we? I think I heard a TA say that we should do it, but I am not sure.  3. All logs should be base-2, right?  4. What does calculate_likelihood() do? Calculate $$\log{P(C|\Lambda)}$$? Does that equal $$\sum_{d \in C} \sum_{w \in V} (c(w,d) * \log{\sum_{j \in topics}}P(topic = j | doc = d) * P(word = w | topic = j))$$ and append it to self.likelihoods?  5. When should calculate_likelihood() be called? At the start of each iteration of the loop in plsa()? At the end of each iteration of that loop?  6. What is to be done with self.likelihoods? Do we stop when the amount by which it increases falls below some threshold (epsilon?), or is it just an output of our algorithm?  7. Are we supposed to do anything using the labels on the first 100 documents? Should they just be discarded?  8. expectation_step() computes self.topic_prob, correct? self.topic_prob[d, j, w] equals the probability that word w in document d is generated from topic j, right?  9. Are there really only 6 unique words and 2 topics in test.txt? Will there be more topics and words in the collection of documents we are getting evaluated on? The documents we were provided look very unnatural.   10. How is our code to be evaluated? Will you check self.document_topic_prob and  self.topic_word_prob? Will self.likelihoods be checked? Will runtimes be checked? How tight is the runtime limit?   Sorry for asking so many questions, but this assignment seems fairly complicated. Thank you for your help.Hi, in most cases the instructions in the code comments should be enough to guide your programming. For your questions: 1. No. 2. No. 3. The default one, "np.log", should be good.  4. Yes, you are required to follow this fomula.  5. At the end of each iteration. Follow E-M-Likelihood in each iteration.  6. Just append each likelihood to it. You can observe whether the algorithm converges by printing it. Epsilon is used to compare with the absolute difference of the continuous two likelihoods.  7. No, ignore them. 8. Yes. 9. In test.txt it is the case. You are required to calculate them instead of hard coding them. It is unnatural indeed.  10. It is to compare the results of your codes with the results of our codes. You should get all parts in the comments finished.   Hope this will help! Thank you for taking the time to answer my questions?  I am still a bit confused about how our code should be run.  Is this the correct structure for the loop in Corpus.plsa()?  ``` for iteration in range(max_iter):         # Do E-step         # Do M-step         # Compute log-likelihood and append it to the list ```  Do we always go through all "max_iter" iterations? Should I have something along the lines of:  ``` if (self.likelihoods[len(self.likelihoods) - 1] - self.likelihoods[len(self.likelihoods) - 2]) < epsilon:         break ```  Thank you!  Edit: Also, is it allowed to create a num_topics field within the Corpus class? This can be calculated on the fly from the size of document_topic_prob or topic_word_prob, but it would be convenient.  Edit: I had a num_topics field in the Corpus class, and added a condition to stop if the log-likelihoods decreased by less than epsilon. I also made the log-likelihoods start at negative infinity instead of zero. By doing this, my code passed the autograder. Yes, it's correct. The stop condition may be either 1) the abs diff < epsilon or 2) reach the max_iter. Thank you. Hi Yuxiang, I find that the printout of my vocabulary contains words like "\tchicago", "\n", "1\tseattle". We have to remove these characters right? The vocabulary size should be 6 after removing them, is that correct? Yes, you may consider to use .split() to remove "\t" and "\n".
https://campuswire.com/c/G984118D3/feed/713 Lesson 4.5 Equation Question Hi, can someone please clarify what the value of n represents in the equation? My interpretation was that it represented the # of terms that were in both the doc and the query. ![Screen%20Shot%202022-09-26%20at%205.54.19%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/37e49eb1-f91f-4adb-b6c5-82cc2fab162e/Screen%20Shot%202022-09-26%20at%205.54.19%20PM.png)I remember n = |Q|, query length  Got it, thank you!
https://campuswire.com/c/G984118D3/feed/722 Exam1 Hi, What's the time frame that we can take the exam? Is it Oct16-Oct22? Is it Oct10-Oct16?https://campuswire.com/c/G984118D3/feed/717 some one said here we should pause until the TAs release more information. Oct 10th - 16th I guess it is the 10th -16th. You can refer to the official page. Oct 10th - 16th. You have to sign up for a time through ProctorU
https://campuswire.com/c/G984118D3/feed/1136 Facing LiveDataLab issue - Submission not RUNNING Hi TAs,  I forgot my password for the livedatalab and I registered again. After following the github linking and course registration, I submitted the assignment without implementation ( just the skeleton code) to check the integration and I am seeing FAILURE.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f4172ae6-451f-48e6-98ef-4526cf0816bd/image.png)If you go to your repository > settings > webhooks > click on webhook, do you see something like this? I am wondering if the deliveries and being successful (green checkmark)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/299611c2-7528-48fd-bf20-03b55f2a04f9/image.png) Yes. The delivery status is OK 200.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1a182ac1-1107-4a54-91a7-b643677bc27f/image.png) Seem like livedatalab is not able to pull your code. This could be because your entered github username and API key are incorrect. Can you try relinking your github account?  Thank you. The API Key was wrong.
https://campuswire.com/c/G984118D3/feed/328 Build success! but Score is 0 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4690bbe8-c79b-4ee4-a041-feda0db2c8ae/image.png)  However from the leader board, the score is 0. Why is that?I believe this means you did not write the code correctly, make sure you define tok and trigrams and use the instructions to tokenized, lowercase, length filter and stemming step by step I went through the exact requirement in the MP1 and realized I did some steps not exactly what it is asking. So that's why I get a score of 0. Now, I have fixed that and got 1. Thanks everyone!
https://campuswire.com/c/G984118D3/feed/793 Exam registration Now that there is a week left till the beginning of the exam period, I am still confused on the registration process. I heard some people who have registered, but have seen other comments to wait for formal instructions from a TA. In the past I've seen TAs email links to students to specifically register for exams. I'm seeing conflicting information. Are we supposed to self register or are the TAs in the process of sending out formal instructions?Please go ahead and register yourself. I know many did. While in ProctorU, please select CS 410 - DSO - Exam 1 -  Fall22. Cost is $16 I believe we are still in the process of sending out the formal instructions. I will let you know when the instructor provides me more information. Please wait for official information to be released before registering or paying any fees. An update: You should receive official information tonight or tomorrow. If you have already registered, I think it should be fine. But the official information is yet to be sent out. Ok thank you for the information. I shall wait then Please take a look at the pinned post #805 
https://campuswire.com/c/G984118D3/feed/939 Exam 1 can’t test equipment  ![image2022-10-13%2019%3A53%3A24.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2616319f-af66-47fd-ac37-6e8bcac10351/image2022-10-13%2019%3A53%3A24.jpg)I got an error message,and I can’t test my equipment Zong-Yun, I do not see a reason for errors in your screen photo. Can you scroll down to see if there is any additional information? Also, are you using Chrome and have you installed the ProctorU extension?  ￼I open the camera and microphone,then refresh the website, but nothing happened  try incognito mode .. if it doesnt work ,you need to provide access to microphoen and camera Make sure you have the proctorU chrome plug in installed. Also, make sure you are logged into your UIUC google acount
https://campuswire.com/c/G984118D3/feed/1248 document topic probability converging to 50% I don't really want to post my code so I'm hoping someone ran into this. I thought I was understanding the e and  m steps but after finishing those I printed out my document topic prob matrix to compare against some of the first couple rows since those have know topics.  All of my topic probabilities slowly converge to 50% for each topic on all documents. Clearly I missed something.I believe this is happening because I was trying to use the normalize function provided in the e step and ended up normalizing across vocab instead of topic. How do I create my own normalize? My attempts failed so far.
https://campuswire.com/c/G984118D3/feed/643 Submission failed with no logs I pushed my code and these are the results I see so Im not sure why my code is failing. Its looked like this for over an hour. I put my p value in significance.txt and made the appropriate changes in score_one() and load_ranker(). What could be the issue?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6a02776b-4dbc-430a-b1eb-7af027b4c500/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e4a1a08e-7dbb-4c69-b596-358609734236/image.png)I had the same problem before. I followed the first answer in #560 and solved the issue. Just delete the linked account in LiveDataLab, go to github to generate a new personal token, go back to LiveDataLab to link the new account again, then git push again.
https://campuswire.com/c/G984118D3/feed/421 Campuswire Class Code I have a friend who joined late trying to get access to this class campuswire, is there a join code anywhere?I think they have to reach out to the TA or the Professor to have them added. There isn't a specific code to join the group When I wasn't added to a particular course in Campuswire, reaching out to MCS Support also helped me. But a TA added me in that case, so Syed's answer may be a shortcut Yup, I got course access late and promptly got campuswire access after emailing TA haritar2@illinois.edu Please ask your friend to email me (czhai@illinois.edu) and I'll add him/her to Campuswire. 
https://campuswire.com/c/G984118D3/feed/355 500 Internal Server Error I'm getting a 500 Internal Server Error response on the webhook for MP1. I saw other posts from students that had this issue. I removed the webhook and created a new one. The webhook url I'm using is: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger  I also deleted the linked GitHub account and recreated the link. and I get the response: Headers: Content-Length: 290 Content-Type: text/html Date: Mon, 05 Sep 2022 20:49:17 GMT Server: nginx/1.23.1 Body: <!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN"> <title>500 Internal Server Error</title> <h1>Internal Server Error</h1> <p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>  It doesn't seem like there's a clear fix to this issue. Is there anything that can be done to fix this issue?  Thanks
https://campuswire.com/c/G984118D3/feed/1001 a higher gMAP and a higher MAP  If X has a higher gMAP, can X also have a higher MAP? Or a higher MAP and also a higher gMAP?Depends on the input values (query scores). gMAP is more affected by small values than MAP. Try making up a few sets of input values, then calculate (in a spreadsheet, for example) the values of MAP and gMAP for each. gMAP and MAP are not related to each other, meaning, a list with a higher MAP can be with higher or lower gMAP. The averages are only the properties of the numbers involved and their distribution.
https://campuswire.com/c/G984118D3/feed/1309 Project Proposal Reviews Now Available Hi everyone,  I've just made the submitted reviews available to each respective group. Thank you to those who have submitted their reviews on time. For the others, please try to submit them as soon as possible, as your comments will provide students with valuable feedback on their proposals.   In addition, the meta-reviews + grades are being entered into Coursera by the course staff. Not all proposals have been graded yet / have meta-reviews, but I'll send an update once everything has been completed.  The next deadline is the progress report, due Nov 14th. For the peer review after this deadline, you'll simply use the same peer review form, but answer the questions marked [PROGRESS]. The progress report is graded based off of completion, asking that your group clearly addresses the following points: 1) Progress made thus far, 2) Remaining tasks 3) Any challenges/issues being faced.  Thanks! -Kevin  
https://campuswire.com/c/G984118D3/feed/311 Additional Issues with MP1 grades on Coursera Hello,  I have been able to submit my lab on LiveDataLab and verify the submission and leaderboard, but the connection to Coursera isn't working. I have clicked on the OpenTool button on the MP1 page on Coursera and resubmitted following the advice on #155 and #249, but that hasn't worked. What else should I try?hi!  I just checked your leaderboard score and updated your Coursera MP1 grade manually. Hopefully this issue doesn't persist for future MPs
https://campuswire.com/c/G984118D3/feed/113 MP1_Pre-trained POS-tagger model  Hi Guys, in the README file in the assignment 1, it mentioned we have to download the pre-trained POS-tagger model to see certain demonstrations.   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a88ad7c3-6e13-4400-8e13-c262df73fbd3/image.png)   I have downloaded the file, but not sure how to incorporate the file into the code?   tagger = metapy.sequence.PerceptronTagger("perceptron-tagger/")  This code is causing some confusion .. appreciate the help in advance I am assuming you got some kind of error message. When you run the code, it should download greedy-perceptron-tagger.tar.gz to your current directory and unzip the file there. If not, you can always do it manually. You just need to make sure the perceptron-tagger folder is in the same location as your script.  ![Capture.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/798873c2-fcff-4a8f-bf48-6f1c72401e48/Capture.PNG) You need to download the zip file and unzip it to your python working directory first. To find out where your working directory is, you can do the following:  ``` import os os.getcwd() ```
https://campuswire.com/c/G984118D3/feed/58 Week 1 Practice Quiz - Pull vs Push About the question from week 1 practice quiz:  Select the following applications that provide push information access.  I selected Bing, because when I go to bing.com, I see a search bar on the page, but below that I see a list of popular articles, top stories in my location, weather, local traffic, etc.  My answer was marked incorrect.  Wouldn't those items be considered recommendations based on my location and interests? Why would this not be considered push information access?  Thank you!I also had the same question when I was doing the quiz. They way I understand those Bing "pushes" is that they are more like **trending stroies and news** targeting all the users in the network (maybe your nearby network or the whole internet) but are not necessarily based on your personal interests. In other words, those breaking news and wehther information are usually considered interesting for alomost all the users. Thank you for the comment.  Here's an article describing how bing.com pushes results based on your personal interests. This is probably gathered from your browsing history and previous searches.  https://www.searchenginejournal.com/bing-introduces-personalized-image-video-feeds/204156/  I'd like to hear from one of the TAs on whether this is considered pull or push, and whether the quiz should be updated based on this information. ![Screen%20Shot%202022-08-24%20at%2010.24.44%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/fd42bdee-e823-4e83-a088-498d80f31338/Screen%20Shot%202022-08-24%20at%2010.24.44%20AM.png) The way I understand it is how we look at Bing and Google as a general search engine tool. They belong to the search engine category therefore the main purpose of them is to provide result based on users input. Recommendation articles and push system are just their side-features, but the main purpose is user init. search queries and it belongs to pull mode. Hope this helps.  Thank you, that information really helps. I think when they mention Bing, they only consider its searching engine part. In general how are we supposed to infer meaning from questions like this?  In general how are we supposed to infer meaning from imprecise questions like this? Thank you To be honest, I will also chose the "wrong" answer in this case. It has to be answered by TAs. When the lectures and textbook talk about Google and Bing, they're always referring to the classic user query search engine functionality, i.e. pull mode, so I suppose that we're supposed to answer these questions in the context of the course material. It is definitely confusing given the wide range of services these companies provide today, though, I agree. I see the point of the intention of this question, but i would prefer a question with answer less debatable. not like I don't understand bing.com is a search engine, but I feel frustrated that I go one step further to check on actual application, and see they are opening recommendation window and trying to get as accurate as possible then instead losing my score :-( Hi Rick,  I had the same thought and selected all the four options since there appeared to be some ambiguity in terms of what we intend to go on the bing or google maps i.e. do we want to just search for a particular address?   However, if you give it a thought again, and try to correlate this question with respect to the concept being taught. The two options are essentially search engines and not recommendation systems. Even when we type in a particular type of food they will generate a list of options available near by but won't recommend this based on your preferences from the past where as the recommendation system stores some of your previous choices in order to push you the information.  To summarize, it goes back to the basic Who takes the initiative? 1. User - Then its pull (in case of maps the user takes the initiatives and not the system) 2. System - then it is push  I hope this helps.
https://campuswire.com/c/G984118D3/feed/112 MP1 - Docker If you don't want to install Conda, you can use docker.   dockerfile ``` FROM python:3.5  RUN pip install --no-cache-dir --upgrade pip && \     pip install --no-cache-dir metapy pytoml  WORKDIR /home/python/app  # infinite loop # ENTRYPOINT ["tail", "-f", "/dev/null"]  CMD ["python", "example.py"] ```  docker-compose.yml ``` version: "3.8" services:     python35:       build:         context: .         dockerfile: ./dockerfile       volumes:         - .:/home/python/app ```
https://campuswire.com/c/G984118D3/feed/340 CS 410 DL - code Hi,  I was wondering if we can access the implementation of this search engine. It will be a good learning and exploring the concepts learned so far and in the future. Thank you for your interest in CS410 DL! Most of the search functionality itself actually uses [Elasticsearch](https://en.wikipedia.org/wiki/Elasticsearch), an open-source search engine based on Lucene. The code itself doesn't actually implement a search engine, rather it communicates with a cluster of Elastic nodes. So, if you're interested in learning more, I'd recommend that you check out Elasticsearch! Thank you! Thank you! Thank you!
https://campuswire.com/c/G984118D3/feed/968 tech review score weight for 4-credit students I was wondering how much percentage is the tech review counted towards the total score? Thanks!I think it's just for completion where you need to have completed the tech review to pass the course, but it doesn't contribute to the final grade formula. Thanks!  Hi Qinjingwen, According to information in Week 1, tech review doesn't have a score weight. But not sure if TA will include them as the information in Week 1 is general for 3 and 4-credit students  https://www.coursera.org/learn/cs-410/supplement/1gMIe/syllabus
https://campuswire.com/c/G984118D3/feed/409 Does it make sense to use a complex Language Model instead of the Unigram LM? This question is mainly out of curiosity but directly related to Week 4.  One of the things that surprised me about the **Unigram LM** is that we don't give importance to the order of the words in the query. Basically, $$\text{p("Today is Friday")} = \text{p("Today Friday is")}$$. From the lectures, I understand that although this assumption isn't true, it is necessary to simplify the model. However, that would mean that documents with many key terms in any order would probably rank higher than real content documents, assuming they don't get too penalized by their length. - If this is true, how do search engines overcome this? - Is the simplification necessary to improve computation?   - Would it make sense to use something like **GPT-3** to replace **Unigram Query Likelihood**?  ThanksGoogle makes a pretty good case that a generalized n-gram model would be sufficient. See https://books.google.com/ngrams/. Based on the limitation that the n-gram need to show up in 40 books, they probably have a decent heuristic for how large to make n for this case. IMHO the more gram you used the more accurate the model is, at cost of computation resources; The large language model, on the other hand, revealed the relationship and similarity of the words at a very high dimensional space thus more accurate. This is an excellent question. The lack of consideration of order of words in the "bag of words" representation is a well known limitation of this kind of representation. As already pointed out in the following posts earlier, using n-grams is generally more accurate than using just unigrams. However, unigrams are often working reasonably well, especially with the use of the proximity heuristic, which prefers matching the query words in a small window of text. A main reason is when we match two words, X and Y, it's not so common that both "X Y" and "Y X", as a phrase, would make sense. That is, the order tends to be the "right" order that we need. For example, if X="stock", Y="price", then once we know we match both words, it's most likely that the matching content is about the "price of stock" or "stock price", making it unnecessarily to explicitly rank "stock price" on top of "price stock". If we use n-grams (e.g., "stock price" as a single term), it wouldn't (automatically) match a single word like "stock" or "price". This means if a document says "price of stock", it wouldn't match the phrase. If we use both "stock price" and "price" as indexing terms, it would also cause complication in assigning weights because we might over-reward matching the phrase "stock price" since it would not only match the phrase term "stock price" but also two single words "stock" and "price". However your idea of using GPT-3 to improve query likelihood is a great idea!  I encourage you to try it in MP2.4 or MP4 (which will be about neural ranking models). 
https://campuswire.com/c/G984118D3/feed/891 W2: what is uniform code? Hi, I am confused about the uniform code (week 2). Could anyone explain what the "uniform code" is? Thanks!  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bb3148dd-f4cd-4548-a1bd-e8ff70a7d3bc/image.png)For the Gamma code, as highlighted, you can read it as "binary code" Uniform code is equal length coding or binary coding. (The first bullet shown on the slide) Uniform code is just binary coding Uniform code is just another name for Binary Code. 
https://campuswire.com/c/G984118D3/feed/116 Practice Q9 measures affected by document length Hi folks, I wonder why the correct answer to Q9 is "L2 distance"? Isn't cosine similarity also calculated by the same vector? My explanation is that the value of L2 distance normally increases as the document size increases; is this correct?  Thanks!This helped me: https://www.baeldung.com/cs/euclidean-distance-vs-cosine-similarity With cosine similarity, you divide out the magnitudes (see #4).  This link is also a good reference for this question: https://stackoverflow.com/questions/19410270/vector-space-model-cosine-similarity-vs-euclidean-distance If calculating similarity using the normalized vectors, Euclidean distance and cosine similarity should yield the same result. If not normalized by length, then cosine deals with the length effect. It's easier to remember the difference by thinking about it this way: cosine looks at the angle while Euclidian looks at the distance.  The link provided below has a good solution to your answer  https://cmry.github.io/notes/euclidean-v-cosine
https://campuswire.com/c/G984118D3/feed/1154 number_of_topics how do we obtain number of topics for E-step, since the function doesn't give it as a argument like M-step did?You can get if from the shape of the document_topic_prob You can either store it as a class variable in the plsa function or extract from the shape of the document to topic probability matrix which u r using in the calculation. I changed the function signature.
https://campuswire.com/c/G984118D3/feed/571 Rank vs. Overall Score Hi, It looks like there are occasions where a submission receives a higher overall score but is ranked lower on the leaderboard: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/77335636-f4b2-46d4-9385-24bf97d04898/image.png) Seems like that rank is currently based on the first score (NDCG@10 on APNews).I think the first score column is our total score. This one is the score you earn on the faculty dataset.  --This was wrong! The last column is our final score. Thank you for answering. I thought about that too, but it looks like that the numbers don't go together with the formula "Overall Score" = 0.1* NDCG@10 on APNews + 0.3* NDCG@10 on Cranfield + 0.6* NDCG@10 on Faculty dataset. TAs just updated the livedatalab. I was wrong! The last column is the final score!
https://campuswire.com/c/G984118D3/feed/705 Lesson 5.7 I was looking back at week 5 lectures and have a doubt with the 'Equilibrium equation'. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7b4c8437-94c4-4bba-b46b-39b08ac0c362/image.png)  This is the formula to predict the probability that we would find page j at time t+1.   1. Doesn't the second term actually sum to '1/N'? The sum of all probabilities of finding any page from i=1 to N at time t is 1. The second term comes to be alpha times 1/N. Thus, the following formula where we remove time index will be much simpler. Is it not done only because it is easy to solve the equation by using the eigen pattern or is there some other non-mathematical reason? 2. When we say both terms loop i from 1 to N, implicitly it means we include i=j also. Which means, we are taking into account the case where a jump from page j leads to page j itself. Is it ok to consider this in the second term (random jump)? And, what meaning or use case does it solve in the real world? Isn't the primary purpose to cover unlinked pages?Below is my understanding but please let me know your thoughts.  1. Yes, the term sums to 1/N, and it makes sense. Because the probability of reaching a specific page through random jumping should be the same, since we have N page, the prob. of reaching one specific page should be 1/N.  Assume alpha = 1, it means we assume a page can only be reached via random jumping.   -  P<t+1>(dj) = prob. of at page dj at time t+1 - 1/N = prob. of jumping to page j (from page i) - P<t>(dj) = prob. of at page dj at time t  2. Since it is a random jump, it is possible the jump to a same page. The random jump will cover unlinked pages, but it does not necessarily mean we should exclude all linked pages (including a page itself) from the consideration set. I was thinking the exact same thing about the 2nd term, but would be interested to hear from an instructor. Conceptually, I thought of it as:  (alpha = the probability of choosing to jump to a random page) * (the probability of randomly jumping to dj = 1/N)  , if there are N nodes in the graph
https://campuswire.com/c/G984118D3/feed/439 MP 2.2 and MP2.3 Hi,  When will the descriptions for MP2.2 and MP2.3 get uploaded?  Currently for 2.2, when I searched using any query, there was no "Relevant" or "Not Relevant" button like what is shown in the screenshot on Coursera for MP2.2.  Thanks,  I am also waiting for the update.   > the UI will be updated mid-morning on 9/12  Let's wait for a moment. To see the update, you need to redownload the extension. The TA just post it. Hope it helps!
https://campuswire.com/c/G984118D3/feed/782 Don't commit the generated idx folder If anyone else is having trouble with MP2.4, make sure you aren't adding the idx folder! This is helpful! Thank you!
https://campuswire.com/c/G984118D3/feed/660 AttributeError: 'metapy.metapy.index.ScoreData' object has no attribute 'doc_tem_count' I have this error: "AttributeError: 'metapy.metapy.index.ScoreData' object has no attribute 'doc_tem_count'". Don't know if anybody experienced the same issue. Any suggestions are appreciated. Thanks.should be `doc_term_count`,  looks like you may be missing the `r` in `term` Thanks! I did missed it :)
https://campuswire.com/c/G984118D3/feed/138 Group Project: misinformation Hi,   I'm looking for teammates that are interested in developing a tool for misinformation identification or related topic- I'm personally interested in this topic. I have some background in social media and audience research as a data scientist. I live in U.S (EDT) and would prefer people with similar schedule (nights/weekends).   thanks~Jianci, identifying misinformation is a very interesting topic for me as well, and was among my reasons for applying to this program. I am in the Central time zone, so I'm sure we could find times to meet.  good - 1 hr difference won't be a problem. i will wait a little bit longer to see if we can get one or two more names on. then let's have a meeting to see how our skillset and expectations aligned regarding to this project. Jianci, shall I post an invitation for additional team members in the #project-team-search chat room that was created today? I would have done so already, but I did not want to claim we started a team to focus on misinformation without your agreement. thanks & pls go ahead :)
https://campuswire.com/c/G984118D3/feed/847 Be: Aux or Verb As for POS tagging, I understand that "be" in "I am studying" is Aux.  Which is "be" in "I am a student," Verb or Aux?I think "to be" in this case is a verb.  Forms of "to be" are auxiliary verbs when they're followed by a gerund (verb + -ing) to help make a progressive verb tense.  Hello Yutaro, "am" (or "be" if stemming is used) is a verb, as Brian mentioned. It could be marked as VERB or VBP (verb present tense), depending on which POS system is used. With a sentence like "Our team was asking some questions", "was asking" could be marked as AUX VERB or VBD (verb past tense) VBG (verb gerund). With a sentence like "We should be asking some questions", "should be asking" could be marked as MD (modal) VB (verb) VBG (verb gerund).  I hope this helps. Thank you so much! Thank you so much! Super helpful.
https://campuswire.com/c/G984118D3/feed/1106 Grading Percentage of Tech Review Hello  May I know what is the grading percentage of Tech Review as I cannot find any information in Syllabus ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/16ee3870-cda3-4946-8a2e-54dea39b4e27/image.png) Looks like it is just a 'completed' thing. It does not contribute to the final grade. Can TA please confirm this. From the course introduction video: > As I said, the technology review is not actually contributing to you grade, but it's a metric component and it will be graded based on completion.  I'm having trouble finding it, but I thought I remember either the syllabus or a lecture video saying that noncompletion of the technology review reduces our overall final grade by a percentage amount? Would need a TA to confirm. Short of a TA confirming, here is the grade page for the final project which does not include a percentage. It is simply for completion  ![Screenshot%20%28391%29.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/101295c4-d84a-4365-b1f0-20d2c566ca4c/Screenshot%20%28391%29.png) Since tech review is only for students enrolling with 4 credit hours, the above is for 3-credit and the above + tech review is for 4-credit. The tentative percentages for the above : tech review is 90% :10%.  What does 90% : 10% mean? I think it means:  above all : tech review = 90 : 10 What does "above all : tech review = 90 : 10" mean? Tentative grading percentage will be adjusted to satisfy  > (Quiz + MP + Project + Exams) : Tech Review = 90 : 10 Okay, thank you.
https://campuswire.com/c/G984118D3/feed/725 grading problem I received the full grade at livedatalab on Sep. 24th  but the Coursera shows unsumbitted. Should I do something to fix it? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/648b6ac6-a19a-4c1c-b98f-30a5006d0a4b/image.png)If it helps, similar query is answered at #700  Thank you
https://campuswire.com/c/G984118D3/feed/620 Error connecting github and livedatalab for MP 2.3 This is my error message. Does anyone how to fix it? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c9d5486e-92cb-4812-93de-d44b8dc1e92e/image.png)when setup the webhook in github, make sure you use http://live**data**lab.centralus.cloudapp.azure.com/api/webhook/trigger not "livelab" Thank you for the reply. I copied the link from the instruction pdf, which doesn't work. However, if I typed the link, it worked. It is weird. You might not have noticed, but some PDF text selection tools like grabbing the sentence-ending period after "/trigger". That tripped me up once.
https://campuswire.com/c/G984118D3/feed/544 Confused on Inl2 ranker function I believe I have everything implemented properly for the Inl2 function, but the one thing I'm confused on is the summation part of the formula. I understand that we are taking the sum of all the terms in the query that appear in the document, but confused on what to actually iterate over. I was thinking of using a loop to create a cumulative sum of the scores, but couldn't find documentation on how to iterate to the next term with the sd object. Ultimately I don't really know what the sd object is and how to use it to implement the summation part of the formula. Any guidance would be appreciated ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9dece4ae-07cb-4133-8d95-d3f50ada388b/image.png)From https://github.com/CS410Assignments/MP2.3:  "As you can see, the sd variable contains all the information you need to write the scoring function. The function you’re writing represents one term in the large InL2 sum."  In other words, you don't have to implement the summation itself, that's taken care of for you...you only need to define the general form for each term. You just need to write one term, without considering the summation. See the instruction below: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/13d873c8-adc1-4145-b35f-d6dd70db1015/image.png)  The instruction also has a link for the data structure of sd Ahhh I see I guess I read too quick and missed that part thank you! Ahhh ok thank you!
https://campuswire.com/c/G984118D3/feed/271 MP1 Status: FAILURE For MP1: All I see in liveDataLab is ``` Submission Number:1 Status:FAILURE Duration:00:00:12.1 (HH:MM:SS.MS) ``` And there are no logs ``` No logs available yet. Your build may still be starting up. ``` Any help is appreciated, thanks!    ![Screen%20Shot%202022-09-03%20at%209.25.39%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/05c89054-aa60-4b66-b9f2-bb7e6996d096/Screen%20Shot%202022-09-03%20at%209.25.39%20AM.png) ![Screen%20Shot%202022-09-03%20at%209.25.57%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c83813c4-d1a3-4659-a542-9782f5e1166e/Screen%20Shot%202022-09-03%20at%209.25.57%20AM.png) Hm, are you still seeing this error? It seems like you've passed the tests and your coursera grade is also updated.  After deleting the linked account and linking again, it worked. Thanks! 
https://campuswire.com/c/G984118D3/feed/657 SystemExit 1 I finished the score_one change and trying to run on Jupiter Notebook. It only run till the usage statement. Anybody know why. Output as below:  Usage: /Users/yiboli/opt/anaconda3/envs/python=3.7/lib/python3.7/site-packages/ipykernel_launcher.py config.toml An exception has occurred, use %tb to see the full traceback.  SystemExit: 1I had to use python 3.5, not sure if that is your problem Did your program correctly locate your config.toml file? Hello Yibo, I can confirm that MP2.3 works with Python 3.7. Instead of Jupyter, please try executing from a command line 'python search_eval._text.py config.toml' and see if your problem goes away. For this to work, search_eval_test.py and config.toml should be in the same directory where you issue the command. David try to use terminal or commend line to run the code instead of using run button. It was samething for me in both Jupyter and VS code. I used command line to execute 'python search_eval._text.py config.toml' and it worked.  This works. although I have to downgrade to Python 3.5 otherwise it will run indefinitely. Thank you so much! Thank you very much! this works! you are right! thank you!!!
https://campuswire.com/c/G984118D3/feed/418 MP1 Due Date Can MP1 be turned in by Sept. 11 11:59 pm or is it before Sept. 11 completely (so by Sept. 10 11:59 pm)? From the TA's post 6 days ago, it looks like there is no late penalty for MP1, Quiz1 and 2. In general, all the deadlines are in Central time. For instance, MP2.1 is due Sep 11th at 11:59 PM CDT On the github page for the MP its saying "Due Sept 4, 2022: For students who just added the class, NO LATE PENALTY will be applied to submissions before Sept 11", is this just not updated correctly then? Can refer to post https://campuswire.com/c/G984118D3/feed/310 Sorry for the confusion. The information there (on github) is probably out of date; we later decided to extend the deadline for everyone to Sept 11. 
https://campuswire.com/c/G984118D3/feed/160 project group Hello i am a first year mcs student from korea and if anyone is looking for a group for the final project. please email me at hyunjun5@illinois.eduLooks like someone (TA?) just added everyone to a chat room called "project-team-search" - might be helpful to post in there!
https://campuswire.com/c/G984118D3/feed/219 MP2.1 sign-on sheet Hello,  I am trying to access the MP2.1 sign-on sheet with my Illinois email address, but failed. Is anyone able to access this sheet now?We are still working on revising MP2.1, which is expected to be released on Monday, Sept 5. So please wait until then to work on it. The new version of MP2.1 will NOT require you to use the sign-on sheet. Sorry for the confusion.   Thank you, professor!
https://campuswire.com/c/G984118D3/feed/246 Releases of future MPs (2,3,4) Is it possible to release future MPs earlier? For online MCS students who are not doing this program full-time, sometimes we don't have time to do lectures, quizs and MPs (especially only released during that specific week) all in one week because of work, business travels etc.   It would be even better to have the leniency to submit all quizes and MPs before the hard deadline (week 7 and 13) without penalities. We've had that structure in CS437 Internet of things and it worked out great. Thanks for the suggestions. Having the course tasks and deadlines spread over the semester discourages students from procrastinating and reduces the chance that a student would be overwhelmed with too many tasks at the end of the semester, but I see that our online MCS students are in a very different situation than our on-campus students, thus we will make an exception to accommodate the special needs of all the students in the DSO section of the class. Specifically, we will allow the students in the DSO section to submit all quizes before the hard deadline (week 7 and 13) without penalty. For MPs, we will release MP2 and MP3 very soon (MP4 will be released a bit later).  However, we are constrained by the dependency among some of the MPs (i.e., one MP may depend on the completion of a previous one by all/most students in the class) and the need for synchronization in a leaderboard-based competition, thus it is infeasible for us to implement a general lenient policy for MPs as we could do for the quizzes. However,  we will be lenient in granting extensions of MP deadlines to all the students in the class.  We will do our best to approve any reasonable request for extension of a deadline and will generally approve the extension if it was due to a factor out of the student's control or an unexpected challenge encountered in finishing an MP (e.g., a problem with computing resources or a "mysterious" bug that couldn't be located exactly). Hope these will help everyone learn all the materials in the class without being stressed out. 
https://campuswire.com/c/G984118D3/feed/679 Confusion: CoLab with MP2.3 On MP2.3, I have a 0.7 on LiveDataLab, but when I try to work in CoLab, I get:  TypeError: unsupported operand type(s) for +: 'float' and 'metapy.metapy.index.PivotedLength'  or  TypeError: unsupported operand type(s) for +: 'float' and 'pybind11_type'  For the part in InL2Ranker where I add tfn to self.param.  Has anyone had this problem? How do you solve it? I don't know how I can complete the second part of the lab without it, because the first part will work in the grader but not on CoLab, and the Scipy stuff won't work in the grader.Hi Rachel,  You can use the online IDE to calculate the p-value. All you need to do is the prepare two lists of average precision, one from new rank function and one from BM25. Follow the photo I shown to calculate the p-value of two lists and save the p-value into significance.txt.  The hard part is that you might need to manually key in the data into list as Online IDE might not support read file from local desktop.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8c610a64-bd6c-406b-a6e6-c76ffab6bc5a/image.png)  https://replit.com/languages/python3 I'm not sure how I would make the lists though, since I'm having trouble getting the code itself to run outside of LiveDataLab. Sometimes it seems to work, but when I paste the code that seems to work on CoLab into my stuff to push to GitHub, it fails on LiveDataLab. So I'm not sure if the values I'm getting are correct.  Do you know if 0.2504 for PivotedLength() and 0.2551 for BM25 seem like normal MAP values? Or values like 0.2057? May be it's happening due to version difference between Colab and grader. I tried with Python 3.6 in my local and it worked fine in grader as well. Try upgrading the package in Colab and restarting the runtime after install. My MAP for Inl2 is 0.22779....     MAP for BM25 is 0.2551186  check post# 654 for your reference  
https://campuswire.com/c/G984118D3/feed/310 No Late Penalty for MP1, Quiz 1, Quiz 2 Hi all, Due to the high volume of extension requests for students just joining the class, we will be **removing any late penalty from MP1, Quiz 1, and Quiz 2.** This means that you can submit these items late and still get full credit/grade. This applies to **ALL** students. You no longer need to post on Campuswire to ask for an extension. Best of luck! 
https://campuswire.com/c/G984118D3/feed/119 quiz 1 due date Hi,  I wonder when is the due date for quiz 1? and how many tries we have for quiz 1?  thanks!!Looks like due date for quiz1 is 28th Aug . There are max 2 tries for the quizzes thanks!
https://campuswire.com/c/G984118D3/feed/1007 Grouping between 3 and 4 credit students? Since the maximum group size is 3 people, can we form a group of 2 undergrad students and 1 grad student? Assuming the project expectation is the same, that is.Sure, you can have a mix of undergraduate and graduate students. A group of 4 or 5 is alright, too.
https://campuswire.com/c/G984118D3/feed/926 How to calculate syntactic structures for a sentence Why there are 2 syntactic structures ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8f02224f-0522-4aa4-bc39-34f3c06964f9/image.png)My understanding is “with a telescope” can be a man or a boy. Therefore, there are 2 syntactic structures. Syntactic structures are the many ways we can interpret the meaning of a sentence.  In this case, this sentence can be interpreted in two ways, which is what introduces ambiguity:  1. A man, saw a boy with (or holding) a telescope 2. A man (using a telescope) saw a boy  Both sentences carry different implications. The number of syntactic structures is how many ways a sentences can be interpreted. Yes , Syntactic is about structure of the sentence. The question is really about how many interpretation of the sentence you see here which is possible. And if you look at the sentence - there can be two  - "Whether man is having the telescope" OR "Boy is having the telescope". technically called - ambiguous prepositional phrase (PP Attachment) The distinction comes from the ambiguity whether we attach "with a telescope" to the man or the boy. A man saw a boy - with a telescope, A man saw - a boy with a telescope 
https://campuswire.com/c/G984118D3/feed/1052 Content for Tech Review Hi, I am a little confused as I didn't find instructions regarding what to cover in a tech review (I am new to this so I am not familiar with the common structure). Is there anywhere I can find related information? Thanks very much!You should be able to choose a topic from this list:  https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit#gid=0  And sign-up for your topic on this list:  https://docs.google.com/spreadsheets/d/1hWAyxd82FcitN9eG3yMW6ckq6l1VASBtYWpaPbywMPc/edit#gid=0  :) Oh thank you Joel, that's very helpful. Actually what I am unsure about is more like how should I review something, i.e. what aspects to cover, how to arrange the structure of the review, etc. Oh, haha... Apologies.  It seems very open as the only requirements are:  "The review must have a coherent storyline (Intro, Body, Conclusion) and cite relevant references. It must be at least ~2 pages"  Edit:  This is identified in Week 8 on Coursera  Edit2:  The syllabus mentions:  "Tech Review (for 4-credit students). It will require you to generate a short 1-2 page review on an interesting course-related cutting-edge technology topic not covered in any lecture." That's good to know, haha, thank you! There is no hard structure requirement beyond the general coherent storyline (intro, body, conclusion). You are free to structure it however you would like.  You can think of it as a "mini-survey" on your chosen topic, where the goal is to teach others about your topic while positioning it within the related work.   Hope this helps! Hi Angela,  Think of Tech Review as writing a blog article on a topic. You can refer to medium.com, and towardsdatascience.com and look at how they pick up a topic and explain the concepts to readers.  That's a good point, thanks Parth!
https://campuswire.com/c/G984118D3/feed/986 Office Hours Are Yuxiang Liu's office hours ending next week? MP3 is due next week, but there do not seem to be many office hours available.You can find the OH information under Live Events. But yes, I also felt that there are not many office hours available. you can check the office hours under "live events" in coursera. it seems like we only have one TA office hours before the deadline next week.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e41f0052-c927-4ba2-a3af-c45ca845fa0a/image.png)
https://campuswire.com/c/G984118D3/feed/527 MP2.2 Grade Hi, I have complete MP2.2 last week but it is currently showing overdue. Any idea when it will be graded?Hi, I see the same, last time for MP2.1 the TA's were able to grade it right away. We will probably hear back soon on that coursera indicates it as overdue as no submission was received before the deadline. it's normal and you'll see your score after the TA team updates it in coursera. it's the same as MP2.1. Hi, thank you all for your patience. I've just uploaded the grades for MP2.2. Please let me know if you can't see it.  I can see the grade for MP2.2 now in Coursera. I can see the MP2.2 grade now. 
https://campuswire.com/c/G984118D3/feed/785 MP2.3 Confusion Despite the extension on this assignment and navigating through various forum posts, I cannot seem to get a general starting point for this assignment. The README syntax is very confusing to read through and does not inspire any ideas on direction. This could be why there are so many questions regarding the objective of the assignment.   We are asked to "Write an InL2 function". Do we actually need to write a function "InL2", or are we merely editing functions within the class InL2Ranker? There are many objectives given in the assignment without much foundational instruction. Perhaps a TA or student who has completed the assignment could provide some clarification.  You need to edit the function score_one within the InL2Ranker class. Below are the instructions/comment from the code under that function: "You need to override this function to return a score for a single term." So in editing this function, the intent is to return the formula displayed in the picture. But afterwards there is confusing instruction on how to send data to files and then perform analysis on said data. I am assuming we will need to iteratively send data to the files? And then perform the analysis using R?   Thank you for your assistance you can modify the output portion of the main function to only output the avg values, one per line, then put that output into a text file using   ``` search_eval.py config.toml > something.txt ```  repeat for both algorithms, then you can read in the files in python using numpy, and pass the two constructed numpy arrays into the given python code to get your answer. it'll be in a string format, so you have to copy over just the number to your significance.txt You basically have to update the score_one function to return the formula for InL2 in the Read Me file as python code. Use this link to find the fields you need for the formula: https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html Since you are passing in an sd object to the function, the fields you use for the formula should be written as "sd.field". So for example, sd.num_docs.  I feel like the second part gives a little bit more freedom in how to get it done. Basically when you run this file, it should give you a list of 225 values that you need to save in a text file. You need to do this part twice. First run the code using the BM25 ranker function. Do this by having the load_ranker function return that specific ranker method. Save the avg_p values only into a text file. Then repeat this again but for the InL2 ranker function (so have the load_ranker function return that ranker). What I found easier for me was to modify the code slightly to have two arrays that have all the avg_p values, one for BM25 and the other for InL2. Then I used these arrays with this python function to get the p-value: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html   You can save the p-value in a text file. I did this part manually and uploaded the text file to github. That seemed to do the trick for me. Hope this helps!
https://campuswire.com/c/G984118D3/feed/622 MP2.4 scores a 1 on LiveDataLab, but 0% in Coursera On my initial submission, I did not meet the baseline and my score in Coursera changed from not submitted to graded with a 0%. After subsequent tuning and exceeding the baseline for many runs now, my Coursera score is still 0%. Is this a problem on my end, or are the scores going to be updated manually after the due date like the previous few MPs?Same here. Waiting for TAs to confirm.  Similar post at #601 .  Same here as well. same here same here In case it helps - launching MP2.4 from Coursera again then adding a dummy commit did it for me. Thanks, that worked for me as well! Thank you, it works! Thanks @Sydney. It worked for me as well. Thanks @Sydney. It worked for me. Thanks very helpful I am having the same issue. Relaunching did not do the trick for me. Requesting TA's to look into it.  Thanks
https://campuswire.com/c/G984118D3/feed/1137 Timeout on livedatalab I previously used three nested for loop to compute E-STEP and M-STEP, but I got the timeout error in livedatalab. Then I changed to two nested for loop, and I still get timeout error. Does anyone have any idea? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ba7bdac3-7dcd-4e1c-a8d1-e4a88c90df09/image.png)may want to double check your code. My code converged at 25 iteration. Based on the screen shot, it looks like the line "max_iterations = 50" in main () has been removed or commented out, causing your timeout. There is a single (not nested) for loop in plsa() that uses this variable. This is different from any for loops in the E-step or M-step functions. Thank you for the reply. I just check my code. the "max_iterations = 50" has not been removed or commented out. Should the program only runs 50 iterations in livedatalab? Thank you for the reply. Should I find the maximum likelihood in the for loop of plsa() after 50 iterations, or should I stop in the middle of the iterations? It seems that main() in LiveDataLab does not have the same limit. This suggests that your likelihood is not converging, as Xinlei hinted. You may wish to check the value of likelihood at the end of each iteration by printing it. If I understand correctly, the iteration will stop when the likelihood converges, is that correct? Do your model converge with the `test.txt` data? It should stop in the middle of iterations. Of course, `max_iteration` cannot be too small. You can try to set it to 500 for `test.txt` to see whether it converges. Oh I think I miss something in the iteration. It does not have and threshold to stop. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2f00e1d3-d1f4-4b32-9c76-70927e675091/image.png) here u go May I ask what is the threshold? I do not see any comment in the script saying about the threshold, but there is an epsilon. Should I use the previous iteration's likelihood minus the current iteration likelihood to check if it is less than epsilon?. If it is, we should stop? I believe the intent is that iteration can stop when the most recent likelihood minus the previous likelihood is less than a particular value (which is given in the starter code). Please set the max_iter as 50 and add convergence check in your iterations. Either that you reach 50 iterations or that you satisfy the convergence condition will make your program stop. Usually it will converge in 50 iterations. Otherwise, you may need to check your codes. Yes
https://campuswire.com/c/G984118D3/feed/1053 Project Proposal Format For the project proposal, does it need to be in essay format? Or should we just submit a numbered list of answers for each question? Please write it in an "essay" format, rather than a numbered list of answers for each question.
https://campuswire.com/c/G984118D3/feed/834 MP3 Released Hi All,   MP3 is released now. Please access through Coursera.   MP3 requires to implement PLSA, and will **take more time** to finish than the previous MPs.   The due date is **Oct 23th 11:59PM** CT. Please take care of the time schedule and **start as early as possible**.   **Update**:  (10/8) Please clone the latest version of MP3.   Thanks,  Yuxiang
https://campuswire.com/c/G984118D3/feed/7 Week1-Quiz Hi,  my Course start date is Aug 22,but the quiz deadline shows on courser is Aug21. It is completely impossible for me to finish this quiz on Aug 21. Please assist.Same here. The deadline is currently incorrect (you will be given a few days to finish the Week 1 quiz). We will fix and let you know the deadlines for the Week 1 quizzes by tomorrow. Same Thank you! Could you extend the deadline a bit longer? I was just been added to the Illinois campus and they said it needs about 24-48 hours for being added to this course. Thank you. The deadline is set to August 28, 11:59 PM, so you still have time till Sunday. Can you get back to me if you are not added to the course by tomorrow? Thank you. Of course! quiz 1 deadline is still set to August 21 PDT on coursera. quiz 2 deadline is set to August 28 PDT on coursera. is this correct? or should we add 1 week to all the quiz deadlines? (could you make a post about the deadlines[or directly update them on cousera], it is confusing if the coursera deadlines are not up to date with what you said)![Screenshot of quiz deadlines](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b6386497-7656-4b5f-80a5-a57102ba196b/Screen%20Shot%202022-08-22%20at%208.04.14%20PM.png) I noticed this as well. It currently still says on the "Week 1 Quiz" that the due date is still August 21. Will this date be updated on Coursera as well? Thanks. That's strange. It was updated earlier in the morning. Let me get back to you on this.  Thanks! This is what I currently see. ![Screen%20Shot%202022-08-22%20at%2010.29.54%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6b7f99f5-7d66-45df-a260-ceab2fadee9c/Screen%20Shot%202022-08-22%20at%2010.29.54%20PM.png) Still showing deadline as 8/21/2022. Can you please help to fix the dates for Week1 quiz and rest of the dates for other quizes.
https://campuswire.com/c/G984118D3/feed/885 Lecture 9.3 Probabilistic Topic models Hello,  I don't get this behabior 2: high frequency words get higher p(w|θd). why we need to have multiple dot procduct for word "the"? As we learned before, we want to exclued common words, such as "the". why we want to assign hign p(w|θd) to word "the"? ![9.3.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3eed6fbb-3867-4353-a3a3-037c0c65300d/9.3.png)I believe what you're referring to as a dot product operator is just an ellipsis to represent that many more instances of "the" are present, but omitted from the graphic for brevity.  You're intuition is correct in that we often seek to reduce the influence of stop words, but in this scenario, we're looking to determine the likelihood of our mixture model generating document $$d'$$ which *legitimately* contains the stop word "the" many times. In order to adjust our model's parameters so our model achieves a high maximum likelihood of generating $$d'$$, we have to increase the probability that the model will produce the word "the" in response to an increase in frequency of that word. As the lecture says, this could be done in a couple of ways. Either by increasing $$p("the"|\theta_d)$$, or by increasing $$p(\theta_B)$$, the probability that the mixture model chooses to generate a word from the background model, which itself already assigns a high probability to "the".
https://campuswire.com/c/G984118D3/feed/1029 MP3 Permission Denied Issue Hello, I'm getting this error in livedatalab, not sure what it means because it says that permission is denied to a specific folder. Below is the attached screenshot.   ![Screen%20Shot%202022-10-16%20at%207.56.35%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9a7ae469-f73a-48e2-85e6-87d4aabc7d2b/Screen%20Shot%202022-10-16%20at%207.56.35%20PM.png)May be you can check the document path declared in your code. I have initialized documents_path = 'data/test.txt' and it worked for me without the permission denied error.  perfect thank you!
https://campuswire.com/c/G984118D3/feed/873 alpha sub-D? In week 4, there are several formulas that contain a variable, alpha sub-d. What is this variable represent?  One of the slides highlights it in a denominator position as being the query-likelihood representation of IDF. That would make me guess that it is the number of document in a collection containing the word, w?  If so, then what is n in the following part of this equation?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/293640e7-4c4d-4622-9bf6-e8f32e7f8925/image.png)    Some other references to variable below.   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c3f439ef-d5b3-435d-a936-5cb2da7cec20/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/15238059-fc5b-4213-b1a6-9e783b1683c3/image.png) From my understanding, $$\alpha_d$$ denotes the parameter to determine how much we distribute the probability for unseen words.  $$p(w|d) = \begin{cases} p_{seen}(w|d)\\ \alpha_d p(w|C) \end{cases}$$  $$n$$ denotes the query length. I think it is more intelligible to use $$|q|$$ instead of $$n$$.  I hope this helps and would appreciate any feedback on my understanding.   based on my understanding, alpha-d is the smoothing parameter. it controls the mass to be assigned for the unseen words and appropriately discounts some probability from the seen word in the document. Thanks Yutaro. Intuitively that would make sense: effectively we're discounting a word's probability when we have to pull it from the Collection or Background model because it's not in our document. That make sense because we probably would place more value on a word probability coming straight from our document model vs one coming from the background model. Agreed. Thank you.
https://campuswire.com/c/G984118D3/feed/1146 Naming Convention and Folder Structure of Project Repo in GitHub Hi,   I was wondering if there is any document that mentions the naming convention and folder structure we should follow when submitting project materials? such as for the proposal, what should be the filename and should I put it under the root folder of the repo?  Can someone please advise?  Thanks I have not found yet but if you click on the former semesters' submission, they seem to follow one format. And there is a post mentions to include group name and some other imformation The link in the instruction on week 9 proposal submission task says there is no naming convention.   BTW, there are so many documents, instructions, forms used in this course and it is very easy to miss information. Why can't we stick to Coursera?   Also we used different platforms to upload the homework of this course. But I feel: livedata one does not seem secured enough with frequent connection issues; and CMT seems like something more serious than a homework that we have to register without another choice.   Whereas in other courses provided by UIUC, all the similar tasks can just be explained and finished within Coursera. No, there is no naming convention. As long as each milestone is easily findable + clearly named, then you should be fine. If I recall correctly, Coursera does (or did) not support (1) auto-grading MPs, and (2) group-based peer reviews. This is why we use LiveDataLab and CMT, respectively.  Both of these are not only fully supported, but extensively leveraged in many other courses in the program. This is likely the source of this student’s point of reference.  Off the top of my head, auto-graded MPs were supported in: - Applied Machine Learning - Deep Learning for Healthcare  While peer review was heavily featured in: - Every MOOC in the Cloud Computing Specialization - Every MOOC in the Data Mining Specialization (including the two based on this course)  IIRC, the MOOCs also leveraged auto-graded MPs, but it’s been a while. Thanks for the information and feedback! We will look into using this for future iterations to make the course easier. 
https://campuswire.com/c/G984118D3/feed/655 MP2.3 is it normal to get a very large p-value? I was varying parameters for the two ranking functions and picked some values that output a relatively large mean average precision. But when I'm doing the significance test the p value I got is very large (larger than 1). Is it normal or does it mean the parameters I picked are still not good enough?I did not change any parameters when comparing to the ones in the example.  My assumption is that you mean the ranker BM25 parameters?  I did have an issue where I did not create the additional files of inl2.avg_p.txt and bm25.avg_p.txt initially thinking all we needed was the significance.txt.    This was not the case because it seems the auto-grader uses the other 2 files to verify your significance.txt p-value.  Hope this helps! I think p value is a probability, so it should not be larger than 1. I would also double check the p value function if other things look good. Check if your output is in scientific notation rather than a float. Thanks that solved me problem!
https://campuswire.com/c/G984118D3/feed/572 MP 2.3 Issue When I run search_eval.py, my code only performs the print statement "Usage: {} config.toml" as seen in the screenshot and then seems to exit, as the rest of the code under it does not run.   How do i resolve this? ![Screen%20Shot%202022-09-22%20at%201.08.44%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/13b52495-99a7-4c42-8c84-9613a6d307f1/Screen%20Shot%202022-09-22%20at%201.08.44%20AM.png)You can run the code locally as: ``` python search_eval.py config.toml ```  My issue with running locally is that it will give me an error in importing metapy since my conda environment for python version 3.7 only works when i change the python interpreter in VSCode. When I run locally, the python version is 3.8.9 even when I have activated my conda environment where i set the version to 3.7.  The issue here I believe is that you may not be providing the parameters when running the code. Make sure you are running the code as specified in the usage message: ``` python search_eval.py config.toml ``` You can do this from within your conda environment. Running the following ``` python search_eval.py ``` will cause the system to exit and not run. I had the same problem - what i did was simply redirect the [cfg] towards my local file location by replacing "cfg = sys.argv[1]" with "cfg = local file dir//config.toml" I am running python 3.5.6 under Conda. You can create a new environment and install this version of python there. Then you can install and import metapy inside. This works for me.
https://campuswire.com/c/G984118D3/feed/62 Pause working on MP2.1 and above As already mentioned in other posts, we are still working on making MP2.1 and later MPs work for the students (they have been locked for now and will be released later). We suggest that you focus on the weekly quizzes and MP1 for now. We will let you know via a note once the further MPs are released.
https://campuswire.com/c/G984118D3/feed/324 kernel appears to have died Hi, I was able to get MP1 instruction working, but I can't do the last step of load an analyzer from this configuration file.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/27547ccb-c1f9-4bf3-b01a-c80acd01981c/image.png)  What should I check?The reason has found.
https://campuswire.com/c/G984118D3/feed/3 Are the due dates of quizzes and assignments on Coursera correct?   Hi there, I just saw the course appeared on my Cousera one hour ago. I saw the due dates of Week 1 activities are the night of tomorrow, Aug 22. So we only have about one day to learn all stuff of Week 1 and complete the two quizzes (one for the course information and one for Week 1 module)?   Besides, it seems that all quizzes and assignments are due on Monday of the corresponding weeks, so we must study and do assignments ahead, is this intended?  ![image2022-08-21%2020%3A32%3A46.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5be6f9af-8031-4207-9c6f-f842bbe2770a/image2022-08-21%2020%3A32%3A46.jpg)![image2022-08-21%2020%3A32%3A53.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/74879363-15a4-40cd-b943-edc7b01055c8/image2022-08-21%2020%3A32%3A53.jpg)This is from the orientation section readings.  It looks like for quizzes 1-6 the hard deadline is end of week 7.  Course Deadlines, Late Policies, and Academic Calendar Course Deadlines Quizzes Assignment  Release Date  Hard Deadline  Quiz 1  First day of class  End of Week 7  Quiz 2  First day of class  End of Week 7  Quiz 3  First day of class  End of Week 7  Quiz 4  First day of class  End of Week 7  Quiz 5  First day of class  End of Week 7  Quiz 6  First day of class  End of Week 7  Quiz 7  First day of class  End of Week 13  Quiz 8  First day of class  End of Week 13  Quiz 9  First day of class  End of Week 13  Quiz 10  First day of class  End of Week 13  Quiz 11  First day of class  End of Week 13  Quiz 12  First day of class  End of Week 13 Also, The hard deadline is the deadline after which you will receive 0 points on assignments regardless how well you did on the assignment. No late submission will be accepted except under extremely rare non-academic circumstances (which usually require approval from the Dean's office).  Each week's Quiz is due on Sunday. Late quiz submissions will be accepted up until the Hard Deadline. However, a late penalty of 5% will be deducted daily until a submission is made or the grade reaches 0%. Oh, I see. So they should be due on Sunday for full credits at least. But they are due on Monday currently.  Looks like that to me! Because you're not using PST Should we use the timezone PST, not CDT or CST? ![Capture.JPG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/40074946-2126-4f36-b021-380c03420bdb/Capture.JPG)  Obviously, it is not correct because it is showing Sunday (Aug 21) as the deadline Actually, the due time is Aug 22, 1:59 AM CDT, which is kind of impossible for me to finish one week's stuff with the four hours left. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6f140558-0a5b-41ad-b8c3-4b9f843ebb3e/image.png) I agree, I think this is now obviously a mistake. Mine is showing Aug 21 hahahaha Yeh, it depends on the timezone you set on Coursera. My Coursera hasn't even been added yet. I'm stressing out. Am I about to lose 5% on all the Week 1 stuff now? same issue with me .Can someone please help? The official start date of Fall 2022 is tomorrow, I do not think the due date is correct. Let us see what our professor and TAs will say tomorrow. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/56a448b0-f3e5-4b9a-8d17-11d8f1803785/image.png) isn't this getting stressful already?  :( The deadline is incorrect. We will fix the deadlines by tomorrow and let you know. Cool! Thank you so much. This must be a typo! We'll correct it. As the discussion thread has already clarified, the hard deadlines for those quizzes are quite late. Thanks to those of you who provided all the useful information and helped clarifying things!  I think the course staff will fix that tomorrow. Hi Harita. Has the fix been implemented yet? It is still showing Aug 21 as the deadline at my end (PST) and it is overdue now
https://campuswire.com/c/G984118D3/feed/249 How to confirm that MP1 is complete? I think I've completed MP1 and I see my submission on the LiveDataLab Leaderboard but I don't see any sort of completion feedback (like a green checkmark) on Coursera.  I just wanted to double check what the expected behavior is after completing one of these assignments. Are there any actions I need to take to ensure I get credit in Coursera? You just have to make sure that you open the LiveDataLab link through Coursera MP1 link. Once you add, commit, push again you will see the grades on Coursera instantly. That is how it worked for me. Hope that helps! Hey please see our discussion in #155. Hope it's helpful. you can check the coursera grade. if it's 100%, it means you have complete and got the full credit.
https://campuswire.com/c/G984118D3/feed/697 MP2.3 submission I can see the submission history at home page, but nothing in the grading update in MP2.3 site. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/61ba2bf9-5ea3-41de-ae73-627514f7c820/image.png) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d04a2d36-ab59-4933-b2e6-598e199608d8/image.png)Press Leaderboard to see if you are graded. this is the leaderboard page ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/775b76a7-2033-48f0-92e2-692380822f79/image.png) There seems to be errors in your later submissions.  Attempt to click on your latest submission to see if there are any logs that would inform you of what went wrong, such as livedatalab not having any modules named scipy Looks like you are trying to import numpy, you shouldn't need to import this package to finish the MP I fixed it, thank you very much! I fixed it, thank you very much!
https://campuswire.com/c/G984118D3/feed/1259 Log Likelihood not monotonically increasing I have the same problem as someone else did two days ago. My iterations with the test data look like this: ``` Iteration #25... -224237.83429232525 Iteration #26... -214537.26590018562 Iteration #27... -207431.20891021765 Iteration #28... -202872.2076288436 Iteration #29... -200336.91548889066 Iteration #30... -199202.84835355126 Iteration #31... -198950.43153064366 Iteration #32... -199210.35306954256 Iteration #33... -199743.62552057777 Iteration #34... -200401.6875146974 Iteration #35... -201091.9353962618 Iteration #36... -201756.27025680034 Iteration #37... -202359.3163707304 Iteration #38... -202881.37697428238 Iteration #39... -203313.6169861956 Iteration #40... -203654.5906963647 Iteration #41... -203907.724130878 Iteration #42... -204079.4847899975 Iteration #43... -204178.1263822302 Iteration #44... -204212.87140975398 Iteration #45... -204193.2891203349 Iteration #46... -204128.71431173678 Iteration #47... -204027.75383974507 Iteration #48... -203897.98950528604 Iteration #49... -203745.89750533825 ``` As you can see, it goes from -224k to -198k to -204k and then back up where it converges at -197864.11650778833 at Iteration #931  Someone replied to check if the E-Step and M-Step were setting variables correctly. My E-Step sets self.topic_prob; my M-Step sets self.topic_word_prob and self.document_topic_prob.  Any ideas for why my log likelihood is not monotonically increasing? Also, if anyone has solved this issue, any tips? Rachel, You may wish to check the normalizations in both E-step and M-step against the lecture equations and constraints. Not normalizing, or normalizing along the wrong axis, could cause likelihood to behave like this.
https://campuswire.com/c/G984118D3/feed/1125 question about remove 0 and 1 on build_corpus Hello, it might be a dumb question, but I'm wondering how to remove 0 and 1 at the start of the first 100 lines?Can you post a screenshot so I can see what problem you are trying to address. I did not remove the 0 and 1 list.remove() can do that. You can do it for test purpose, but when submit to grader you don't need to remove them. I too did not remove 0 & 1 from the list.  what I did was in build vocabulary, I created a set and add to it if word is not 0 and 1 i tried to go for something "generic" by first checking if i could cast the element of the list to an int; if i could, i discarded it. if i couldn't, it was a valid string and i added it to the document. i did this with a try/except block re My understanding is `test.txt` is an example of a collection covering two topics. First 100 lines illustrates what the topics are and dominating words in two topics. It's fine to test the code with the rest 900 lines only. And then check the final `topic_word_prob` matrix to verify whether two topics have reasonable word distributions. The first 100 rows of test.txt seem intended as a training data set: actual topics are specified by 0 or 1 so we could compare to the model results. In any case, the first 100 rows all begin with a digit and a tab character (\t). The remaining 900 rows all begin with a tab but no digit. So, one approach is always to drop the first character. This will remove the digit from the first 100 rows and the leading tab from the other 900 rows, and prevent '0' and '1' from becoming part of the vocabulary.
https://campuswire.com/c/G984118D3/feed/66 quiz hi,  how many attempts do we have for quiz?two. max 2 attempts two two Just to add to the other answers: You should be able to see the number of attempts allowed on the quiz page. You can see the number on the page. If not, you can try unlimited.
https://campuswire.com/c/G984118D3/feed/114 Textbook Error? In chapter 2.1 of the textbook, the section on conditional probability contains an example problem.   Am I missing something, or is the probability calculation (1/3 vs 1/6) incorrect here?  Thank you!  ![comp410_textbook_error1.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e877b042-03c9-4beb-8384-c4826bd2112f/comp410_textbook_error1.PNG)  ![comp410_textbook_error2.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/66805c19-80cf-400d-8f8a-e812d5de7498/comp410_textbook_error2.PNG) Check the paragraph that opens the conditional probability discussion in your first screenshot: "Let's use the original die with six unique colors". The author is moving back to the probability space where item 4 is a blue circle. Item 4 being a green circle was for the joint probability discussion.
https://campuswire.com/c/G984118D3/feed/155 MP1 Grade on Coursera Hi All,  I submitted MP1 a few days ago and made sure the scoring shows a 1. However, on Coursera, I haven't seen a grade for the assignment yet. Is anyone experiencing the same issue? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c541c979-7de4-42d0-ba7c-c43a41673842/image.png)![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/71ab51a6-00c8-482b-959d-9f24fde349bc/image.png)Did you access the MP via the "Open Tool" button in Coursera? Or alternatively, by logging directly into LivaDataLab and then navigating to the MP?  Only speculating here, but wondering if that has something to do with establishing the connection between the two.  I'm not able to find it (yet) but I vaguely recall instructions advising us to *only* use the Coursera link to start MPs. I just tried to resubmit by clicking the link through Coursera...that did the trick! I now see 100% as the grade. Thank you for your help! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ec328276-a8ef-4a83-a7d1-60d414647e11/image.png) Hey Ginna, wondering what happened when you clicked the link? I am having the same issue however clicking the lick does not help. For me, after clicking the "open tool" button from coursera MP1 page it just asked me to go into LiveDataLab and after which nothing happened... Many thanks. Hey Hanyin,  Yes, what you describe was what I saw as well. All I did was make sure I accessed the MP1 through Coursera. Then navigate to my IDE (I'm using Pycharm) and re-push the code out to GitHub (this should resubmit the assignment again to LiveDataLab). Within seconds I saw my grade on Coursera. Let me know if you're still having issues! Thank you so much! I just tried this and magically it worked! Many thanks! Just adding if it helps anyone, Another way to go about this from Github is to go to your Settings in Github, hit Webhooks, go to the link mentioned there (this should open the web hook you created), hit recent deliveries, and hit the redelivery button and follow the instructions to redeliver. This worked out for me and the grades showed up on Coursera
https://campuswire.com/c/G984118D3/feed/729 Average score about the MP2.4 I have tested all the listed rankers and the inL2 ranker created in MP2.3 and have had no luck scoring 1 after 23 tries.  I did the grid search of the optimal parameters for relative rankers by testing only the Cranfield data before submitting it to the liveDataLab. The problem is that the best parameter+ranker never gives me a score >0.35 for Cranfield data. I am not surprised that the test will fail in the liveDataLab.  Please provide more hints to pass the test. Thanks a lot!I actually found that setting the parameters around default suggested values tends to be good starting point and it gives a fairly decent score. Grid search is a good starting point. Remember that the best parameters one dataset (e.g., Cranfield) may not be the best for another dataset (e.g., faculty). Also keep in mind that the Cranfield data is only weighted at 30% of the composite score and the faculty data is weighted at 60% - so in this environment, attempts should focus more towards maximizing the faculty score even at the expense of the AP and Cranfield scores (to a certain degree). As a hint, I found that even seemingly insignificantly small changes of some parameters can have a significant impact on certain scores, so make sure that your grid search isn't taking too large of steps. Steve, Have you been able to test against the faculty database without submitting? It is frustrating to see Pivot length ranker failed after testing 20 values with an interval of 0.1 It was interesting, I was able to overfit the Cranfield data and get a poor score when I submitted my search_eval.py. I don't believe you can do that. This is correct to my understanding. It's a commit and push to remote for each parameter test, unfortunately Thank you both! You are correct: I just checked with the TAs. I was hoping for a better process. 
https://campuswire.com/c/G984118D3/feed/1006 Question about JM smoothing - Effect of Lamda Since lambda ranges from 0 to 1, it would mean when,  1. lambda is close to 0 => p(w|D) is close to ML Estimator   2. lambda is close to 1 => p(w|D) is close to language modelYour understand is correct, what is your question? Thank you. Was just confirming. Absolutely
https://campuswire.com/c/G984118D3/feed/301 Delta encoding example ![Screen%20Shot%202022-09-04%20at%2010.49.50%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d826242d-98db-46fd-99ef-eb0cf22dcd37/Screen%20Shot%202022-09-04%20at%2010.49.50%20AM.png)  Could someone decompose the steps for getting the delta code of 3 as 1001.  Thanks a lot!let's first understand how we get gamma code. 1. how to map 3 to unary code "110"? step 1, we need 3 - 1= 2 digits of "1"  step 2, we need it to be followed by 1 digit of "0". So we have "110"   the total digits ("110" has 3 digits) of the mapping means the actual number value 3.  Now let's see how we map 3 into gamma code? 1. gamma code has two parts, first part is unary code, the second part is uniform code. The unary code part value is 1+ floor(log x ), so it is actually has floor(log x) digits of "1" and then "0". For example, if x is 3, btw the log here is base of 2, not base of e or base of 10, then floor(log 3) is 1. so we will have just 1 digit of "1" and then "0". Then, the rest part is the uniform code of 3-2**(floor(log 3)) = 1 in (floor (log 3)) bits, so it's just "1". That's why we have 3 -> "101"   Next, how we map 3 to delta code? Remember in gamma code we have unary code part and uniform code part?  for 3, the unary code part is "10" and uniform code part is "1". So for delta code, we just need to replace the unary code part to be gamma code. The unary code part "10" means 2, so we need the gamma code of 2. The gamma code of 2 is actually "100". That's why we have "100" + "1" = "1001" to be the delta code of 3.  I don't know if you can follow it. Any suggestion is welcome! I was able to follow through! Thank you so much for the detailed explanation! Ojasvi, I made a spreadsheet to understand better the calculation and efficiency of different encodings. Here is a picture. I hope it can be helpful to others. ![Bit_encodings.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/30bd7359-73d3-4ef0-a740-8488e2700e48/Bit_encodings.png)  Thanks! This is great!
https://campuswire.com/c/G984118D3/feed/201 webhook issue ![Screen%20Shot%202022-08-30%20at%206.57.15%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6fa1d494-3238-4579-ae7d-a55a2d87fb1a/Screen%20Shot%202022-08-30%20at%206.57.15%20PM.png)  the webhook is still not working for me. the link I used is  http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger when I git add, commit, push no submission shows upIf it helps, similar issue & the fix has been discussed at https://campuswire.com/c/G984118D3/feed/25 I think a lot of people are having issues with the webhook right now- it looks like the error now is a 500 Internal Server Error. You can find this if you click on the webhook and go to Recent Deliveries -> Response. The submission history on livedatalab also does not seem to load at the moment. Ok that makes sense. Hopefully this can get fixed soon then. Try to delete and readd, should be up and running again Looks like redelivering the payload is working now. Thanks! However, I still cannot see my submission history of any of my submissions on livedatalab. Are there any additional steps I could take? works now, thank you! I'm having the same issue! Wondering what I can do. You may not be able to see your old submissions, but scores will still be recorded on Coursera. Your new submissions should appear on the submission history.
https://campuswire.com/c/G984118D3/feed/214 Successully submitted but there is no score log. I re-did the whoe process twice, but still not work....  I hope we can use a new system like Jupyter Notebook..  This can save a lot of our time.  I see the tests passed on our backend. Looks like you've linked the same github account multiple times ... likely why you are facing an issue. You might need to relink the github account again after this MP.  Regarding Jupyter Notebook, what I did was write my code there, and then paste it into the example.py file in order to submit it. I was unable to figure out how to install metapy locally, but doing so was easy on Jupyter Notebook.
https://campuswire.com/c/G984118D3/feed/1034 Will we able to see what questions we got wrong in exam1? As title.after 17 I'm not sure. But, since this is marked as resolved, is anyone able to see
https://campuswire.com/c/G984118D3/feed/61 [OPTIONAL] content of the lecture Will the optional lessons (like the below screenshot) be included in the exams? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a9ef74ab-6cd1-4492-82db-3c8a25df3fa5/image.png)  Thank you!It is highly possible that these optional lectures won't be included in the exams, but you are expected to double check the review lectures for information about exams.
https://campuswire.com/c/G984118D3/feed/755 Release of MP 3? Is it possible to release MP 3 soon? Have some free time now and would like to get started on MP 3 as soon as possible. Otherwise it is hard to balance between project, midterm, and work travels during that week. In addition - If possible please release other MPs as well if they are ready. For working professionals like me , we can try and complete them and save some extra time for project given our working schedules. Actually  MP3 should have been released this past Monday according to the syllabus/course deadline. ![Screen%20Shot%202022-09-30%20at%205.54.27%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/85fa73e6-b921-4815-acb7-c46f9ae72e2e/Screen%20Shot%202022-09-30%20at%205.54.27%20PM.png)   We plan to release it on Monday. Sorry for the inconvenience.  I dont see Programming Assignment 4 in Coursera. Can only see till MP3. Do you see MP4 ?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/27736f50-a4b6-4a15-9c86-152ff395d311/image.png) See #795  Hey Yuxiang, I hate to be that guy but just wondering if the assignment will be released soon? Sorry for the late. The MP3 was released on LiveDataLab this Monday but remained locked on Coursera. I just unlocked the MP3 item. You can now access through Coursera. Thanks so much!
https://campuswire.com/c/G984118D3/feed/736 Project update! Hello everyone,  I hope you all are enjoying the semester so far. This note is to provide you with an update on the course project plan.  It is time to start thinking about groups/topics for the course project. The timeline can be found [on Coursera](https://www.coursera.org/learn/cs-410/supplement/vtHNZ/course-project-overview). The first deadline is October 17th, where you must form a team. Please see the beginning of the Overview Section of [this Google doc](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit) for instructions about the sign-up sheet and about CMT (platform that we are using to manage submissions, peer grading). To reiterate the document ****all students must create a CMT account****.  Moreover, if you notice any inconstancies or have any trouble, please let us know.   For the topic selection and proposal submission (deadline October  23rd), the directions for the CMT submission process can be found [here](https://docs.google.com/document/d/1XFvippp_jyiUUXpWsps4Geh5KsyZRkJq/edit?usp=sharing&ouid=115921160486510493648&rtpof=true&sd=true). Only one team member should make the submission, but they should add all team members under the Authors section. After you add the Github repo link, you can put all submitted content in the repo, and peer reviewers will be able to visit your repo link to find your project content.  Looking forward to seeing your projects!  -Kevin 
https://campuswire.com/c/G984118D3/feed/677 Running Search Eval I keep getting the error "'python' is not recognized as an internal or external command, operable program or batch file." when trying to run python search_eval.py config.toml are you inside python environment? You have to activate python first If you are using anaconda prompt, make sure you are activating the virtual environment by typing conda activate <your python environment>, this should resolve it you get that error because there is no executable named python on your machine or you do not have the PATH updated for the python executable.  Check where you have installed python and make sure you have the path ready.   Problem is no one can actually say why it would happen on your machine unless someone actually looks at your machine.
https://campuswire.com/c/G984118D3/feed/365 MP2.1 Where to Check the Queries I Submitted Hello, I was wondering how do we check to see the queries we inserted specifically?Go to "my submissions" after signing in here: http://timan.cs.illinois.edu:4000/auth I noticed that all of the submissions are different than the ones I submitted, is this normal? For me, http://timan.cs.illinois.edu:4000/auth shows me all my submissions with the highlighted text Hi Da'Mon,  Do you mind explaining a bit more about what you're seeing / how they're different than the ones that you submitted?  I was actually searching "My Submissions" in the search bar so that was the issue.
https://campuswire.com/c/G984118D3/feed/415 Cisco vpn connection failure I have been trying to connect my vpn for a bit now and I keep getting the two popups down below. It has been a while since I've used the vpn but it has worked before, I'm just confused as to what I'm missing as everything on my laptop seems to be running fine in the background. Does anyone have any suggestions or know what the issue is? ![2022-09-10%20%284%29.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/7f6c1bfd-569d-499d-b5a7-92081c7d9e87/4635ca02-b184-4379-95e6-c834cff4b208/2022-09-10%20%284%29.png)  ![2022-09-10%20%285%29.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/7f6c1bfd-569d-499d-b5a7-92081c7d9e87/326e4481-cae6-4c1e-9625-036effc6e1ea/2022-09-10%20%285%29.png)Are you using vpn.illinois.edu? Yea, every time I log in it comes up with this screen and then a couple minutes later it gives me the two photos from before ![2022-09-10%20%287%29.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d08af3e8-b9ad-4543-aa62-c443813ac385/2022-09-10%20%287%29.png) There is a support line number on the VPN webside. May be you should try calling them. I copied from that website here. > Please contact the Technology Services Help Desk at consult@illinois.edu or (217)244-7000 for support. I would try updating the VPN software and checking the firewall. How can I check the firewall? Ok I finally got it to work, not entirely sure how but its working at least One of the ways, is to restart your computer especially if this is the first time you are trying to connect to a VPN.
https://campuswire.com/c/G984118D3/feed/1145 Do we need to wait for a grade on the technology review proposal Do we need to wait for a grade on the technology review proposal before we start working on the technology review paper or is it fine to start working on it now?Hey!  My understanding is that the Tech Review is separate from the course project. We simply choose a topic in tech that we would like to review and write a 2 page paper of it. ( I wasn't sure if you were referring to the Technology review or not, so I included the syllabus excerpt for it) Here is the tech review sign up sheet: https://docs.google.com/spreadsheets/d/1hWAyxd82FcitN9eG3yMW6ckq6l1VASBtYWpaPbywMPc/edit?usp=sharing  And the tech review topic selection sheet: https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit?usp=sharing  *Tech Review (for 4-credit students). It will require you to generate a short 1-2 page review on an interesting course-related cutting-edge technology topic not covered in any lecture.*  *Final Course Project. There will also be one culminating project due at the end of course. It will require you to work in groups of at most three. You will build a tool using methods from the course to perform data analysis and also generate documentations and presentation.* -course syllabus  For the final project, typically we can start without the proposal feedback, but if the TA's have significant feedback it can alter the work you may have already completed. It may be beneficial to wait for feedback if you have chosen an out of the box free topic, but if you feel confident or have chosen one of the 4 main topic themes, you should be fine to begin your implementation.   *For all the categories, the instructors will meta-review your project proposals (following a peer-review round by other students) and provide feedback. If we find your project topic/plan has some limitations, we will provide suggestions to improve it or suggest you pick one of the sample topics. You’re allowed to change topics after the proposal stage based on our feedback. If you have not received any suggestion from the instructors, you can generally assume that your project proposal is fine and you can proceed to carry out the proposed project work.* -CS 410 Project Topics document found under project instructions  Here are the project instructions: https://docs.google.com/document/d/1XFvippp_jyiUUXpWsps4Geh5KsyZRkJq/edit  And the project topic sheet: https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit Ok so to reword my question, after you have signed up for a topic in the tech review sheet, can you immediately start working on the tech review paper or do we need to wait for something.  I believe that you can start working on the tech review paper as the content has been described on week 9 of coursera. You don't need to wait for anything. It's better to start early as it will due soon, also the same due time as MP4. I am not seeing an MP4 in the grade summary or in the weekly assignment information. Could you please expand on this?  What I know is that there will be a MP4 soon.  It feels very late in the semester to be adding another unscheduled assignment, as well as modifying the current MP grade structure as many of us work and have planned our schedules weeks to months in advance. When can we expect more information to be released as november 6th is 13 days away? I'm sorry for that. It seems that at the beginning of this semester the instructor has mentioned MP4. I'm not sure when it will be released. Another TA Daniel, who specializes in MP4 will announce soon and start his OHs. Thank you for the information
https://campuswire.com/c/G984118D3/feed/902 Lecture 9.2 Typo  It seems there is a typo here:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/abb6f4ac-b46f-4eff-bfb2-83e3b081ed1f/image.png)Thanks for catching the typo! I'll correct it. 
https://campuswire.com/c/G984118D3/feed/888 lecture 2.1 IDF I do not quite understand where are these IDF values coming from. Can anyone help? I understand the equation mentioned in the previous slide which is log[(M+1)/K]. But how are these values calculated from this equation?![Screen%20Shot%202022-10-10%20at%2010.38.39%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/37a2c19f-83ee-4bbb-8c0d-2a4d66f33e81/Screen%20Shot%202022-10-10%20at%2010.38.39%20PM.png)I don't recall exactly if this was used an example based on the previous examples used in the lecture. I am not sure if the slide had listed the specific occurrences of the term in terms of the number of document it exists and total number.  I feel it just gives an intuition of how it works. Basically, rare words get high IDF where as common words get lower IDF.  I feel these numbers are just representative of how IDF value of the one word(a rare word)  would look like comparative to that of other words (commonly occurring word).  Thank you that’s how I feel but wanted to make sure  It is just representational, to capture the idea that IDF for rare words tend to be high
https://campuswire.com/c/G984118D3/feed/1203 practice quiz 10 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1cf9c95c-5f50-46da-9c88-e52b22229caf/image.png) Don't understand the option 1 here.my interpretation of the meaning of option 1:  we need a similarity measure to indicate whether a data point belongs to a specific cluster, to compare it to other points (or maybe a centroid) in the cluster. for example, maybe we use euclidean distance. in model-based clustering algorithms, is it challenging to substitute the euclidean distance measure for another similarity measure, like the dot product, or pearson correlation coefficient? or is it easy to swap those similarity measures out? Unlike similarity-based approaches where we can explicitly specify a particular similarity function to inject a particular view of similarity, in model-based approaches, there is no direct/easy way to control the similarity measure (the similarity measure is implicitly determined in the model). They do not give us the flexibility of specifying a particular view of how things should be measured as similar.
https://campuswire.com/c/G984118D3/feed/1295 Topics for Tech Review Hi, How are you selecting the topics for tech review paper? There are no papers listed in the excel sheet, we just randomly search on the net and find a paper. And do a tech review of it?  ThanksHi, We are provided with some sample topics to choose from under the Week 1 Tech Review content. If we don't pick any of those, there are certain requirements to make sure that the topic is related to the course. This is all available under Week 1 Tech review I chose the topics that have the paper provided
https://campuswire.com/c/G984118D3/feed/833 Initial Proposal Deadline I see in W1, initial proposal deadline is oct 23 but in week 9 it shows as oct 17. Which is the right deadline to follow ?It should be October 23rd. I've changed the proposal deadline in week 9 to October 23rd.   Thanks for pointing this out! Thanks a lot! 
https://campuswire.com/c/G984118D3/feed/600 confused on Statistical significance testing I am a bit confused on what we are supposed to do with this portion, I just run my code using "python search_eval.py config.toml" in terminal and get the average precision value and put in the significance file, is it good enough?  ![Screen%20Shot%202022-09-23%20at%209.45.32%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b7db0252-b1b1-4b25-97f4-4083c0c62305/Screen%20Shot%202022-09-23%20at%209.45.32%20PM.png)You need to compute the p-value between two different datasets, which are: 1. APs from your BM25 ranker 2. APs from your InL2 ranker  One way to do this is to write the APs in 2 separate text files and read them to compute the p-value. The python method is provided in readme please take a look. You just need to pass 2 arrays of APs into the function.  Or you can just create the 2 arrays directly in search_eval.py and calculate the p-value. But I think the auto-grader doesn't recognize scipy, and you need to do the calculation elsewhere.  I also have questions about this part. is the avg_p here the list of average precisions from 225 queries?  Do we need to find the best value of a chosen parameter for this relative ranker? Thanks! yes, I believe you need to copy your 225 results to two separate .txt files and read in them to use the function in the instruction, but I am getting some TypeError here.. Thank you! I did what you said. I created two .txt and saved my 225 AP results in each of them. I read in the two files and use stats.ttest_rel(A, B). however it gives me this error, do you know why?![Screen%20Shot%202022-09-23%20at%2011.19.37%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7ec74fc7-68b9-46c1-b9f9-a91aa596f6af/Screen%20Shot%202022-09-23%20at%2011.19.37%20PM.png) What are "a" and "b" here in your line? It looks like their type does not support the minus operand. I am not sure what you are trying to do here. You just need to pass your 2 arrays into the function and you get the p-value. thank you, I figured it out. 
https://campuswire.com/c/G984118D3/feed/451 MP2.3 - No module named scipy Hello TAs,  FYI, it seems like there is no spicy package in the grading environment ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dff6c555-195d-4417-9014-641f5224961f/image.png)  The submission is passed once I removed the spicy code and keep the calculated signficiant.txt in the directory  ThanksI didn't remove my scipy code. I created a separate file to do the t-test. Everything looked fine for me and I got full grade on Coursera.  I believe because the grader will not run your separate python file that's why you didn't get the error I think so. It only checks the search_eval.py and significance.txt.
https://campuswire.com/c/G984118D3/feed/34 Error Installing metapy on Windows 10 Hello, I am having problems installing metapy on my Windows machine (Python 3.10, CMAKE and all support libraries installed.) I am getting the following error: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4cc9c74c-6ad7-48d6-b7a5-ac73fcaf8d05/image.png) I have verified that all the binaries in the system path as well. Any ideas? Unfortunately metapy only works with Python 2.7, 3.4, 3.5, 3.6, or 3.7.  You can keep Python 3.10 as your default and do an altinstall of 3.7, then run any scripts related to this class under 3.7  Reposting my solution from #28: I got metapy to work by installing Python 3.7 and then using the appropriate cp37 built distribution from here:  https://pypi.org/project/metapy/#files  You can install a built distribution using pip install <whl_url> where <whl_url> is the appropriate link for your environment from the PyPI metapy link above. Thank you, this fixed my issue. Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153  
https://campuswire.com/c/G984118D3/feed/672 Final project done independently?  I just saw the project requirement from week 6 and that team formation is required.   Could TA confirm that final project can still be done independently? I have a hard time finding teammates due to working in the CEST timezone in Europe.   ![Screen%20Shot%202022-09-25%20at%2010.50.10%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c8236daa-ef7d-4024-90c5-52dd8e5d7d17/Screen%20Shot%202022-09-25%20at%2010.50.10%20PM.png)from the project topics description document in the final project proposal submission [here](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit): >  Because the course project is only one of the many tasks that you have to complete for this course, we do not expect you to spend more than 20 hours of quality time on the coding part of the project. For a group of N students, the expectation is 20*N hours total. Thus, you are strongly encouraged to form a team to work with others so that you can accomplish something more substantial than what you can on your own. In fact, we also encourage multiple teams to coordinate with each other for potential integration of multiple projects so as to maximize the impact of your course project work.  If you can combine your effort to finish a larger project successfully, you would all be able to mention this accomplishment on your resume as your course project for CS410.  from that it sounds like 1 person teams are fine, you just need to tackle a project that should take $$\sim 20*N$$ hours to complete, with `N` being the number of members on the team (in this case N=1). i also plan to do the project independently  can wait for TA to confirm though Yes, the final project can be done independently.  Yes. Make sure your project is 20 hours effort atleast. 
https://campuswire.com/c/G984118D3/feed/1335 Technology Review + Project Progress Report Due Dates The due time is at 1:59am for these two assignments rather than 11:59pm like most of the others. Seeing as they're the odd ones out I'm assuming this is an error. Wondering if we can get confirmation on the day/time these items are due?   ![duedate.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2b3978bb-ce26-4a94-931a-f31cd493c1b0/duedate.png)  I noticed the times appear to be correct for the syllabus, but they don't match what is on the page for each submission:   ![Screen%20Shot%202022-11-02%20at%204.55.50%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d2fe2860-a331-4c3b-bdf1-6cf51177fade/Screen%20Shot%202022-11-02%20at%204.55.50%20PM.png)  ![tech.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1ca00381-a813-4458-94cb-2f0f58a0211b/tech.png) Please follow the dates in the syllabus. I will update the individual submissions later tonight. 
https://campuswire.com/c/G984118D3/feed/331 Question on Document length normalization The BM25 TF transformation with Document Length normalization formula is like this:  $$f(q,d)=\sum_{w \in q \cap d}c(w,q) \frac{c(w,d)(k+1)}{c(w,d)+k(1-b+b\frac{|d|}{avdl})}log\frac{M+1}{df(w)}$$  I wonder why it is not like this: $$f(q,d)=\sum_{w \in q \cap d}c(w,q) \frac{c(w,d)(k+1)}{(c(w,d)+k)(1-b+b\frac{|d|}{avdl})}log\frac{M+1}{df(w)}$$  Why is the normalizer only multiplied to the $$k$$, the denominator, not the whole score function $$f(q,d)$$ itself?From a review [here](https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf) in page 360, I feel that the direct reason behind the first formula is, $$B=1-b+b\frac{|d|}{avdl}$$ is a factor to directly normalize $$c(w,d)$$, not the TF-transformed result $$\frac{c(w,d)(k+1)}{c(w,d)+k}$$. So the middle term is originally $$\frac{\frac{c(w,d)}{B}(k+1)}{\frac{c(w,d)}{B}+k}$$ which is equal to the first expression But I am also expecting a deeper explanation of why choosing this formulation. One thought: if using the second formulation, the upper bound is no longer $$k+1$$.  The deeper philosophy is probably this: document length might be more closely related to $$c(w,d)$$ than TF-transform, so should be applied first, and TF-transform second. Sounds very reasonable. Thank you for a detailed explanation and sharing your thoughts! thank you too for this inspiring question! inspiring discussions
https://campuswire.com/c/G984118D3/feed/809 Exam schedule unavailable ![q.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/45b6f6fd-7df7-4f5d-96d0-c1d4a2d32527/q.jpg)  The exam should be available from Monday October 10, 2022 9:00AM CDT - Sunday October 16, 2022 8:00PM CDT and I think I chose the available period, so does anyone know why my attempt failed?Same here, trying to pick a different date like 16 Oct. Many available spots on 16 Oct. im also having this issue, none of the time slots seem to be available That works! Thanks were you able to see anything for oct 16 now? nothing is showing up for me You need to hit the submit button in the bottom left corner, and then there will be a lot of optional times shown on the right in the afternoon of the 16th #812 I found "next available times" bottom can show more available times than just submitting in the select time section.
https://campuswire.com/c/G984118D3/feed/1094 question about the M step ![Screenshot%202022-10-19%20234143.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c430a1d5-4282-467c-9668-48e18b5b478e/Screenshot%202022-10-19%20234143.png). I try to label the terms in the MP3 code to the equations in the lecture note.  From the equation, I believe the nominator can be obtained by dot product between matrices. and the denominator means the normalization of the product matrix. however, I don't know to match the dimensions of the matrice. It is confusing to me. Is there any good tutorial demonstrating how to convert the arithmetic equation to matrix equation? Thanks!I will try to attend today's office hour. Take a look at this: https://stackoverflow.com/questions/4455076/how-do-i-access-the-ith-column-of-a-numpy-multidimensional-array  For example if you do something like ``` self.topic_prob[:, 0][:, 0] ``` This will give you a 1-D array that contains all the topic probs of the 1st word for the 1st topic among all the documents. This method allowed me to match dimensions of matrices so I could do the dot product TA is  not showing up in the office hour. I just hope TAs can respond to this thread at any convenience. Thanks! Please see the update. I'll start OH at 8PM. Sorry for the inconvenience. If you prefer to use matrix operations, make sure you understand the calculation process clearly and first get yourself familiar with numpy functions such as dot, matmul, *, @, and also take a look at the broadcast. In addition, Abi seems to present a good point below. 
https://campuswire.com/c/G984118D3/feed/659 Tips on MP2.3 submission If you see a score of 1 in the livedatalab MP2.3 Leaderboard and do not see a score on Coursera, make sure to click through the Coursera MP2.3 link again and re-login to livedatalab, do another empty commit or commit with a small change (e.g. add an empty space) to trigger the autograder again. There seems to be a discoonection between the webhook and coursera that logs you out, if you have been working on the MP for some period of time. 
https://campuswire.com/c/G984118D3/feed/1071 Will we be able to see which questions we got wrong for Exam 1? Someone asked this already, but we didn't quite get a reply. I was wondering if we are able to somehow see the official solutions, or know the questions we got wrong for exam 1?  The coursera platform has the exam locked currently.  Also, will we able to know the class average and std deviation?  Thanks!Yeah, I am not able to access the Exam Quiz solutions either, but I think they should be available in some time.  Yeah, I can't see which questions I got wrong. wait for the unlock of exam 1 feedback. Not sure the prof will do this, because otherwise he will have to come up with a different set of questions for next year’s course. This is only my guess. In my experience, professors are reluctant to release an answer key online. Best bet is to attend office hours and see if the professor is willing to discuss the exam questions. I guess we will not be able to see it. still no updates?
https://campuswire.com/c/G984118D3/feed/197 question about MP1 submission -- webhook error code 401 I followed the instructions at https://d18ky98rnyall9.cloudfront.net/wpBeUpALTruQXlKQCx67EQ_e8d6e5e5963d4438b1ace359c0c775f1_mp-setup-final.pdf?Expires=1661990400&Signature=UxD4kdca66TefCjmVQOKTt4jOGuUIuwlb2GuFnPdYZuHnkGI-M~ULOT7RP9rlwb67dfPt-6hq7XiSQoPdNQUzwVYNja8-B~9dgJMLlw4FJk70~aDLxO~einLqCmswWvMQKli5V8kSX437K0zSDhccjAp9UcCUgfmr-VUSYXvmFw_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A  for the MP setup.  I am able to find MP1 in livedatalab ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e5e2726c-1eab-4216-8130-382943d24ce9/image.png)  I believe I successfully cloned the project code to the private repository of my GitHub account https://github.com/yuliuliuliu/MP1_private. And I already finished the required modification on example.py. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/27cb2a3f-2dfe-421f-af8b-cf8ae8c72470/image.png)  I added the webhook with the updated link http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/248c121f-ac06-43c3-a8f6-4aa3e19ba0f8/image.png)  I tried to git push from my local folder to submit my result.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4ec8b640-16b4-43ae-9aef-bfdd6b696363/image.png)  However, I got the following error message as below ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ad89e1e9-eede-4f2a-8b92-a6faee1754c1/image.png)  I cannot find any submission log on livedatalab either. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/544fd16c-e672-44b5-8600-f271d0d4d7aa/image.png)  Can anyone help? Thanks a lot in advance.You can try to drag and commit your py file to your Github repo directly. Can you elaborate on it a bit more? I tested the connection between my local file and GitHub remote repo, and it works. If I push something from my local folder, the GitHub repo can reflect the change. I think the problem is the connection between my GitHub and webhook (livedatalab), and I don't quite understand the error code 401, does it mean the webhook cannot find my git account?.... Is there any way I can directly submit my code to livedatalab? Hello, May I ask you if you generated a github token for your livedatalab and set your github username and token into your livedatalab? Looks like your livedatalab can not connect to your Github. How is your webhook set-up? Could you share a screenshot? fantastic! I relinked my github account with PAT and it works! (I don't think I did this step wrongly before because I just followed the instruction and entered my username and PAT. but never mind since it works now) Thank you so much for your help! Now I am able to see my submission on livcdatalab. The screenshot below means I passed the MP1, right? Thanks! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6a5f57ae-d3ac-402d-a30d-8db98a75f0c0/image.png) Thanks for asking. Dajun solved my problem. I relinked my github with PAT on LiveDataLab and I am able to submit my code now. Thank you all the same. 
https://campuswire.com/c/G984118D3/feed/569 MP2.4 baseline score where can we find baseline score?We are still working on to set the baseline score. It is not yet shown on LiveDataLab. Sorry for that! The baseline score is now set to 0.4170671141624618 (per: #583)
https://campuswire.com/c/G984118D3/feed/14 Access issue Hello Team, good morning! I'm sorry, not sure if there's a problem with what I'm doing - but I still don't see the course in my Coursera page nor does the link work. The account is on my Illinois account only.  Please feel free to suggest some tips.It might take some days after you register for the course before it appears on your home page. If you registered a long time ago probably emailing Coursera could be a good idea. Hey, Thanks. I'll be emailing them. I registered for this course at least 2-3 weeks back.  I sent an email to uiuc coursera suppot team. They told me that there is a technique issue about account linking invitation. And they are fixing it now. They also told me to wait patiently util they fix it. I didn't see the course in my Coursera either. Wait to see if it can be fixed in the next few hours.  I had the same issue but after emailing the support team and waiting a couple of hours it was resolved and I can now access course content. I believe they're working to resolve the issue on their end. I contacted the staff and they said it will need 24 - 48 hours for them to add me to the course.
https://campuswire.com/c/G984118D3/feed/821 Week 7 Practice Quiz ![Screen%20Shot%202022-10-05%20at%204.27.14%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/406f8f25-0a85-4def-a984-367fb3850259/Screen%20Shot%202022-10-05%20at%204.27.14%20PM.png)  I am getting an incorrect answer in the practice quiz of week 7 where it is talking about opinion mining/sentiment analysis. According to slides, I feel this is the correct answer, am I missing something here ? I'm attaching a specific slide that is particularly relevant for this question:  ![Screenshot%20from%202022-10-05%2017-38-50.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cb346cda-401b-4878-9959-0cf161fdd44c/Screenshot%20from%202022-10-05%2017-38-50.png)  This table nicely summarizes the various text representations and how they are used. I also had this same confusion, until I looked for `sentiment analysis` specifically on this slide  it also makes sense thinking about it. in a text conversation, what is the most important thing that gives us context about sentiment? is it structures or the way someone says something? is it the entities/relations involved in the text? or is it the words themselves, which tend to have their own connotations? This helps! Thank you! I respectfully disagree on this question. I feel "words and syntactic structures" should be the answer, because at least we need to know what object that sentiment is associated with.
https://campuswire.com/c/G984118D3/feed/1285 How flexible are tech review topics? As the title asks, how (un)related a valid review topic can be to the course. I'm interested in machine translation, so I'm wondering if I can choose something related to that despite it not being related to the course content aside from being under the NLP umbrella. Topic example would be understanding the current limitations in machine translation modelsSure, general AI/ML under the NLP umbrella is related to the course!  Awesome, thank you!
https://campuswire.com/c/G984118D3/feed/730 Webhook ping fail Github web hook failed to connect with LiveDataLab. Reconnect Github account with new token also no luck. Any suggestions? Thanks! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d9e7d999-63d2-4fea-b218-91e01a84eba6/image.png) web hook settings ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d398a71e-1949-4400-a5d1-2572fbda6a8b/image.png)Xintei, after generating your new token, please try deleting the webhook and creating a new one. yep that's what I did, but still... Did you also delete linked accounts in LiveDataLab, and then link new account?  yep, delete->new token->link->delete web hook->new web hook When I had a similar experience, I believe that I added the webhook before linking. nah no luck LOL, but really appreciate your replies My only other thought is to recheck the webhook setup for typos (like the now-infamous 'livelab' vs. 'livedatalab'), to confirm content type is 'application/json' (which is not the default), and to be sure that 'just push' and 'active' are checked.  livelab or livedatalab? I used the one from pdf http://livelab.centralus.cloudapp.azure.com/api/webhook/trigger ha, u know what, YOU are right. it should be http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger PDF has wrong URL LiveDataLab, definitely. No such website as livelab. That document has a typo. not sure how I got it working previously Thanks man! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/771fadd5-f73a-483b-b30e-e0b86fcba752/image.png) I had the same problem.  I had to unlink my github account with my livedatalab account, generate a new token and then relink my github account with the new token.  I used the delete linked accounts button in the upper right hand corner of the livedatalab screen.  Be careful though, it doesn't ask you to verify your decision!  Just deleting the webhook alone didn't work for me.  I also picked 90 days for the lifetime of the token. found the reason, the LiveDataLab link in PDF is wrong
https://campuswire.com/c/G984118D3/feed/76 MP1 grading My submission for MP1 shows a green submission along with this: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/17fa5886-9706-4ad5-9025-8f03e6a1cb2e/image.png)  However, my "score" for the leaderboard is still 0. Does this mean that I did not pass some tests?  You can also check the result on Coursera Grades. It’s quite strange. You can also check your logs generated on LiveDataLab.  If you have less than a 1 on the leaderboard, you didn't pass all the tests. If you have a 0 you didn't pass any of the tests. This [page](https://www.coursera.org/learn/cs-410/gradedLti/zbBWu/mp1) says: For this MP, if you see a score "1" on the Leaderboard on LiveDataLab,  it means that you have successfully completed the MP and should see a 100% grade on Coursera.  same issue happened. It said build success but score is 0?
https://campuswire.com/c/G984118D3/feed/129 About language model Just want to confirm: Is the **maximum likelihood estimate** a (special kind of) **language model**? Usually the ML needs to be smoothed before it can be used as a LM.   Why I ask this question: In lesson 5.3 at 3:04, professor Zhang said that the **P seen (w | d)**  is the document language model.   I would like the instructor or TA to confirm but I am also open to your suggestions. Thank you in advance! The definition given in the textbook best explains what a language model is - A language model is defined as a probability distribution over word sequences. When we have a language model, we can sample word sequences based on the given probability distributions. If we assume a 'unigram language model', we need to calculate p(w|D) for all the words. Maximum Likelihood Estimation is a way to calculate these probabilities for the language model by making the probabilities of the words relative to the frequency of the words in the observed data (i.e, the document D). I would suggest looking at section 3.4 (Statistical Language Models) of Chapter 3 of the textbook.  Thank you! This answered my questions.
https://campuswire.com/c/G984118D3/feed/162 Status: Failure in LiveDataLab Hello, for MP2.1 Autograder, I notice that I keep getting a Status:Failure for the autograder run.  ``` Beginning execution...  DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.  Defaulting to user installation because normal site-packages is not writeable  Collecting metapy  Downloading metapy-0.2.13-cp27-cp27mu-manylinux1_x86_64.whl (14.3 MB)  Collecting pytoml  Downloading pytoml-0.1.21-py2.py3-none-any.whl (8.5 kB)  Collecting tqdm  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)  Collecting requests  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)  Collecting glob2  Downloading glob2-0.7.tar.gz (10 kB)  Collecting importlib-resources; python_version < "3.7"  Downloading importlib_resources-3.3.1-py2.py3-none-any.whl (26 kB)  Collecting idna<3,>=2.5; python_version < "3"  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)  Collecting certifi>=2017.4.17  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)  Collecting chardet<5,>=3.0.2; python_version < "3"  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)  Collecting urllib3<1.27,>=1.21.1  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)  Collecting typing; python_version < "3.5"  Downloading typing-3.10.0.0-py2-none-any.whl (26 kB)  Collecting singledispatch; python_version < "3.4"  Downloading singledispatch-3.7.0-py2.py3-none-any.whl (9.2 kB)  Collecting contextlib2; python_version < "3"  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)  Collecting pathlib2; python_version < "3"  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)  Collecting zipp>=0.4; python_version < "3.8"  Downloading zipp-1.2.0-py2.py3-none-any.whl (4.8 kB)  Collecting six  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)  Collecting scandir; python_version < "3.5"  Downloading scandir-1.10.0.tar.gz (33 kB)  Building wheels for collected packages: glob2, scandir  Building wheel for glob2 (setup.py): started  Building wheel for glob2 (setup.py): finished with status 'done'  Created wheel for glob2: filename=glob2-0.7-py2.py3-none-any.whl size=9308 sha256=d4aecf1d5f3023962f25c149b6562aa845e43ab4c58de18b660a1550d115f297  Stored in directory: /home/livelab_jenkins_user/.cache/pip/wheels/3e/a4/8e/d0e602135b46828fbd59fbb2e394bda746cb96d99487cc2a24  Building wheel for scandir (setup.py): started  Building wheel for scandir (setup.py): finished with status 'done'  Created wheel for scandir: filename=scandir-1.10.0-cp27-cp27mu-linux_x86_64.whl size=11146 sha256=b4fb74d7011e73a0918ad2dac3b623d1b0a72b783c620c673a2cdd6e97d63d4e  Stored in directory: /home/livelab_jenkins_user/.cache/pip/wheels/58/2c/26/52406f7d1f19bcc47a6fbd1037a5f293492f5cf1d58c539edb  Successfully built glob2 scandir  Installing collected packages: metapy, pytoml, typing, six, singledispatch, contextlib2, scandir, pathlib2, zipp, importlib-resources, tqdm, idna, certifi, chardet, urllib3, requests, glob2  WARNING: The script tqdm is installed in '/home/livelab_jenkins_user/.local/bin' which is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  WARNING: The script chardetect is installed in '/home/livelab_jenkins_user/.local/bin' which is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  Successfully installed certifi-2021.10.8 chardet-4.0.0 contextlib2-0.6.0.post1 glob2-0.7 idna-2.10 importlib-resources-3.3.1 metapy-0.2.13 pathlib2-2.3.7.post1 pytoml-0.1.21 requests-2.27.1 scandir-1.10.0 singledispatch-3.7.0 six-1.16.0 tqdm-4.64.0 typing-3.10.0.0 urllib3-1.26.12 zipp-1.2.0  File "mp2_grader.py", line 153, in <module>  grade()  File "mp2_grader.py", line 126, in grade  resp = requests.post(get_mongo_url , json={'collection':'faculty_bios','query':{'email':{ '$ne':email}}})  File "api.py", line 117, in post  return request('post', url, data=data, json=json, **kwargs)  File "api.py", line 61, in request  return session.request(method=method, url=url, **kwargs)  File "sessions.py", line 529, in request  resp = self.send(prep, **send_kwargs)  File "sessions.py", line 645, in send  r = adapter.send(request, **kwargs)  File "adapters.py", line 519, in send  raise ConnectionError(e, request=request)  ConnectionError: HTTPConnectionPool(host='livelab.centralus.cloudapp.azure.com', port=80): Max retries exceeded with url: /api/mongodb/getrec (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f823c5515d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',))  Build step 'Execute shell' marked build as failure  Finished: FAILURE ```  What could I do to fix this?I think the TAs are still working on some updates for MP 2.1 (#62) This may have to do with the fact that MP2.1 hasn't been unlocked in Cousera yet.
https://campuswire.com/c/G984118D3/feed/662 MP2.3 Status failure, No logs available yet. Your build may still be starting up. Anyone see this in LiveDataLab? I already have an extension, but this is all I am seeing...![fail.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e7e7a202-befd-4f9d-8b5b-284f7969a666/fail.PNG)This happened to me because my personal access token for my github account expired. To resolve,  I deleted my linked account in LiveDataLab, generated a new personal access token, and then re-linked my github account with the new access token. After that I pushed again and it worked. Hmm...I thought I went through that, but am willing to try anything at this point.  There is a lot of room for bugs in these programs.  Thanks!
https://campuswire.com/c/G984118D3/feed/751 Exciting Digital Library Update! Hi everyone,  We have also some very exciting changes to the CS410 Digital Library. It is now called **The Community Digital Library**. This is because we have added functionality to support group creation, where only members of the group can see and search over submitted content. You all should have been automatically added to the "CS410 Fall 2022" community, and to "Your personal community" where you can make private submissions. Please check out the [updated overview document](https://docs.google.com/document/d/1o8jHEO8xYtfbqThIT8afnRQ7vdfxZ7WqdmvlNxyQ_wI/edit?usp=sharing) for a complete description of the feature changes, including how to create and join communities. **To use the new features, you'll also need to re-download the Chrome extension** (instructions in the linked overview document, should take <5 minutes).  We envision that this community update will allow you to create communities for your project group, your personal archive, other classes, research labs, or any other scenario that you find helpful!  And as always, if you run into any bugs or have any suggestions, please let me know.  Thanks, Kevin
https://campuswire.com/c/G984118D3/feed/93 Make questions about quiz answers visible only to TAs and Instructors If you are creating a post discussing the quiz answers, please make the post visible only to "Instructors and TAs" in the visibility settings (when you are posting, you can see a dropdown arrow beside "Post to everyone" in which you can select "Post to Instructors and TAs"). Other students might still be waiting to attempt the quiz. If you are asking for a clarification of a question, it should be fine to post it to everyone, as long as you are not discussing the possible solutions in the post.
https://campuswire.com/c/G984118D3/feed/346 MP2.1 Access Issue Hi All, I am trying to access MP2.1 google link and running into issues. Can someone verify what they did to access it. ThanksI did.  There were a few things, like making sure that I had liked my google account to my Illinois account, etc.  I then set up the VPN.  For some reason I couldn't do this on my ubuntu box, but it worked just fine on Windows.  I have been playing with it.  Where are you stuck?  I run into the You need access google doc page. I tried with my google as well as Illinois account. How do I setup VPN?  I connected to the VPN and hit the request access button, did you have to go through the same path and wait for access? Hey Syed,  To access the Google doc page, you do not need the VPN. Everyone with an Illinois email should have access. However, you may need to activate your Google Apps @ UIUC service: https://www.nuestraverdad.com/post/activating-google-apps-uiuc Thanks. This worked That was where I got stuck initially.  In order to access any of the google docs from Illinois, you have to link your account.    Maybe this helps: https://www.nuestraverdad.com/post/activating-google-apps-uiuc   Thanks I got it figured out using this
https://campuswire.com/c/G984118D3/feed/943 Q3 question Why is this false?![Screen%20Shot%202022-10-13%20at%209.07.04%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7fe9f757-d315-4cb8-b119-777dc6eb3016/Screen%20Shot%202022-10-13%20at%209.07.04%20PM.png)You can take a look at this post #373 thanks, I understand now! F = 2PR/P+R . putting the numbers to the equation the R is coming to 1.5, which cannot happen. The ideal system recall = 1
https://campuswire.com/c/G984118D3/feed/956 How could we check our output gives reasonable result? As title. Could anyone give some hints or suggestions? I think I already calculated the likelihood after the EM steps, but I wonder how I could reasonably check my outputs.  ThxI can suggest two ways to check: 1. Consider the range of log likelihood and check whether your output likelihood at each iteration satisfies or not.  2. Run multiple times and check whether it will converge most cases. (the abs diff of two continous likelihoods is decreasing) Yes, I think it will converge. Because I noticed in the problem statements it mentioned like first 5 topics are related to "Chicago", I am wondering if there are any available methods for us to know our results represent what kind of topic. You may need to output the words with high generation probabilities for each topic to get a sense of the corresponding topics.
https://campuswire.com/c/G984118D3/feed/795 Do we have MP4?  On the Course Deadlines overview in week1, it says we have MP4, but inside the grades the last MP is MP3.   Also, what would be the timeline for MP3? On the overview and grades, it says deadline is 10/23, in week 7, it says available from 10/17-12/31. We are currently unsure whether there will be a MP4. Please wait for relevant post if there will be.   MP3 is due on 10/23, after which you can still access but get late penalty.  But MP 3 is still not available. Can we have MP 3 relased earlier so that we have more time to work on. We don't want another MP. Please don't please. There will be a MP4. Hi Yuxiang, is MP4 going to be similar workload as MP3? It's hard to say. My feeling is that the workload of MP3 isn't large. The core is to understand PLSA, which is covered by the course content.   MP4 will require knowledge not from the course. You may need to get familiar with advanced NLP knowledge, but the coding shouldn't be hard.  How much percentage of grade would MP4 be accounted for? Thanks It hasn't been decided yet.  Will relevant information about "knowledge not from the course" be provided in a way that we can digest without knowing other NLP or machine learning techniques? It does not seem fair to have such an MP if it requires something that is considered advanced but not covered in this course.... I agree, especially when we are doing a technology review and have a project going on at the same time, and an upcoming exam as well. Sorry that I have mentioned MP4 in an informal way. For any detailed information, please wait for the formal announcement. If you have further request, feel free to make a post once MP4 is released. 
https://campuswire.com/c/G984118D3/feed/916 Probability of generating phrases using unigram LM You are given a vocabulary composed of only three words: "text," "mining," and "research." Below are the probabilities of two of these three words given by a unigram language model:  text: 0.4 mining: 0.2  What is the probability of generating the phrase "text mining research" using this unigram language model?  The answer to this question is 0, which I assume is because we are not provided the probability of the word "research".  However, for this question: You are given a vocabulary composed of only four words: "the," "computer," "science," and "technology." Below are the probabilities of three of these four words given by a unigram language model.  the: 0.4 computer: 0.1 science: 0.2  What is the probability of generating the phrase "the technology'?  The answer to this is 0.12, because we assume the word technology takes on the probability of 0.3 (since 1 - 0.4 - 0.1 - 0.2 is 0.3)?  There might be some logic I'm confused about. I am just unsure why the answer to the first question is 0 and the answer to the second is not 0 (since "technology" probability is not provided). Any clarification would be appreciated. Thanks!You can obtain the probability of "research" by inspection because it says the vocabulary only contains 3 words, and we know the probability for all words in the vocabulary must sum to 1.   The answer provided above for the Practice Quiz 4 question in incorrect.  Use the same technique you describe for the Quiz 4 question and you can determine the correct answer to the practice quiz. We are given that vocabulary composed of only **4** words: "the," "computer", "science",  "technology." Sum of probability of all the words in the vocabulary= 1  So probability of "technology" =  1- probability of ("the" ,"computer", "science") = 1- (0.4+0.1+0.2) = 0.3  Oh I see now, that was my mistake. Following the quiz 4 logic, should the answer to the first question really be 0.4 * 0.2 * 0.4? Yes, that's correct. The probability of “research” = 1 – P(“text”) - P(“mining”) = 1 – 0.4 – 0.2 = 0.4. So, the probability of generating the phrase “text mining research” = P(“text”) x P(“mining”) x P(“research”) = 0.4 x 0.2 x 0.4 = 0.032, not zero.
https://campuswire.com/c/G984118D3/feed/275 Is it possible to have MPs due on the same day of the week? I noticed MP1 is due on a Sunday, but subsequent ones are due on Saturdays.  In order to allow for better planning/preparation, could I request that MPs be due on the same day of the week (Sunday, for instance)?  Thanks.I agree. If MPs are all due on Sunday, it will be very helpful.  same here, and I think most students have full time job and can only spend most of the time on the homework on weekends, it would be more reasonable to have the deadline on Sunday Ok! They should all be due on Sunday. The due dates are subject to change as we haven’t released them. We will release MP2.1 next Monday. 
https://campuswire.com/c/G984118D3/feed/1205 MP3 Coursera Grade Not Updating ![2022-10-25%20at%2003.38.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a9ff3a5f-d1ac-436d-bd41-aee25d2d7ad5/2022-10-25%20at%2003.38.png)  I know this question was asked twice, but conflicting answers were given for them. One (#1162) of the answers said as long as I have a one on the leaderboard, I will be fine. But the other (#855) said I need to further improve my code. Which one is correct?   My NetID: rw17 DataLab username: shanengysAs per my understanding, as you have one in leaderboard you are good. May be TAs can confirm that your submission is successful. Score 1 should be ok. 
https://campuswire.com/c/G984118D3/feed/1186 Proposal peer review Are there instructions for the peer review?https://campuswire.com/c/G984118D3/feed/1187
https://campuswire.com/c/G984118D3/feed/903 Wasn't able to start my exam  Hi, I wasn't able to start my exam with the proctor. Anybody can help?   The proctor key in the password several times but wasn't able to get me unlocked to the exam. Issue resolved after I logged out and logged in for a new proctor. 
https://campuswire.com/c/G984118D3/feed/5 1-hr for each quiz? Hi Professor and TAs, Interested to know if each quiz has a one-hour time limit.  Does it mean there will be two attempts, each taking one hour?  Thanks!  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/46ac8e58-b2bf-4373-b828-0129a9025a0d/image.png)Just basing off of the orientation quizzes. You should get an hour per attempt on the quiz, and I believe you can end it early if you finish. I'm not a TA though so maybe someone else can answer but that's what I'm expecting. I think the 1h is just an estimate and not a hard limit, I just took the quiz and there was no timer ticking. And yes it's two attempts. From Coursera - Syllabus Section :  **Quizzes**. Most weeks will include one for-credit quiz. You will have two attempts for each quiz, with your highest score used toward your final grade. Your top 10 quiz scores will be used to calculate your final grade (i.e., we will drop the two lowest quiz scores). It is an estimate of the time we need to finish the quiz so that Coursera can calculate the total approximate workload of each week based on this information.  Typically, you can complete one attempt in time much less than an hour (you can just consider 1 hr as the maximum time for an attempt) because the quizzes are short. As already mentioned by others, you will get two attempts.  Hi Harita, if I am unable to finish an attempt within an hour, will it be considered a failure? The 1 hr is just a time estimate entered on Coursera. I do not think we have set a time limit for a given attempt (I believe there is no timer that gets activated when you start the quiz).
https://campuswire.com/c/G984118D3/feed/1296 Request Peer Review I do not see any assignment that need to be reviewed. Can I get some of the reviews please?Are you sure you accepted the link sent by email and switched to reviewer mode? Yes, I just accepted the link. There are nothing after I switched to reviewer mode. I think TA need to assign reviews manually? Make a request at https://campuswire.com/c/G984118D3/feed/1187 to Kevin Ros (TA) Correct, they're assigned manually by TAs after you accept the reviewer role. i had to toggle the "author/reviewer" button once to see the submissions for peer review. the page is loaded as "author" by default, but my role is displayed as "reviewer". not sure why.
https://campuswire.com/c/G984118D3/feed/1021 ProctorU Firefox Extension is down Heads up that the extension is currently broken, and they didn't alert me until I had already logged in to start the exam. The recommendation was to use Chrome, so that's what I did, for whatever that's worth.
https://campuswire.com/c/G984118D3/feed/1061 File encoding for MP3 Testing with DBLP.txt at first generated this error message: ``` UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 630: character maps to <undefined> ``` Manual inspection shows that the data file does contain some unusual characters. Opening the file with UTF-8 encoding removed the error message, and did not create any issues with test.txt. However, the change seems to cause this LiveDataLab error: ``` 'encoding' is an invalid keyword argument for this function ``` Has anyone else encountered this or a similar issue?yeah it was building with python 2.7 so I used io open and passed encoding like this. This cleared the unicode errors.   import io   with io.open(self.documents_path, encoding='utf-8', errors='ignore') as fh: Use io module and use its parameter to open the file. Version incompatibility Charles and Sri, thank you very much for calling out this incompatibility between Python 2.7 and 3.x. I had not realized previously that LiveDataLab's use of Python 2.7 could affect coding of file open.  
https://campuswire.com/c/G984118D3/feed/236 Output MP1 What is the expected output look like for MP1? I am not sure why testing on LiveDataLab failed.  Edit: Solved Now. I asked this question because I set up the LiveDataLab earlier last week. When I just submitted the MP1, there was no logs. #203 said that we need to reconnect again, which help to solve the problem. I think the test on LiveDataLab calls the tokens_lowercase function and compares the output with what is expected. +1 to the other message, it might also be helpful to ensure your stemming and trigrams have been instantiated correctly for your token stream. 
https://campuswire.com/c/G984118D3/feed/661 Significance test - write as float or read as float I can only write as string in a file in bm25.text and inl2.text. The avg_p are float numbers. When I am trying to read it and save in a list , I am getting an error while typecasting to float. We will have to convert the avg_p into float to run on the stats.ttest_rel() function , right ?  Is there a better way to read as float the avg_p and store in an an array/list. ?You can use numpy array to do that. like Create a numpy array:   result_inl2=np.array([],dtype='float') and put all your avg_p in it. You can do something like this For array conversion: ``` flt_avg_p = [float(x) for x in str_avg_p] ```  For reading from file you will need to process each line and then append to a new list like below: ``` inline = []  for line in file_read:   inline.append(float(line)) ``` i think it's easier if you run the two ranker in one test, and just append them into an array [ ] when you calculate them, instead of reading from the .txt Thank you !!
https://campuswire.com/c/G984118D3/feed/787 Exam 1 * Just double-check,  1. the exam 1 actually takes place in the Coursera in the week 7 session? 2. we still need to log into proctor U before the test? Thanks! Per the course academic calendar, exam 1 is available to be scheduled through ProctorU between: Mon., Oct 10, 9:00am CDT and Sun., Oct 16, 10:00pm CDT. The proctor will direct the students to take the exam on Coursera. All operational/technical information regarding the exam is available as reading in Coursera on week 7 [here](https://www.coursera.org/learn/cs-410/supplement/GlP57/how-to-schedule-and-take-the-exam).
https://campuswire.com/c/G984118D3/feed/922 How to make sure the proposal will be approved ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0da36796-dcf2-49de-a291-21d0a333a679/image.png)  Might be related : https://campuswire.com/c/G984118D3/feed/749Sorry for the late reply - generally, as long as it is related to the CS410 topics, it will be approved. Please feel free ask any questions about specific topics that you may be considering, and I can give you a more specific answer.
https://campuswire.com/c/G984118D3/feed/50 Unable to submit MP2.1 Hello Professor and TAs, my MP2.1 submission is failing. Totally understand if the grader is not yet set up, just want to ensure I'm not doing something wrong and bring visibility to this issue. Thank you!  ```/home/livelab_jenkins_user/.local/lib/python2.7/site-packages (from pathlib2; python_version < "3"->importlib-resources; python_version < "3.7"->tqdm) (1.10.0)  File "mp2_grader.py", line 153, in <module>  grade()  File "mp2_grader.py", line 126, in grade  resp = requests.post(get_mongo_url , json={'collection':'faculty_bios','query':{'email':{ '$ne':email}}})  File "api.py", line 117, in post  return request('post', url, data=data, json=json, **kwargs)  File "api.py", line 61, in request  return session.request(method=method, url=url, **kwargs)  File "sessions.py", line 529, in request  resp = self.send(prep, **send_kwargs)  File "sessions.py", line 645, in send  r = adapter.send(request, **kwargs)  File "adapters.py", line 519, in send  raise ConnectionError(e, request=request)  ConnectionError: HTTPConnectionPool(host='livelab.centralus.cloudapp.azure.com', port=80): Max retries exceeded with url: /api/mongodb/getrec (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f14a84355d0>: Failed to establish a new connection: [Errno 101] Network is unreachable',))  Build step 'Execute shell' marked build as failure  Finished: FAILURE```I got the same error when submitting MP 2.1 Thank you for the information. Indeed, we are still testing it and will release it a bit later after we finish the testing. We may also use an alternative way to collect content in MP2.1. For now, to avoid any confusion, we have tentatively locked MP2.1 (as well as many other future tasks) so that you can concentrate on learning the Week 1 materials and finishing MP1. 
https://campuswire.com/c/G984118D3/feed/588 MP 2.4 Question This might be a dumb question but I just want to understand the requirements for MP 2.4. Do we need to implement the function so that it will find the best values for k1, b, and k3? Or we just need to test it manually? I have made 19 submissions right now and the best overall score still looks horrible. Please see the response from TA below (I post my question to TA&Instructor only) :  "We just found a possible error in the uploaded faculty data set, which we think is causing low scores, and we plan to resolve it shortly. Moreover the 0.7 baseline is a bit outdated, as it refers to last year's baseline. So once the data set is updated, we will run a baseline and provide everyone with an update."  Hope it helps. Thanks for the update. I was concerned seeing the current leaderboard. Not sure why the baseline would use last year's faculty data since we generated an entirely new dataset. Thanks for letting me know Hi all, please refer to the pinned post on MP2.4, we've lowered the baseline score and updated the dataset. You should be able to score higher now. 
https://campuswire.com/c/G984118D3/feed/651 error with submission ('ERROR: ', AttributeError("'module' object has no attribute 'log2'",)) My feedback showed me this, how should I fix that?livedatalab does not support log2. try log(x,2) Thanks! python 2.7 is used by autograder and it does not support log2() .
https://campuswire.com/c/G984118D3/feed/1209 metapy for python 3.9.x If anyone is feeling adventurous and using metapy for the course project, I found a fork of metapy that supports newer versions of python.  https://github.com/tajshaik24/metapy  I was able to install it and successfully run the MP2.4 project with it on python 3.9.12, Ubuntu 20.04.4 LTS. I had to install cmake first.  ``` sudo apt-get install cmake ``` To install, you'll just need to add the latest commit sha onto your pip install command.  ``` pip install 'metapy @ git+https://github.com/tajshaik24/metapy@06380750eb20761de629563db826c3fab26458b6' ```  ``` (base) ➜  CS410_MP2.4_private git:(main) python search_eval.py config.toml Building or loading index... Running queries NDCG@10: 0.3624443289858917 Elapsed: 0.1597 seconds ```
https://campuswire.com/c/G984118D3/feed/480 MP 2.3 Has MP 2.3 been released?  I can't find anything about it anywhere.  Thanks!!It is available on Coursera. https://www.coursera.org/learn/cs-410/gradedLti/026Fl/mp2-3 GitHub link: https://github.com/CS410Assignments/MP2.3 LiveDataLab link: http://livedatalab.centralus.cloudapp.azure.com/project/view/535442415 Yes. Week5 of coursera. On the GitHub page it says it is due this week. That must be inaccurate right, since MP2.2 is due this week. Just wanted to confirm In Coursera MP 2.3 is due next week.  I had the same question, but it pretty unambiguously says 9/25 on Coursera.
https://campuswire.com/c/G984118D3/feed/723 MP 2.3 grade incorrect in Coursera Hi,  I noticed my grade for MP 2.3 is still at a 70% from my initial submission with the correct equation implementation. I had later finished the significance.txt portion (before the due date for full credit), while livedatalab updated to a 1 score instead of 0.7 coursera still reflects that 0.7. Is this the case for anyone else? Do I need to be concerned or is the system just delayed by update?  Any thoughts on this would be great! Thank you.try use coursera to open the link again and make a minor change and push again  Similar problem here, I just get overdue. Same here. Completed with full score on livedatalab last weekend, but no score on Coursera. Apparently my github token expired so I had to go through the gymnastics of token/webhook/livedatalab linking etc. I've made some trivial changes and pushed a bunch of times but have low confidence at this point. Does anyone know how quickly grades are reflected on Coursera if successful?
https://campuswire.com/c/G984118D3/feed/899 E step self.topic_prob shape? What should be the shape of the self.topic_prob in the E step part of the MP?it should be `[self.number_of_documents, number_of_topics, self.vocabulary_size]`. see the original `plsa.py` line 178 for reference.
https://campuswire.com/c/G984118D3/feed/208 Different Compression Encoding I found the textbook seems to define the encoding methods differently from the slides. (Is it based on different unary encoding?)   For the quiz, TA Assma let me know we should use the one in the slides (Thank you!) But which one is the general convention we should follow? Or is it based on different use cases etc  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/280f623e-f6bd-4935-9271-ab9019e835a4/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0ea13ab4-25da-4df9-8c78-b1fce7d3cdae/image.png)It seems to be use case specific. The size of the compressed outputs of these methods vary so that could be possible one criteria to be considered. This is a superficial difference due to the difference in unary coding where we could switch the roles of 0 and 1.  That is, a unary code can be either many 1's followed by a "0", or many 0's followed by a "1". In the slides, we used the former, whereas the book used the latter. In practice, the decoding algorithm would work in exactly the same way in both cases and the compression performance is also identical.  For all the quizzes and exam questions, please use the convention adopted in the lecture slides/videos. 
https://campuswire.com/c/G984118D3/feed/1044 Ask for any insights in MP2.4? I am curious about how other classmates achieved good scores in MP2.4 and what they learned from this parameter tuning.  Are we allowed to share parameters, codes, or any insights here, as the deadline has already passed?My score is not that good, this is how I did it: 1. Choose the right ranker, it seems BM25 is the best out of them. 2. Parameters tuning, start from the default value, tuning one parameter at a time, start from large step, submit, check the result, increase/decrease the parameter according to the feedback. 3. I feel that there are several local maximums, if you find (or guess) the global maximum at the first time , you are lucky. I used Okapi BM25, as it seemed like a good starting point. I implemented a grid search algorithm, trying all combinations of   b = [0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]  k1 = [1.2, 1.24, 1.28, 1.32, 1.36, 1.4, 1.44, 1.48, 1.52, 1.56, 1.6, 1.64, 1.68, 1.72, 1.76, 1.8, 1.84, 1.88, 1.92, 1.96, 2.0]  k3 = [0.0, 50.0, 100.0, 150.0, 200.0, 250.0, 300.0, 350.0, 400.0, 450.0, 500.0, 550.0, 600.0, 650.0, 700.0, 750.0, 800.0, 850.0, 900.0, 950.0, 1000.0]  While waiting for this to run, I decided to submit by using Okapi BM25's default values of b = 0.75, k1 = 1.2, and k3 = 500. This beat the threshold.   I eventually got some hyperparameters that resulted in the highest nDCG. These parameters lead to slightly better results on the test data. Interesting, I did not beat the threshold by using default Okapi BM25 value. Thank you so much for your sharing! Thank you so much for your sharing! In retrospect, I found that I learned next to nothing from this. I suppose this MP was meant to be a demonstration in the fine-tuning of hyper-parameters? Though, as we are all data scientists, this is likely something we have all done multiple times in order to achieve best performance. This MP seemed like an unnecessary program after MP2.3. 
https://campuswire.com/c/G984118D3/feed/1271 Tech Review Citations What citation format should be followed for our sources? MLA? If so, which version? APA?  Also, do we have to cite things we have learned from this and other classes? How would we do that? > What citation format should be followed for our sources? MLA? If so, which version? APA?  The citation format does not matter, you can use whatever you prefer.  > Also, do we have to cite things we have learned from this and other classes? How would we do that?  No, for the purpose of this tech review, you do not need to cite content that you've learned in class.  Is Wikipedia an acceptable source? Generally, I would avoid citing Wikipedia. Instead, you can begin with their references. 
https://campuswire.com/c/G984118D3/feed/1139 calculate_likelihood Hello, I'm a little confused about what should we return for the function `calculate_likelihood`. From the problem description, we only need to "Append the calculated log-likelihood to self.likelihoods".when you call `calculate_likelihood`, you will calculate the current log-likelihood of the model (given the current probability matrices). this value (the current log-likelihood of the model) should get appended to `self.likelihoods` as you suggested. i'm not sure if a return value is strictly speaking necessary, but you will need some way to calculate the difference between the likelihood at the current step and the previous step, to see if you have converged (given parameter `epsilon`). for me, i did return the calculated likelihood and that made it simple, but you should just as easily be able to just use the `likelihoods` list to accomplish this As return type is not defined in the problem statement, you can decide what works for your implementation. I have returned the computed likelihood as I then calculate the diff between the current & computed likelihood to compare with epsilon value.  The function is to calculate the likelihood score of the model, and you can find the formula in the lecture. But the exacct value depends on how your implement your model.
https://campuswire.com/c/G984118D3/feed/658 The repo name for mp2.3 in GitHub? I donot know if any repo name works for mp2.3 in GitHub.  I have tried use mp23_private and mp2.3gy_private as name. But it doesn’t work. So I am wondering if I need use mp1’s existing repo mp1_private? Any suggestions are appreciated. Thanks. I believe the correct name should be MP2.3_private I used MP2.3_CS410_private and it worked.  It seems that as long as you have MP2.3_private in the naming structure, you should be fine.  My webhook  was acting funny so I actually played with the naming of my repo.  I deleted my webhook url, saved it, switched pages and came back to try it again and that worked.  I'm not sure if something was cached when the first url was errored out but this worked for me.   Thanks. Will try today.  Thanks Thanks Thanks
https://campuswire.com/c/G984118D3/feed/1227 e step what is e step supposed to do? i know the formula for e step given in lecture, but i am unable to figure out how that translates to the e step calculation here.Luckily, for the assignment we don't have to worry about the background portion of the e-step. That mean's we only need to focus on this formula (which you know from the lecture):  $$p(z_{dw} = j) = \frac{\pi_{dj}^{(n)}p^{(n)}(w | \theta_j)}{\sum_{j' = 1}^k \pi_{dj'}^{(n)}p^{(n)}(w | \theta_{j'})}$$  or, simplifying:  $$\begin{aligned} p(z_{dw} = j) &\propto \pi_{dj}^{(n)}p^{(n)}(w | \theta_j) \\ where \\ \sum_{j = 1}^kp(z_{dw} = j) &= 1 \end{aligned}$$  To describe the above formula in words: the probability of `z` being topic `j` given `d` and `w` is proportional to the document topic probability for `d` and `j` ($$p(z|d)$$ or $$\pi_{dj}$$ given `d` and `j`) times the topic word probability given `j` ($$p(w|j)$$ or $$p(w|\theta_j)$$)  we either get our $$\pi_{dj}$$ and $$p(w|\theta_j)$$ values from the m-step, or initially from our random initialization. from there, it may be helpful to think about what we would do to get the value for a single document, a single topic, a single word. well, then the answer would just be $$\pi_{dj} p(w|\theta_j)$$ (i am dropping the time superscript from here) given that `d`,`j`, and `w`.   #1132 is a great post to help visualize the e-step a little better for scaling beyond just the case of one document, one topic, one word. to be succinct, you are calculating $$p(z | w,d)$$ in the e-step. because of this, it may be helpful to think about what the dimensions of an array representation of this should be? what would a numpy array's "shape" be to hold these values? it may be helpful to think of the "brute force" solution to how you would construct these results. and finally, the last step would be to normalize.  i tried to answer the question directly without exposing too much of the answer. but please let me know if that wasn't clear enough, i am happy to discuss and clarify further
https://campuswire.com/c/G984118D3/feed/583 MP2.4 Update Hi all,  MP2.4 has been released and is available on LiveDataLab.   For the leaderboard, you should be able to see 5 columns: AP Score, Cranfield Score, Faculty Score, Overall Score, and Final Score. The first 3 columns are the NDCG@10 scores for the respective datasets. The overall score is calculated as per the MP2.4 READme ( 0.1* NDCG@10 on APNews + 0.3* NDCG@10 on Cranfield + 0.6* NDCG@10 on Faculty dataset). Finally, in order to score a 1 for the assignment, you will need to *beat*  the baseline overall score (*0.4170671141624618*). If your overall score is higher than the baseline, you should see a 1 here and a 100% on Coursera.   Thanks for your patience and good luck on the MP! 
https://campuswire.com/c/G984118D3/feed/312 MP1 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d934bf99-0096-4c17-885b-36ca2d6881ff/image.png)  Does anyone know why time out?  {'$_CD': 2, 'subtree-(ADVP (RB))': 2, 'subtree-(VP (MD) (RB) (VP))': 1, 'subtree-(SBAR (IN) (S))': 2, 'subtree-(VB)': 2, 'MD_RB': 2, 'RB_VB': 2, 'subtree-(NP (QP))': 1, 'subtree-(PP (IN) (NP))': 1, 'subtree-(VP (VB) (NP) (PP) (ADVP))': 1, 'subtree-(JJR)': 1, 'subtree-(S (NP) (VP) (.))': 2, 'VB_IN': 1, 'subtree-(IN)': 5, 'RB_VBZ': 1, 'IN_JJR': 1, 'PRP_VBD': 1, 'subtree-(CD)': 2, 'subtree-(VBZ)': 1, 'CD_.': 1, 'subtree-(RB)': 3, 'subtree-(VBD)': 1, 'subtree-(S (NP) (VP))': 1, 'subtree-(VP (VB) (SBAR))': 1, 'subtree-(.)': 2, 'VBZ_$': 1, 'IN_$': 1, 'JJR_IN': 1, 'subtree-(VP (VBZ) (NP))': 1, 'VBD_IN': 1, 'RB_.': 1, 'subtree-(QP (JJR) (IN) ($) (CD))': 1, 'subtree-(VP (VBD) (SBAR))': 1, 'subtree-(VP (MD) (ADVP) (VP))': 1, 'subtree-(S (NP) (ADVP) (VP))': 1, 'subtree-(PRP)': 5, 'find': 1, 'VB_PRP': 1, "can't": 1, 'PRP_RB': 1, 'CD_RB': 1, 'subtree-(NP ($) (CD))': 1, 'subtree-(MD)': 2, 'subtree-(NP (PRP))': 5, 'believ': 1, 'subtree-(ROOT (S))': 2, 'PRP_MD': 2, 'subtree-(ADVP (IN))': 1, 'PRP_IN': 1, 'IN_PRP': 2, 'subtree-($)': 2, 'cost': 1} ['$_CD', 'subtree-(ADVP (RB))', 'subtree-(VP (MD) (RB) (VP))', 'subtree-(SBAR (IN) (S))', 'subtree-(VB)', 'MD_RB', 'RB_VB', 'subtree-(NP (QP))', 'subtree-(PP (IN) (NP))', 'subtree-(VP (VB) (NP) (PP) (ADVP))', 'subtree-(JJR)', 'subtree-(S (NP) (VP) (.))', 'VB_IN', 'subtree-(IN)', 'RB_VBZ', 'IN_JJR', 'PRP_VBD', 'subtree-(CD)', 'subtree-(VBZ)', 'CD_.', 'subtree-(RB)', 'subtree-(VBD)', 'subtree-(S (NP) (VP))', 'subtree-(VP (VB) (SBAR))', 'subtree-(.)', 'VBZ_$', 'IN_$', 'JJR_IN', 'subtree-(VP (VBZ) (NP))', 'VBD_IN', 'RB_.', 'subtree-(QP (JJR) (IN) ($) (CD))', 'subtree-(VP (VBD) (SBAR))', 'subtree-(VP (MD) (ADVP) (VP))', 'subtree-(S (NP) (ADVP) (VP))', 'subtree-(PRP)', 'find', 'VB_PRP', "can't", 'PRP_RB', 'CD_RB', 'subtree-(NP ($) (CD))', 'subtree-(MD)', 'subtree-(NP (PRP))', 'believ', 'subtree-(ROOT (S))', 'PRP_MD', 'subtree-(ADVP (IN))', 'PRP_IN', 'IN_PRP', 'subtree-($)', 'cost']  And which one is the expected output?Did it work locally in your code editor ? If yes, then I am assuming it could be related to the server problem. You can try again in few mins.  ['$CD', 'subtree-(ADVP (RB))', 'subtree-(VP (MD) (RB) (VP))', 'subtree-(SBAR (IN) (S))', 'subtree-(VB)', 'MD_RB', 'RB_VB', 'subtree-(NP (QP))', 'subtree-(PP (IN) (NP))', 'subtree-(VP (VB) (NP) (PP) (ADVP))', 'subtree-(JJR)', 'subtree-(S (NP) (VP) (.))', 'VB_IN', 'subtree-(IN)', 'RB_VBZ', 'IN_JJR', 'PRP_VBD', 'subtree-(CD)', 'subtree-(VBZ)', 'CD.', 'subtree-(RB)', 'subtree-(VBD)', 'subtree-(S (NP) (VP))', 'subtree-(VP (VB) (SBAR))', 'subtree-(.)', 'VBZ_$', 'IN_$', 'JJR_IN', 'subtree-(VP (VBZ) (NP))', 'VBD_IN', 'RB_.', 'subtree-(QP (JJR) (IN) ($) (CD))', 'subtree-(VP (VBD) (SBAR))', 'subtree-(VP (MD) (ADVP) (VP))', 'subtree-(S (NP) (ADVP) (VP))', 'subtree-(PRP)', 'find', 'VB_PRP', "can't", 'PRP_RB', 'CD_RB', 'subtree-(NP ($) (CD))', 'subtree-(MD)', 'subtree-(NP (PRP))', 'believ', 'subtree-(ROOT (S))', 'PRP_MD', 'subtree-(ADVP (IN))', 'PRP_IN', 'IN_PRP', 'subtree-($)', 'cost']  This is what I get from local If you have used the same doc content that is provided in the question PM1 , then this result is not correct. I would suggest you to go through the Readme file of the assignment and carefully look at the examples. That will help you ! Read the instructions thoroughly would help you! I think you might miss one or two requirements
https://campuswire.com/c/G984118D3/feed/545 Question about MP2.3 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1ceacf29-e42f-48cc-b262-c64ab0c6f0f5/image.png) When I want to index the data by using the give code in MP Instruction. It has a runtime error that: RuntimeError: config.toml could not be opened for parsing.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6b24610c-788a-4b0a-97af-b2c2f82b7468/image.png) Also, I don't know why some part of original code is unreachable? Is that a problem? Thanks!For  RuntimeError, can check if config.toml is present in the same folder as search_eval.py Yes, they are in the same folder. I think it’s because the system cannot locate config.toml.  Try typing “cd Desktop\2022_fall\CS_410\MP2_private” on the terminal you already opened on VSCode and rerun the code.   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9d1ba500-51fb-462c-8a69-8a59b290bdc4/image.png)  My vs code also shows me that region of code is unreachable when in fact it is and actually does run. Not too sure how to fix that The issue with pylance is that it assumes a context manager (the with statement) does not catch exceptions by default, which makes the code after it appear unreachable even though it is not. https://github.com/microsoft/pylance-release/issues/773  You can wrap the context manager in a try-except block to fix it ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b6845022-127c-4a06-b323-1828028c8ac9/image.png)  However, this fix is purely cosmetic and your code should run regardless.     For the runtime error, you need to run python from the same directory as ```search_eval.py```. You can use ```pwd``` to print the current working directory. Use ```cd Desktop\2022_fall\CS_410\MP2_private``` to change it
https://campuswire.com/c/G984118D3/feed/1316 Calculate likelihood with Laplace smoothing It's about the 8th question in practice quiz 11, how to calculate in specific? Why is 1/10 * 3/10? Thanks!$$d_{3} = (w_{3}, w_{4}) $$. $$p(d_{3}|\theta_{1}) = p(w_{3}|\theta_{1})p(w_{4}|\theta_{1})$$. To calculate $$p(w_{3}|\theta_{1})$$, we use the Laplace smoothing formula given in the question. $$p(w_{3}|\theta_{1}) = \frac{c(w_{3}, T_1)+1}{|T_1| + |V|}$$. We know that the $$c(w_{3}, T_1)$$ = 2. $$|T_1| = 6$$ (number of terms in $$T_1$$), and $$|V|$$ is 4 (total number of unique terms in vocabulary in the training data). So we get 3/10. Similarly, if we substitute the values, we can get 1/10 for $$p(w_{4}|\theta_{1})$$.
https://campuswire.com/c/G984118D3/feed/1118 Git push mirror step stuck for very long - MP3    Hi everyone ,  My git push mirror step is stuck for a very long time. Do any of you have ideas on how to fix this? My internet connection is fine, and I am following exact steps in the MP set up guide as I’ve always done.  I really appreciate any help. Thank you. I had the same issue last night. I just restarted the terminal and pushed it again, and it worked. That worked for me, thank you so much. Really appreciate it.
https://campuswire.com/c/G984118D3/feed/917 Extra Instructor Office Hour for Exam 1: 8-9pm, Fri, Oct. 14 Please note that I've added an extra office hour at 8pm-9pm CDT this Friday, Oct. 14,  to accommodate those students who were not able to make my office hour on Tuesday and help answer any questions that they may have about Exam 1. Please visit this office hour if you have any questions about the course materials covered in Exam 1 (Weeks 1-6). Of course, please also post any questions on Campuswire. The TAs and I will do our best to answer your questions as quickly as possible.   ---------------------- **Instructor Extra Office Hour for Exam 1** **Time**: Oct 14, 2022 08:00 PM Central Time (US and Canada) **Join Zoom Meeting** https://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09  Meeting ID: 897 6334 2791 Password: cs410  One tap mobile +13126266799,,89763342791# US (Chicago) +13017158592,,89763342791# US (Washington DC)  Dial by your location         +1 312 626 6799 US (Chicago)         +1 301 715 8592 US (Washington DC)         +1 470 250 9358 US (Atlanta)         +1 470 381 2552 US (Atlanta)         +1 646 518 9805 US (New York)         +1 651 372 8299 US (Minnesota)         +1 786 635 1003 US (Miami)         +1 929 205 6099 US (New York)         +1 267 831 0333 US (Philadelphia)         +1 253 215 8782 US (Tacoma)         +1 346 248 7799 US (Houston)         +1 602 753 0140 US (Phoenix)         +1 669 219 2599 US (San Jose)         +1 669 900 6833 US (San Jose)         +1 720 928 9299 US (Denver)         +1 971 247 1195 US (Portland)         +1 213 338 8477 US (Los Angeles)         +1 778 907 2071 Canada         +1 438 809 7799 Canada         +1 587 328 1099 Canada         +1 647 374 4685 Canada         +1 647 558 0588 Canada         +49 69 7104 9922 Germany         +49 695 050 2596 Germany         +44 203 481 5237 United Kingdom         +44 203 481 5240 United Kingdom         +44 131 460 1196 United Kingdom         +81 3 4578 1488 Japan         +61 3 7018 2005 Australia         +61 8 7150 1149 Australia         +61 2 8015 6011 Australia         +52 554 161 4288 Mexico Meeting ID: 897 6334 2791 Password: 814961 Find your local number: https://illinois.zoom.us/u/kcvtuGrnG  Join by SIP 89763342791@zoomcrc.com  Join by H.323 162.255.37.11 (US West) 162.255.36.11 (US East) 221.122.88.195 (China) 115.114.131.7 (India Mumbai) 115.114.115.7 (India Hyderabad) 213.19.144.110 (Amsterdam Netherlands) 213.244.140.110 (Germany) 103.122.166.55 (Australia Sydney) 103.122.167.55 (Australia Melbourne) 209.9.211.110 (Hong Kong SAR) 64.211.144.160 (Brazil) 69.174.57.160 (Canada Toronto) 65.39.152.160 (Canada Vancouver) 207.226.132.110 (Japan Tokyo) 149.137.24.110 (Japan Osaka) Meeting ID: 897 6334 2791 Password: 814961  Join by Skype for Business https://illinois.zoom.us/skype/89763342791 
https://campuswire.com/c/G984118D3/feed/1187 Peer review instructions Hi everyone,  If you made a CMT account, you should have received an email inviting you to be a reviewer. **Please accept by clicking the link ASAP.**  Once you do this, I can assign you 2-3 project proposals to review. To view the proposals to review, you'll need to switch your role to "Reviewer" at the top of the CMT console. The instructions (questions that you'll answer) are on CMT.   During this time, the TAs will also go through all submissions and provide a "meta-review" with any additional feedback as well as enter grades on Coursera. Of course, we recognize that peer review grades may be inconsistent or unfair at times, so we will do our best to ensure that your Coursera grade is as accurate as possible. We will update you once all of the submissions are graded.   Thanks, Kevin
https://campuswire.com/c/G984118D3/feed/964 deadline and tasks of MP/Tech Review/Project Hi all,  I have posted a question (#961) about the summary of Tech Review/Project deadline and tasks, which has been confirmed by a TA. Below is the info copied from that post:  ### MP - End of Oct 23, Sunday: MP3  ### Final Exam - End of Dec 4, Sunday  ### Tech Review Info available at Week 1 (Orientation Information) and Week 8 - End of Oct 23, Sunday: select a topic  (possible topics are available at [HERE](https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit?usp=sharing)) and sign up [HERE](https://docs.google.com/spreadsheets/d/1hWAyxd82FcitN9eG3yMW6ckq6l1VASBtYWpaPbywMPc/edit?usp=sharing)  - End of Nov 6, Sunday: submit the Tech Review via Coursera (Week 11: Technology Review Submission)  ### Project Info available at Week 1 (Orientation Info - Course Project Overview) Detail available at [HERE](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit?usp=sharing) - End of Oct 16, Sunday: **register for a  Microsoft CMT account** [HERE](https://cmt3.research.microsoft.com), and **sign up the form about the team** [HERE](https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit?usp=sharing) - End of Oct 23, Sunday: submit a proposal via CMT (instruction [HERE](https://docs.google.com/document/d/1XFvippp_jyiUUXpWsps4Geh5KsyZRkJq/edit)) - End of Oct 30, Sunday: peer-review project proposals, everyone will review 1~2 proposals - End of Nov 13, Sunday: submit a progress report - End of Nov 20, Sunday: peer-review project progress reports, everyone will review 2~3 reports - End of Dec 8, **Thursday**: **software code submission with documentation**, and **software usage tutorial presentation**   - End of Dec 16, **Friday**: peer-review project code, documentation, and presentations, everyone will review 2~3 groups
https://campuswire.com/c/G984118D3/feed/972 Zipf's Law with inverted index ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b68a0f5a-ea84-4000-817a-0e1161d35e2e/image.png)  Hi there, may I ask why we say "Exploit Zipf’s law to avoid touching many documents not matching any query term"? How do we achieve this for the fast search? Which step utilizes Zipf’s law? And how do we do that?Hey Yang; I also discussed this in #135 for further analysis to summarize, when you are compressing d-gap numbers there you are storing the differences of numbers so you expect those to be small for terms that occur in a lot of documents (since d-gap points to the "consecutive" relevant document) and in rare cases you will get large d-gap for rare terms. since it is for rare terms, you can handle a few large numbers. this is what i discuss primarily in my second response to Jeremy's question once i properly understood the question  please let me know if this and the referenced answer help, i am happy to discuss further
https://campuswire.com/c/G984118D3/feed/440 MP2.1 and MP2.2 Updates Hi all,  First, thank you for completing MP2.1! We've collected over 3,000 unique webpages related to CS410, all of which are searchable via [the CS410 search engine!](http://timan.cs.illinois.edu:4000/)  I've just batch-uploaded the grades for M2.1 to Coursera. If your grade is missing, please let me know, as it is possible that I may have missed something.   I've also just pushed the UI changes for MP2.2. The directions can be found [here on Coursera](https://www.coursera.org/learn/cs-410/gradedLti/KGkqH/mp2-2). Please read them carefully before doing the assignment!  Finally, unrelated to the MPs, I've added a bit more functionality to the Chrome extension. Now, when you open it on a webpage, it will tell you (1) how other users have described the page, if any, and (2) if there is highlighted text, it will search the DL using the highlighted text as a query and show results in the extension itself. To view these changes, you'll need to redownload the Chrome extension from Google Drive.  Thank you for your patience, and good luck on MP2.2!  Best, Kevin
https://campuswire.com/c/G984118D3/feed/1116 where do we define "number_of_topics"? assuming 2 (chicago vs seattle) for the test file, but can't see where/how it was defined otherwise...The number of topics is explicitly set in the main function at the bottom of the code. It gets passed in as a parameter to the plsa method omg - being super silly here. can just drop and die lol I think the number of topics is a hidden variable which we usually don't know, but for the MP it is set.  i think it's actually a good question - i'm not sure. if they are not that will be great, or we have an other dimension to optimize.
https://campuswire.com/c/G984118D3/feed/845 Quiz 2 Concepts Understanding  I just wanted to make sure my understanding was correct.  Question 2) Originally the answer is 1000 because we have to use the highest doc frequency query word since the question is asking for the minimum number of accumulators for the query. Now if it asked for the maximum number would it be 1100 because you would have to add all the query word doc frequencies?  Question 10) If the question asked for IDF weighting instead would the answer be "computer", because that would appear infrequently compared to "baseball" which would be a more common word if we are looking a term vector for a baseball sports news article?  Seems right to me
https://campuswire.com/c/G984118D3/feed/1092 PLSA EM Algorithm Hi everyone,  Could anyone show me the normalizer for the probability of the background hidden variable? And its derivation? Thanks!   ![Screen%20Shot%202022-10-20%20at%2012.10.04%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/edc500f7-4fc1-4e8a-829e-6246f174b186/Screen%20Shot%202022-10-20%20at%2012.10.04%20AM.png)  Let us try to think about what components will be present in the normalizer. If we are normalizing $$\lambda_{B}p(w|\theta_{B})$$, then $$\lambda_{B}p(w|\theta_{B})$$ should definitely be present in the normalizer as well, right? So now we have have part of the normalizer, i.e, $$\lambda_{B}p(w|\theta_{B})$$. What would be the other part? It would be something multiplied by $$(1-\lambda_{B})$$. This "something" would be the sum of all the ways of generating the word if the background was not chosen, i.e., $$\sum_{j=1}^{k}\pi_{d,j}^{(n)}p^{(n)}(w|\theta_{j})$$. So, the normalizer will be $$\lambda_{B}p(w|\theta_{B}) + (1-\lambda_{B})\sum_{j=1}^{k}\pi_{d,j}^{(n)}p^{(n)}(w|\theta_{j})$$ Thank you so much for your detailed answer, Harita! However, the formula has been presented on a previous slide. The professor indicated that this normalizer is a constraint about p(z = B). I guess that the constraint should be expressed in terms of p(z = j) and p(z = B). Do you think if it is possible? Hi Harita. If we plug the normalized p(z = B) and p(z = j) into the normalizer that you derived above, it will be p(z = B) + (1 - lambda) * sum(p(z =j)) = 1. Since sum(p(z = j)) = 1, we can obtain the normalized p(z = B) should be always equal to lambda. If this is correct, the normalization for p(z = B) will be very convenient. However, I don’t think that it makes sense. Can you confirm it? Thanks! I'm not sure how to interpret your question here, but as Harita has already explained, the normalizer for computing p(z=B) is λB​p(w∣θB​)+(1−λB​)∑j=1k​πd,j(n)​p(n)(w∣θj​). Note that in this case, the alternative (when z is not B) is all other (non-background) topics. That is, you should think about is the constraint : p(z=B)+ p(z=NotB)=1. One potential source of confusion is that according to our assumption (i.e., the two-step decision, first decide whether to use B and then decide which of the k topics to use), p(Z=B)+p(Z=theta1)+....p(Z=thetaK) doesn't sum to 1. Instead, p(Z=theta1)+...p(Z=thetaK)=1.  Does this make sense?   Note that the normalizer is not equal to 1. This is precisely the reason why we need to divide each individual estimated event count by the normalizer. If the normalizer were 1.0, we would not need to do this kind of division.  Hi Professor Zhai. I feel privileged that you can respond to my question! I greatly appreciate your detailed explanation, which offer me a deeper understanding. But I am still wondering if there is relation between p(z = NotB) and p(z = theta_j), where j = 1, ..., k. Do you think if the relation can be established? Thank you so much for your help!
https://campuswire.com/c/G984118D3/feed/341 CS 410 DL - Unable to specify video time Hello,  I am trying to capture a youtube video from a specific time but could not get it to work properly. The link to the video is youtube.com/watch?v=rsxT4FfRBaM&t=443s, but when I check my submission, it seems like everything after the ampersand was omitted.   Does anyone else have the same issue with the extension?  Sean Hi Sean,  This is likely an issue on our end. The database is showing that the video URL is saved correctly, so please proceed with saving videos with timestamps. I'll look into fixing this error today.  Thanks for pointing this out! Update: I think I fixed it, as now clicking your submission directs me to timestamp 443 seconds. Please let me know if it doesn't work on your end! It's working now. Thank you!
https://campuswire.com/c/G984118D3/feed/716 MP2.4 low faculty score I cloned the MP 2.4 today and still see a low score for faculty.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6707d567-4c80-4349-86d0-efc2a9c074b7/image.png)  what do I need to do so that the score gets to correct value.This happened to me. Removing the "idx" folder helped. Check out this [post](https://campuswire.com/c/G984118D3/feed/633) who had similar issue. thanks that worked.
https://campuswire.com/c/G984118D3/feed/1330 scrap google search results Hi,  I'm trying to scrap google search results (together with some other popular online forums as well) for my team project, but I have a hard time to find the proper attributes/classes to get the reference link via the HTML source code using chrome "inspect" function. Is it normal such info is hidden? do i need to get an API for such project?   thanks.
https://campuswire.com/c/G984118D3/feed/192 Issue about MP submission Hi,  When I try to submit my MP, I received this from the log. Could anyone give me some hints or suggestions on how to fix this? I don't know why I fail to submit the MP. I followed the steps in the PDF. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6298c11e-bbb8-4318-a326-9293050ad08b/image.png)Hi, It shows up like that, but takes a few minutes to build. You should see more logs in a bit ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1b4fa796-f5c6-4b60-bae2-f1b1d1e1deea/image.png)This is my submission history, is it normal? Can you try again? You should see it turn green instead of red. As Priyanka mentioned, give it a try again and see if the issue is resolved. Good luck! OK, I will try it again. And BTW, how could I resubmit or 'try again'? I always use git push to merge my updated code to GitHub, is it the correct way? Yes, each update to the repository is a submission. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3eb793a1-e5b5-40c6-9cc8-80a3e9da3c59/image.png) Why I get this? I followed the setup pdf to set up my account. Could you please give me some hints or suggestions?
https://campuswire.com/c/G984118D3/feed/783 Where to find AP, Faculty Datasets and Config Files I am working on MP2.4, and have been testing my parameters using the Cranfield data set provided in MP2, but where can I find the AP and Faculty datasets to test on those as well? I don't think we get those datasets. If you want to see the performance, submit your code on LiveData. I just used the Cranfield set on my computer, and then mad multiple submissions to LiveDataLab to optimize my parameters for the other sets. Wow, that doesn't seem very fair to us, but thank you  Keep submitting... super effective 👍 (but no)
https://campuswire.com/c/G984118D3/feed/1322 Final date for MP3 Late Submission Hello,   For those who have not submitted, what is the final date of submission for the MP3?  Thank you
https://campuswire.com/c/G984118D3/feed/1115 Question about the print statement in main() So there are three print statements in main(): ``` print(corpus.vocabulary) print("Vocabulary size:" + str(len(corpus.vocabulary))) print("Number of documents:" + str(len(corpus.documents))) ``` But when the code is run, these three lines are not printed out. I'm not sure if that's normal, and should I worry about it?  ![Screen%20Shot%202022-10-21%20at%2011.08.19.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d2d173e5-8256-4a7a-8124-7f96608a0036/Screen%20Shot%202022-10-21%20at%2011.08.19.png) Was your build successful?  It seems grader doesn't show this information, you don't need to worry about it as long as the result is ok. Yes. From my experience with MPs, I suppose only the print statement inside the class will be printed. The main function will be different for the grader and therefore, the debugging statements in the main function will have no effect. Resolved? 
https://campuswire.com/c/G984118D3/feed/1289 Optional Video Lectures - Week 10 Is it okay to leave the Optional videos from week 10 ? Does quiz have questions based out of the optional video ? Will Exams have content from these optional videos ?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a91a920f-4f33-47b5-a71a-1fd249fff213/image.png)I don't recall any of the optional lecture material in this course appearing on the quizzes. I believe it is also confirmed by the TAs that optional material won't appear on the exams, either. thank you ! . I watched those videos. They help understand Naïve Bayes Classifier in text categorization.  If your question is with respect to the exam, then yes you can leave the optional videos.  If your question is with respect to learning, then I would recommend you to go through them at least once. There is a reason why the professor chose to record these videos and made them available for students.
https://campuswire.com/c/G984118D3/feed/269 MP1 - What method can return variable like 'trigrams'? Hello,  I am trying MP1, learning metapy tutorial.  However, I can't see any example that unravels variable like trigrams with .items() method that returns token and count.  I am trying code like ``` trigrams = metapy.analyzers.NGramWordAnalyzer(3, tok) ``` but it throws following error. ``` AttributeError: 'metapy.metapy.analyzers.NGramWordAnalyzer' object has no attribute 'items' ```  Could you give me a little hint what kind of methoud should I use?  Thanks,There was a comment ``` (name the final call to ana.analyze as "trigrams") ``` to hint this issue.  I will change this issue to resolved.  Hope this help! Look at the example about how they make unigrams and bigrams. The process is very similar
https://campuswire.com/c/G984118D3/feed/731 MP2.3 Deadline extended to Oct 2 Given the number of requests we were getting for an extension, we have extended the deadline for MP2.3 to Oct 2. This extension should be reflected on Coursera.
https://campuswire.com/c/G984118D3/feed/1228 Unchanging log likelihood Hello-  I'm having issues where my log likelihood and the values for each probability matrix are not changing after each iteration.  I've implemented the E and M step so that I'm updating each matrix according to the formulas given in the lecture, and I've calculated the log likelihood according to the formula specified in #988 .   One thing that was unclear to me was the normalization part - is it sufficient to compute the numerator of the formulas and then call the given `normalize()` function? Do we still have to normalize if we've implemented the denominator of the formulas (isn't that just normalization??)  Any insight is much appreciated - thanks for your help!You can use the provided normalize() function as long as you remember to normalize along the right axis based on the the array you are using it for. To answer your question, yes- if your numerators are implemented correctly, you can use the normalize function to have the denominator implemented across a given axis. If you have already implemented the denominators, you do not need the normalize function. You can see a response from the TA regarding this here: #1093  Marking this question as unresolved because i have the same question, and although I followed the formula specified in #988, my log values are unchanging between the first and second iteration.  It can be used in M step but not E step. Before using normalize() function, make sure you are clear about normalize over which dimension and how normalize() function works. 
https://campuswire.com/c/G984118D3/feed/547 course project for undergrad students Just to clarify, is the course project a requirement for undergrad students taking the three credit hour section?Yes, the course project is a requirement for every student in the class (including undergrad students taking the three credit hour section). 
https://campuswire.com/c/G984118D3/feed/1176 Tech Review with different reference I am wondering if we can choose a topic from the provided sample topics ( https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit?usp=sharing ) but use different reference.Yes, using a different reference is acceptable. 
https://campuswire.com/c/G984118D3/feed/20 Group Project and Technology review Hi,  Should the group for final project and technology review be the same group(same teammates), since someone might choose the 3 credit option which do not need to finish the tech review?  Thank you!I believe that the technology review is an independent assignment ![tech%20review.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ba62b629-7b2e-4696-95ff-578dfbb48c9d/tech%20review.png) please see screenshot of the orientation view and it shows that the tech review is in group. There seems to be an inconsistency between the course introduction video and the technology review information reading. The video seems to indicate that it can be done in groups, but the reading says, "The Technology Review should be completed individually.", doesn't mention groups, and refers to the "student" rather than "students". Could an instructor please provide some clarification? Please follow the reading on the Coursera website --> the technology review should be completed individually yes the tech review is an individual assignment! not done in groups Sorry for the inaccurate description about the technology review in the introduction video/slides. It's an independent assignment. I'll correct this mistake and update the introduction video and slides.  Thank you for your updated!
https://campuswire.com/c/G984118D3/feed/742 Coursera Score not updated for MP2.4 I got a 1, however the Grade in coursera still says 0%,. Any suggestions what I might be doing wrong?  BTW I also tried [this](https://campuswire.com/c/G984118D3/feed/622) suggestion, but did not work for me  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8124177d-0131-4461-ae81-0a957a82abe4/image.png)It's synchronized. There might be some delay when transferring grades. 
https://campuswire.com/c/G984118D3/feed/403 MP2.1 issues Hi,  I noticed I cannot create an account following the instruction provided on Google Docs, and there is no MP2.1 project section on Livedatalab. Could anyone give some hints or suggestions?  ThanksYou'll have to be on the Illinois VPN in order to create an account - otherwise, you might find yourself clicking the "New Account" button and nothing will happen! You don't need Livedatalab for MP2.1. Just create an account and save at least 15 links related to the course. As another student answered, make sure you are on the VPN. Hi, MP2.1 assignment changed and is no longer a **Livedatalab**. You can refer to the new instructions here: [https://www.coursera.org/learn/cs-410/gradedLti/FO1Ol/mp2-1](https://www.coursera.org/learn/cs-410/gradedLti/FO1Ol/mp2-1).  OK, thanks for your explanation. I did use VPN this time, but it still doesn't work. the digital library was down earlier in the day. are you still having problems to create an account? Thanks for your following up. I completed MP2.1, I create an account at 23:00 pm successfully. Thanks again.
https://campuswire.com/c/G984118D3/feed/518 Spoiler alert Use `math.log2` will fail passing the grading since Python2.7 doesn't support it.Just used the Math.log(x, 2) in python 2.7 and the grader should be working properly. I have not looked at the MP so do not know if python 2.7 is mandatory or not. But 2.7 reached its end of life January 2020. Why use it? I think python 2.7 is still used in several operating systems.  I asked this question to a prof a while back.  Their response was 2.7 is not going anywhere quick.   Well yes, it's not, but there has been no official support for 2.7 since 2020 Jan so was just curious why use it altogether. Valid point.   Because Metapy is not supported in newer python version. For mac I think both 2.7 and 3.5 will work. For windows, 3.7 also works.  yeah I setup pyenv for 3.5 and used that. Thanks. This worked for me. Got 0.7 instantly!  so happy
https://campuswire.com/c/G984118D3/feed/488 Parameter of JM/DP smoothing & query likelihood function ![Screen%20Shot%202022-09-17%20at%2010.14.15%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/df788603-ff4f-4973-8b67-439897306f23/Screen%20Shot%202022-09-17%20at%2010.14.15%20AM.png)  How do we come to the conclusion that $$a_d$$ in the general query likelihood function equal to $$lambda$$ for Jelinek-Mercer smoothing?In Query likelihood function, ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c2ece84f-9a09-4a10-a182-468bc3f4751e/image.png) is (probability of seen words)/ (probability of unseen word in document). The probability of unseen word  in JM smoothing is represented by ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b0ad32cf-727e-455d-87f2-f1f8c88c9bed/image.png) where lamda is the coefficient to control the amount of smoothing. So comparing these two help us equate lambda to ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/331be2ce-2c53-4ddd-a896-54e0d2588db5/image.png) as 1. Essentially they both are smoothing coefficients. In Lecture 4.4 slide 5, the high-level formula is a good reference for this. Alpha and Lambda are the weight of pseudo word count in collection LM. ![Screen%20Shot%202022-09-19%20at%2009.36.34.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e757d8cc-2ae7-4043-8a2c-8984ef34e235/Screen%20Shot%202022-09-19%20at%2009.36.34.png)
https://campuswire.com/c/G984118D3/feed/820 Lecture 8.10 I am taking lecture 8.10. It talk about the simple case with one document and one topic. later in the slides, it talks about language model setup, why there is M theta.  What does M mean?  Why the sum of M theta will be 1? I am still confusing how the likelihood function will find the topic. ![simplest%20case.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3bdfd793-7d2c-4c28-91b7-02b134a8ce4b/simplest%20case.png) ![model%20setup.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/68206ecf-6f92-489a-b226-9009537b56cf/model%20setup.png)I believe `M` is the size of the vocabulary (note: one reason $$d = x_1...x_{|d|}$$ instead of $$d = x_1...x_M$$ is because we are not accounting for repeated words here, but for the vocabulary we are only counting unique words) Thus, $$\sum_{i=1}^M\theta_i=1$$, because when we sum the probabilities of words from the vocabulary in the document ($$\theta_i=p(w_i | \theta)$$) , at least following the general Maximum Likelihood estimator approach, we expect to get a perfect picture of how the document is made up. For a simple example, if our vocabulary is $$V = \{the, text, mining\}$$, and in the document "the" occurs 2/5 times, text occurs 2/5 times, and mining occurs 1/5 times, that makes sense, because they sum to 1. We wouldn't expect a new term that isn't in the vocabulary to appear and throw off that sum. We also wouldn't expect a term to show up 2/5 times and have higher probability than 2/5. This goes back to Lecture 4.2 (and 8.8) where we see that the maximum likelihood estimation of a unigram LM is just $$=\frac{c(w, d)}{|d|}$$ Finally, we can follow from there to the steps in the later slide of Lecture 8.10 where the Lagrange Multiplier is used to obtain the topic word distribution using this Maximum Likelihood estimation  Let me know if that made sense, I am happy to try to explain more clearly especially if you have specific follow up questions (or if I missed answering anything from your post) what does θi mean in θi=p(wi∣θ) For my understanding, we have documents and we want to find out the topics from these documents. I still cannot get the mechanism of the probabilistic model to find out the topic from the document? $$\theta_{i}$$ is just a shorthand representation for $$p(w_{i}|\theta)$$. We try to estimate the topic using the documents. If we use maximum likelihood estimation, the estimate of $$\theta$$ will be the value of $$\theta$$ that maximizes $$p(d|\theta)$$. We can use the Lagrange multiplier method to finally arrive at the value $$\theta_{i} = \frac{c(w_{i},d)}{|d|}$$ to be the optimal value for the parameters..
https://campuswire.com/c/G984118D3/feed/1206 When will MP4 be released?  I saw the thread in https://campuswire.com/c/G984118D3/feed/795 that we should expect an MP4, but haven't given any more details other than that.   Given the course schedules and deadlines on coursera, it shows that MP4 will be due in 2 weeks. Will MP4 be released some time this week?   ![Screen%20Shot%202022-10-25%20at%2010.52.06%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0a9e08a6-2ea0-4636-a674-45ad5f66a617/Screen%20Shot%202022-10-25%20at%2010.52.06%20AM.png)I think this is the old schedule, no?  If you look at the grades section on Coursera, it lists only up to MP3. I agree, this looks like an old schedule. The current grade breakup for MPs is 25% and all seems to be divided among the existing MPs(MP1- MP3).  I assume that if we end up having an MP4 we will be given enough time to complete it, so the schedule would be obsolete Yeah it seems like there is no MP4 for now. It is not listed under the grades section on coursera. Hopefully, they’ll offer it as extra credit or something.
https://campuswire.com/c/G984118D3/feed/1150 Same topic here Our group plans to build a chatbot, any group share the similar interest or anyone has advice for this?  Thank you.Just a question which top of your chatbot will focus on
https://campuswire.com/c/G984118D3/feed/233 MP1 Content Hi there, I wonder whether we need to learn the lessons of Week 2 first to do the MP1. Is it sufficient to do MP1 with only the lessons of Week 1?MP1 includes a brief introductory Meta tutorial that covers requirements of the todo portion of the assignment. Also, the chapter 4 reading from week 1 is dedicated to Meta, so yes, I would say MP1 could be comfortably completed having only completed the week 1 material.
https://campuswire.com/c/G984118D3/feed/374 Will the MP2 be graded with regards to the submitted content Will 15 submissions guarantee the full score?I don't think so. I think we just need to submit 15 links  Do you have reliable information source  Yes, submitting 15 links will guarantee the full score.  Thanks for the confirmation! I'm guessing the TAs are not keen to read/watch 15 articles/videos X 400 students. :-)  But I tried to find interesting content, as that's what we're all going to be relying on as a group for future exercises, I imagine.
https://campuswire.com/c/G984118D3/feed/228 Piazza Hi everyone, I am new to UIUC and I have never used Piazza. I believe the professor and Onboarding Course mentioned using that platform but I do not see it listed anywhere.  Are we also using Piazza or just Campuswire and Slack for class communication?  Thanks!This class is using Campuswire not Piazza this semester. Awesome, thanks!
https://campuswire.com/c/G984118D3/feed/460 Reminder: GitHub Token for livedatalab Just a reminder, your github token that you use to link to the livedatalab may expire. Make sure you update the token before next MP. First delete the current linked accounts. Then you can regenerate a new token (set it to 90 days as least) and link again. 
https://campuswire.com/c/G984118D3/feed/1336 Technology Review I just want to make sure that technology review is only for students who register 4-credit hours section. So, if I register 3-credit hour section, then I do not need to finish tech review right?That's what it mentions in the syllabus. Maybe TA's can confirm Yes that is correct. You can check out https://campuswire.com/c/G984118D3/feed/1288 as well
https://campuswire.com/c/G984118D3/feed/319 wget won't work ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e8e87a99-1383-4d08-b91c-5fe1e6e27529/image.png)  When I execute this step: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0519e3e5-29dc-44d8-9400-618e9fb617df/image.png)  And I have tried !wget or @wget, neither of it works. Hey! I was getting same error,  I downloaded the whole text file and I am not getting error calling that module. You need to just paste that link in browser and download the text file. Make sure your code can access it, while using that module in next line.   You are running wget CLI in the Python environment. Try call wget in the terminal instead. Thanks everyone. I think the MP1 should note that.  You don't need to install or use wget. Just copy and paste the link in that command to a new tab in your browser and you can download the documents easily   Hello Xin, wget is a linux command, and unfortunately it does not work at the Windows command line or within Python. However, it IS possible to run wget in Windows Powershell, which can be found on the Windows menu. Pasting the link into your browser, as Maahi and others recommend, is just as fast. I learned this only through experimentation, so I agree with you that it would be helpful if the assignments can advise where each command or step should be executed.  I think for python you need to import wget, for windows you need to use curl command. See #321 
https://campuswire.com/c/G984118D3/feed/395 MP2.1 Submission after 15 links does not update coursera Hi, I have submitted 15 links as instructions provided for MP2.1. Output on Grades shows no submission. How do we know if what we submitted is accepted or not? ThanksGrades will be uploaded by TAs https://campuswire.com/c/G984118D3/feed/352 I cant find the campuswire post, but there is a post from Kevin saying that you just have to be sure that you have done 15 submissions.  That is all you have to do.  There is a counter now in the upper left hand corner of the "my submissions" page.  For me it looks like:  Search Results Search Again Your submissions  Total results: 19 Where is this counter?  Where is the "my submission" page you mentioned? Disregard this question.  Campus wire #365 has this answer.  
https://campuswire.com/c/G984118D3/feed/837 Whiteboard on Exam I see that we're allowed 2 scratch papers for the exam. Could we be allowed to use a whiteboard instead?  If so, can you explicitly list that under allowed resources on the exam rules in proctoru?   Thanks!yes, no problem. I added whiteboard as a permitted resource. Thank you!
https://campuswire.com/c/G984118D3/feed/128 MP1 - Jupyter Notebook For those of use who prefer to work in an interactive environment in lieu of terminal, I've transcribed the README from [https://github.com/CS410Assignments/MP1](https://github.com/CS410Assignments/MP1) into a Jupyter notebook:  [CS%20410%20-%20MP1.ipynb](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/fd2b76af-249b-48fe-b706-24bf85ce4992/CS%20410%20-%20MP1.ipynb)  Simply save the JSON-encoded file with the extension '.ipynb' (eliminate the '.txt' extension if saved as text file) and I believe it should work; let me know if you encounter issues.  If enough people find this useful, I'm happy to do the same on future MPs as they become unlocked...please upvote to indicate interest.    This is very helpful. Thank you! This helped a lot！ Wonder how do we make MP2.3 as .ipynb
https://campuswire.com/c/G984118D3/feed/687 config.toml can't open using Colab I have tried using VS code but there are some problems installing metapy so I started trying Colab.  Then it showed an error loading the .toml file. How can I open it? Thanks! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/561e0c4f-1869-4b67-bb4f-6ff865e09113/image.png)Your current directory is /content. You need to use %cd to the directory where you save the config file. Use !pwd to show your current directory.   I changed the directory and it solved the problem. Thank you Chaochao! My pleasure!
https://campuswire.com/c/G984118D3/feed/1050 Difficulty of connecting to LiveDataLab How difficult is it to connect a leaderboard competition to LiveDataLab? I think it would be feasible to do some sort of competition but I am not sure what process it would take to put on LiveDataLabI'm not sure, as it likely depends on the skill level of the group working to implement the leaderboard. But if I had to guess, if one has strong Python development skills and familiarity with Flask, Docker, and MongoDB, then it should be feasible. 
https://campuswire.com/c/G984118D3/feed/43 Practice Quiz 1 I am somewhat confused regarding the following two questions on the practice quiz (and have not yet tried the regular quiz):  8. How many syntactic structures can you identify in sentence “A man saw a boy with a telescope”?  a. 1 b. 2 c. 3  What do you mean by "syntactic structures"? Isn't "a man", "a boy", and "a telescope" noun phrases, "saw a boy" a verb phrase, and "with a telescope" a "prepositional phrase"? The answer is not 3. How does this work?  9. In VSM model, which of the following similarity/distance measures would be affected by document length?  a. L2 distance $$||v_1-v_2||_2$$ b. Cosine similarity $$cos(v_1,v_2)$$  I don't think these were discussed in the lecture. Wikipedia says Cosine similarity is independent of length, so I chose that, and it is correct. Why does L2 distance care about document length? What are the contents of $$v_1$$ and $$v_2$$ here? Counts of words?  8. was mentioned in lesson 1.1. "with a telescope" could be referring to the man or the boy.  9. I found this explanation for cosine similarity vs L2 https://stackoverflow.com/questions/19410270/vector-space-model-cosine-similarity-vs-euclidean-distance  Please correct me if I'm wrong! What does "was mentioned in lesson 1.1. "with a telescope" could be referring to the man or the boy." have to do with the number of syntactic structures? I saw that in the lecture, but do not understand how it relates to anything. Jeremy, I also was confused by the phrase "syntactic structure" in this question. But I think here it refers to different ways to understand the sentence, such as "a man saw a boy who was holding a telescope" or "a man saw a boy by looking through a telescope". This ambiguity was mentioned briefly in the lecture. How many syntactic structures can you identify in sentence “A man saw a boy with a telescope”?  The way I understand: how many ways can you interpret this sentence? Two ways. A man with telescope or a boy with telescope.  Thank you. Thank you. I think the syntactic structure may means different ways of understanding this sentence.  For L2 distance please see screenshot![L2.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/75dd1c1a-2875-489f-8001-de6cabc9640a/L2.png)
https://campuswire.com/c/G984118D3/feed/387 Coursera can't open? -----------Never mind, it is back online again----------- I have beening working on the assignments tonight until just a few minutes ago. Seems coursera is down? Anyone else experiencing the same issue? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/83da3725-4775-4d07-a0a0-45bdadfd2efb/image.png)Maybe different regions block it?
https://campuswire.com/c/G984118D3/feed/373 Question of Q5 in Quiz3 Hey TAs,  I was working on quiz3 for week 3 - TR system evaluation. I have a question regarding the Q5 answer why a retrieval system **can not** have an F1 score of 0.75 and a precision of 0.5. Based on the definition of F1, as long as the beta could be bigger than $$\sqrt{2}$$, then this retrieval system will have an F1 of 0.75, with a precision of 0.5, and a reasonable recall. Any explanation will be great!  Thanks!`F1` specifically means `F-score` with `beta` set to 1. Following off Dave, since F1 has beta =1, that means R would have to be 1.5 for this to be possible  Gotcha! Thank you all for the explanation! Don-Thuan Le already said that R would have to be 1.5 for this to be possible, but here is a more detailed explanation of why:  We know that $$F_1=\frac{2*P*R}{P+R}$$.  Suppose $$F_1=0.75$$ and $$P=0.5$$. Then $$0.75 = \frac{2*0.5*R}{0.5+R}$$ So $$0.75 = \frac{R}{0.5+R}$$ So $$0.75*(0.5+R) = R$$ So $$\frac{3}{8}+0.75*R = R$$ So $$\frac{3}{8} = 0.25*R$$ So $$1.5=R$$  R is recall, which is the number of true positives divided by the total number of positives, or, in other words, the proportion of actually relevant documents returned by some text retrieval algorithm. It is impossible to return more relevant documents than there actually are, so this case is impossible.
https://campuswire.com/c/G984118D3/feed/610 Not receiving grade on Coursera I got 1 point on livedatalab but I had nothing on my Coursera. I have waited for like 15 minutes and tried unlinking my account and resubmitting again. May I ask how to fix this problem? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f6a19f18-b816-4a97-a984-84c0112bb36c/image.png)Same for me. Seems like this is a common issue. I've seen similar posts like [this one](https://campuswire.com/c/G984118D3/feed/591). I have the same issue. Me too Same for me. Have any of these been resolved? Same issue login to livedatalab one more time and submit your code again. This method works for me.  
https://campuswire.com/c/G984118D3/feed/911 technology Review - Proposal - Deadline Is the technology review proposal submission also due on this Sunday October 17th  ?No, the technology review has two deadlines: 1. Select a topic, and fill in the Google Sheet (October 23rd) 2. Submit the review via Github/Livedatalab (November 6th) got it , thank you !
https://campuswire.com/c/G984118D3/feed/1256 LiveDataLab unaccessible Hello, I am working on MP3 with last couple of implementations, and I just cannot access the livedatalab website. It continues to load the webpage(indefinitely) but doesn't work with several reloads.  And I got warning that the page is not responding. What should I do? Anyone else with this problem?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/31216ffc-93f1-4f03-9b19-487dec5cb0a9/image.png)Oh, no worries! It came back after 20~30 minutes!
https://campuswire.com/c/G984118D3/feed/117 Looking for project teammates in Pacific time zone Hi classmates!   Anyone on the west coast want to form a group for the final project?  I work full time and mostly will be doing coursework on weekends.   Please reach out on slack if you are interested :)   Thanks, Akshat AgarwalI just see your post. I just email you and hope it gets through.
https://campuswire.com/c/G984118D3/feed/377 MP2.1-submittion I have entered one link it looks good , but now if I add another link it does not looks okay. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cc779ad7-37d1-485b-bb5d-b8bb757f31d5/image.png)   Can you please help with which box do i need to enter url and which box do i need to add caption ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/58a63864-6f6f-4f1e-a572-ae69aeca9db8/image.png)It is my understanding that you don't need enter url. Just fill out highlighted text and its explanation blanks, then submit to CS410 DL. That's all. Thanks , for clearing confusion. Nusrat, the URL is entered automatically. If, in your example, you highlight the blog title "Integer Compression" before clicking the "DL Extension" icon, then the phrase "Integer Compression" should appear above the first box, just after "Highlighted text:". (The first box remains empty, which was a little confusing at first, but words shown next to "Highlighted text:" actually are stored in the database.) For the second box ("this is useful because") it seems okay simply to type or paste your own answer, such as "explains step by step how to compute unary, gamma, and delta encoding".  I guess the next exercise will give an idea of how well this works for determining relevance.
https://campuswire.com/c/G984118D3/feed/1008 Question regarding DP Smoothing In the lecture, it is said that,  1. mu is the count of the pseudo words inserted into the model 2. if mu is decreased p(w|D) will become closer to ML estimate.  Given, mu can range from [0 to infinity], it is safe to assume that for lower value fo mu i.e. when mu is less that 10 (for example), the p(w|D) will be close to ML estimate?  I ask give the range of mu is significant large.The first part of the coefficient of the DP equation = |d| / (|d| + mu), you can see when mu is decreasing, this part close to one. while the second part of the coefficient : mu / (|d| + mu) close to 0, so the final p(w|D) = c(w,d)/|d|, which is the ML estimate. mu ranges from 0 to infinity, the logic should be true to any value of the mu. As mu increases, P(w|D) should move towards collection. As already explained in the replies here, it's the two coefficients, |d|/(|d|+mu) and mu/(|d|+mu), that control how close the estimated p(w|D) using Dirichlet prior smoothing. In general, as you increase mu, you'd increase the weight on the collection language model, whereas reducing mu would make it closer to the ML estimate. Think about mu=0, it would put all the weight on the ML estimate (i.e., no smoothing).  Also, note that for any fixed mu, no matter how large it is, as we increase document length |d|, it would also cause the coefficient on the ML (|d|/(mu+|d|) to increase, causing the estimate to be closer to the ML estimate, which makes sense intuitively as it simply says that as we collect more data (longer documents), we'd trust the ML estimate based on the counts in the data more. If we let |d| approach infinity,  the estimate would become essentially the same as the ML estimate.   Thank you Professor. I just making making sure. Given it is the interpolation of ML estimator and language model, I was not sure if there was a pivot number that balance the two given the range is from 0 to infinity.
https://campuswire.com/c/G984118D3/feed/97 rm -rf Error For some reason, when I attempt to remove the old repo like stated on Step 9 of the MP instructions, I am getting an error. Here is the error: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/052ea8b8-d602-4488-ab57-96f4a7367e22/image.png)How can I solve this error? Can I just move on with the steps, or will this cause errors? You can use rmdir for Windows or just delete the folder manually. The step just declutters so it's more or less optional. In general, you're probably going to see a lot of Linux commands and for the most part you can easily find the Windows equivalent (another answer here says to use rmdir). It's likely going to be useful just to have a Linux environment to work in anyway. For example, you could have a Linux Subsystem through [WSL](https://docs.microsoft.com/en-us/windows/wsl/install)
https://campuswire.com/c/G984118D3/feed/612 name 'pytoml' is not defined error ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/727e1994-c100-4344-a89e-56ce92bf45ed/image.png)  Hi, anybody has the same issue when working on MP3.5 and how did you fix it?  so i'm using conda for python 3.5 environment. Everything was working fine when i use the same laptop for MP1. This time I have this name 'pytoml' is not defined error. I have no idea why.   I could import metapy, but I'm stuck at  ``` >>> with open('config.toml', 'r') as fin: ...         cfg_d = pytoml.load(fin) ```  I guess I have metapy pytoml installed correctly, otherwise i wouldn't have finished MP1 last time.  I tried to remove py35 environment from conda and recreate python3.5 in conda, uninstall and reinstall metapy pytoml -- unfortunately it still did not solve the issue.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8515a21b-d42a-4152-b36f-7c3416bca54f/image.png)   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ac49e459-cd28-4917-9701-8bfd1879207a/image.png)  Someone help me please.  Thank you!SiCheng, did you already 'import pytoml' in addition to 'import metapy'? I do not see it in your screen shots.  pytoml is it's own package so that last line meaty.pytoml isn't going to work.  But as David suggested, make sure you include "import pytoml" in your document ahead of using it. The README for the MP includes the import for metapy but not pytoml. I was hitting my head against the wall trying to figure out why it didn't work before I realized!
https://campuswire.com/c/G984118D3/feed/441 MP2.2 Confusion I am a bit confused as to what we are doing on Mp2.2. Are we searching topics and using our judgement to determine if the query results are relevant/not relevant to the class?Yes, as the lectures also mention we are making binary judgement. Relavence feedback needs user to provide explicit judgement. See below: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e4e805b6-a2c9-421d-a6f5-5c29be6cd8a3/image.png) Hope it helps! To add on to the answer, the judgment of relevance should be based on the query, not necessarily the class. For example, if your query is "explanation of k1 b in bm25", then you should mark as relevant results that explain the parameters k1 and b of the formula bm25. 
https://campuswire.com/c/G984118D3/feed/277 Webhook fail to connect I tried to connect to webhook by following the instruction and use git to  submit example.py to my github MP1_private repo. But I am not able to see any submission on LiveDataLab   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/80c1cea4-497a-4e6f-9cb0-4a1ec1d10c7c/image.png)What does your webhook config look like? It should look like this: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a9628592-db9d-4e5d-b985-b2e6bbd7fc69/image.png) Hi, my webhook config looks like this, but I am facing a similar issue Are you sure you are using: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger and not livelab? If not, please share an image of your config.  I am also having the same issue. I've included the photo of my configuration. It's the same as the one posted.  ![Screen%20Shot%202022-09-03%20at%203.38.03%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/738170fa-bfc7-42ac-b56e-6e97f23a3085/Screen%20Shot%202022-09-03%20at%203.38.03%20PM.png) With similar results as the original post: ![Screen%20Shot%202022-09-03%20at%203.39.04%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7d94b303-e5a7-4662-af5d-67a3a9baef94/Screen%20Shot%202022-09-03%20at%203.39.04%20PM.png) Can you try deleting the webhook and readding? Absolutely. Just did and got the same error. For reference the response I am getting is 401, Github account not found. I tried deleting and relinking my GitHub to my livedatalab but no luck. ![Screen%20Shot%202022-09-03%20at%203.50.00%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/24949113-a01b-4d80-b686-a9bc106a5ffd/Screen%20Shot%202022-09-03%20at%203.50.00%20PM.png)![Screen%20Shot%202022-09-03%20at%203.51.13%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/25784996-3e94-4ac6-bd60-067afa174a17/Screen%20Shot%202022-09-03%20at%203.51.13%20PM.png) This probably means that your PAT key might not be right. Are you sure you are copying the full value? And it is not expired?   I have deleted your github account from our backend, can you try to relink it one more time?  I figured it out. The issue was my computer kept default capitalizing to GitHub.com I re-added the account with "github.com" all lower case and that solved the issue. Try re-adding your account on LiveDataLab making sure the host is github.com (all lowercase) this resolved my issue. And got my first successful post!![Screen%20Shot%202022-09-03%20at%204.03.23%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/97a872ba-a571-4a64-821b-00b97086f3dd/Screen%20Shot%202022-09-03%20at%204.03.23%20PM.png) I solved this problem by re-adding my account on LiveDataLab and using the host name as github.com(lowercase). Hey, try checking if the webhook link that you provided is correct. I believe that it should be http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger. This happened to me too. I believed it was something wrong with copying and pasting on my computer. I solved it by typing the URL manually.
https://campuswire.com/c/G984118D3/feed/456 MP2.2  It seems all these search returns come from the same source,  are they considered one judgment or in this case, seven judgments?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3d2bbba5-efa6-4d94-ba16-93f70f71c31c/image.png)I think it gave me credit for each one.  Could you click on the 7 that seem the same and then 3 other judgments to see if the search engine accepts them as your total of 10 judgments?  I have hit submit with only 9 judgments and the search engine displayed an error. I think one "click" equals one judgement. For the same website different sections maybe highlighted. I think each icon count as one try. Just for safe, I submitted at least 10 links for each judgment.   A judgment would be selecting the radio button with the option (Relevant/Not Relevant) so in your case for the query you have given you have provided 7 judgment.
https://campuswire.com/c/G984118D3/feed/1306 calculate likelihood Hi I am not sure what I am doing wrong on calculate likelihood:      def calculate_likelihood(self, number_of_topics):         """ Calculate the current log-likelihood of the model using         the model's updated probability matrices                  Append the calculated log-likelihood to self.likelihoods          """         for d in range(self.number_of_documents):             for w in range(self.vocabulary_size):                 temp = 0                 for z in range(number_of_topics):                     temp += np.log(self.document_topic_prob[d][z] * self.topic_word_prob[z][w])                 self.likelihoods.append(self.term_doc_matrix[d][w] + temp)    Please someone help???? I am getting error   IndexError: index 2 is out of bounds for axis 0 with size 2  You need to ensure the value inside of np.log is > 0, otherwise it will cause error. That says that if value inside of np.log is < 0, you need to neglect it (not add to your temp).  Also, you need to self.term_doc_matrix[d][w] * temp instead + based on the slides provided in week 9.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/356ea634-f967-41b7-8ec2-52cc6c911e49/image.png)
https://campuswire.com/c/G984118D3/feed/375 Submission of MP2 I just want to confirm that do we need to submit our MP2 to LiveDataLab? Beucase when I clicked the "Open Tool" on Coursera under Week3 Assignment that directed me to LiveDataLab, I could not find MP2 in Projects section.Hi, I believe it was mentioned in a few other posts as well that as long as you submit 15 responses to the Digital Library you will receive full credit. There is nothing to be done with LiveDataLab for MP2.1 You don't need to do anything about MP2.1 and MP2.2 on LiveDataLab. However, you need to submit MP2.3 and MP2.4 on LiveDataLab. Clarification on scope of MP2.1 by TA https://campuswire.com/c/G984118D3/feed/359
https://campuswire.com/c/G984118D3/feed/680 MP2.3 In MP2.3, are we allowed to change the name of the function load_ranker? Also, are we allowed to add parameters to the load_ranker function? I'm not sure about the function name change but since I was able to add parameter to the load_ranker function and receive success in LiveDataLab it might be possible. Thanks!
https://campuswire.com/c/G984118D3/feed/152 Problem with MP1 submission Hey there,  I am having problems for submitting MP1, I followed the instructions in Coursera, but there are still some error in pushing the code to Live data labm, anyone can help? Thanks ![%E6%88%AA%E5%B1%8F2022-08-28%20%E4%B8%8B%E5%8D%889.19.22.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1e1fc0c5-33f5-45cd-9178-d8149dbb1de4/%E6%88%AA%E5%B1%8F2022-08-28%20%E4%B8%8B%E5%8D%889.19.22.png)  Just found the answer, the link provided in instruction is wrong. The correct one is :http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger
https://campuswire.com/c/G984118D3/feed/557 Feedback Mixture Model ![ymail-tmp-8509540692894373742.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b8c2344a-3e71-466a-9148-a8d592e23578/ymail-tmp-8509540692894373742.jpg) I don't understand when we are using the mixture model for feedback why we don't use theta T (in my version of the equation) for feedback.  Doesn't theta F contain all of the common words (a, the, etc)?  Thanks!!Could clarify what $$\theta_{T}$$ is here? I couldn't see  $$\theta_{T}$$ used on the lecture slides. It is just $$\theta$$ on the slides.  I just came up with the idea of $$\theta_T$$ because I wondered if that was the topic model we wanted to use for feedback. Thank you! Sorry, I did not understand. What is the difference between $$\theta$$ and your $$\theta_{T}$$? It seems that the $$\theta_F$$ model contains all of the common words like a, the, etc.  I don't understand how this is helpful if we combine all of these common words with the query model for feedback.  Shouldn't we combine the topic model with the query model to make the updated query model?  Thank you!! I got it now!!!  I just didn't understand the argmax operator.
https://campuswire.com/c/G984118D3/feed/234 MP1 vs MP1Project Hi, in my LiveDataLab, on the project overview, there is a "MP1" and "MP1Project". Just want to double confirm to be safe: we only need to complete MP1, and don't need to care about "MP1Project" - it's for the developer to test... right?![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/885c3159-ecde-449d-8288-cd81c1e320de/image.png)Yes, as per my understanding we need to complete MP1.
https://campuswire.com/c/G984118D3/feed/1260 mp3 live data lab failure i've been trying to submit my MP. it runs fine locally, but immediately fails on livedatalab with no logs. any idea what could be the problem?  UPDATE: unfortunately, it kept failing. i got logs stating that there were errors on a certain line after updating my token but i have no clue what the problem is since my code runs accurately locally with no issues. is it possible for a staff member to look into this? im using the python version specified to use in MPs Have you tried #1249?  thank you! ill try that. its past the deadline, now but let me see if it works I've noticed it can take some time for the log to show up as it's big.
https://campuswire.com/c/G984118D3/feed/41 Git clone/push needs auth token, not your password Team, you might already know this, but sticking it here just in case need to see/refer.  If you perform Git operations and try to authenticate using the account username and password, you will get an error remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.  Ref: https://itsmycode.com/support-for-password-authentication-was-removed-github/
https://campuswire.com/c/G984118D3/feed/894 Issues with ProctorU Password I saw a post that some folks have had trouble with the ProctorU password for Exam 1.  Is this resolved or do the TAs recommend taking this exam on weekdays or earlier than the last date?   Considering that the ProctorU folks have to reach out to others for potential issues. Just asking since this is the first time I am using this service and I am scheduled to take the test on Sunday.I think now we can take the exam without any trouble.  I had trouble with the password for the first time, but when I reconnected with a new proctor later, she entered the correct password without any difficulty. So the problem we had at the beginning seems to be resolved now. Sounds good. If you have trouble the first time, can you reconnect/reassign to a new proctor the same day? Yep on the same day. I hope you have no trouble. Sounds good, thanks! I took the exam last night and I also experienced the password issue. My proctor failed 5 times and finally was able to figure out the correct one. I had to point out like 'you might have a wrong password' as he had no idea why it is not working!!  Sounds good, thanks for the info.
https://campuswire.com/c/G984118D3/feed/855 MP3 Grade on Coursera not showing up I got a success response on my latest push and grade shows as one on the leaderboard in LiveDataLab, am I missing something?I reduced the tolerance on my iteration, it completed in fewer steps, and now my grade shows up Although this didn't work for me, re-launching the assignment from Coursera and then again pushing to github did fix it for me. I did something similar by updating `epsilon` to a larger value in order to make it easy to converge. the 2nd attempt got my grade to show up. (although both attempts show "1" in the leaderboard) When we check for convergence, does that mean if our current log likelihood is <= epsilon then it has converged? Or is the case for >=? nevermind  May I ask what's the issue? 
https://campuswire.com/c/G984118D3/feed/1249 LiveDataLab FAILURE without logs **SOLUTION:** I deleted the linked GitHub accounts from LiveDataLab, relinked GitHub to LiveDataLab and then it worked!  On LiveDataLab, I am getting FAILUE status, but my code is not actually running. I commented everything and kept only a 'Hello World' printing statement, that didn't show up either in the log.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/501e10b1-a0b3-45cd-b554-8cb7d4edc6db/image.png)!  - My GitHub Personal Access Token had expired. I created a new token from GitHub, relinked that to LivedataLab. - I deleted my GitHub repo and redid all the cloning and pushing steps from the start. - I reopened the LiveDataLab link from the Coursera 'Open Tool' link.  None of these helped.   My issue is similar to the following post. I tried the solutions mentioned in the replies of this post, but these are not working.  https://campuswire.com/c/G984118D3/feed/1138. I also encountered the same problem. Kowshika, this is the error for your code:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e4d43f4c-ea8b-4d12-92fa-3cfc66f44715/image.png) Yijing, you are also getting the same error as above Thanks a lot for getting back, Priyanka. I later could see the logs after deleting the linked accounts from LiveDataLab and Coursera is also showing my grade for MP3 now. As Kowshika said, I relinked my GitHub account and it worked! I guess it is because I used a new GitHub token.
https://campuswire.com/c/G984118D3/feed/780 Please help on submission not going through on LiveDataLab Hi, I am trying to submit my assignments 2.3 and 2.4 but I am unable to on LiveDataLab. My webhook on github is detecting the pushes, but not directing submission to LiveDataLab.   Please help!  ![Screen%20Shot%202022-10-02%20at%204.07.44%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/55e68557-c00e-4747-8330-cd72a50e2a3a/Screen%20Shot%202022-10-02%20at%204.07.44%20PM.png)  ![Screen%20Shot%202022-10-02%20at%204.07.58%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/589d9ac2-6c1c-45b6-aba5-bbd3b0b5c4fc/Screen%20Shot%202022-10-02%20at%204.07.58%20PM.png)![Screen%20Shot%202022-10-02%20at%204.09.04%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/15bb3aeb-efbe-4ff1-b0d9-6ffc43dc2013/Screen%20Shot%202022-10-02%20at%204.09.04%20PM.png)   The submission does not appear for 2.3 or 2.4 assignments. Please help! Hm, I am not seeing any of your submissions. Can you try to delete and relink your github account? Can you please try deleting the linked account and relink? Also, please make sure to check your token to if it is still active. By default, the token is set to expire after 30 days. Are you using the correct URL for the webhook? It is http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger And not the one listed in the MP2.4 set up instructions
https://campuswire.com/c/G984118D3/feed/115 Combined Slides for Each Week? Hi Professor and TAs, I would appreciate it if you could kindly upload weekly "combined slides" (or a single pdf for the entire class). This will be very useful for learning and reviewing.  Many thanks!I agree if that won't add too many burdens to the professor and TAs. What I followed is to download the course content using the https://github.com/coursera-dl/coursera-dl. This way I do not need to download the pdf's separately. Hi,  One challenge would be the size of the document and its organization? We currently have a way to download the slides for a particular week. It becomes easier to organize it so we can only differentiate and easily pick up the slides for that week.  If required you can also try appending the pdf using different tools available in the web. 
https://campuswire.com/c/G984118D3/feed/120 can someone help with MP1 i do not find clear instructions on MP I am completely lost . Can some one pelase help ?If you have problems with the MP setup, you may refer to this post #24   For the MP1, you only need to use some methods in **metapy** to define 2 variables. The github page (where the readme file will be shown by default) provides detailed instructions on how to use those metapy methods. You can start by following the instructions on Coursera. Once you git cloned the assignment, go though the README file, it has very detailed instructions.  The README file in the git repository should have all of the instructions for the MP. I had the same problem but realized the only way to see the project instructions is to follow the mp setup process and clone the git repo first. You won't see the lab instructions on Coursera or LiveDataLab. I converted the README to a Jupyter notebook, if that helps...check out [#128](https://campuswire.com/c/G984118D3/feed/128) thanks All for helping :) thanks for helping.
https://campuswire.com/c/G984118D3/feed/203 MP1 Submission I managed to submit successfully, but the auto-grader does not provide me logs, even after waiting a while.  First Submission: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/04e64ffe-992f-4b94-87ac-8a03e989dc12/image.png)  Current time as of post: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/fd300e60-0bb5-470b-ba44-656f9d3257b9/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bf5e38fe-44b2-4767-88e9-2c2832227355/image.png)  1. What should I do to resolve the issue with the logs not showing what happened?  2. Do we need to have the config.toml file as part of the submission (or if the auto-grader is looking for that file)?I got this error too!! So what I did was that I had to delete the linked account on livedatalabs and link it back again. I created a new authorization token as well. I then copied my code from my previous project, deleted that github repo,  made the repo again, and then linked it to the likedatalabs with the webhook again. I then pasted my old code and submitted it and I passed!! These instructions fixed my issue. Thanks.
https://campuswire.com/c/G984118D3/feed/944 practice quiz9- Q2 confused.  i think it should be (0.3x0.1+0.7x0.5)x(0.4x0.3+0.005x0.7)=0.0589The key to this question is this sentence in the directions "To generate a document, first, one of the two language models is chosen according to P(θi), and then all the words in the document are generated based on the chosen language model." In other words, draw $$\theta_i$$ first, then multiply by the probabilities of each word given $$\theta_i$$. There are two cases so will need to add these probabilities for each $$\theta_i$$  P("the technology") = P($$\theta_1$$)P("the"|$$\theta_1$$)P("technology"|$$\theta_1$$) + P($$\theta_2$$)P("the"|$$\theta_2$$)P("technology"|$$\theta_2$$) = (0.3x0.4x0.1) + (0.7x0.05x0.5)
https://campuswire.com/c/G984118D3/feed/1132 some helpful visual if you just start the project - hope it will help. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f11e614d-2a40-4572-820c-b724ca3606f7/image.png)Thank you Jianci! I'm curious what's the data type of term_doc_matrix? Is it a 2d numpy array or is it a pandas dataframe? if 2d array, how do you add column headers to this matrix? For this project, we are all using matrix. I manually created the chart in excel  I know we’re using matrix, just as its name suggests… but how do you represent the the column header in term_doc_matrix?  The column heads are not required that’s why I usually create visual charts to help me think … as long as the order of ur variables don’t change, you will be able to calculate without column/row names …
https://campuswire.com/c/G984118D3/feed/794 Question in Practice Quiz 05 Hi,   In the practice quiz of week 05:  In PageRank, one needs to estimate the transition probability between pages. Which one of the following statements is true:  - The sum of probabilities of jumping from one page to all other pages is one - The probability of jumping to page A from page B is the same as that from page B to page A - The number of parameters in the transition matrix is smaller than the degree of freedom of the transition matrix  Could someone please help me understand why the 3rd option is incorrect?  Also for the 1st option, if a page has no outgoing links, should the sum of probabilities of jumping to other pages equal zero?  Thanks Degree of freedom can't be larger than the number of parameters because it's a measure of how many things are allowed to vary, you can't have 8 degrees of freedom when there's only 7 variables. PageRank also has random hopping, so that might be related to the second question? in addition to Robin's point about degrees of freedom, to more directly attempt to answer the second question:  in the case where a page has no outgoing links, it is probably safe to assume that the `Transition Matrix` probabilities of jumping to other nodes from that node (without random jumping) are 0, though maybe we could add a special case here where $$M_{i,j} where \ i == j$$ would be 1 for that node. However, with random jumping our probabilities from the original Transition Matrix plus the random Transition Matrix (which has uniform probabilities for all entries) get mixed together. In this way, we can still maintain that with random jumping the sum of the probabilities of jumping to other pages equals 1 (but they will all be from random jumps) thank you! thank you!
https://campuswire.com/c/G984118D3/feed/1131 don't print output, or you may not pass I have used a lot of prints to debug .. then i found with print() I can't pass livedata and it keeps telling me I have errors in implementation... until I commented out those print... just FYI.Thank you that is really useful.
https://campuswire.com/c/G984118D3/feed/1108 not able to run through E step and forward I think I completed all sections but I have trouble running the code through E step: M step and forward. My results are like this:![Screen%20Shot%202022-10-20%20at%2010.57.53%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d5a06d67-3f76-4214-97ff-88d459754533/Screen%20Shot%202022-10-20%20at%2010.57.53%20PM.png)Seems you are not calling E and M steps in plsa() function. thank you! that is the issue
https://campuswire.com/c/G984118D3/feed/200 CMake build failed when install metapy at m1 mac Updated: Thank to Charles Stolz. I solved this problem by  ``` conda create -n cs410project1 python=3.5 conda activate cs410project1  pip install metapy pytoml     Python 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:30:03)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin Type "help", "copyright", "credits" or "license" for more information. >>> import metapy ```   Hi I follow all the mac m1 anaconda instruction as other posts mentioned but still encounter this weird error when I try to pip install metapy. Thanks in advance for any help  ```  ```   [  0%] Building CXX object deps/meta/src/util/CMakeFiles/meta-util.dir/progress.cpp.o       [  1%] Linking CXX static library ../../../../lib/libmeta-util.a       [  1%] Built target meta-util       Scanning dependencies of target ExternalICU       [  1%] Creating directories for 'ExternalICU'       [  3%] Performing download step (download, verify and extract) for 'ExternalICU'       -- Downloading...          dst='/private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz'          timeout='none'          inactivity timeout='none'       -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'       -- [download 100% complete]       -- verifying file...              file='/private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz'       -- MD5 hash of           /private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz         does not match expected value           expected: '68fe38999fef94d622bd6843d43c0615'             actual: 'aed0fdbf7e96b35513dc6b22ac01c642'       -- Hash mismatch, removing...       -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'       -- [download 100% complete]       -- verifying file...              file='/private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz'       -- MD5 hash of           /private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz         does not match expected value           expected: '68fe38999fef94d622bd6843d43c0615'             actual: 'f8d9c014c5d29bbae0dbb87337859678'       -- Hash mismatch, removing...       -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'       -- [download 100% complete]       -- verifying file...              file='/private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz'       -- MD5 hash of           /private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz         does not match expected value           expected: '68fe38999fef94d622bd6843d43c0615'             actual: 'e9656697bd628d7733f020da9fd0f576'       -- Hash mismatch, removing...       -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'       -- [download 100% complete]       -- verifying file...              file='/private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz'       -- MD5 hash of           /private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz         does not match expected value           expected: '68fe38999fef94d622bd6843d43c0615'             actual: '04d52b5f5b3337a9d207d4384c5512fa'       -- Hash mismatch, removing...       -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'       -- [download 100% complete]       -- verifying file...              file='/private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz'       -- MD5 hash of           /private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz         does not match expected value           expected: '68fe38999fef94d622bd6843d43c0615'             actual: '2a869cb94c97a409ee60844d50f4511a'       -- Hash mismatch, removing...       -- Using src='http://download.icu-project.org/files/icu4c/61.1/icu4c-61_1-src.tgz'       -- [download 100% complete]       -- verifying file...              file='/private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz'       -- MD5 hash of           /private/var/folders/y0/rz1nk38x1dzbl3zqm230_mw00000gn/T/pip-install-0y3s7mmq/metapy_a11e6f32d883408eac2d760a1c73801c/deps/meta/deps/icu-61.1/icu4c-61_1-src.tgz         does not match expected value           expected: '68fe38999fef94d622bd6843d43c0615'             actual: '2547c7ffd36ff90dfa8ef42ed4a15192'       -- Hash mismatch, removing...       CMake Error at ExternalICU-stamp/download-ExternalICU.cmake:170 (message):         Each download failed!                                          make[2]: *** [deps/meta/deps/icu-61.1/src/ExternalICU-stamp/ExternalICU-download] Error 1       make[1]: *** [deps/meta/CMakeFiles/ExternalICU.dir/all] Error 2       make: *** [all] Error 2       error: CMake build failed       [end of output]      note: This error originates from a subprocess, and is likely not a problem with pip. error: legacy-install-failure  × Encountered error while trying to install package. ╰─> metapy  note: This is an issue with the package mentioned above, not pip. hint: See above for output from the failure. (python=3.7) ➜  ~  ```I also own an M1 Machine, and could not get metapy to install correctly. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153 I have an M1 machine and was able to get it to run fine using the anaconda distribution and creating a python 3.5 project  ``` conda create -n cs410project1 python=3.5 conda activate cs410project1  pip install metapy pytoml     Python 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:30:03)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin Type "help", "copyright", "credits" or "license" for more information. >>> import metapy ``` Note from MP1 -  We'll use metapy---Python bindings for MeTA. , use the following commands to get started.  Please note that students have had issues using metapy with specific Python versions in the past (e.g. Python 3.7 on mac). To avoid issues, please use Python 2.7 or 3.5. Your code will be tested using Python 3.5 Thank you so much. You are my life saver!!! I will update your comment to my post. Thanks
https://campuswire.com/c/G984118D3/feed/803 Which grades are final? Hi everyone,  Will the grades posted on Coursera for each quiz/assignment be the correct/final grades, or is there somewhere else we have to check our grades? Thank youI believe up until our final grades are posted on our Student Center profile, what we see in Coursera should be correct and "final". (TAs can probably confirm this) Your Coursera grades should be correct (please let us know if they are incorrect). The final grade that you get will be posted on self-service/myIllini, but for the individual assignment grades, you should use Coursera as reference.
https://campuswire.com/c/G984118D3/feed/482 Project team size How to find out what's the workload and plan the size of the team. Does it matter if we plan to have 5 people in the team, only to find out that 20*5 worktime is not required or we need more time(in case we have 3 members). Is there any team size that's suggested based on the project (1-5) topic?> How to find out what's the workload and plan the size of the team  We ask that you provide your best estimate. There is no way to know for sure. Ideally, you can design the project in a way where the workload is flexible. For example, in the proposal, you can write something like "If we have additional time, we will add X and Y".  > Is there any team size that's suggested based on the project (1-5) topic?  No, there isn't. It depends more on your specific project than the topic. 
https://campuswire.com/c/G984118D3/feed/747 Signing Up For Exam 1 Is it called "CS 410 - DSO- Exam 1 FA22" (I am in the DSO section, with a CRN of 67393)? When are we supposed to sign up?Yes, I have signed up for this section.  Can TAs pls confirm Hi, it's better to wait for the formal anouncement of exam registration.  Based on my experience with other courses, sometimes the link will be changed, then we have to sign up again. I also signed up for the same session, is it still valid?  you can signup now!
https://campuswire.com/c/G984118D3/feed/525 Now that everyone is finished with MP2.2 I am curious to see the parameters in which each of you determined relevancy. As relevancy was a binary choice, I found choosing relevancy of a document to be sometimes difficult. Some results would have partial information on what I was querying for but missing the general point of the question. For example, in a search for "Jobs in SEO", a Wikipedia article telling me about Google SEO is related, but not entirely what I was looking for.   Thank you,  DillonI think the answer really depends on what the user is looking for to find. If he is looking for a particular answer to his query where the result to be expected is just one document then in the above case, he would categorize the document as not relevant, but in other cases, where the user is just browsing as discussed in the lecture videos, the user could categorize it as relevant. I think starting with a particular idea in mind before querying would help us better decide whether the query is relevant or not relevant. I believe there is no right or wrong answer here and that it is really dependent on the user, what may be relevant to one user might not be relevant to the other as discussed by the Professor in the lecture videos as well. The usefulness or relevancy of each document with respect to a query should vary on users' implicit intention. This is why, e.g., the search engine algorithm gets feedback on the click-through rate. Clicks on the link is a good estimator of relevancy, and the click-through rate is not a binary but a continuous variable. You have a valid point. I believe others would have had a similar experience. In addition, we were also restricted to only 50 searches on the page (if I am not mistaken). I feel there could be more relevant document that could have not returned in the search and were missed since we were not able to click on the next page. To your point, we have covered multi level judgement in the class which could be helpful in understanding the true relevance of a document.  Having said that, I believe the intent of the assignment is for students to get familiarized with the concepts and hence would have been presented with the binary choice instead of multi level judgement to reduce the complexity.  It would be interesting to see how the MAP changed over time. Like others have said, relevancy is subjective based upon the expectation of the users. For me, I determined relevancy based upon whether I felt that the resource sufficiently resolved my question, e.g., was it sufficient to explain a concept or help me in a code implementation, or at least refine my understanding to the point where I could go back to submit a more appropriate query. Resources that simply regurgitated my query terms but didn't further help develop my understanding I marked as non-relevant. For example, I was surprised at how many people submitted the Coursera syllabus or weekly overview pages as resources. I don't understand how those could be thought of as having reasonable utility. They simply state terms and concepts and aren't even a true hub page with outlinks. I marked all of those results as non-relevant.
https://campuswire.com/c/G984118D3/feed/95 MP1 Tip: Webhook for LiveDataLab  Everytime I try to add webhook to my github it always shows sending ping failed to the address I copy pasted from mp setup.docx.   I just noticed if you copy paste directly from the document the web url is wrong  **http://livelab**.centralus.cloudapp.azure.com/api/webhook/trigger  Correct one is below:  **http://livedatalab**.centralus.cloudapp.azure.com/api/webhook/trigger  Hope this helps. 
https://campuswire.com/c/G984118D3/feed/608 MP2.3 Coursera Grade Not Updating Hi everyone,  I submitted MP2.3 a few times on the LiveDataLab and finally got a score of 1. However, my grade on Coursera doesn't update and still shows "30%" as the grade I got at the first time. Does anyone know what is going on? Thank you!    ![grade_question.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9ee0fae6-f8a2-4d0e-b3f9-db5de2fa9d2d/grade_question.png)I got 1 on livedatalab but 0% on coursera.. I saw a answer in another post(which I connot find anymore) says the TA says there is an error in Coursera system and they are working on updating, Just wondering where can i see the score? I saw "SUCCESS" in my submission but I don't see any score/grading. Cousera doesnt have grading either.  Click "Leaderboard" on the left of the Submission History page. The score will then show up. Thanks for the answer! I also cannot find the post. I might contact TA later if my score has not been updated. Hi i'm facing the same issue as well. Have you got any replies from TAs yet or can you share the TAs contact info? Thanks! login to livedatalab one more time and submit your code again. This method works for me. Hi, I followed the tip on #659 and succesfully renewed my grade on Coursera. Resolved. Thank you!
https://campuswire.com/c/G984118D3/feed/816 Questions in Practice Quiz 06 Hi All,   I have 2 questions in Practice Quiz 06:  ##### Which of the following tasks can be solved as a classification problem? I'm not sure why the option "Ranking" can also be considered as a classification problem, doesn't rank always requires a non-binary value to rank?  ##### When a new user comes, which of the following will NOT help for recommendation? - Ask user to first select a few items that he likes - Ask user to provide a short description of himself - Recommend user with random selected items  I was wondering why option C won't help? Since the user might be able to select a few of those randomly given items, then the recommendation system can implicitly learn the user's preferences from it to improve the recommendation system.  I was wondering if anyone can help me to understand them?  Thank you so much For ranking: First, I'll say that classification problems don't necessarily have to be binary. Classifying a picture as being a dog, cat, or bird is also classification (in this case, 3 classes).  However, in Lecture 6.2 `Learning to Rank -- Part 2`, we see a Logistic Regression approach to ranking. In this approach (which is also hinted at in lecture 6.1), we do classify documents as "Relevant" or "Not Relevant" and can rank them based on this, for example by their probability of being relevant.  For the second question mentioned, I think you have to take each of these 3 options at face value. Will `Ask user to first select a few items that he likes` help for recommendations by itself? Yes, it will allow us to leverage content-based filtering to determine what the user likes. Will `ask user to provide a short description of himself` help for ranking by itself? Yes, we can use this description to see how similar the user is to other users in a collaborative ranking type approach. Will `Recommend user with random selected items` help by itself? No, we would need to pair it with some other strategy, like allowing the user to rank the randomly provided options. Since this option does not mention allowing the user to rank those options, we should assume that we are only presenting random items to a user here, which will not help recommendations. While the other two options explicitly provide us with user input to either compare that user to other users or compare content that user likes to other content, the third answer does not give us any information about the user and is just the action of presenting random recommendations.  I'll mark your question as unresolved in case others can think of more to add here, but let me know if that made sense or if I can clarify further
https://campuswire.com/c/G984118D3/feed/366 MP2.1 hi,  When can I get access to MP2.1 overview and google document access. I have already submitted request. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dee58194-4983-4c43-be88-a42443cff215/image.png)Make sure you switch to UIUC ID while requesting access.  You will first need to activate Google app in your illinois account: https://www.nuestraverdad.com/post/activating-google-apps-uiuc  Then you can log into google using the illinois edu email address to view the document. thanks . I am not able to create an account  Hi Nusrat,  1. The first field in the "Create Account" section is meant for the email, but the second field is meant for your username (which is supposed to be your NetID). So when signing up, please change your second email to your NetID. 2. Have you downloaded and started the VPN, as described in the overview document? thanks kevin, it worked for me :)  Great! Glad it works We should include this info at the first place. I bet a lot of people don't know about this Hi Kevin, unfortunately I am still running into issues and could use some assistance. I am getting access issues for the "sign up sheet" still, and I have followed the directions activating my google account (when I logged onto it it was already on), as well as tried access both with and without VPN. I am unsure where to go from here, if I could get some guidance that would be great! Thank you.  I finally can access Google doc with my linked account through private mode of chrome You do not need to access the sign-up sheet. The only documents that you need to access are listed in the MP2.1 description on Campuswire. 
https://campuswire.com/c/G984118D3/feed/362 MP2.1 ConnectionError I still got this   ``` Beginning execution...  DEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.  Defaulting to user installation because normal site-packages is not writeable  Collecting metapy  Downloading metapy-0.2.13-cp27-cp27mu-manylinux1_x86_64.whl (14.3 MB)  Collecting pytoml  Downloading pytoml-0.1.21-py2.py3-none-any.whl (8.5 kB)  Collecting tqdm  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)  Collecting requests  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)  Collecting glob2  Downloading glob2-0.7.tar.gz (10 kB)  Collecting importlib-resources; python_version < "3.7"  Downloading importlib_resources-3.3.1-py2.py3-none-any.whl (26 kB)  Collecting idna<3,>=2.5; python_version < "3"  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)  Collecting certifi>=2017.4.17  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)  Collecting chardet<5,>=3.0.2; python_version < "3"  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)  Collecting urllib3<1.27,>=1.21.1  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)  Collecting typing; python_version < "3.5"  Downloading typing-3.10.0.0-py2-none-any.whl (26 kB)  Collecting singledispatch; python_version < "3.4"  Downloading singledispatch-3.7.0-py2.py3-none-any.whl (9.2 kB)  Collecting contextlib2; python_version < "3"  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)  Collecting pathlib2; python_version < "3"  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)  Collecting zipp>=0.4; python_version < "3.8"  Downloading zipp-1.2.0-py2.py3-none-any.whl (4.8 kB)  Collecting six  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)  Collecting scandir; python_version < "3.5"  Downloading scandir-1.10.0.tar.gz (33 kB)  Building wheels for collected packages: glob2, scandir  Building wheel for glob2 (setup.py): started  Building wheel for glob2 (setup.py): finished with status 'done'  Created wheel for glob2: filename=glob2-0.7-py2.py3-none-any.whl size=9308 sha256=eba2562907773ff71f8e58eaf6b6d330a55c518211dd35e23048bca2e5bc18fb  Stored in directory: /home/livelab_jenkins_user/.cache/pip/wheels/3e/a4/8e/d0e602135b46828fbd59fbb2e394bda746cb96d99487cc2a24  Building wheel for scandir (setup.py): started  Building wheel for scandir (setup.py): finished with status 'done'  Created wheel for scandir: filename=scandir-1.10.0-cp27-cp27mu-linux_x86_64.whl size=11146 sha256=a6666da9d6b54f592ba76ebd5d67375bf9686c8c976e6cf973844fc73233bcd0  Stored in directory: /home/livelab_jenkins_user/.cache/pip/wheels/58/2c/26/52406f7d1f19bcc47a6fbd1037a5f293492f5cf1d58c539edb  Successfully built glob2 scandir  Installing collected packages: metapy, pytoml, typing, six, singledispatch, contextlib2, scandir, pathlib2, zipp, importlib-resources, tqdm, idna, certifi, chardet, urllib3, requests, glob2  WARNING: The script tqdm is installed in '/home/livelab_jenkins_user/.local/bin' which is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  WARNING: The script chardetect is installed in '/home/livelab_jenkins_user/.local/bin' which is not on PATH.  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.  Successfully installed certifi-2021.10.8 chardet-4.0.0 contextlib2-0.6.0.post1 glob2-0.7 idna-2.10 importlib-resources-3.3.1 metapy-0.2.13 pathlib2-2.3.7.post1 pytoml-0.1.21 requests-2.27.1 scandir-1.10.0 singledispatch-3.7.0 six-1.16.0 tqdm-4.64.1 typing-3.10.0.0 urllib3-1.26.12 zipp-1.2.0  File "mp2_grader.py", line 153, in <module>  grade()  File "mp2_grader.py", line 126, in grade  resp = requests.post(get_mongo_url , json={'collection':'faculty_bios','query':{'email':{ '$ne':email}}})  File "api.py", line 117, in post  return request('post', url, data=data, json=json, **kwargs)  File "api.py", line 61, in request  return session.request(method=method, url=url, **kwargs)  File "sessions.py", line 529, in request  resp = self.send(prep, **send_kwargs)  File "sessions.py", line 645, in send  r = adapter.send(request, **kwargs)  File "adapters.py", line 519, in send  raise ConnectionError(e, request=request)  ConnectionError: HTTPConnectionPool(host='livelab.centralus.cloudapp.azure.com', port=80): Max retries exceeded with url: /api/mongodb/getrec (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f218e04b590>: Failed to establish a new connection: [Errno 101] Network is unreachable',))  Build step 'Execute shell' marked build as failure  Finished: FAILURE ```Can you try to see if you are following the latest MP2.1 instructions posted yesterday? The Lab requires you to only use DL Extension in the web browser after connecting to the Illinois network using a VPN client. could you pleas provide the link  It is updated in Coursera Week 3 - MP2.1. so this assignment will be graded manually? It's already answered here #352. Also, can refer the below discussions for further details of M2.1  https://campuswire.com/c/G984118D3/feed/359 https://campuswire.com/c/G984118D3/feed/354  So this one becomes obsolete????? https://github.com/CS410Assignments/MP2.1 I spent at least 5 hours on this assignment and you guys just changed the assignment for no reason.... Probably the course staff can comment on this. If you want you can change the status to Unresolved. I'm just your fellow classmate, trying to guide you where you can see the latest MP. :) Hi Jay,  My apologies for the misunderstanding on MP2.1. We tried to make it clear with Campuswire posts +  locking the Coursera assignment description that it would be changing from the previous year. We're also in the process of removing the old assignments from Github and LiveDataLab, which will hopefully prevent future misunderstandings.  We made the changes to MP2.1 and MP2.2 so that the students' efforts towards completing the assignments would build something that brings benefit to the entire class (i.e., a searchable collection of useful webpages related to the class). Additionally, the overhead for completing MP2.1 is lower than the previous version (e.g., no coding is required). We hope that the increased benefit and lower overhead offers some consolation.   Again, sorry for the confusion!
https://campuswire.com/c/G984118D3/feed/1185 Proposal Submission showed over due  Hi,      I submitted the Project Proposal to github and added the link to CMT. But today I saw the "over due" in Coursera. Is it a correct status? and when will we have the grade usually? Any idea? Thanks.   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3e4c703a-a758-4a5b-a7bb-e87cab476b9e/image.png)Yes, this is correct. The grade will be entered after the peer review phase.  Thank you.
https://campuswire.com/c/G984118D3/feed/1066 TA Office Hour Canceled Hi all, It appears that I've caught the dreaded flu, and I won't be able to hold my usual office hour tomorrow. I will keep the zoom link open and accessible if you want to use it to help each other out. Stay warm out there! ~Assma
https://campuswire.com/c/G984118D3/feed/1045 Instructions for project proposal Do we have a detailed instructions for the project proposal? I think we are required to answer a few questions in our proposal.  ThanksI found them in the link below: https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit Similar question is answered here #1033 i still cannot access this folder..How long it takes to get access after we request ? You need to use your Illinois email to access it. Other emails won’t work.  Log in Google using your Illinois ID and password
https://campuswire.com/c/G984118D3/feed/36 Issue installing metapy How can I fix this error?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c03a3de5-0ed2-4ab6-bbf5-c98c45d8bf2d/image.png)From the error log , it looks like you are missing cmake executable. You have anaconda installed so try to install cmake via Conda https://anaconda.org/anaconda/cmake   See #34 Also you can see explanations givens in #28  Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153  
https://campuswire.com/c/G984118D3/feed/130 Group project achievable with one person? Hello,  I just wanted to ask and see if the group project is achievable with one person?  I do find the group coordination to be quite interesting as I haven't seen that in other courses before!  Thanks!Yes, it is certainly possible to do the final project independently. But we do encourage students to work in groups! Thank you for your response! I will definitely consider group work too!
https://campuswire.com/c/G984118D3/feed/1123 Question about SmartMoocs for Course Project My group is interested in extending or improving upon SmartMoocs for the CS410 term project. Is it possible to get access to the source code?  Thank you   https://smartmoocs.web.illinois.edu/courses/textretrieval/lectures/0-1  
https://campuswire.com/c/G984118D3/feed/212 Metapy Documentation? Is there a list of documentation for what the various functions in Metapy do? I looked in the github repository, and while there are some tutorials, I did not find a place that listed all the functions and what they did.  I am currently somewhat confused about PennTreebankNormalizer, default-unigram-chain, default-chain, ptb-normalizer, how toml files work, and what the tree stuff produced by the final code block in the README is about (though I finished the MP).Is this what you are looking for? https://meta-toolkit.org/doxygen/namespaces.html You may have already known, but there are also many tutorials at https://meta-toolkit.org/   
https://campuswire.com/c/G984118D3/feed/181 Is the platform problem or my problem Submitting Mp1, can run in my PC but seems some problems in LiveData  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/681edfbf-6a4e-4939-b629-4945c99cce86/image.png)   Update: This is also related with the network problem. Same code, one day after submission, passed successfully.You might have to check if the webhook is set up correctly. This post might help #109 Can try to restart and may also refer to https://campuswire.com/c/G984118D3/feed/174  restart what?  Seem s not the same problem... seems My LivedDataLab has been linked but the version problem It doesn't seem like the version problem. I pass this one and my log is showing the information about Python version as well. You may want to focus on what is under "Traceback (most recent call last):".  The "TraceBack" says that that's the problem of mp1_grader instead of my code... How to use this instruction to find the corresponding code What is your netid? hc56@illinois.edu Seems the test_tokenizer.post_results() problem... Can not debug further... What's your username on livedatalab? Is this the username you need? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/09ba107d-8cac-4c06-aa1c-8f2f38af6478/image.png) No, the one you use to login to livedatalab. It is: chy147258 correct? Yeah... That seems be my password? 🤔  I use my NetID to login  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/27fea5c0-3d23-4531-bb2c-407601777f65/image.png) Hm.. that seems to be the username stored in our db but that must be what you entered in the username:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ab91b2ad-9f16-45d6-bab9-4e7f43ce5b6c/image.png)  In any case, I already see you have a score in coursera. I think this JSON issue might be due to some network error we were facing yesterday. Your coursera grade should be up to date. Let me try it again Yes!!! It right the network problem. This time, it runs successfully!!! so many time due to the network problem 🤣   Could you please delete my password due to this is a public post. Thank you!
https://campuswire.com/c/G984118D3/feed/101 Lecture 2.1: IDF Hi, Are these IDF values calculated based on the example sentences literally or just some numbers to make a point? Thanks! ![Screen%20Shot%202022-08-26%20at%2010.59.55.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d19fc28d-57d2-4a35-82c6-96ffaf2b1e54/Screen%20Shot%202022-08-26%20at%2010.59.55.png)IDF values seem to be indicative of the logic being explained. If it helps, a similar post is answered at [](url)https://campuswire.com/c/G984118D3/feed/96 thanks a lot, saw that post but want to get confirmation from TA Yes, as some of you have already explained, the IDF values here are meant to illustrate the idea behind IDF; they aren't the exact values of IDF that we'd obtain if we were to use this toy data set. Thanks to all of you for the timely answers to the question here and a similar question in another post! 
https://campuswire.com/c/G984118D3/feed/452 Office hours going over the quizzes? Hi, I wonder are there any recordings of office hours that go over how to solve the quizzes' questions? Would help a lot for learning. Thanks!So far, my office hours have been mostly used for discussing some issues with MPs and project ideas. If there is any critical elaboration of any concept or quiz question beyond what the lectures have covered or the responses already posted on Campuswire, we will post the explanation/elaboration here on Campuswire, so that  those of you who didn't visit the office hour would also be able to see it.  Note that we generally wouldn't discuss test quiz questions publicly or with anyone who hasn't taken it yet. Such discussion, however, can be done with a "private" note posted by the student to the TA and instructor on Campuswire. For example, if you feel a question in a test quiz is ambiguous,  you should post a private note to ask us to clarify. Of course, you should feel free, indeed are encouraged, to discuss any *practice* quiz questions here on Campuswire or any concept covered in a test quiz without referring to the specific question in the test quiz. 
https://campuswire.com/c/G984118D3/feed/1072 Example Tech Review Hi, maybe I missed it but could we by any chance get an example tech review? I know the general structure is intro, body, conclusion but it would be helpful to see previous student submissions just to see good examples of what the instructors are looking for.I don't know if any example reviews will be provided, but the instructions are pretty open-ended so as long your review meets the general requirements it should be fine. Yes example tech reviews would be very useful. The instructions for this are in week 8 right? Apart from this, are there any other specific instructions we have to follow? Hi I know I'm a little late to this one. But I found this github page of a past student https://github.com/raman162/cs410-tech-review/blob/master/techreview.pdf
https://campuswire.com/c/G984118D3/feed/936 Gamma and delta encoding ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/182186c4-0ca8-41e8-ada9-9a2a29b09950/image.png)  Hi, can someone walk me through how in gamma encoding 3 is 101, and in delta encoding 3 is 1001? Having a hard time figuring out to calculate these.see #301 Here are step-by-step calculations. I hope this will be helpful. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/306f7555-bce6-47fc-a79e-c12c233a3e92/image.png) You can easily (inverse) gamma encode without the floor or log functions.  **Encode** - Convert the number to binary:    - $$3 \to 11$$ - Create a unary prefix equal in length to the binary encoding.     - The binary encoding is length n = 2, so this means (n - 1) 1's followed by a 0   -  The prefix is $$10$$ - Drop the leading 1 from the binary encoding and attach the prefix for the gamma code:   - $$10|1 \to 101$$  **Decode** - Drop the unary prefix, which is everything up to and including the first 0   - $$101 \to 10|1 \to 1$$ - Prepend a 1   - $$1|1 \to 11$$ - Decode the binary   - $$11 \to 3$$
https://campuswire.com/c/G984118D3/feed/915 Q2 from Practice Quiz 6 Can anyone help as to how Ranking is considered as a classification problem. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d81444ed-4a05-4772-9a21-2d765e945f4f/image.png)#816 If you are familiar with ML methods, think of ranking as a multiclass problem using the softmax function to return the "rank 1" class among k possible outcome classes.    In this context, the k possible outcomes are the ranks (i.e. 1 to 10) and instead of returning the top probability, i.e. softargmax, you return the full ordered list of probability rankings.  Thanks, this makes sense. Thanks for the pointer.
https://campuswire.com/c/G984118D3/feed/471 Lesson 4.1, 4.2 & 4.3 Hello,  After I take Lesson 4.1-4.3, I am still confusing about probabilistic retrival model, language model and query likelihood retrieval function. Can anyone explain the relationship among these model and functions?   In addition, is the query likelihood function a ranking function to rank the documents based on a specific query?  Thank you!In my understanding, probabilistic retrival model is a ranking function, but it has many specific forms. You can implement a probabilistic retrival model through the language model technique. One specific implementation of language model is query likelihood retrieval function. In other words, query likelihood retrieval function is a kind of probabilistic retrival model through the language model techinique.  So, yeah, the query likelihood function is a concrete ranking function to rank the documents based on a specific query.  Probabilistic retrieval models are models in which we "define the ranking function based on the probability that a given document d is relevant to a query q". Using language models for retrieval is one case of probabilistic retrieval models. The query likelihood retrieval model is a one language model approach.  Query likelihood is an approach wherein we assume that each query word is obtained from the document and each query word is generated independently. "The probability of relevance can be approximated by the probability of a query given a document and relevance, p(q | d , R = 1)".  I guess what you really want is p(R=1|q, d), but you don't have the "big table" of relevance judgements available to you.  You do have the documents available, so you can calculate p(q|d), and p(q,d) is approximately p(R=1|q,d).  Is it that you use a language model of the document to calculate the probability of the query? from my understanding:  a **language model** is really just a probability distribution over word sequences. this can also be called a *generative model* because it can be used as a probabilistic mechanism for generating text. however, language models are also useful for information retrieval.  the **query likelihood** is the probability that, given a certain relevant document that the user wants to view, how likely is it that the user would provide a certain query? we tend to make the assumption that each query word is generated independently, with each word picked from the user's ideal "imaginary document" (though this assumption can be revised to prohibit zero probabilities from occurring by saying the user is picking query terms from a *language model* instead -- one that doesn't have zero probabilities. in fact, even just using the "document" is a language model, it is just a "document" language model instead of something like a "collection" language model that is built off of the entire collection, so it is more restricted). this query likelihood can then be used to calculate rankings.   a **probabilistic retrieval model** is an information retrieval technique that then tries to use the statistical *language models* to generate *query likelihoods* in order to rank relevant documents and return them to the user  happy to clarify further if that didn't make sense
https://campuswire.com/c/G984118D3/feed/51 Import "metapy" could not be resolved I have installed metapy. But, when I ran the code, it still showed that "No module named 'metapy'" and has an error that "Import "metapy" could not be resolved". ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/86d8530e-95bf-4454-b932-7f313b75131b/image.png)Python 3.9 does not support metapy for sure. You can check this link https://campuswire.com/c/G984118D3/feed/19, same issue with you has been solved. If this is visual studio, you may wish to check if you have selected the correct python interpreter version under command patellate.  Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153  
https://campuswire.com/c/G984118D3/feed/168 Submission History Hi, I was able to git add, push and commit my code to my private repo and see the change on github. But I don't see anything in the submission history. Wondering how long it took for you guys to show up. I followed all the 14 steps in the instructions pdf. Hoping that I didn't miss any stepIt took a couple of minutes.  The first time I forgot to push, though.  It's been 20 minutes and I still don't see it Oops I had the wrong link to the webhook, just changed it and redelivered and it went through fine. Thanks for you help :) is the webhook link: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger? Is it correct? I met the same issue.
https://campuswire.com/c/G984118D3/feed/768 Syllabus for Exam 1 Hi,  Will exam 1 cover Week 1 - Week 6 ? Or till Week 7?I think it covers week 1 - week 6. Based on lesson 6.10 "summary for exam 1", I believe that yes, the first exam should only cover weeks 1 - 6.  As well, it looks like weeks 13 - 16 are dedicated to our group project work with no introduction of new lecture material, so exam 2 should be focused on week 7 - 12 (splitting the material into 2 - 6 week segments). Exam 1 will cover topics from week 1 to 6. Please refer exam 1 summary video posted by professor -  'Lesson 6.10: Summary for Exam 1' on Campuswire.
https://campuswire.com/c/G984118D3/feed/1089 library/package restriction? Hey,  I want to check in and see whether we have any restrictions in using python libraries or packages to complete MP3-PLSA implementation.  Thanks!I used  only numpy and math library in python for this assignment. Same here, I too have used just math & numpy I'm not sure about restrictions but I don't think we need additional libraries. I didn't see any restrictions in the instructions so you should be good
https://campuswire.com/c/G984118D3/feed/602 no grading in liveDataLab Hi, I just push my code with the p-value file to the github.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f1351286-bc27-4076-9c88-c3ffd249f733/image.png). The latest upload is about 10 min ago.  I haven't seen any feedback in liveDataLab.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ecc006ac-1394-46cd-a755-fb058d430648/image.png). The only score is from the time when the project is created.  Could anyone help? Thanks! does it say in the log that the submission is successful and no faults? what do you mean about the log? I didn't see any log in liveDataLab. sorry when you click your submission, there is a submission log on the right which tells you the running process, if there is any issue, whether submission is successful, etc. I am guessing you might have some error there ok, i see it. it said "FEEDBACK: InL2 not implemented correctly!!! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f806a007-8f9f-42a7-9c42-42cb8abe1cb0/image.png) is the related error "the module has no attribute log2"? What I can think of is I used math.log2() here.   try use math.log(x,2) instead, I think livedatalab does not support log2() many thanks! this time,  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/1984322f-32ad-4628-ab9c-afd436011778/image.png) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4f1b44a0-f0df-4fe7-956b-cc657a4b10ed/image.png) I do not have the p-value txt. Does this mean that at least the code for the InL2  works that I can  proceed to the 2nd part? Thanks! it does not allow me to answer your last update so answering here. No, it looks like your equation is still not correct. You can create significance.txt and put your p-value number in it manually.
https://campuswire.com/c/G984118D3/feed/1129 Question about practice quiz of week 9 The question is **Which of the following generative descriptions is not TRUE about PLSA?**  And the incorrect answer is: **To generate a topic assignment for a word,  a coin is tossed to decide if the topic is from the background topic or not, and the probability of the background is a constant specified by the user.**  The explanation is:  **The probability of background component weight is learned not fixed.**  I am confused by this explanation. Isn't the probability of the background component fixed?  Thank you.  The algorithm tries to learn the parameters that is best likely to generate the topic assignment for a word. One of the parameters is the probability of background model. This means, the algorithm 'predicts' the most likely probability for the background model. But, the option implies that the probability is fixed by the outcome of a random event. Therefore, that is not true.  The background language model is obtained using the whole collection. It is not specified by a user. That is what the explanation means.
https://campuswire.com/c/G984118D3/feed/1273 Tech Review Page Count We are told that "It must be at least ~2 pages" long. Does this mean two pages double spaced, or two pages single spaced? Because we don't require a specific format, single or double spacing is fine. 
https://campuswire.com/c/G984118D3/feed/1080 Additional office hours for projects Hi everyone,  As we are entering the "project" phase of the class, I will begin to hold office hours for answering questions about any aspect of the projects.  The office hours will be held [on Zoom](https://illinois.zoom.us/j/3277733087?pwd=VXZNMGJ4WENvS0Yrd1hlNTFWelRDQT09) during the following time:  Every Sunday (starting the 23rd) , from 8:00pm - 9:00pm CT.   If you cannot make office hours, feel free to post your question(s) on Campuswire, and I will try to get back to you as soon as possible.   Best, Kevin 
https://campuswire.com/c/G984118D3/feed/760 OkapiBM25 parameters Hi Everyone, I am am using OkapiBM25 and have tried several times by tunning b,k1,k3, but was not able to beat the baseline. Can anyone provide any hint on which direction I should go with to tune these parameters? ThanksYou are already in the right direction. Hyperparameter tuning can be a bit tricky and manual at times. First, it's definitely helpful to understand the actual purpose of these parameters and range of values they can take. Then based on those you can mess around with some values just to gain a better intuition (which I'm sure you've already been doing).  From there, you can simply iterate through the different combinations of b, k1, and k3 and save the hyperparameter tuple that results in the best performance (but be careful about overfitting maybe?). The easiest way to do this is probably to modify your search_eval.py to have a triply nested loop. You could also write a shell script which calls search_eval.py with different params.  I hope this points you in a helpful direction without explicitly saying "Hey, use parameters like this..." part of this link helped me in understanding and tuning the parameters.  https://www.elastic.co/blog/practical-bm25-part-3-considerations-for-picking-b-and-k1-in-elasticsearch
https://campuswire.com/c/G984118D3/feed/414 MP1 - Can't load config.toml correctly Hi, I'm following the guide and everything worked fine until this last step. Can anybody tell me what's wrong here?   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bdab0d6e-ee8f-4e23-99be-ed643df28fab/image.png)  I've copied the config exactly as it is in the guide and saved it in my working dir. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5b1563d4-cb0a-41fb-9eb5-e8b42c23e5af/image.png)   Thank you!Can you check if any special characters were added while copying the file or any character removed Maybe try to put it in your python script folder MP1_private?  Hi. Thank you for your answer, but that’s not my working directory…  Given the error message, your `config.toml` may have a syntax error. You can check [their docs](https://toml.io/en/) and examples to ensure all the values are set up correctly. Additionally, did you download the extra packages required to run the pipeline? These are: - https://github.com/meta-toolkit/meta/releases/download/v3.0.2/crf.tar.gz - https://github.com/meta-toolkit/meta/releases/download/v3.0.2/greedy-constituency-parser.tar.gz  As a side note, you can produce the output required by the **autograder** only modifying `example.py` to implement the token stream defined in the guide.
https://campuswire.com/c/G984118D3/feed/1033 Proposal content The project overview says:  "Detailed instructions about the required content of the proposal will be provided by Week 4, including a short set of questions that need to be answered in the proposal. As long as those questions are addressed (wherever applicable), the proposal does not have to be very long. A couple of sentences for each question would be sufficient."  Where can we find the list of questions?List of questions are given after end of every topic in CS410 Project Topics.docx . Look for the below section in the document ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d891ef49-62dc-4354-a0f5-ca91e83c2bd5/image.png)
https://campuswire.com/c/G984118D3/feed/423 Unable to add Youtube and PDF to CS410 DL. The extension doesnt highlight the PDF Text and doesnt accept youtube URL. Any idea how to submit these ?Not sure about the PDF text as I haven't tried it yet, but I just submitted a youtube url and that worked for me. What is the format of your youtube url? I followed the instructions in the file CS410 Digital Library Instructions and was able to figure it out. As per the instructions you have to do this for the YouTube video:  **If you want to ask a question at a specific time, you can change the webpage URL to include the video time (right click video → copy URL at current time → paste into browser), and re-open the extension.** If it isn't highlighting for you, it can be copy pasted as per the instruction in the google doc: **For some pages, your highlighted text will not be captured by the extension. You can tell that this is the case when the “Highlighted text:” area doesn’t display any text. If you want to save the highlighted text in the context or ask a question about the highlighted text, then first copy-paste it into the field right below the highlighted text display.**  Adding one more point to the discussion: you should not copy-paste URLs into the extension itself (e.g., pasting URLs into the highlighted text field or into the explanation field). Moreover, you should not make any submissions from "chrome://*" page. If you try either of these, you'll receive an error.   And a general note: this change was relatively recent (noticed in the logs that this was happening), so no worries for those who have already submitted content this way (will not affect grades). But if you have time, you can visit Your Submissions on http://timan.cs.illinois.edu:4000 and delete + resubmit any malformed submissions. One easy way to check if a submission has the correct format is if you click the submission and the URL redirect works.  For YouTube videos, the highlighted textfield will be empty, right? The highlighted text field can be used if the highlighted text did not show up above the field. You should be able to highlight any text on the youtube page, but it may not always pop up so copying the text prior to opening the extension may be necessary! I am confused what can be the sources of text in case of YouTube videos.  As YouTube doesn't have video transcriptions like Coursera videos and usually the video descriptions are not informative, shall we copy texts from comments of YouTube videos?  I was able to highlight the text of the video title and see it when I open the extension. Please see screenshot below:  ![Screenshot%20from%202022-09-11%2015-47-26.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a753baf6-c170-4b4a-9ed2-50e2e0492f88/Screenshot%20from%202022-09-11%2015-47-26.png) Got it, thanks a lot! Sorry to say but the instructions on adding Youtube videos didnt made sense to me initially. After the fields didnt accept the URL , Based on the screenshot in on of the replies below , I highlight the title of the video which opens the Youtube link in the tinman link. I hope thats acceptable.  to be honest , this was an accidental discovery. It seems to be a good extension but it can be better by having a better user guide. Example with screenshots. Thats would have helped saved a lot of time . it was the URL from chrome https://www.youtube.com/watch?v=vZAXpvHhQow  But based on the screenshot above , when i highlight the title of Video and submit to extension , seems like it recorded the video link. I wish this was more clearer in the user guide. I guess I got confused thinking videos would be recorded in a different way then regular websites.
https://campuswire.com/c/G984118D3/feed/44 setting up MP1 I'm using Mac M1, and previously I have the same issue as #19, then I tried to create a conda virtual environment with python 3.5, as suggested in #39 student answer. Now my terminal looks like this, does it mean the package is installed successfully and I could start working on the MP? ![Screen%20Shot%202022-08-23%20at%2022.14.38.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cb986024-9947-46e5-bce9-4ef18ff71ab7/Screen%20Shot%202022-08-23%20at%2022.14.38.png)  Thanks in advance.Hello there, as I just posted in the answer to #28, for me I got things figured out following this link: https://stackoverflow.com/questions/70205633/cannot-install-python-3-7-on-osx-arm64  Hope you find it helpful.  Thank you! I also have an M1 machine. To solve this issue, I am simply using Google Colab for MPs, because metapy doesn't work on M1. Refer to this post - https://campuswire.com/c/G984118D3/feed/153
https://campuswire.com/c/G984118D3/feed/985 from collections import Counter Are we allowed to import Counter from collections?No, the provided import should be enough. The autograder may not support other packages.  I did and the autograder worked fine. If it's not allowed, then it's not being enforced, and given the low engagement of the staff, it's unlikely to be.
https://campuswire.com/c/G984118D3/feed/843 Cannot Get the MP3 Resource PDFs Hi,   I'm getting "This site can’t be reached" error when downloading those 3 provided resource PDFs (at the bottom of the `readme.md` ). I have tried to use the UIUC VPN but still getting the same error. I was wondering if anyone else sees the same issue?  Thanks I have the same issue. Confirming I also see the same issue, seems like this will require guidance from one of the TA's or professor.   It looks like the material for the course hasn't been updated in about 5 years, because all the links are at least that old. Therefore, the hosting procedure for a lot of these resources (e.g.: faculty homepages, course websites) has changed in that time, sometimes more than once.  That being said, it was straightforward to find [Prof. Zhai's new homepage](https://czhai.cs.illinois.edu/) by googling him (just like we learned in class!), and then applying the same relative path to resolve the first link successfully: * [Cheng's note](https://czhai.cs.illinois.edu/pub/em-note.pdf) on EM algorithm  I'm thinking the other two are lost to history as, unlike faculty homepages, they don't bother to migrate course pages across servers / services / hosting platforms, as far as I can tell. In fact, the link to the course ([CS 598 CXZ - Advanced Topic in Information Retrieval (Fall 2016)](https://cs.illinois.edu/academics/courses/cs598cxz)) from Prof.'s homepage is likewise dead. If you answer the question with a statement, it marks it as resolved, and then the TA's can't see it. As above, if you answer the question with a statement, it marks it as resolved, and then the TA's can't see it. I noticed that and promptly changed it back to unresolved. If it showed as resolved, that was the result of the other response. 
https://campuswire.com/c/G984118D3/feed/1196 M-step Normalization Hi,   I feel a little confused. So for the content in the video, the normalizations for pi and p are different. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f823916a-26f9-4e8b-a571-16284447769c/image.png)  Do we need to implement different normalizations for them in MP3? Thanks!$$\pi$$ and $$p$$ are different values: the first is the document topic probabilities $$p(z | d)$$, the second is the topic word probabilities $$p(w|z)$$  generally, without a background model, PLSA EM has two steps: 1. E-step: calculate $$p(z_{dw} = j)$$ 2. M-step: calculate updated values for both $$\pi_{dj}^{(n+1)}$$ and $$p^{(n + 1)}(w | \theta_j)$$  so if i understood your question correctly, the answer is yes. the M-step calculates two quantities (or two matrices), each with a different denominator for normalization. please let me know if i misunderstood, im happy to try to clarify further The normalizations are basically identical to the formula in the slides, except that you do not need to consider the background model. 
https://campuswire.com/c/G984118D3/feed/823 Quiz 1 Review Hi I'm reviewing quiz 1 for the exam and was confused on a couple of the questions.   Question 5 In the "simplest" VSM instantiation, however, using word count instead of 0-1 bit vectors, if we concatenate each document by itself, will the ranking list still remain the same?  Ans) Yes, it will remain the same  Question 6 In Text Retrieval problem for N distinct documents, select statements below that are correct?  Ans) Doc selection is 2^N and Doc ranking is N!  For question 5 I'm confused on what it means by "concatenate each document by itself" and also what exactly it means to use word count instead of 0-1 bit vectors. Does this mean just a singular number representing the total number of words in the document?   For question 6 I understand how doc ranking is N! because you have to compare a document to every other document, but I can't comprehend why doc selection is 2^N. I thought doc selection would be 2 * N since you have to use absolute relevance, so there can only be 2 outcomes per document, whether it is relevant (value of 1) or not (value of 0). For question 6: Since there can be 2 outcomes per document, the selections would be: 2x2x2.... (n times) = 2^N For Question 5: Let's consider a simple document, d1: "this is a text mining document about text"  Word Count will just count up the number of words in the document (ie count of word "text" in d1 is 2, instead of 0-1 bit vector which would just put 1 for presence of the term or 0 for absence of term) Now, if we concatenate the document by itself, d1 becomes: "this is a text mining document about text this is a text mining document about text". Well, this changed the count of the word text in d1, as it is now 4. However, we've concatenated *every* document by itself, so the relative ranking will remain the same.   please let me know if that made sense, i can clarify further  Oh i see I was thinking about it in the wrong manner then. Thanks! Ahhhhhh I see that makes much more sense. I now see what the word count is and understand it's concatenation every document with itself. Thank you for the example and explanation Ryan!
https://campuswire.com/c/G984118D3/feed/511 DL Search Engine Website Down? Can't seem to access the website and the extension isn't responding either. Can an instructor look into this? Trying to get the MP done on time. Thanks!  ![Screen%20Shot%202022-09-18%20at%209.58.06%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dc50798a-0308-44be-a0cf-45c83ea981ee/Screen%20Shot%202022-09-18%20at%209.58.06%20PM.png)Are you connected to the VPN? That was it lol my bad No worries! Glad it was an easy fix
https://campuswire.com/c/G984118D3/feed/510 MP 2.2 Not seeing submissions Hello,  I would like to view my queries but the "My submissions" page is showing me this. Is this an issue on my end?  Thanks!![Screen%20Shot%202022-09-18%20at%208.19.01%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bda4d2bd-089a-4027-816f-2820e786b900/Screen%20Shot%202022-09-18%20at%208.19.01%20PM.png)I think the error is happening because you have not made any traditional webpage submissions (via the Chrome extension). Seems like this is a problem with people who needed to make new accounts for the webpage. There is a bug in the code which does not render the number of submitted judgments when the submissions are empty.   There are three options:  1. Make a submission using the Chrome extension with the account currently logged into the webpage. 2. Right-click, inspect element, check the response from the server for the key "num_judgments". If this number is >=5, then you will get full credit. 3. Tell me your email that you used to sign up, and I can quickly check the backend for you.
https://campuswire.com/c/G984118D3/feed/599 scipy.stats.ttest_rel, typeerror I am doing the following for the Statistical significance testing: 1. created two separate .txt file for bm25.avg_p and inl2.avg_p with the 225 results saved separated in there. 2. read in the two files and use stats.ttest_rel (I have scipy installed)  when I run my locally, I got below error, can anyone help?![Screen%20Shot%202022-09-23%20at%2011.19.37%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c39561f6-6b26-4b0a-b45e-e910b755e9fe/Screen%20Shot%202022-09-23%20at%2011.19.37%20PM.png)Make sure 1. All numbers are float type 2.Numbers are saved in list
https://campuswire.com/c/G984118D3/feed/325 doc content won't recognize It was running fine for  doc = metapy.index.Document() doc.content("I said that I can't believe that it only costs $19.95!")  But when I execute the last step of MP1 instruction, the error message is like this:   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b9dbbafb-9585-4c6d-8186-e6cadb603909/image.png)Try execute the command line by line instead. Also the commands order looks different from the instructions  ana = metapy.analyzers.load('config.toml') doc = metapy.index.Document() doc.content("I said that I can't believe that it only costs $19.95!") print(ana.analyze(doc)) solved. issue is the config file that I accidentally commented out.  Even though the readme file states to start Python command line , I found it easier to execute these steps in a code editor ( like VSCODE).  
https://campuswire.com/c/G984118D3/feed/908 EM Algorithm   What is the w' prime in the M-step?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/494032e2-e4a0-41ac-af10-447d2f3138af/image.png)The denominator of that equation is a normalizer. Whereas the numerator operates on a given word, $$w$$, whatever that may be, the denominator divides the result of that expression by the same expression summed across *all* of the words in the vocabulary $$V$$; $$w'$$ simply represents the current word in that sum of all words in $$V$$. That helps, thanks.
https://campuswire.com/c/G984118D3/feed/171 Git Add, Commit, Push Hi All, Just wanted to share the link that worked out great for me for the steps where we have to add, commit and push our code once it's completed. This is after you finish the first 14 steps mentioned in the instructions to setup for all the MP's.  https://www.youtube.com/watch?v=wrb7Gge9yoE
https://campuswire.com/c/G984118D3/feed/465 Lecture 5.7 Is there a slides for this lecture? cannot find it in download dropdown. Thx!Right, I too don't see the option for pdf download for both lectures 5.7 & 5.8. Also, slides on educational web at http://timan102.cs.illinois.edu/explanation//slide/cs-410/31 doesn't seem to be working. May be TAs can help. Let me check this and get back to you. Hi Harita, appreciate it if new slides can be uploaded soon! lecture 5.7 and 5.8 is using the slides in lecture 5.6.
https://campuswire.com/c/G984118D3/feed/992 MP2.3 Still Says Overdue in Coursera Hello, who can I contact about my grade for MP 2.3. I submitted it on time and live data lab says that I have passed, but Coursera still says that the assignment is overdue.May be you can drop mail to TAs. They are usually very prompt and helpful in such cases.  Thanks - where are their emails listed?  Have not been able to find anywhere in Coursera. Hello Steve, I will take a look at this and get back to you. Does Coursera not show any grade? Correct. Coursera shows no grade and Overdue. I have informed the TA who will take a look at this issue. Updated Thanks, I have received a grade now.
https://campuswire.com/c/G984118D3/feed/991 Zipf's law 10. Question 10 What does Zipf's law tell you?  There are only a few words that have a small probability.   There are many words that have a small probability.   Words are evenly distributed.  The answer is - There are many words that have a small probability.  Could someone please explain why is that the case ? Basically it is referring to the fact that there are in general many words from the vocabulary that are less frequent in a document. If you pick a large sample of the documents you will find many words that are less frequently used in the document related to the other words. If you think about the set of stop words - like "a", "the" "is" etc . but the list of such word is still limited. Hence it is mentioned , there are few words(like above) that have high probability occurring in any document. But there are many many other words that have smaller probability and are rare. Hope this helps ! If it helps, can refer to the video explaining this law in detail  https://www.youtube.com/watch?v=aVmf8MKev5M You can refer to the video and slide. But briefly, many words are seldom to use.
https://campuswire.com/c/G984118D3/feed/239 tar xvf syntax error Hi, I am having syntax error of the line tar xvf greedy-perceptron-tagger.tar.gz, can anyone help me with it?Will you please confirm you are trying to execute this from a shell? Syntax error sounds like you might adding this as a code block? The command works like this from a shell. (cs410mp1) ➜  d tar xvf greedy-perceptron-tagger.tar.gz x perceptron-tagger/ x perceptron-tagger/feature.mapping.gz x perceptron-tagger/label.mapping x perceptron-tagger/tagger.model.gz (cs410mp1) ➜  d ll total 12936 -rw-r--r--  1 rtikes  wheel   6.3M Dec  6  2021 greedy-perceptron-tagger.tar.gz drwxr-xr-x  5 rtikes  wheel   160B Sep  1 17:18 perceptron-tagger (cs410mp1) ➜  d   Can you pls share the error msg.  tar -xvf filename usually works from the command prompt on windows Thanks! How to get to the file location in the shell if I am using Colab for the mp? Is the cloned file the location I should go in the shell? ![Screen%20Shot%202022-09-01%20at%205.37.22%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f70b3b9c-6732-44f3-a461-2c38000a0e1d/Screen%20Shot%202022-09-01%20at%205.37.22%20PM.png) Thanks a lot!! This isn't working for me. I keep getting "SyntaxError: invalid syntax". I also tried putting the filename in quotes, but that didn't help Please disregard. I figured out my mistake. As what Charles showed, I think you can put an exclamation point at the beginning of the line of code, like "!tar ...". I solved a similar issue.
https://campuswire.com/c/G984118D3/feed/288 Accessing Additional Readings Does Illinois offer access to all the additional readings for this class? For example, I would love to read the Diagnostic Evaluation of Information Retrieval Models through ACM, but do not have access. Thanks!Yes, you can access it by searching for it in UIUC Library Database: https://www.library.illinois.edu/ Thank you so much! I have not mastered navigating the school site so the link is much appreciated!  Attached for your convenience: [Diagnostic%20Evaluation%20of%20IR%20Models.pdf](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f52f055c-f747-4f91-bfa4-ec468472162e/Diagnostic%20Evaluation%20of%20IR%20Models.pdf) Thank you!
https://campuswire.com/c/G984118D3/feed/333 MP2.1 Due Date On Coursera, the due date for MP2.1 is Sept. 11, **1:59am CDT** , but the instructions say that the due date is **11:59pm CDT**. Which one is the actual due date? ![Mp2_1DueDate.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e01681b8-c3aa-4f38-94fa-16125aad071b/Mp2_1DueDate.PNG)  Thanks!I believe it's going to be 11:59 PM CDT, similar to any other assignment timing deadline. TAs probably need to update it in Coursera.  The actual due date is Sept. 11, 11:59pm CDT. I've updated Coursera to reflect this change. Thanks for pointing it out!
https://campuswire.com/c/G984118D3/feed/970 Quiz 4 Problem From my perspective, if we double terms in one document, C(w,d) should double, but |d| remains same, then probability should change, why this choice is correct?  ![%E6%88%AA%E5%B1%8F2022-10-15%20%E4%B8%8B%E5%8D%881.36.52.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/df2ecda8-5be5-45ec-8488-8300bc06ca0f/%E6%88%AA%E5%B1%8F2022-10-15%20%E4%B8%8B%E5%8D%881.36.52.png)The question says "If word count **for every term doubles**...". So from my understanding, the count every individual term would double but so would the document length, therefore the term remains the same value. When word count doubles for each term then the document length |d| is also gonna double.  Thanks bro, quite confusing. JM Smoothing interpolates between the ML Estimate and the Collection model. Doubling term count will not impact the collection model. Will it impact the Maximum Likelihood estimate? Consider a simple document "the text" the ML estimate for `the` is $$\frac{1}{2}$$ and for `text` $$\frac{1}{2}$$; what happens if we double the terms? "the text the text" well now, not only does the numerator for the ML estimate change, but so does the denominator: $$p("the"|d) = p("text"|d) =\frac{2}{4} = \frac{1}{2}$$  While Dirichlet-prior also interpolates between ML estimate and collection model, the smoothing coefficients are based on document length and thus will be impacted by changes like this. however in JM smoothing the ML estimate is just multiplied by a constant coefficient, so it will not be impacted
https://campuswire.com/c/G984118D3/feed/551 MP2.3 Question Do I have to change the return of load_ranker? I have been a bit lost as to what I should do after implementing the score for a single term in single_score.That's correct.   This is from https://github.com/CS410Assignments/MP2.3:  "Do not forget to call the InL2 ranker by editing the return statement of load_ranker function inside search_eval.py." Do we have to invoke the score_one function after creating an instance of the InL2Ranker class? If we have to call the score_one function, what would the input parameter of the score_one function be? How do we get access to the score_data file to pass it in as input into this score_one function? You don't need to invoke the score_one function, just return an instance from load_ranker. So we simply return InL2Ranker() in the load_ranker function? correct
https://campuswire.com/c/G984118D3/feed/501 LiveDataLab hanging when I try to see submission logs Has anyone seen this problem? The grader seems to be working at least partially because I am getting a partial score.Make sure you have done the two tasks. I have both modified the search_eval.py and included the significance.txt. The grader gave me full score.  Thanks. I figured out that a print statement that I had in score_one was generating so much output is was causing the logs screen to hang Nice!
https://campuswire.com/c/G984118D3/feed/398 MP1 Overdue in Coursera Hello, I noticed that I saw my MP1 was considered overdue in coursera's Grade tab, but I have turned it in on time and fully functional. I followed the instructions in setting up and my livedatalab is linked to gitlab.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a9932231-70f2-49d2-b3eb-d1f382dcb33c/image.png)  From opentool(coursera) are you able to point to livedatalab. If that works, it should immediately update the results. It always takes me to a webpage saying it is not secure, I have to click the link to manually redirect to livedatalab and login in each time.  Are you still seeing this error? Looks like coursera is updated. If Datalab works, then it's ok
https://campuswire.com/c/G984118D3/feed/878 Generative Mixture Model I have a question regarding week 5 contents. In Generative Mixture Model, lambda is referred to as noise in feedback documents. I understood that if the lambda value goes up, a larger portion of the collection/background model is used as part of the feedback language model. However, on the next slide which gives an example, if the lambda value is larger, it more uses the topic model. I think these two slides are contradictory. Which point did I misunderstand? ![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-10%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.56.46.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a5b5f9b8-e5ea-4fdc-9c05-b9b66de8c95a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-10%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.56.46.png) ![%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-10%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.56.53.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2d80c441-9ce9-449b-9ef2-1aab3d571d30/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-10%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.56.53.png) From my understanding, the explanation of this lecture sounds reasonable.  Here we have two language models $$\theta$$ for topic words and $$C$$ for background words. As you understand, $$\lambda$$ denotes the noise amount in the feedback document.  Then, if we set $$\lambda$$ to be small, $$\theta$$ has the responsibility to allocate more probabilities to common words instead of $$C$$ in order to achieve $$\arg \max_{\theta} \log p(F|\theta)$$.  I hope this helps and would appreciate any feedback on my understanding! Thanks a lot! Your explanation was helpful. My question has been resolved.
https://campuswire.com/c/G984118D3/feed/1056 Lecture 9.2 x=y calculation Hi, I am confused about the calculation of p("text"|$$\theta_d$$) and p("the"|$$\theta_d$$). Ho do we come to the equation p("text"|$$\Lambda$$) = p("the"|$$\Lambda$$)?  We know that p("text"|$$\theta_d$$) + p("the"|$$\theta_d$$) = 1. Do we assume that p("text"|$$\Lambda$$) + p("the"|$$\Lambda$$) = constant, so that we can use the above equation? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/845cecfa-1605-4bd5-87d2-a391a79daaaa/image.png) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f3d6ee8c-0fe2-429c-b9b9-30f4787d46ed/image.png)  You can prove it by calculating derivative.  x = p("text"| Λ ) y = p ("the" | Λ )  We want p(d | Λ) to be the maximum, this is when x = y. We know that $$p("text"|\theta_{d}) + p("the"|\theta_{d}) $$ is a constant. This would mean that the sum of  $$[0.5*p("text"|\theta_{d}) + 0.5*0.1]$$ and $$[0.5*p("the"|\theta_{d}) + 0.5*0.9]$$ is a constant. Which implies that $$p("text"|\Lambda) + p("the"|\Lambda)$$ is a constant. Three points: 1. We are doing an estimation among the two to maximize their product.  2. Since we know their sum is always equal to 1, we exploit the general mathematical rule that, when two variables sum to a constant, their product is maximum when they are equal. 3. Thus, to maximize product using the 2nd rule, we assume that they should be equal to each other (which has to be 0.5 in this case).
https://campuswire.com/c/G984118D3/feed/562 MP2.3 Valid P-Value So I submitted the MP and I noticed that I get the following error in my output in LiveDataLab:   ``` Requirement already satisfied, skipping upgrade: scandir; python_version < "3.5" in /home/livelab_jenkins_user/.local/lib/python2.7/site-packages (from pathlib2; python_version < "3"->importlib-resources; python_version < "3.7"->tqdm) (1.10.0)  FEEDBACK: significance.txt not found or does not contain a valid float value!!!  Finished: SUCCESS ```  I noticed taht the file is included in the github, but I want to check to see if the following value in the file is valid for the test cases: ``` Ttest_relResult(statistic=3.6197939408479907, pvalue=0.00036453477845628475) ``` Is this a correct value/form of output? the `Ttest_relResult` is an object that represents two values: a `test statistic` and `p-value`. MP2.3 asks to extract just the `p-value` from this result. From the MP2.3 `README`:  > Write the p-value in a file called significance.txt. Do not include anything else in the file, just this number!   Oh okay! Thank you!
https://campuswire.com/c/G984118D3/feed/2 Coursera Access Hi all! I would like to know how many students are having issues accessing the Coursera for this class. Please choose one of the options below. Option 1: I was able to complete the Onboarding course AND I was automatically enrolled in CS 410 on Coursera. Option 2: I was able to complete the Onboarding course BUT I was not automatically enrolled in CS 410 (I can't see the course on my Dashboard). Option 3: I am not able to complete the Onboarding course.
https://campuswire.com/c/G984118D3/feed/960 update the due of Technology Review on Coursera? Hi Professor and TAs,  The Information about Technology Review on Coursera is very confusing and not consistent. Could you please update them?  Thanks!  Week 1 (Technology Review Information):  - Topic proposal - Oct 23, 2021 - Review submission: Nov 6, 2021  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9538bcd5-d642-4c46-8e8b-57cfd56e1c95/image.png)  Week 8 (Technology Review Information):  - Topic proposal - Oct 17, 2021 - Review submission: Nov 6, 2021  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7791da86-371e-488a-88dc-82212edeada1/image.png)Sorry about this - I've changed the second week 9 document to be October 23rd.  Thanks! But I think the document is available under week 8:)
https://campuswire.com/c/G984118D3/feed/710 Submission Failing without any Logs Just in case you run into a similar issue, I had an error where when I submitted MP 2.4, the submission would immediatly fail in live data lab and no logs were displayed. I realized that the personal access token I used to link live data lab with my github account had expired. To fix it, just delete your linked account on live data lab and redo the [steps](https://d3c33hcgiwev3.cloudfront.net/56yaHRT_TDSsmh0U_zw0pg_0a5a1bc92b31433ca575124f8b5cfbe4_LiveDataLab-setup.pdf?Expires=1664323200&Signature=AQux~EYBec9st2X6d~SRj816Fq99vNcdnEItobI3m7-IuMrnrMeg0o3K7ChI1Y4gIV6mcIP~4xpwAXay-dtAExxMCiCuPgFsepf12INVmMynMW2-bNnMfoJB9m3nQkr7vqaumGBSmYr4qozMEiqoVcsjzSJNwgNWCgulNjMJWHc_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A) to link your github account to live data lab. 
https://campuswire.com/c/G984118D3/feed/870 Doubt about Scoring Question in Practice Quiz 4  If a document does not contain all of the words in the query, then based on the formula in the lecture, S1 would be 0 but there is a chance that S2 would not be 0 hence I am confused about how this won't change the ranking of the documents. (I understand how the ranking would not change if S1 and S2 are nonzero but confused about the case when S1 could be 0)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/47f16524-e209-408a-af28-22d29c9cd885/image.png)So based on the assumption if the term is not in the document we will have the probability replaced by probability for the term in the Collection multiplied by alpha d.  I think S1 will never be 0. Thank you, that makes sense! But I guess then are we just supposed to assume that in this question, the scoring function includes smoothening? My understanding is yes, reason being, if not then the probability will become 0 because of  unseen words. Oh okay, thank you! The professor’s presentation style being both iterative and expository may be causing some confusion; specifically, building up ranking algorithms from scratch, asking students to think about failure modes, and then extending based on guided intuition. In practice, though, my guess is that nothing in history was called a “scoring function” unless it included smoothing, because that failure mode is so glaring and catastrophic as to make any system not based on that assumption unusable.  Here’s a really trivial example based on a navigational query for “the Facebook”. If there’s no “the” on the home page, it fails completely to deliver the only page the user cares about. That makes sense, thank you for the example
https://campuswire.com/c/G984118D3/feed/1067 MP3 Deadline Now Oct 26 Hi all, due to popular demand, I've extended the MP3 deadline by 3 days. It is now Oct 26. Happy coding! ~Assma
https://campuswire.com/c/G984118D3/feed/28 Problems with installing python 3.5 and metapy on M1 MacBook Hey there,  I am having problems installing python 3.5 (with Pyenv or anaconda) and metapy on a clean M1 MacBook Pro (despite installing cmake as suggested here https://campuswire.com/c/G984118D3/feed/19).   After quite some research, it appears numerous postings have described similar issues with installing python 3.5 on M1 Mac. Please see [here,](https://stackoverflow.com/questions/70472350/issue-in-installing-python-3-5-10-using-pyenv-in-macos-12-0-m1-arm64) [here](https://github.com/pyenv/pyenv/issues/1643) and [here.](https://stackoverflow.com/questions/65653464/installation-of-earlier-versions-of-python-prior-to-3-8-fails-on-mac-with-m1-c)   Any help would be appreciated.   And to our TA, I am curious what did people with M1 Mac do in previous years?  Thanks.Rather than try to get the older version of python running on my M1, I set up a free-tier Amazon EC2 instance, and am using the Remote SSH VSCode plugin to connect to it. Heres a summary of the process https://medium.com/mpercept-academy/connecting-aws-ec-instance-with-visual-studio-code-f7bbf388e9df   Hello ,   I have Python installed on my MacBook Pro. The best way to do it is via Anaconda. Install anaconda and use Python in VSCode with Conda installation. https://docs.anaconda.com/anaconda/install/mac-os/  https://docs.anaconda.com/anaconda/user-guide/tasks/integration/python-vsc/ Thanks Prateek! Do you mind I ask if you installed python 3.5? I did try Anaconda however could not get Python below 3.7 installed this way. Thanks! That’s a great idea Nate and I may take this route! I assume you had Metapy installed this way without problem as well? Yep! There's just a bit more setup overall with installing Git/pip/etc. Also need to be careful to commit your changes.. since they won't be saved locally I got metapy to work by installing Python 3.7 and then using the appropriate `cp37` built distribution from here:  https://pypi.org/project/metapy/#files  You can install a built distribution using `pip install <whl_url>` where `<whl_url>` is the appropriate link for your environment from the PyPI metapy link above. I have Python 3.9 installed. Just noticed that the default MacBook came with 3.8 pre installed. My apologies , as I didn't notice you were trying to get 3.5 installed. May I know why? In past example while doing AML Labs , latest version of python worked fine for me.  The MP1 setup instructions mention to use 2.7 or 3.5. Not sure what effects other versions may have. oh ok... have'nt started MP1 yet. I would ask TA if we can use higher versions. Yes as Gautam mentioned python 3.5 is recommended for MP1. Plus metapy could not be installed with the python 3.10 I currently have. Thank you there! Wondering how did you get python 3.7 installed? Seems the earliest version anaconda offers is v3.8.11. Thanks!  Hello all,  Just wish to let you know thanks to all replies and after quite some struggling, I was able to install python 3.7 and Metapy with conda following method in think link: https://stackoverflow.com/questions/70205633/cannot-install-python-3-7-on-osx-arm64  5 hours of life that will never come back...:) Which command did you use to install metapy at the end after setting up the conda environment py37? . For anyone facing the issue of installing metapy on their Mac M1/M1 Pro machines, I believe there are a couple of reasons: - Metapy does not work after python 3.7 and most of the Mac machines come with python 3.8+ installed.  - The metapy package requires "cmake" to be installed and it is not found.  The easiest way to resolve both of these issues is to install Anaconda and use it to both install cmake as well as get the right python version. Here are the steps that worked for me:  1. Download the Anaconda from their website. 2. In your terminal, enter (this will download cmake for you) ``` conda install cmake ``` 3. Next, enter the following to create an environment to use python 3.7. ``` conda create -n $PYTHON37_ENV_NAME python=3.7 anaconda conda activate python=3.7 ``` 4. Next, in your terminal, type in  ``` conda install python=3.7 ``` 5. Next activate your environment ``` conda activate python=3.7 ``` 6. You can check your python version is 3.7 here by typing "python --version" 7. Now try installing metapy again in your python 3.7 environment: ``` pip install metapy pytoml ``` metapy should successfully install now. Hope this helped!  Reference: https://stackoverflow.com/questions/43630002/conda-install-downgrade-python-version  I believe I first activated the new condo environment and then used pip to install. Hope this helps.  Thanks for this great summary Gautam and well stated!  One caveat I would mention is that you downloaded the Anaconda or Minoconda version for M1 Mac, you would not be able to find python 3.7 in their repository. In this case I used the method mentioned here (https://stackoverflow.com/questions/70205633/cannot-install-python-3-7-on-osx-arm64) to first create an environment with x86_64 architecture, after which python 3.7 can be found. Just to add you could find you condo build use "condo info" and look for platform. For me it says osx-arm64. I suppose if you installed condo for Mac intel and Rosetta you may find python 3.7 in the repository right away. I believe I did see a message about Rosetta when I got Anaconda. This is is the link I used for the download: https://repo.anaconda.com/archive/Anaconda3-2022.05-MacOSX-x86_64.pkg I believe this was the one for x86 anyways. Here you go and I think you are totally right! Boy I wish I had just downloaded the x86 version of anaconda instead of M1 version, that would have saved a ton of trouble!!! I have a MacBook Air M2. This works for me.![Screen%20Shot%202022-08-29%20at%2016.37.57.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b06da986-ca96-4c2c-a825-18b74a1b2cb2/Screen%20Shot%202022-08-29%20at%2016.37.57.png) I think as long as you have rosetta installed, either docker or anaconda will work. Personally I like docker better. Too much dark magic in conda for me. I also have an M1 machine. To solve this issue, I am simply using Google Colab for MPs, because metapy doesn't work on M1. Refer to this post - https://campuswire.com/c/G984118D3/feed/153 Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153   The older version of python works fine on the M1.   I have an M1 machine and was able to get it to run fine using the anaconda distribution and creating a python 3.5 project ``` conda create -n cs410project1 python=3.5 conda activate cs410project1  pip install metapy pytoml   Python 3.5.6 |Anaconda, Inc.| (default, Aug 26 2018, 16:30:03)  [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin Type "help", "copyright", "credits" or "license" for more information. >>> import metapy ``` I'm stuck on the `conda activate python=3.7` step. Trying to run this after step three, or skipping it and then trying to run step 5 both cause this error: ``` Could not find conda environment: python=3.7 You can list all discoverable environments with `conda info --envs` ``` Running that list command shows this: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/30ac4407-dfa6-4740-a0b6-bcf032f25753/image.png) I'm on windows, but facing the same problem so I'm not sure how to continue. I know that it's possible to just use google colab, but I want to get it working on my computer :( I will try to answer here. I think what Gautam meant in step 3 is:  ``` conda create -n $PYTHON37_ENV_NAME (this should be the environment name you create; say cs410) python=3.7 anaconda conda activate python=3.7 (this could be a typo; I think the right command should be condo activate $ENV_NAME; say condo activate cs410)  ```  In your case, you can try condo activate $PYTHON37_ENV_NAME (though I would try an easier env name)  Hope this help This may be an issue on Windows- on a Mac machine the conda activate command I stated above works as expected. It is possible that the solution that Hanyin proposed works on Windows machines.
https://campuswire.com/c/G984118D3/feed/450  Unable to access the CS410 DL Search Engine website Dear TA's.  The  CS410 DL Search Engine website appears to be not accessible at the moment.  Can you please help?  I can access to it right now. Make sure you have connected the VPN.
https://campuswire.com/c/G984118D3/feed/72 MP1 project Account is linked to github project for MP1, but i dont see how to submit the code and run the test on it?  Thanks. You commit your code to your github project, then the webhook will submit it to LiveDataLab automatically. 
https://campuswire.com/c/G984118D3/feed/1122 change topic after proposal Hi TA/Instructor,  Are we allowed to change the topic after 10/23, for example,  if we find the topic we choose is not manageable or achievable at a later stage?   Or for the same topic, we changed some implementation details such the "expected outcome" mentioned in the proposal.  ThanksI guess we are free to change the topic after the review process as writen in the project ideas. Yes, you are allowed to change the topic + implementation details after the deadline. 
https://campuswire.com/c/G984118D3/feed/230 Cannot create a private room for the project Hello, does anyone know how to create a private chatroom in Campuswire?  The button is disabled for us for some reason. ![chatroom.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/518e0097-e675-45e2-9495-923659229905/chatroom.png)   I've always just done Slack for group project chats True, Slack works as well. But it would be better if we could consolidate everything into one place instead of having to switch between the two platforms.  I think I just gave permission for students to make private chat rooms - please let me know if you can do it now! Hi Kevin, yep just created a private chat room for my group. Thanks!! Great, no problem!
https://campuswire.com/c/G984118D3/feed/1078 MP3 Shows as Green in Datalab but no score is shown 0 in leaderboard I am facing an issue similar to #941, where the submission is showing as SUCCESS and green in livedatalab but my score on leaderboard is showing 0. My netid is ojasvia2. Can i get some guidance on this ? I was in the same situation and found that the SUCCESS label on the submission log does not mean the correct implementation. So you should probably find any mistakes in your codes. Were you able to get the right score after correcting the mistakes ?  Yep. I can see the log likelihood values converging, still i am seeing a score of 0. It took 9 iterations on the autograder for me.  I see. As for your codes, it might be better to ask TAs! Yep, I had the same situation where my log likelihood was increasing but I was still getting '0'. Turns out I did have calculation errors - it's quite hard to debug unfortunately.  I had the similar issues in the beginning. For my case, I didn’t update the numbers in my EM steps correctly while all other operations were correct (for example the shapes of all the matrixes and calculating the likelihood). This caused I passed but scored a zero. The auto grader not only checks if you converged or not, but also checks if your final EM matrixes are correct. Sometimes incorrect matrixes can lead to convergence of likelihood. After I fixed my EM steps, the score became “1”.  Also try to open the tool via Coursera first
https://campuswire.com/c/G984118D3/feed/1287 Proposal reviews on CMT For the proposal peer-reviews on CMT, is it adequate to answer the questions prefixed by [PROPOSAL] and then "Save Draft", rather than "Submit"?  Or is there some other way to submit our review and comments.Sam, I believe the directions say to answer non-proposal questions by typing "N/A" or clicking the "Not due yet" radio button. The other answer is correct - please put whatever for the non-proposal questions and click "Submit" Thanks! Do you happen to know where these instructions are defined? Hi Joel - these instructions are embedded within the CMT review page itself.  I missed them when I first looked at the site.
https://campuswire.com/c/G984118D3/feed/1294 Will we have an MP4 ?  Hi Ta’s and Professor,   Will we have an MP4, and if so when will it be due? What will be the percentage that this MP will be worth if we have it?   Thank you,  Sruthi In an earlier post, one of the TAs said there will an MP4 but no other details were given. This post mentions that there will be a MP4 in the responses: https://campuswire.com/c/G984118D3/feed/795 Aren't the project and tech review not enough? Changing the course schedule like that with 1 month remaining in the semester is pretty annoying since some of us already planned everything according to the deadlines given in the beginning.   Please, instructors and TAs, no MP4 The semester is coming to an end. Soon, we are going to have our Winter break, which just left us with around 3 weeks till the exam to do the MP. As of today, it is not released yet, which is weird. Usually, the rest of the MPs were released very quickly. And on top of all this, we also have our tech reviews + project. Please, NO MP4.
https://campuswire.com/c/G984118D3/feed/100 The dates on the page are outdated ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/127ab6b5-899b-4adc-aa23-3c73e77f8a95/image.png)  On week 1, technology-review-information section has deadlines and  they are outdated.  Probably not an important issue because I expect that there will be another actual guideline for 2022 and regard this page just as an overview of what we will be doing in this course.  Thank you.Thanks for catching the problem, Danwoo! We'll correct them.  We are in the process of systematically reviewing all the content, which, as you noticed, was copied from last year, and will make sure to correct all such problems.  
https://campuswire.com/c/G984118D3/feed/605 MP2.3 - ranker.score(...) taking too long  I am going through MP2.3 tutorial, and got to the, "Varying a parameter" section. I am trying to run the loop where we test our scoring function on our queries and docs, but the ranker.score function is taking too long... I am stuck on [*] on jupyter and am not sure what the issue is. I simply copy and pasted the code from the MP2.3 github. All of the code above this section ran as expected.   I am using Python 3.7.13   ![Screen%20Shot%202022-09-24%20at%2012.00.00%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9b60e603-6ea0-44f4-ac7d-05e7589c1e0f/Screen%20Shot%202022-09-24%20at%2012.00.00%20AM.png)  there are some past posts maybe discussing similar issue #478 , maybe try version 3.5? I did not Jupyter, but I use python 3.5 which seems working Thank you, I switched to version 3.6 and it ended up working!
https://campuswire.com/c/G984118D3/feed/174 LiveDataLab don't sync github submission Background: I used to successfully link my github and livedatalab and had seen grade on livedata. But the ta told me since I old livedata account email address is not school email, I can't sync it to Coursera. So I created a new account and set a new link. Howerver, on github it shows ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/689300b0-b5eb-472e-a7b3-0edbdd0f9afc/image.png) But on livedata it can't recieve any submission. I already have the mp1 pushed to my github( repushing didn't help).Hello, this is because our backend does not allow two livedatalab accounts to be linked to the same Github account. I've deleted both of your accounts, can you recreate your livedatalab account with the illinois email and then resubmit?  Got it fixed, thx! Hm. Yes, that is the right link, can you try reloading?
https://campuswire.com/c/G984118D3/feed/82 Pre-Quiz 7 Cross Product of Matrix ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/713d250e-1c62-4baa-b949-24b63c4ba8b8/image.png)  Hi there, in this question, $$x^T$$ is just an easy form to display the matrix? So the actual expression is still $$M \times x  $$, and also the result is transposed?  I am confused about this question.That's what I assumed as well. Or you could do $$ x^T \times  M$$ and end up with a 1x2 vector. Unclear. This was unclear to me as well, it should have been written:  What is the result of multiplying matrix M = $$\begin{bmatrix}1 & 2 \\2 & 1\end{bmatrix}$$ by a vector **$$\vec{x}$$ where** $$x^T = [1,1]$$?  Then the problem becomes $$y = M \cdot \vec{x}$$ $$x^T$$ is the notation for the transpose of $$\vec{x}$$.  Since $$M$$ is $$2 \times 2$$ and $$\vec{x}$$ is $$1 \times 2$$, the transpose allows us to have proper dimensions for matrix multiplication. I agree. Looks like there is a mistake in the way the question is written. You can interpret it in the way Rick has suggested. I agree. It's the only way we can get answers. I agree
https://campuswire.com/c/G984118D3/feed/533 How are Feedback methods used? I am having some difficulties understanding how the feedback method we discussed are used. From the user's query, you can generate a new query (in the Rocchio Method) or a list of words (for Language Models).   What is then done with this new query or words? The user has already done their search, so it is not immediately usable. How is it used to improve future searches?  Can the new query be used to modify queries seen in the future, somehow? How do you know when it should be applied? Good question! There are multiple ways of using the new query.  First, in case of pseudo feedback, the new query can be executed immediately to provide the user with potentially better results. (In such a case, we just assume the top-k results from the initial query are relevant.) Second, if it's relevance feedback where the examples are explicitly labeled by the user as relevant or non-relevant, the expanded query can still be used to re-rank the unseen documents for this user. E.g., if the user interacted with the first page of results (e.g., 10 docs), the expanded query can be used to re-rank the results on the remaining pages that the user hasn't seen yet. Third, the expanded query can be useful for future users who might type in the same query or a similar query. 
https://campuswire.com/c/G984118D3/feed/1319 what does score 0 mean for tech review report submission? I submit my tech review in pdf file. It score 0 in livedatalab. What does that mean for a score 0?? confused!It usually means that the pdf file is named incorrectly or not in the correct directory level in the repo. 
https://campuswire.com/c/G984118D3/feed/1079 Why is PLSA "latent"? Hello,  As far as I understand, PLSA seems like EM algorithm with multiple topics.  Why would the name of PLSA have 'latent' in it?From my understanding, PLSA aims to clarify latent topics in documents.  The hidden variable $$z_{d,w} \in \{B,1,2,...,k\}$$ is also called a  latent class.  https://people.cs.pitt.edu/~milos/courses/cs3750-Fall2007/lectures/plsa.pdf ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/edc6f83a-b64d-4cda-9f93-255b105ca053/image.png) Thanks :) just to add a little additional intuition/understanding of "latent" on top of Yutaro's answer  we consider these "latent" or "hidden" variables simply because they are not observed. we assume that there is some variable or some state there, that results in the observation we see. for example, let's see we have values that are drawn from 1 of 2 normal distributions. a value can be drawn from either: $$N(0, 5) \\ N(2, 7)$$ (so the means $$\mu_1 = 0, \mu_2  = 2$$, variances $$\sigma^2_1 = 5, \sigma^2_2 =  7$$)  If we draw a value and get "16", it is much more likely that the value came from the second distribution than the first; meanwhile a value of "1" could have easily come from either distribution. all that we observe here is the outputted value. However, there is a "hidden" variable for whether it was generated from the first distribution (we'll say `z = 0` for this case) or from the second distribution (`z = 1`). while we don't observe this, it is a part of the process and can be very helpful to model.  Because these underlying values are "hidden" we also refer to them as "latent" which has a similar meaning to "hidden" Thank you!
https://campuswire.com/c/G984118D3/feed/598 confused about the score_data I am lost in the file "score_data.h" which describes the fields of the sd object.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f4bc72a3-d5da-47cb-a740-1172d554418a/image.png) What is "struct score_data"? is the "inverted_index& idx" two fields or one field? could I  call the field just like the class?  the lines below the big chunk of comments are also confusing.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/51a5c407-85d6-4363-882d-f5313119047e/image.png) the arguments in the score_data() don't appear below the separating comment chunk.   Many thanks if anyone can help clarifying it.    You can treat score_data like an object that has all the needed attributes for your calculation. The score_data attributes are supposed to be used inside the score_one(self, sd) function. An example is below. Hope it helps. ```python  def score_one(self, sd):         """         You need to override this function to return a score for a single term.         For fields available in the score_data sd object,         @see https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html         """         example_calculation= sd.doc_term_count + sd.avg_dl +  sd.num_docs + sd.query_length ``` I find this link is much easier to understand https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html
https://campuswire.com/c/G984118D3/feed/4 Cannot access LiveDataLab I cannot access the LiveDataLab page via the link posted on [the Coursera orientation page](https://www.coursera.org/learn/cs-410/supplement/irzBO/programming-assignments-overview).  http://livelab.centralus.cloudapp.azure.com/We are still working on it. It should be up in the coming week.  Hi there, please try the following link: http://livedatalab.centralus.cloudapp.azure.com/course/join/WZS77P00E570J74   You should also see MP1 available now.    Thank you for your confirmation. It works for me, thanks! I am getting an internal server error when I try to access this livedatalab link. Is anyone else facing this? What is the correct link to access live data lab?
https://campuswire.com/c/G984118D3/feed/215 I can not see the submission history in LiveDataLab I pushed my code yesterday, but when I loged in my account on LiveDataLab, I only see "Loading submission history...".  How to solve this problem?Have you linked your github account on livedatalab and is your webhook working? Yes, I think the webhook is working. I now can see the score in Leaderboard is updated, but there is still no submission history, but now it is showing "No submissions are available or match the filter." on the page, does it matter? Are you seeing your grade on coursera? Yes, MP1 is shown as "Submitted"
https://campuswire.com/c/G984118D3/feed/983 Midterm extension Hello. The course is down the night before my exam  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d81cbb82-67ab-4da5-9e55-5722ceb6c640/image.png)  can you please extend the deadline? We are not able to study the quizzes and view the slides now...Thanks!  EDIT: the videos are back but we still can't view quizzes.i can view the quizzes now :)  I too can view the quizzes now Yeah, looks like everything is back now. Are you still seeing this issue?
https://campuswire.com/c/G984118D3/feed/548 MP2.4 Specified leaderboard does not exist Hello there,  Started working preliminary on MP2.4 and encountered this error when submitting.  ```  Running AP news queries Running cranfield queries Running faculty queries Specified leaderboard does not exist File "mp2.4_grader.py", line 175, in <module> test_mp24.post_results() File "mp2.4_grader.py", line 168, in post_results raise Exception("GRADING FAILED!!! Please check the logs above for details") Exception: GRADING FAILED!!! Please check the logs above for details Build step 'Execute shell' marked build as failure Finished: FAILURE ```  Not sure if this is a problem with mp2.4_grader.py. Many thanks! I am also getting the same error msg. Can TAs pls confirm. Got this problem! I also got the same issue. I believe TAs will address it soon. Hi all, issue should be resolved. Please try now.  Thanks for the kind work. Seems everything is working now! 
https://campuswire.com/c/G984118D3/feed/770 NDCG@10 and overall score  sometimes i have a lower ndcg@10 score locally but my overall score on livedata is higher. Why? I thought the overall score should go up when ndcg@10 score go up.I believe it has something to do with overfitting. I have the similar experience where my local ndcg score is high, but livedatalab overall score is low. It is not always true that higher ndcg score equals to high livedatalab score. Locally we are able to test only on Cranfield data and therefore the local score is a reflection of how well the algorithm & parameter selection works on Cranfield dataset.   However, the overall score is calculated on 3 different datasets with varying weightage.  As per the MP2.4 READme  OverallScore  = 0.1* NDCG@10 on APNews + 0.3* NDCG@10 on Cranfield + 0.6* NDCG@10 on Faculty dataset.  Zhouya, The Cranfield, AP, and Faculty data sets are very different. As a result, there is no correlation between Cranfield NDCG and Faculty or AP NDCG, as can be seen in these scatter plots: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/737469f1-18be-4029-ad1e-27eab48e0076/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4359af55-8f43-43c6-8eae-59a58bdfadd9/image.png) These charts were made with Leaderboard data. The red one is Cranfield vs Faculty NDCG. The blue one is Cranfield vs. AP NDCG. I made these charts to see if any maximization strategy was possible. Unfortunately, it seems that the only strategy is guessing many times (or being very lucky). Nice! Thanks for looking into this.  I’m wondering what insights others have on this.
https://campuswire.com/c/G984118D3/feed/1111 build_corpus When I print out the number of documents, it is 10. But it is supposed to be 1000. This might be a dumb question, but what is the correct way to read the document?Are you are referring to 'data/test.txt'? That file contains 1000 lines, where each line represents a single document. io Standard file open() should be fine. Just append the doc to the document list. you can read the document using standard python i/o functions such as `f = open(filename, 'r')` as others have described. then you can either iterate through the document and increment the counter every line for number of documents, or if you follow Anonymous' answer: > Standard file open() should be fine. Just append the doc to the document list.  then you should be able to get the number of documents from the length of the document list Open the file using any standard library and read each line using the respective readline function in the library u r using. Consider each line a document.
https://campuswire.com/c/G984118D3/feed/369 Reminder: CS410 MP2.1 released, due Sept 11th Hi everyone,  This is just a reminder that MP2.1 is due September 11th, 11:59pm CDT.   The description can be found [here](https://www.coursera.org/learn/cs-410/gradedLti/FO1Ol/mp2-1).  Please take the time to read through the linked description and general overview documents. Also, note that MP2.1 (and eventually, MP2.2) are different than the old versions on LiveDataLab/Github, so please take a look at and follow the directions in the above link.  Thanks! -Kevin
https://campuswire.com/c/G984118D3/feed/1253 MP3 Document Term Frequency Matrix I have been stuck on the Document Term Frequency for quite some time. If anyone could point me in the right direction as to if there are simple numpy functions that would allow this to happen or if this is truly meant to be done with for loops. I would assume the former to be true.   Thank youDillon, Please be more specific about where you are stuck. Are you referring to writing the build_doc_term_matrix function? In theory, you should be able to complete MP3 with numpy matrix operations in place of nested loops, and I'm sure it would be more efficient. I found it easiest to use nested loops, though, because I had to think very carefully about which iterables I was looping over (words in vocab, docs in the collection, or topics in the topic set).  In a couple of cases, though, I did use matrix operations and it seemed more logical. For example, you might want to divide one dimension of a 3D matrix by a certain value for normalization purposes...in several different spots. In that case, you might do something like...   `your_matrix[i, j, :] = your_matrix[i, j, :] / norm_value`  You could certainly re-write that with a triple-nested 'for' loop, but since you're doing the same operation (scalar division) on each iteration, the matrix operation made the most sense to me.  Hope this helps! Seth, thank you for your input. I believe I'm fit to be tied for this 11:59 due date, but regarding this method, my logic is as follows:  the matrix will be in a shape of (d,v) where d is num_docs and v is vocab_size. I believe it easy to instantiate an array of zeroes of that size, but from there, I would need to adequately loop through each w in v in d. A triple for loop as you mentioned. I have seen several mentions of for loops in this forum tonight, though I notice the Github README heavily disavows using for loops.   I wish reliance on a library we haven't used in this course was not so prevalent for this assignment, for I do not know any numpy methods that could perform that level of conditional counting at that iteration, not even a mask.  David,   You are correct. Please see my response to the answer above.  If (d,v) refers to the shape of your `term_doc_matrix` then you're on the right track! You'd only need a double-nested 'for' loop here, because you're only looping through two dimensions (a later function in MP3 uses three dimensions, though).  Honestly, I didn't see any major performance hit primarily using nested loops. The fastest code is the code that runs...write it to do what you need and worry about optimizing later. And if you're using nested loops, you don't need to know anything about `numpy` besides how to instantiate a matrix with zeroes - good luck! You've been a great help Seth, hopefully this time I've got it.   Thank you much!
https://campuswire.com/c/G984118D3/feed/12 Question on setting up MP 1 Hey friends,  I am following the steps in "[mp setup.pdf](https://d18ky98rnyall9.cloudfront.net/Dzke2aP9TGC5Htmj_Sxgjg_59737a6e41dc41efae3c1c5ca47d5da2_mp-setup.pdf?Expires=1661299200&Signature=j2-82x7IQnd7nkD3sQutp5oQA3wiwriWkEVa2Pwnr3znpYV9vTYZ6ytgTj9F-oVN0z2\~gMnHZtOYcLMWhA-hbav6mfVDUeKxua0CkzrDda4e9HSyej9D2j88GpGIL2rZH6iexo5gi18mOvZr95TutrecYDPlY8o20uvtI8dG5uM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)" but run into some issues. I am wondering for the step 6:  1\. Where do we enter the command of "*git clone --bare <git url>*"? Should we do into in git hub or terminal of laptop?  2\. Which git url should we use? The one from the private repo we created from step 1?  Also should we enter the command from step 7 in our local laptop's terminal?  Many thanks!  HanyinHi Hanyin, I think we need to do the cloning locally, so you'll have to use the terminal of your PC. The URL is https://github.com/CS410Fall2020/MP1.git, and all you need to do is type this on the terminal:  git clone --bare https://github.com/CS410Fall2020/MP1.git  Hope this helps!   In this case, we are free to use our own IDE such as **VS code** to do our homewrok.  Please note after some testing it appears the git url to be entered in step 6 should be: https://github.com/CS410Assignments/MP1.git page not found error :(  Ah, I think they've updated the link. Try this instead: git clone --bare https://github.com/CS410Assignments/MP1.git 
https://campuswire.com/c/G984118D3/feed/74 Due date for MP1 Hi, I saw this in the Github for MP1 "For students who just added the class, NO LATE PENALTY will be applied to submissions before Sept 11". Does anyone know who are "the students who just added the class"?  The dute date Sept 11 is for everyone?  Thanks. MingqingProbably it refers to the students who have enrolled for the class in last 1-2 days as they would still be waiting for coursera access. I have Sept 4th as the due date so doesn't look like Sep 11 timeline applies to everyone. I will need to confirm this with the instructor but no, the Sep 11 deadline is not for everyone. Usually it should be for students who got Coursera access very late.
https://campuswire.com/c/G984118D3/feed/183 Issue on Livedatalab I met some issues while following the setup pdf. 1). After I set up the webhook on my git account, I would find nothing in my 'project' on livedatalab page. Here is the screenshot below. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/18f5ca3d-ae5a-487a-a7bf-bed3b41b86bc/image.png) 2). After I submitted my first code on git, it seems the submission was not submitted successfully. Here is the screenshot below. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2b32029e-badb-4e1e-97e8-1380819facb1/image.png)  Could you please give me some hints or suggestions? Thanks,  Here are errors I get:  ![error_log.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/7a8d89ce-808d-4002-8630-4a1f0481d882/error_log.png) Ya, I can no longer see my score on Livedatalab. Looks like they have removed all the projects there.  I did everything and got a full grade yesterday. Maybe there are changing something now. Same here. Got full grade yesterday but unable to access any project today I keep seeing this error when I try to open the livedatalab site using this link :  http://livedatalab.centralus.cloudapp.azure.com/ As a result I am completely blocked. Please let me know if you have ideas. Thank you  ![Screen%20Shot%202022-08-30%20at%208.46.51%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5ade8526-ab94-4668-a751-5bf73f6e5de7/Screen%20Shot%202022-08-30%20at%208.46.51%20AM.png) 
https://campuswire.com/c/G984118D3/feed/764 MP2.4 ipynb (from Readme) See attached:  [CS410_MP2_4_README.ipynb](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bf3775cc-f0ea-44d9-9345-510240036166/CS410_MP2_4_README.ipynb)How do we convert it in to this format? No tricks. I just converted it manually from the raw data of the markdown file.  Thank you!  Sure thing!
https://campuswire.com/c/G984118D3/feed/357 What is treated as a submission? Hi Kevin,  For curiosity I wanted to know why the my submission is showing the result as count of number of unique URLs instead of counting the number of different contexts being provided?  For example:  Query 1 --> Document 1 Query 2 --> Document 1 Query 3 --> Document 1  Query 4 --> Document 2 Query 5 --> Document 2  I see the total submissions being shown as 2. Is that expected?No sorry - that is not what I intended. In your example, I think the correct could should be 5, not 2. I'll fix this ASAP. Update: I've made the fix, total submissions should count each individual unique URL submitted. Thank you for prompt response!!
https://campuswire.com/c/G984118D3/feed/187 Issues with LiveDataLab Hi. I'm having issues with LiveDataLab account: - I cannot see any of the MPs. Under **Projects in this Course** for CS 410 it states "This course does not have any projects." - Webhook deliveries were never successful, including the initial ```ping```. The URL used is [http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger](url)  Additional information 1. The Personal access token created is shown as "Never used" while my LiveDataLab account is linked to Github 2. LiveDataLab is created with Illinois email id; Github is under my personal email 3. I recall creating an additional LiveDataLab account w/ Illinois email id a while ago. I forgot the password and attempted to re-sign up and was able to "reset" my password this way. This could have created duplicate accounts?  Can someone please take a look at my account and let me know how to fix it? Thank you.Me too. Just joined the class and the website for livedatalab told me This site can’t be reached Not sure if this is the case for everyone, but I've already submitted MP1 (been enrolled since first day of class), yet I'm seeing the same thing you are. My guess is this may have something to do with the server itself, maybe some sort of changes being done on the back-end?   I'm still seeing submission history though. My guess is this will probably have a resolution very soon. Just checked again actually and I can no longer access livedatalab at the moment as well, so you're definitely not alone. Similar as Raza, but I can still access livedatalab at this moment. I submitted my MP1 already and I can see my submission history.  ![Screen%20Shot%202022-08-30%20at%2011.34.36%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8af35920-d141-4fc3-9142-b279f4a49919/Screen%20Shot%202022-08-30%20at%2011.34.36%20AM.png) I am seeing same issue today. However it was working yesterday though. Everything works for me now! You should check!
https://campuswire.com/c/G984118D3/feed/1200 Reviews assigned! Hi all,  Thank you for those who have accepted the CMT reviewer invitation email. If you haven't accepted yet, please do so ASAP.  I've now assigned the reviews. After logging into CMT and selecting role "Reviewer", you should see 2-3 papers. Clicking "Show Abstract" should give you the link to Github, which should contain the proposal. Clicking "Enter Review" will let you answer the review questions. For this stage, only ones that are marked [PROPOSAL] are necessary to fill out. For the others, you can put anything. Please try to complete the reviews by  October 30th.   I will be assigning new reviews daily to those who accept the invite late.   Best, Kevin
https://campuswire.com/c/G984118D3/feed/1127 E-step  I have trouble getting the right output at E-step. What I did is for numerator, I  multiply document_topic_prob and self.topic_word_prob. For denominator I just did np.sum of the numerator. Then I normalize it. But I get this error. why? It seems my calculated output cannot be normalized![Screen%20Shot%202022-10-21%20at%2010.22.46%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/82f6c423-7f44-43e5-93a9-778a46ee6788/Screen%20Shot%202022-10-21%20at%2010.22.46%20PM.png)I suggest to test your algorithm on a smaller matrix and see if it gets what you want. i also had some errors like this. there could be a number of possible reasons. one thing i recommend is checking the shape of your matrices at various points.  another thing that helped me was that sometimes i found i had to index my 3-d numpy array like `[d][j][w]`, and sometimes i needed to use `[d,j,w]` -- the latter seeming especially important when indexing a slice like `[d,:,w]`...there may be a formal reason for this in numpy, as most cases online seem to use that method: https://www.w3schools.com/python/numpy/numpy_array_slicing.asp https://stackoverflow.com/questions/4257394/slicing-of-a-numpy-2d-array-or-how-do-i-extract-an-mxm-submatrix-from-an-nxn-ar My guess is you were trying to normalize a 1-dimension array with the given `normalize` function? If so, `normalize` function assumes input is a 2-dimension matrix.
https://campuswire.com/c/G984118D3/feed/883 Is there a practice exam for exam1? Question as aboveNo. However, the exam should be similar to the quizes
https://campuswire.com/c/G984118D3/feed/25 webhook link ![WechatIMG14898.jpeg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d098050c-2815-4f6c-abdb-7e344c3a7ecd/WechatIMG14898.jpeg) after I put the link it, it looks like above . Has the link problem been solved or not? I am not sure which link to use.Please try **livelab** instead of **livedatalab** I have tried and it's still working. this:http://livelab.centralus.cloudapp.azure.com/api/webhook/trigger is working for me (X). Edited: Neither of the URLs is working. This url is not working for me either.  I am getting the same warning as above.  Hopefully this will get ironed out with #21  Sorry, it now shows error for me as well. Are you still getting an error? The webhook URL is: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger. Please make sure you are selecting application/json as the Content Type. Hey Priyanka,  I am still getting this error after using the link you provided and selecting application/json content type. ![Screen%20Shot%202022-08-22%20at%208.58.06%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/383878de-dbd9-49e0-94ec-24ee21657df2/Screen%20Shot%202022-08-22%20at%208.58.06%20PM.png) What is the error you are receiving? Is there anything written below this or any message when you hover over the red warning symbol? This is the error I see when I hover over it. ![Screen%20Shot%202022-08-22%20at%209.03.53%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f6192e98-c38b-422d-89e5-5eef4cab4dad/Screen%20Shot%202022-08-22%20at%209.03.53%20PM.png) I think my hook works. I use my email as the username when link account in livedatalab. I delete the wrong link and link again. It says: the last delivery was successful!  I linked a new account with my github login email and this new account replaced the old linked account. then I tried the above link: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger and it's still not working. Can you please share a screenshot of your webhook config and the error you are getting?  I still see this the same error as above: Last delivery was not successful. failed to connect to host. Here is my webhook config: ![Screen%20Shot%202022-08-22%20at%2010.05.06%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6b6e32b2-e219-4901-b7f0-c6777a3fee7b/Screen%20Shot%202022-08-22%20at%2010.05.06%20PM.png) Looks like you are still using the old webhook config, try changing to livedatalab instead of livelab I had the same error so I completely removed the webhook. Then recreated with the URL: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger. Hi Priyanka, Thanks for the heads up! I was mixing up the new and old links each time I started over so I was getting a bunch of different results. Set everything up from scratch again with the new link and it works now- "last delivery was successful" Thanks! I saw you have setup successfully. What did you delete and restart from which step? ![Screen%20Shot%202022-08-22%20at%2022.32.13.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/52d34195-f698-4f99-9aeb-880f5d752d7a/Screen%20Shot%202022-08-22%20at%2022.32.13.png) looks like above Hi Yulei,  I clicked on the "Delete Linked Accounts" button on the top right in LiveDataLab. I re-entered the information with "github.com", my github username, and the API key (use plain text- not sure if this matters but thats what I did). Add the linked account. Then I deleted the webhook config on github and added a new one with the correct link (http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger). This fixed the issue for me. Posted on answer above! Thank you! I finally add the webhook successfully! I have used the URL: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger. I also left the security blank. You might have to delete the existing webhook and create a new one. Once created, give it a minute to establish the communication. After doing this, mine shows green check-mark. 
https://campuswire.com/c/G984118D3/feed/1121 LiveDataLab Forgot Password Hi,  Looks like I have forgotten password for LiveDataLab.  Can somebody advice what the next step is? I just signed up again. Will this be a problem?Have you signed up with the same email address? If so, you shouldn't have any issues. give 1 like because it cracks me up XD Yes. I have signed up with the same email address, however I am facing another problem as mentioned in the post #1136 . Thank you for the help!
https://campuswire.com/c/G984118D3/feed/11 Issue with MP1 Hi, when I click on the "open tool" button of MP1, an error shows up. Seems TA may have answered in a previous post:  *Hi there, please try the following link: http://livedatalab.centralus.cloudapp.azure.com/course/join/WZS77P00E570J74*  *You should also see MP1 available now.*   ``` ``` Just wanted to leave a note for anyone else going through this LiveDataLab/Github process.  The MP1 "Open Tool" button is broken.  The address above will allow you to continue with the instructions given in the MP setup doc.  The next step will be to select the "Project" nav bar tab.  
https://campuswire.com/c/G984118D3/feed/457 MP2.2 Judgments Hello,  I did two queries and at least 10 judgments for each query, but it shows my submitted judgments are only 3, any ideas?   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6357cd7b-4f6f-446b-9015-ee932c5940e6/image.png)It seems the system only records the number of queries, not the judgments, please confirm. It is 5 searches, 10 clicks (relevant, not relevant) each, so 50 clicks. As per my understanding, the system is recording only the number of queries I agree with our classmates. It only means you did 3 searches. The counter will not tell you how many judgements you give for each search. I did 6 searches (for each I have more than 10 judgements submitted) and for me the number is 6.  Yes sorry - that number is the number of judged queries, not the total number of judged results. When I get a chance, I'll update the UI to make this clearer. And to receive full credit for the assignment, that number should be >= 5.  I think we should submit >= judgments with at least 10 relevant results each and the number of relevant results will not show on this page. Seems it hasn't been updated yet? Yes, your understanding is correct it only counts the number of query submission (and doesn't count /display) the judgement for each query.
https://campuswire.com/c/G984118D3/feed/495 About the probabilistic model Hi everyone. p(R = 1 | q, d) is approximated using the query likelihood, p(q | d). I wonder why not using the document likelihood, p(d | q). That is, the probability of a document given a query. I think this is more meaningful. Thanks!It might because  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ad101542-8d63-45d0-b241-0ecf4f30e7e3/image.png)  From wikipedia: https://en.wikipedia.org/wiki/Query_likelihood_model Great, thanks for the answer! In my mind, One of the reasonings behind exploring the probability of a query given a document p(q|d) is the challenge behind getting the relevant document given the size of the internet. p(q|d) model explores on extrapolating on the probabilistic retrieval model by thinking there is a imaginary document that exists and has a certain content , how would user formulate a query given language model approach, more like a reverse engineering approach.  Thanks for the insight.   I agree that p(q | d) is much easier to estimate. But for p(d | q), smoothing is impracticable using a “query collection”. So Bayes’ theorem is used here.  Just to add to the answers given by others, in the query likelihood model, we think of this question "If a user is thinking of this document, how likely is the user going to pose the given query Q?" Document likelihood, i.e, the probability of a document given a particular query is definitely another possible approach. But what makes query likelihood convenient is that since queries are small and documents are big, we assume that the query is generated from the words in the document. Predicting the probability of a large document based on a small query is much more difficult. Thanks Harita for explaining this!
https://campuswire.com/c/G984118D3/feed/1110 Proposal Submission via GitHub Hello, I just wanted to confirm that we are supposed to add our Project Proposal to our forked Course Project GitHub repo only, correct? We have to create a CMT account and update it with the link to our project repository, but we are not adding the proposal to CMT, right? Thank you.I believe that's the correct understanding. The Last point in the CMT Submission doc mentions it.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/213b5be7-d4de-4f51-9d52-72124afe47e4/image.png) Yes, this is correct! You will add all project content to your GitHub, which will be linked in the abstract submission on CMT.
https://campuswire.com/c/G984118D3/feed/59 Have installed metapy, but "import metapy still can not be resolved" in visual studio I think I have successfully installed Metapy.![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0f11a166-6dc0-470d-b506-46d1aecaf5f8/image.png) But, in the VSC, it is still has error.![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b1929992-cd4e-4f96-a391-74af38454419/image.png) "tok" and "trigrmas" are not defined. Is this normal because I havent finished the code? Or, my installation has some problems. ThanksYou need to define tok and trigrams in the portion above. Starting at line 7 (‘’’Place your code here’’’) you’ll add your own code to complete the assignment, and you’ll define these variables in doing so You need to initialize these 2 objects in your function. The metapy analyzers functions will return your "tok" and "trigrams". Then the following code can use these 2 obj to finish the work. You can refer to the MP1 readme where you can find all you need to complete this assignment. Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153  
https://campuswire.com/c/G984118D3/feed/135 Lesson 2.5: Compressing Document Ids On slide 5 out of 7 of the Lesson 2.5 slides, we are told that the differences between consecutive document IDs of tuples describing the number of occurrences of a specific term in different documents are likely to be small, and that this allows us to compress. Why are these differences likely to be small? I did not understand the explanation in the video,We expect the distribution of words in collections to follow Zipf's Law. This means that the distribution of terms is quite skewed. Though the most frequent words occur quite a lot, if we reserve these as stop words and remove them during the parsing step, then we can expect our specific terms to occur much more rarely. Because these rare terms occur less often, their counts are smaller, so we expect to see smaller integer numbers there. Even if we do not remove the stop words, I believe since there are far fewer stop words than rare words (in terms of count of unique terms), then there will also be fewer large numbers than small numbers. A large number occasionally is fine for something like $$\gamma$$ encoding or moreso for $$\delta$$ encoding That's not answering my question. You are talking about the compression of term frequencies, not about the compression of document ids. i see, sorry i misread. in that case, the document id's are stored via `d-gap`; you subtract the document id that a term appears ($$d_i$$) in by the document id that the term last appeared in ($$d_{i - 1}$$). then you store those `d-gap` differences. there are thus two cases for when we are indexing document ids that a term occurs in: 1. the term is a very common word that occurs a lot of the time; so it occurs in a lot of documents. in this case, we expect the `d-gap` to be small for the documents it occurs in (since we are accessing sequentially). this is the most frequent case by far, again according to Zipf's law 2. the term is a rare word and only occurs in a few documents. in this case, it is possible we get a large `d-gap` value. however, this is rare and won't happen much, since only a  few documents contain this query term to begin with  So we can leverage this skew in the term distributions when compressing values here as well  Please let me know if that clarified things or if I misunderstood again, and apologies for the confusion!  Okay, thank you. Hi guys. I am confused about the term in d-gap. Should a specific term be considered in d-gap? Could you offer me a simple example of d-gap? Hi guys. I think I have understood d-gap. I noticed that compression is implemented for postings. Thanks!
https://campuswire.com/c/G984118D3/feed/384 Lesson 4.5 - Formula question Hello TA,  For lesson 4.5, the formula in the video is different from the one in the slide, may I know which one is correct?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9c4b88ee-5376-412f-b8db-044bcf76f2a8/image.png)  ThanksThe issue with the first formula is that there is $$c(w,q)$$ even though w is not defined. If it was $$\sum_{w\epsilon V, c(w,d)>0}c(w,q)log\frac{p_{seen}(w|d)}{\alpha_{d}p(w|C)}$$, it would have been correct because we are considering the count of words in the query. $$\sum_{w\epsilon V, c(w,d)>0}c(w,q)log\frac{p_{seen}(w|d)}{\alpha_{d}p(w|C)}$$ is equivalent to $$\sum_{w_{i}\epsilon d, w_{i}\epsilon q}log\frac{p_{seen}(w_{i}|d)}{\alpha_{d}p(w_{i}|C)}$$ in the formula on the second picture.  Hello Harita,   Thanks for the answer! It seems like they are equivalent when $$c(w,q)$$ is equal to 1 for all $$w$$ under $$q$$ only to me, if $$c(w,q) > 1$$, they will be not equivalent, is that correct? It looks like the slides have skipped some relevant context. I would suggest looking at the formula in page 120 and 121 in the textbook.  ![Screenshot%202022-09-08%20at%2012.56.48%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bd591fce-abda-4dae-967c-1a4521673a49/Screenshot%202022-09-08%20at%2012.56.48%20PM.png) ![Screenshot%202022-09-08%20at%2012.56.57%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e1bbcb11-54d8-4a34-913e-989341def89e/Screenshot%202022-09-08%20at%2012.56.57%20PM.png) 
https://campuswire.com/c/G984118D3/feed/337 MP2.1 - Linking Specific PDF Page via DL Extension - Not working Dears,  I was trying to see if I can link a specific PDF page while adding searchable content via DL Extension (as per the given instructions), but it seems the extension is not able to understand the #page=<pageno.> in the link as shown below:  **Snapshot 1: Linking the Page by giving the "#page=111" at the end.** ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b458c659-cfc0-4a8e-b85c-c5b25942a24d/image.png)  **Snapshot 2: The Link created is not tagged with "#page=111" at the end.** ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e05e03f8-093f-457d-81bf-1ba31a3b9cd2/image.png)  When I open the above link it opens the PDF from Page 1 instead of directly going to Page 111. Maybe I'm doing something wrong, appreciate your input.  Thanks, AyushThanks for noticing this! From the backend, it looks like the page fragment in the URL is being stored, so you're doing it correctly. I'll look into fixing this error.   In the meantime, please feel free to still add specific PDF pages. Alright, this error should be fixed! I checked your example, and it opens to page 111 now.   Also, one thing to note: The URL for the first (non-indented) link in your example will NOT have the fragment, because it is meant to be a pointer to the entire resource. However, the second (indented) link, which is meant to point to specific submissions, will have the pdf page fragment.  Thanks, Kevin for your quick help on this. I'm now able to link specific PDF pages. Appreciate your help. Wonderful! Glad that it works on your end
https://campuswire.com/c/G984118D3/feed/1274 Where to Put Tech Review? Sorry for asking so many questions about this, but where do we put the file "techreview.pdf"? Directly in the downloaded repository? Within the tech_review folder (so /tech_review/tech_review)?I think you can just put it in the /tech_review/ folder. If that doesn't work put it in both the root level dir and the /tech_review/ dir.  You can tell whether or not the autograder found your file by clicking "Leaderboard" and seeing if your score is a 0 or a 1. I have not submitted the PDF yet and mine is a 0. You can see others have a 1. ![Untitled.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/01a41772-6b73-4228-9337-402b39271915/Untitled.jpg)  A TA can answer more direct, but I don't see putting it in two places hurting anything.
https://campuswire.com/c/G984118D3/feed/877 Decode gamma code  Can anyone please give an example of how to decode a gamma code?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4a427f4e-6e29-4978-b7d2-654ae48c23f3/image.png)Lets decode 1110010  K+1 = 1110 K+1 = 4 K = 3 (which is floor(log2 X); we are calculating X)  Now 010, remaining bits are in binary So R = 010 binary = 2 decimal   X = 2^K + R = 2^3 + 2 = 10  Hope this helps. That helps, thanks! Hitesh Yadav gave an example, but I would like to say that it might be good to try and convert the answer you got back into gamma code and see if it equals the provided gamma code, in order to make sure that you did not make any mistakes.  Likewise, when asked to convert a number into gamma code, you could try to convert it back into a number to check for errors. The easiest way to decode (inverse) gamma is to replace the unary prefix with a 1.  Steps: - discard the unary prefix, which is everything up to and including the first 0  - prepend a 1 to the remainder   Example:  $$ 1110010 \quad (\gamma   \text{ encoded})\to 1110 \mid 010 \to 1 \mid 010 \to 1010 \quad (\text{binary encoded}) $$
https://campuswire.com/c/G984118D3/feed/142 Tech Review Signup Sheet Edit Access  > "Topic proposal: Every student is required to select a topic from a provided topic list or propose a topic by the end of Week 9 (Oct 23, 2021) in the signup sheet (access with illinois.edu email address): https://docs.google.com/spreadsheets/d/1hWAyxd82FcitN9eG3yMW6ckq6l1VASBtYWpaPbywMPc/edit?usp=sharing"  Is anyone else having trouble editing the tech review signup sheet?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/680a952e-604e-4072-9150-891e58b8c2ea/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/503eb37c-52fb-4d01-871d-c8aaee731089/image.png) Sorry, you should have edit access now! I requested access to this google sheet with my Gmail account a week ago, and have not gotten it.  I also cannot create an account or directly sign in with my illinois.edu account. Would you please advise on how to do it? Thanks! Please refer to post #23. It has instructions on how to directly sign in with your illinois.edu account. https://campuswire.com/c/G984118D3/feed/23 Thanks, it's working now!  Thank you. Shengyuan Sorry, another question. I remembered there was another google sheet other than this "tech review signup" requiring access when doing the orientation in Coursera. I cannot find it now on Coursera. Perhaps I have mistaken it. If there is one, could anyone post the link here? If not, do you mind confirming it here? Many Thanks! Are you looking for the sample topics doc? > Some sample topics are provided here: https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit?usp=sharing    no, this link is provided in the tech review sign up form so I can find it. I suspect that I missed a link about the topics for a group project. 
https://campuswire.com/c/G984118D3/feed/890 Question about CMT platform It appears I have to manually search for our course in the "All conferences" list. Is there a way I can add the course to "My conference" list to make it easier to access? (Google didn't seem to help me)  Thanks!You can try to remove some characters from the query. For example just type "CS410", our conference will show up. For your second point, I also want to know the answer.
https://campuswire.com/c/G984118D3/feed/759 Can we choose any task if we plan to make a Leaderboard Competition? For the [CS 410 Project Topics](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit) guide, under Leaderboard Competitions it says "You can create a competition on any task that is interesting to you." Does this mean we can make competitions which are not directly relevant to the course material?  Thanks for the clarification.The proposed leaderboard competition should be relevant to the course material or to the general theme of the course (information retrieval, language modeling, etc.). If you are unsure, please feel free to send me the topic(s) that you're interested in and we can have a discussion.  Thanks, I was thinking of creating a competition for the task of identifying sarcasm in news articles (like The Onion, for an extreme example) based on the headline and the contents of the article itself. It's in the realm of text but it's more of a sentiment analysis/classification problem than what we've been doing in the course so far. Do you think this would be an appropriate topic for the project? Yes, that would be an appropriate topic for the project!
https://campuswire.com/c/G984118D3/feed/78 LiveData not updating Hi,  I linked my LiveData lab account with my GitHub account and the Webhook in my private repo also shows recent deliveries from when I pushed my code (also tried redelivering the payloads). However, it's been over 20 minutes and the grading script doesn't seem to have completed.. I don't see any recent submissions on LiveData lab or a score on Coursera. Livedata lab also says that I have not started any projects. Not sure if I'm missing something?Same thing is happening to me. Actually, I had this for the webhook address `http://livelab.centralus.cloudapp.azure.com/api/webhook/trigger`, when it should be `http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger`. Works for me now. Hmm, yeah I used the updated webhook link. Not sure why it’s not linking the two as it should  This may be due to the server instability. You can try to resubmit it at another time. Or you can check the generated logs as this might also be caused by the incorrect codes. I think the problem is with the PAT. I deleted linked accounts on LiveData lab, regenerated the PAT, and linked again. However, I see "Never Used" next to the CS410 PAT on GitHub, so I don't think it was properly read in by LiveData lab. Any recommendations? Can I link my account a different way? it is not working for me , can someone please help. Did you get it work? As Priyanka can see your netid in the leaderboard. No, still not working If possible, can you take screenshots and create a new post explaining your issue? Thanks! Done..post is #94
https://campuswire.com/c/G984118D3/feed/105 General MP question I always have trouble setting up environments.  Would it ever be possible to put everything you need to get the MPs up and running in a container?  I just don't have any experience with this.  Thanks!!Have you tried the setup and faced any issues? The process is mentioned step-by-step in the instructions. I got it working.  No worries!! It is more along the lines of installing Python 3.5, making sure that Python 3.5 opens instead of Python 3.9, getting pip to work correctly, etc.  Environment variables, etc. I wound up just making a conda environment, but this took a little bit of doing. Thank you!
https://campuswire.com/c/G984118D3/feed/1179 MP3: log-likelihoods decreasing, but consistently Getting an interesting result with my log likelihoods at the end of each iteration, and the algorithm doesn't converge in the default 50 iterations.  My log-likelihoods are negative (as I'd expect) and decreasing, but decreasing by a consistent amount (roughly 1200). The algorithm does not converge after 50 iterations and ends in the -58,000 range.  Questions that'll hopefully point me in the right direction...  * should this algorithm converge within 50 iterations to pass the autograder?   * is -58,000 (approx). in the right ballpark for a log likelihood after 50 iterations?  * what should normalization look like in the expectation step? This is where I currently suspect I'm having issues - shouldn't each document/vocab word pair be normalized across all topics?  * Guessing that LiveDataLab uses a different implementation of `main` since I'm seeing 300 iterations in those logs...does it also use the `DBLP.txt` input file rather than `test.txt`?im answering from my understanding and looking at my results:  1. not necessarily; i believe this might depend on a few things, including the results of your random initialization 2. i had different results than that, again that may be related to random initializations 3. i recommend #1132 as a helpful visual for this. generally, when you want to normalize a distribution, you can coerce that distribution at add up to 1. the reason this works is because when you $$\sum{x \in X}$$ and set that equal to 1 it adds an implicit denominator to all the term; see https://math.stackexchange.com/questions/278418/normalize-values-to-sum-1-but-keeping-their-weights reference the `softmax` function; fundamentally you are making sure that for every `$$k$$ topic, you normalize $$p(Z_{dw = j})$$  tp 1 4. i think it does and maybe even with hidden inputs but im not sure here Got things working a couple hours after this, figured I'd answer my own questions in case anyone else runs into similar issues -  * doesn't necessarily have to converge within 50 iterations for `test.txt` to pass the autograder (mine finally converged in ~150 iterations and then passed the autograder in ~25 iterations)  * log likehoods are pretty meaningless by themselves (it'll depend a lot on how you preprocess the input...for example, I wasn't parsing out the \t token in `test.txt` to begin with), don't look at their values, just their values and deltas in comparison to other iterations  * normalization is definitely needed in the E step, learned that the hard way after some crazy likelihoods and ultimately divide-by-zero errors :)  * seems like yes, different implementation of main for the autograder, so changing `main` locally doesn't matter and can be useful for testing Thank you! Finally got this working and came to similar conclusions - stumbled onto #1132 as well, super helpful great! glad you got it working I was also in this situation. What worked for me was to double check if variables in the E step and M step were updated correctly. same issue ,my E step was wrong
https://campuswire.com/c/G984118D3/feed/852 Quiz question I still don't understand this question:  Question 7 Which of the following is NOT correct about the unigram model?  1 / 1 point  1. The probability of generating the word sequence "A" "B" "C" is the same as generating "C" "B" "A."   2. The probability of generating the word A OR B is the sum of the probability of generating A and the probability of generating B.   3.  The probability of generating the words A AND B is the product of the probability of generating A and the probability of generating B.  I would think answer 2 is true.  If I am picking the first word of a sentence out of a bag of words, the chance that I pick A or B is p(A)+ P(B).  They are independent.This is my understanding: Query likely hood model (probabilistic language model) defines what is the probability of user entering a query that is likely to match a document. As we assume that each outcome is independent, the probability of a sequence of outcomes is calculated by multiplying the individual probabilities.  For example what is the likelihood of user entering the query that contains "Presidential" and "Campaign" in the query. Both of these are independent keywords but what is the probability of user entering them together that matches the document. P(A and B) = P(A) * P(B) Would love to get insight if the above is incorrect. Feel free to mark it as unresolved.  Thank you Hitesh.  I agree with everything you say.  My problem is that the key gives choice 2 above as false.  I think it is true.  Sorry to not be clear.  I asked this question a few weeks ago, and the response was that #2 is incorrect because we should subtract the intersection of A & B.  I don't understand that if picking A and B are independent.   Thank you!!  I am not sure why we would intersect the words (probability wise). I think intersection might not apply in Unigram as we are only predicting one word at a time. There is no dependency or interaction between the words while predicting probability of the word. Exactly! Hi Frank, p(A or B) = p(A) + p(B) applies if A and B are mutually exclusive, i.e., if we know for a fact that A and B are different words. I think we cannot assert that, since every unigram word is independently chosen. Then we're left with the more general formulation, which is p(A or B) = p(A) + p(B) - p(A ∩ B).  I hope this helps.    That is it!  Thank you David!!  It never occurred to me that A could equal B.  If this is the case, you can only pull it from the bag once.  Very tricky!
https://campuswire.com/c/G984118D3/feed/334 MP 2.1 Start Date Hi, I see that MP 2.1 is open on Coursera, can we start working on it? Is MP 2.1 finalized ? I am still not able to access the sign up sheet though.  Thank you. ![Screenshot%202022-09-05%20at%202.38.58%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/561d4259-d2e8-46ae-836f-5770ef0ddcf9/Screenshot%202022-09-05%20at%202.38.58%20PM.png)I can access the doc after logging into G Suite with my illinois.edu email  How did you get your G Suite account? I tried to login with my illinois.edu email but failed.  Probably this may help https://www.nuestraverdad.com/post/activating-google-apps-uiuc  I don't remember how I got it since I've used it in a previous class. Maybe this document might help you? https://help.uillinois.edu/TDClient/42/UIUC/Requests/ServiceDet?ID=135 Thanks, I followed this  https://www.nuestraverdad.com/post/activating-google-apps-uiuc and the problem is resolved.
https://campuswire.com/c/G984118D3/feed/144 For anyone who wants to read C.J. van Rijsbergen Here is the link.  http://www.dcs.gla.ac.uk/Keith/Preface.html  The chapter 6 pdf is http://www.dcs.gla.ac.uk/Keith/pdf/Chapter6.pdf , which is on the above website.    Thanks for sharing. This is helpful. Thanks for sharing!  Thanks for sharing this link! This is very helpful! Thanks for sharing! Thanks for sharing!
https://campuswire.com/c/G984118D3/feed/250 Colab Options If you are still having troubles configuring metapy on your local machine and you would like to try Colab, this will help you get started.  First, add this in a code cell at the top of your Colab notebook and run it:  ``` !pip install metapy pytoml ``` Now you have two options:  Option A: - Write and test all the assignment code in the traditional interactive notebook format using multiple code and text cells - When done, comment out the pip install line  ``` # !pip install metapy pytoml ``` - File > Download > Download .py to download `example.py`   Option B: - Create a second code cell and put this cell magic statement at the top ``` %%writefile example.py ``` - Write all the assignment code in that single code cell  - To test, you can either comment out the magic statement, or you can add a third code cell with this line magic to run the output script: ``` %run example.py ``` - When done, manually download `example.py` from the files tab on the left side of Colab Thank you for sharing this. I found another post very helpful. I believe these really help folks who are having issues with installing metapy on their machine.  I would just add that once you have tested the code, push it to the git which automatically runs it and update livedatalab with the result.
https://campuswire.com/c/G984118D3/feed/1005 MP3 reference links Has anyone been able to access or download the references mentioned in the MP3 instructions?  I get an "unable to connect" error even when logged in to UIUC and connected through the UIUC VPN.  The references are: [1]	[Cheng’s note](http://sifaka.cs.uiuc.edu/czhai/pub/em-note.pdf) on the EM algorithm   [2]	[Chase Geigle’s note](http://times.cs.uiuc.edu/course/598f16/notes/em-algorithm.pdf) on the EM algorithm, which includes a derivation of the EM algorithm (see section 4), and   [3]	[Qiaozhu Mei’s note](http://times.cs.uiuc.edu/course/598f16/plsa-note.pdf) on the EM algorithm for PLSA, which includes a different derivation of the EM algorithm.See #843  I engaged the TA's in the comments of that discussion, and they never got back to me. Dave, thank you very much for the link to Professor Zhai's note. Following your lead, I found papers by Chase Geigle, but it's not clear which one (if any) is the one intended for us. Was that your experience also? Yeah, that's as far as I got. I've been following up trying to find him for `metapy` project work (he's a maintainer of the project) and even the course staff can't get in touch. I don't think there's much hope that it gets resolved. Hi sorry for the confusion, I thought the links were being accessed from the Github repo: https://github.com/CS410Assignments/MP3  I've updated the links there, and they work on my end. Thank you, Kevin! I can see the papers now. Links in the MP3 instructions are pulling from locations other than "timan".
https://campuswire.com/c/G984118D3/feed/494 how do I submit mp2.3 I think I am overlooking some instructions somewhereI think the way to submit MP2.3 is the same as MP1.  Programming Assignments Overview: https://www.coursera.org/learn/cs-410/supplement/irzBO/programming-assignments-overview  Coursera: https://www.coursera.org/learn/cs-410/gradedLti/026Fl/mp2-3  GitHub: https://github.com/CS410Assignments/MP2.3 Yes, its similar to MP1. File search_eval.py has to be modified to incorporate the asked functionality in the assignment. Also, need to create a txt file named significance.txt with just the p-value.  Both the files have to be added to Github using the same steps as done in MP1.  Thank you both!
https://campuswire.com/c/G984118D3/feed/516 MP2.4 Coursera LiveDataLab link expire Hi,  I tried accessing the LiveDataLab link on Coursera (the blue button) and it's already expired. Please fix.  P.S.: Also on LiveDataLab, 2.4 github is not released.  Thank you,Could you elaborate on what you mean by the link "expired"? Is there a specific error/issue that you are seeing? The link seems to open up a web page with an error webpage not available. Have you tried clicking on it? I think MP2.4 has not been released yet. We were targeting to release it on Sep 21. Oh okay. Thank you
https://campuswire.com/c/G984118D3/feed/53 Unable to access sign up sheet of MP 2.1 Hi, the sign-up sheet of MP 2.1 is stating that I need access even though I logged in with my Illinois id.   Thanks for advice. Thanks for reporting the problem. I'm sorry that we have prematurely made MP2.1 available to you, causing the problem you've encountered. MP2.1 was scheduled to be released a bit later, and we are still working on testing the whole MP2, including MP2.1, which will be different from what you've seen in the current description as we will ask you to use an alternative way for collecting content so as to eliminate the need for the sign-up sheet. So please wait a bit until we officially release MP2.1. We'll post a note here to let you know once it's officially released and available for you to work on. 
https://campuswire.com/c/G984118D3/feed/775 Practice Quiz 5 random jumping ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/55877ca2-c384-4508-bf83-c6a63f13652b/image.png)  May I ask why the answer is this one? How about other two choices?I'll go through these one by one to discuss how they relate to PageRank's random jumping in inverse order:  1. *Otherwise disconnected page has zero probability*: This is a problem that random jumping *does* help solve; imagine a completely disconnected page. Without any incoming links, there is no way to follow a link to reach this node. Thus, without random jumps the probability of reaching this node is 0. Adding random jumping means this node will have some probability to be reached no matter what node you are currently on, solving this issue. So, this is not an answer for this question, since it *is* a benefit of introducing random jumping 2. *Otherwise for zero-outlink nodes will receive all the probability*: Without random jumping, if you end up in a node that has no outlinks, you will get stuck there. You have zero probability of traversing to any other node. Random jumping introduces probability that you will go to another random node even when in this situation, thus mitigating this issue. Since random jumping does solve this issue, this is also not an answer to the question 3. *Otherwise PageRank will favor nodes with fewer incoming links*: Without random jumping, nodes with fewer incoming links will be reached less often, since it is less likely to reach them at most time steps. So PageRank will *not* favor nodes with fewer incoming links. With random jumping, nodes with fewer incoming links may actually be reached *more* frequently, since there will always be some non-zero probability to reach that node. So, not only is this not a problem with PageRank in general, but random jumping also does the opposite of what this option suggests. So, this is *not* a benefit of introducing random jumping, and is the correct answer for this question  Please let me know if that made sense, I am happy to go into more detail, especially around how PageRank and random jumping work If random jumping is not there, nodes with no outgoing links will get all prob. bcuz, if the control reaches that node, we can not go out of that node. But, random jumping helps here. If random jumping is not there, nodes that do not have incoming links will not be parsed at all. Thus, random jumping helps in ensuring that disconnected pages do get some prob. Thus, second and third options are valid. But, first option is the opposite. Without random jumping, PageRank generally pushes down pages with fewer incoming links. So, neither is the reasoning correct nor does PageRank play a role in first option here. 
https://campuswire.com/c/G984118D3/feed/654 Confused about MP2.3 Hello, I'm a little unsure if I'm doing MP2.3 correctly. I feel like I understand what is needed, but I'm not an expert in python so I don't know how to approach some issues. First, this is how I updated the score_one function. Am I on the right track of what is needed here? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e2e56643-f595-4675-a9f6-4eac7d71e241/image.png)Then do I just modify the load_ranker function like this:   return InL2Ranker(metapy.index.RankingFunction) ?  Then I get stuck on how to create two txt files, one for BM25 and another for InL2. The instructions give the below code under the "Varying a parameter" section and say to modify this to create the txt files. But the search_eval.py file doesn't have this exact code in the main function, so I'm confused what I'm supposed to modify here or how to add the results to txt files.  ![](data:image/png;base64,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) 1. You are on track for the score_one function. I did not check in details for your formula but you can try push, if it is correct you get 70% points.  2.inside the inl2ranker() you have to put parameter=certain number)  3.when it says modify it means change the ranker parameter to another function just like it shows in the snippet. Once you run both code, you get 225 results, you copy paste them into two separate .txt file and read them in as float array, then run the statistical comparison to get p-value Yes you need to modify the scoring function in score_one.  And for the txt files, I think you may just copy the lists of printed scores by calling BM25 and InL2 into two seperate txt files, then read the values from the two files and compute the p value. This part is not given in search_eval.py so I guess you'll need to add this part yourself (either in search_eval.py or outside of it)  Initially, I did my scipy.stats.ttest_rel(a, b) with avg_p list variables.  After realizing you actually needed the text files, I used my empty list a variable to append the avg_p per ranker and ouput it to a file like the code below:  ``` with open(r'C:/~/~/MP2.3_CS410_private/inl2.avg_p.txt', 'w') as fp:         for item in a:             fp.write('%s\n'% item) ```  Do it for both rankers and you should be on track.  Hope this helps! Just wanted to say thank you cus i realized that use the wrong variable for c(t, D). After modify it I got a 0.7, so i guess your score_one is correct. Thank you so much! I think I was overthinking this assignment and didn't realize that I was on track with the first part. I had to fix my webhooks and then I got 70% for the score_one function. Then I was able to get the 225 results for each function and find the p-value. Your comment helped guide me in the right direction! Thank you! It took me some time to figure out how to read the values from the txt files into arrays to calculate the p-value, but once that was done, I was getting full score. Thank you so much! This really helped me. I was stuck on how to read in the values from each function. That's awesome, glad I could help!
https://campuswire.com/c/G984118D3/feed/601 MP2.4 1 on Leaderboard but 0% in Coursera Hello, I noticed that I beat the baseline score with my submission, but I still have a 0% in the Coursera for MP2.4. Any idea as to why this is?Same here. Showing score 1 in leaderboard but 0% in Coursera. May be TAs  can help
https://campuswire.com/c/G984118D3/feed/294 Week 1 Lecture 1.4 In Lecture 1.4, it is mentioned that the key idea behind probabilistic inference model is to associate uncertainty to inference rules. Shouldn't it be certainty instead ? Is this a typo? It would be great if someone could provide more info regarding this concept. Thanks!Probabilistic Inference model associates probability that a function will take value 1 or 0. So looks like it should be 'associating certainty(or probability) to inference rules'. For details can refer to http://deepdive.stanford.edu/inference With probabilistic models, given that what you quantify is the standard error/deviation, I could see how this could be considered “associating uncertainty”. Like with true/false, I think it’s an equivalent formulation to state it either way.
https://campuswire.com/c/G984118D3/feed/590 MP2.4 success rate I am trying various ranking algorithms for MP2.4, without that much success to date. It's a little demoralising to see that no one has actually passed the baseline yet, including people who have submitted dozens of times. What is the baseline? Is it far above what is being achieved by the leaders?According to the TA in #583, the baseline is about 0.7, but another student posted that the TAs told them there might be a bug causing current scores for the faculty dataset to be extremely low. I tried about 90 submissions yesterday before they reset the leaderboard with different permutations of BM25, JM, etc., as well as combination of multiple rankers, but never exceeded ~0.16. At this point, I'm not quite sure where to go from here, so I'm just going to wait to see if somebody is able to discover success. Same here, have tried multiple times. Will wait for update from TAs to see if the baseline is modified  The bug for me is that my **faculty score** is very low like less than 0.0006 regardless what parameters are used. Also the baseline should be adjusted.  I got an answer from a TA. For now, the TAs are working on fixing the problems: 1. some mistakes in the dataset 2. recalculate the baseline score  They will let us know once it is done. So maybe we should pause for now. Yes, I should have specified. Agreed, my maximum faculty dataset score was 0.00079, and even wild parameter variations could hardly get it to budge more than +/-0.0001 Same here, multiple tries.
https://campuswire.com/c/G984118D3/feed/644 MP 2.3: c(t, Q), c(t, C), and c(t, D) In the equations for MP 2.3, what do the following values represent? - c(t, Q) - c(t, C) - c(t, D)  Thank you!You will have to replace them with the correct variables from the sd object file that is linked to the MP2.3 assignment. c(t, Q) = count of the term in the query c(t, C) = count of the term in the collection c(t, D) = count of the term in the document  all of these can be extracted from the `sd` object, and there is helpful documentation for what accessors you have there, though some of the terms have a bit of different descriptions Thank you but I am wondering how to find c(t, Q)[Is that query_term_weight?] and c(t, C)? Since there is no apparent information for these two parts in score data. for this i reference #598  which also links to [this documentation](https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html)  one way to "weight" a query term is by its count in the query  i also recommend thinking about your `corpus`, and what it represents in this case, especially compared with the above documentation  please let me know if that made sense
https://campuswire.com/c/G984118D3/feed/1272 Project proposal submission says Overdue Hi all,  My team leader has submitted the project proposal for my group but it says overdue for me. Is there any step I am missing here or will this be fixed? ![Screen%20Shot%202022-10-27%20at%202.52.03%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/113af6de-1f7a-4bd4-9d23-752b39adabd6/Screen%20Shot%202022-10-27%20at%202.52.03%20PM.png)Just ignore them. We will manually grade your projects and enter your grades.  Sure thank you
https://campuswire.com/c/G984118D3/feed/805 Exam 1 Hi all,  Exam 1 is coming up!  Here is some important information:  **Exam taking window:** Mon 10/10/22 9:00 AM CDT - Sun 10/16/22 8:00 PM CDT  **Scheduling your exam:** You should receive an email invite to schedule your exam on ProctorU soon. Follow the instructions on that email.  **Students in the DSO section:** you will need to pay for your ProctorU exam  **Students in the TIU and TIG sections (on-campus sections):** you will NOT need to pay for your exam. ***Please follow the link in the email you receive.***  **DRES accommodations:**  If you need special accommodations for the Exam, please send your DRES letter as an attachment by email to JC Morgan (jcmorgan@illinois.edu) and cc Assma (boughou1@illinois.edu). According to your accommodations, you might need to follow a different link to register for the exam, if so you will receive an email from JC with instructions.  **The exam will be 60min long (timed exam)**  **Material covered:** Week 1 to Week 6. Watch "Lesson 6.10: Summary for Exam 1." 
https://campuswire.com/c/G984118D3/feed/560 MP 2.3 Failure on LiveDataLab HI, I keep getting a failure on Livedatalab for mp 2.3, and I am not sure why. The logs never appeared to tell me why, and it has been over a day since I pushed and still no logs appeared. How do I find out what went wrong with my code? Thanks.  ![Screen%20Shot%202022-09-21%20at%206.59.46%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/00a7626d-3383-4455-bc6d-40754c38edc1/Screen%20Shot%202022-09-21%20at%206.59.46%20PM.png)  ![Screen%20Shot%202022-09-21%20at%206.59.43%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b3242efd-c00d-4898-b6c1-3472c399a3b9/Screen%20Shot%202022-09-21%20at%206.59.43%20PM.png)Did you get the result from the local? Nothing went wrong locally. It ran just fine  Be really careful about C(t, x), they are three different implementations.  see post #501  I had problems with print statements hanging up the grader I just commented out all my print statements, and I already had created a significance.txt for testing purposes and I just put a random value in it. The submission still resulted in a FAILURE, and logs showed up. I just don't even know why it is coming out as FAILURE.   For reference, I added logic in the score_one function and did return InL2Ranker(some_param=1.0) in the load_ranker function. I haven't modified any of the other code yet for Varying the parameter except commenting out the print statements this is not the issue. Like my issue is that the livedatalab just completely fails to read my code for some reason Resolved. I just deleted my linked accounts in LiveDataLab and generated a new personal access token and linked my github account with the new personal access token and pushed again and that worked.
https://campuswire.com/c/G984118D3/feed/314 Any installation guide for python 3.5 I'm using windows 11  I have tried install ubuntu and it is using python 3.10. I tried the  pip install metapy pytoml but it failed. So I think I need help on how to properly install python 3.5.  I was having the issue and the default python version my computer uses is python 3.9. What I did is to create a virtual Conda environment and set the python version to be 3.5 and then you should be able to run python programs in that environment using python 3.5 I am on Windows 11 as well. Uninstall Python 3.10 and install 3.5 version. here is the link - https://www.python.org/downloads/release/python-350/   you can install the executable installer and then follow the pdf instructions , it should work !  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/70c656d1-d2b7-4626-836d-4c8d6b225321/image.png) You can download Anaconda and create a new environment with Python 3.5 I found the answer in post #28 very helpful I installed anaconda and created a virtual environment with python=3.5, under this environment, I can move a little further. Thanks!  This is the right answer, except create an environment using Python 3.5, not Python 3.7 (which is what the post says).
https://campuswire.com/c/G984118D3/feed/71 Practice Quiz Q4 We are asked "Which of the following application scenario in text retrieval relies LESS on NLP?" The possible answers are "compare homework submissions from students to check if plagiarism occurs" and "Use query in English to retrieve documents in French". The correct answer is "Compare homework submissions from students to check if plagiarism occurs".  Why is this the case? In the second case, couldn't you just translate each word in the query and treat it as a bag of words? That would be fairly shallow NLP. However, in the first case, your application must have a deep understanding of each sentence in order to tell if two students wrote the same thing, as they are unlikely to copy word for word.The question asks which one relies **less** on NLP. Translation (from English to French) relies more on NLP than comparison in the same langauge. BOW will be sufficient in the second case. I think the translation itself could involve some NLP technique like a transformer This is the correct explanation.
https://campuswire.com/c/G984118D3/feed/338 Cannot Create Account I've downloaded the chrome extension, but when I press the button "Create Account" nothing happens. I don't get redirected or an error doesn't display. Are you connected to the Illinois VPN? That was the issue. Thank you! No problem!
https://campuswire.com/c/G984118D3/feed/576 LiveDataLab web session/logout issues Is anybody else having issues with their web session on LiveDataLab? I frequently get returned the the login page while trying to navigate to or refresh the MP 2.4 leaderboard. Once logged back in, I'm taken to the landing page rather than the page I was trying to access, with a good chance that I'll get logged out again while trying to navigate back to where I was.I have not faced exactly the same issue. However, I do get frequently returned to the login page after almost 10-15 refresh to MP2.4 leaderboard
https://campuswire.com/c/G984118D3/feed/1119 Submission of the proposal Hello TAs,  Just wish to make sure only one submission by the group leader is needed for the proposal. From this [document](https://docs.google.com/document/d/1XFvippp_jyiUUXpWsps4Geh5KsyZRkJq/edit) it appears to be the case, however in Coursera it says "*Only the group leader needs to enter the project topic. However, every student needs to enter all other details*." Not sure what are the "other details" rest of group members need to enter.  Many thanks.By other details, I think they are referring to the google doc https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit#gid=0 Every member needs to fill that out with their name and email address. TA's can confirm The "other details" was referring to the name, group name, etc. on the Google sheet.  For the proposal submission on CMT, only the group leader needs to make the submission. But make sure that all members sign up for CMT and the respective account emails are added when making the submission. 
https://campuswire.com/c/G984118D3/feed/995 Week 8 - Quiz - Question 5 can someone help explain the concept in this question   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/3682a96b-d4d1-4c0b-88ab-7eee463e9607/image.png)Think of "the" as a coin that is lands face-up 98% of the time and face-down 2% of the time. "Celestial" is a coin that lands face-up 2% of the time and face-down 98% of the time. The uncertainty of these two coins would be same. entropy, ($$H(X)$$) is a way to measure the randomness of variable `X`. see [this video](https://www.youtube.com/watch?v=YtebGVx-Fxw) for a much more detailed look at entropy.  the common way to describe this concept is with a coin flip. there are two extremes to consider.  1. a perfectly balanced coin that has $$P(X="heads") = P(X = "tails") = 0.5$$ 2. a completely biased coin that shows only either heads or tails (let's pick heads for now): $$P(X="heads") = 1$$  if we want to encode the "randomness" of this coin flip, we may first notice that option 2 isn't really all that "random". there is one outcome that will happen with 100% probability. so, if we wanted to measure the "randomness" of this, what would you want the measure to look like? thinking it through, we'd probably want the "randomness" of this case to be 0 -- there is nothing random here, it is pre-determined. In fact, Entropy for this case is just that: $$H(X) = 0$$ in this case.  for the first case (1), the result is actually random. in fact, for entropy $$H(X) = 1$$ in this case. one way that is handy for me to think about this is "how many bits are needed to encode the possibilities?" in a case of a perfectly balanced coin, you need two possibilities to be fully represented -- heads and tails (1 or 0). this leads to 1 bit, or $$H(X) = 1$$. though, you can have measures of entropy in between 0 and 1 as well: for example a "partially biased coin" that lands on heads 98% of the time and tails 2% of the time. also note that the entropy curve is symmetric; as David says a coin that lands on heads 98% of the time is just as random as a coin that lands on tails 98% of the time  but basically, the less random something is the smaller the entropy. the more random, the more entropy. also, it is helpful to remember the equation for entropy:  $$H(X) = \sum_{v \in {0,1}}-p(x = v)log_2p(x =v)$$ As per my understanding, Entropy is the representation of the uncertainty in the outcome.   With the word 'the' we are almost certain that it will occur most of the time. Similarly, with the word 'celestial' we are almost certain that it will not occur most of the time. As the level of certainty is similar for both the words, they both have the same entropy.  Thank you very helpful ! thank you ! thank you
https://campuswire.com/c/G984118D3/feed/163 Quiz answers Will we be able to see the correct answers to the quiz after the deadline has passed?Have you tried the "View Feedback" button on the bottom right of the quiz screen in Coursera?   That just shows which questions I got right and which are wrong. I can't see what the correct answer for the wrong ones is. Can you see that also? I see, same here. My mistake - I thought you simply wanted to see the questions. I'll investigate this, you should be able to see the feedback after the quiz deadline So the feedback settings for quizzes is already to provide all feedback once the quiz is submitted. see the screenshot. can someone confirm you can see the correct choices for quiz 1? ![Screen%20Shot%202022-08-31%20at%201.42.30%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d726946f-c6e5-4a95-8658-fd44430e558b/Screen%20Shot%202022-08-31%20at%201.42.30%20PM.png)
https://campuswire.com/c/G984118D3/feed/523 Office hours Sept 19 5pm PDT? Hi everyone,  I have joined the office hours but I dont see anyone joining and cannot enter the zoom. Are there office hours today at 5pm PDT?The meeting seems to be at 5 PM PDT, which is 7 PM Champaign time. Isnt Champaign time in PDT? I am in the PST timezone. I believe Champaign's timezone is CDT. PDT refers to Pacific Daylight Time.
https://campuswire.com/c/G984118D3/feed/897 Practice quize 8 Hi,  Can someone please help me with this practice quiz 8 question :( ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6df81136-9266-4de1-bb63-5bd5dc5c9b81/image.png)in a unigram model, you will draw one word at a time from the overall distribution until you have completed your "sentence". each draw from the unigram model will be completely independent.  when we have independent events and want to know the probability of all of those events occurring, you must multiply the probabilities of each individual event to get the answer  so, let's say you generate 3 words from a unigram language model, and get words `x`,`y`, and `z`. then, your sentence is essentially `"x y z"`. so, what is $$P("x y z"| \theta)$$? Well, since we draw each word independently, it is equivalent to the product of the individual probabilities: $$P("xyz"|\theta) = P("x" | \theta) * P("y" | \theta) * P("z" | \theta)$$   Thanks 🙏
https://campuswire.com/c/G984118D3/feed/942 Quiz 3 question Can anyone help explain this question? I am confused. ![Screen%20Shot%202022-10-13%20at%208.57.12%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/5573f737-d5f2-4622-bdbc-6e47534190b9/Screen%20Shot%202022-10-13%20at%208.57.12%20PM.png)The condition suggests that A has higher number of relevant docs at any k. Recall at position k is calculated as #relevant doc retrieved at k/# total relevant docs, therefore A will have higher recall at any position.  Thank you! "Any K " is the trick here !For both precision and Recall "Relevant Retrieval" is still important. Recall the formula from the slide or book, P=a/a+b , R = a/a+c.  Since at no point in the entire K doc set , the Precision is better for System B, hence intuitively and also mathematically the recall for System B is lower than System A. This graph will further help to understand.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/05c2cdc6-d806-4332-9162-b5fe2fbdede4/image.png)
https://campuswire.com/c/G984118D3/feed/323 LiveDataLab not syncing with Git I can't even see my pushes to the git repo, and under LiveDataLab it just shows a blank space.  I read a couple of solutions related to this: 1. I registered LiveDataLab with my school email and that only 2. I deleted and re-linked my git account multiple times 3. There is only me as the author in the git repo, aside from the original author that I cloned from  but LiveDataLab still decides not to sync.....So the real problem seems that I cloned directly from the git repo provided, which caused git to think I'm not the only author(?) After redoing a new repo via **uploading** all 3 files, I was able to get the webhook attached and working.
https://campuswire.com/c/G984118D3/feed/539 Already have environment set up and work in mp1 but failed in mp2 Have import error  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a4365d63-bd43-41fc-a312-b085c2169955/image.png) But when install the terminal says it is already satisfied. (Mp1 code could run on the current environment.)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/eac656cc-7180-4b86-a3a0-49c8ccb5b677/image.png) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/16d7ea7e-e9f9-41aa-8bd9-852a7c6db415/image.png)what interpreter path is listed in VS Code on bottom right?  Are you refering to this?![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/13cad854-a4d6-40b5-a7a9-496f2a7c5a5f/image.png) it is different from the one shows in the terminal too The issue then may be how you are executing the script.  If you are pressing the run button in vscode, it will use the interpreter shown in the bottom right. If that's the case, then click the interpreter in the bottom right and select the appropriate interpreter path.  If you activated the environment using a command in the terminal, then you can run the file directly in the terminal  ``` python search_eval.py config.toml ```
https://campuswire.com/c/G984118D3/feed/211 Metapy installation issue solved using conda I know many classmates have already said the metapy installation worked with conda, I just wated to list the steps here, if that helps anyone still stuck in that step.  ``` //know where python is installed **where python** C:\Users\SAHARX3\Anaconda3\python.exe C:\Users\SAHARX3\AppData\Local\Microsoft\WindowsApps\python.exe  //create a new environment under conda  **conda create --name cs410_env1** environment location: C:\Users\SAHARX3\Anaconda3\envs\cs410_env1  //activate the new environment **conda activate cs410_env1** (cs410_env1) C:\Users\SAHARX3> //conda deactivate will deactivate the environment and bring you out of it  //install python3.5 **conda install python=3.5**  //install metapy and pytoml **pip install **metapy** **pip install pytoml**  //testing **python** **import metapy** //if metapy is installed successfully, this should not give any error  **python example.py** ```Thank you very much for sharing the useful tips!
https://campuswire.com/c/G984118D3/feed/342 CS 410 DL Not Functional After downloading the extension, when I try to signup clicking the buttons in the UI provides no results. I can't sign up nor utilize the tool. Anything I can do to fix this issue?I fixed it, I don't understand what happened during installation but I reinstalled in it works fine now! The silly mistake that I keep making is forgetting to start the VPN! Once I do, the extension and web site seem to be working well.
https://campuswire.com/c/G984118D3/feed/611 MP2.3 Grading Hi,  Just wondering where I can check my score? I still see 0% on Coursera. Though I saw some posts about the leaderboard, but mine shows like below: ![Screen%20Shot%202022-09-24%20at%201.50.52%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e9dc361e-69a9-468c-9b22-f92b915ef3a0/Screen%20Shot%202022-09-24%20at%201.50.52%20AM.png) Anyone maybe know why?   And my submission shows like this:  ![Screen%20Shot%202022-09-24%20at%201.51.29%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8a3dc1f4-9b5a-4327-abfc-bff2d3e5e033/Screen%20Shot%202022-09-24%20at%201.51.29%20AM.png)  Thanks! The leaderboard with your score is on the left of the Submission History page. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/50c5aeb9-3be9-4b22-8f7d-176aede44796/image.png) Same here. I cannot see the updates on Coursera. Same issue here Try navigating from coursera to live data lab and doing a dummy commit (add a space or something small).
https://campuswire.com/c/G984118D3/feed/237 Lesson 3.2: precision vs recall Like discussed in the lecture, having only precision or recall be perfect (1.0) isn't very useful.  If we were to choose only one metric, would precision or recall be more important in application? Is one metric considered more applicable for most tasks, or is it pretty dependent on the task at hand? Or do we not worry about that since the F-score rewards similar values while penalizing extreme values?  Thanks!In most situations, we just use F-score to compare and rank methods. To analyze and improve performance of these methods in details, we need to analyze Precision, Recall, F-score all. I think three metrics are equally important. Thanks for your insight! Michael, it seems that the beta coefficient allows either precision or recall to be weighted more heavily when calculating F-measure, depending on the application.  In these three charts, the vertical (Z) axis is F-measure; recall and precision are the X and Y axes on the base. The first chart is F1. When beta is less than 1, precision gets more weight. When beta is greater than 1, recall gets more weight. I made these to help myself understand the formula. I hope you find them helpful as well.  ![F-meas_beta-1.0.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/0bd4f46a-4d1a-43e9-b72c-050996fca4b2/F-meas_beta-1.0.png) ![F-meas_beta-0.3.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bde8275a-a8ef-46ed-a949-26686ec1274e/F-meas_beta-0.3.png) ![F-meas_beta-3.0.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d1c05cfd-04b1-4542-99a6-572b4067158b/F-meas_beta-3.0.png)  Thank you for sharing the excellent analysis and beautiful plots! It shows nicely the tradeoff that we can make by adjusting the parameter in the F measure and helps interpret this tradeoff in applications. (Note that the F measure is not so interpretable as Precision or Recall alone. )  Thank you for posting this good question. As the responses from the students already said, the "right" measure to be used is generally task-dependent. For example, if a user cares a lot about Recall (e.g., doing a literature survey), then when it comes to the tradeoff between Precision and Recall, having higher recall (at the cost of lowering the precision) is desired, whereas if you just want to check out the major events that happened today in news article search, Recall would not be important. One thing often missing in research publications is that they would use just one parameter setting (F1 is very popular), but depending on the specific application, F1 may not be the best. Thus varying the parameter beta may be needed; at least, showing F measure values with multiple settings of beta would be more informative. That said, however, when we compare two retrieval algorithms/systems, we are mainly interested in their relative performance. For such a purpose, using just F1 is also acceptable since any inaccuracy of F1 in capturing real application utility would unlikely favor a particular algorithm or system involved in the comparison.  Thanks for the clarifications! This is helpful for understanding the topic as a whole!  This is an amazing break down! Thanks for your response! So this is how I understand it. Since the problem is essentially empirical and would definitely need user interaction on relevancy of the retrieved data it is totally on the system to give the weight to precision or recall.  This is explained in the practice quiz 6 of week 3 where you do not want to miss a tweet so you make sure that the recall for the function is high and you do not miss any relevant tweet about threat. On the opposite we would want the retrieval function to be more precise on occasions where we are retrieving documents on a news about election. The retrieval function needs to be more precise in terms of selecting the actual facts vs. any random article that has query terms like election etc.
https://campuswire.com/c/G984118D3/feed/962 questions about project Hi Professor and TAs,  I have two quick questions about the project: 1) Where should the team leader be designated? In the [Google Sheets](https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit?usp=sharing)? 2) Where should the proposal be submitted next week? Through CMT or Coursera? - There is a portal available at Coursera (Week 9), but the [Google Doc](https://docs.google.com/document/d/1XFvippp_jyiUUXpWsps4Geh5KsyZRkJq/edit) shows that we need to submit via CMT  Thanks!1. You can designate the team leader in your proposal submission pdf. 2. The proposal should be submitted through CMT. You do not need to submit anything on Coursera.  Thanks, Kevin!
https://campuswire.com/c/G984118D3/feed/247 MP1 cant install metapy I followed the steps on post #211 but I still get this error message in my conda env when I try to install metapy. Has anyone seen this before or have suggestions?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/89ccba94-c21a-4fda-a21e-9d9b217c7f67/image.png)Yes, I am getting a similar error and I believe it has to do with the python version which should be Python 3.5 or 2.7. Specific Python versions (e.g. Python 3.7 on mac) could  cause issues with metapy. I had similar errors. Here is what I did to fix this.  1) Install latest version of Anaconda ( Note that It will come with Python 3.9) 2) Create Anaconda Env with Python 3.5 and activate it.  3) Install cmake while you are in python 3.5 env. 4) Install metapy and pytoml  Yes. I removed my python 3.10 version and installed with python 2.7. Then I can install metapy successfully. https://www.machinelearningplus.com/deployment/conda-create-environment-and-everything-you-need-to-know-to-manage-conda-virtual-environment/  This link may be helpful to you. I used some of commands from it.
https://campuswire.com/c/G984118D3/feed/404 Practice Quiz 3 question 3 - average precision In lecture 3.3 (approx. the 11:50 mark), the formula for Average Precision includes the denominator 10. I'd originally assumed this was the number of potential steps AKA the number of total documents at which we could evaluate precision (which is 10).  Based on the practice quiz (question 3), however, it seems like the denominator should be the total number of relevant documents in the collection (which, in the lecture video, is also 10).  Could someone confirm whether the denominator for Average Precision is (a) the number of relevant documents in the collection, or (b) the number of steps at which precision is evaluated? Thank you!It is the total number of relevant documents in the collection. The formula is also given on page 177 of the textbook (section 9.3). average precision = $$\frac{1}{|Rel|}\sum_{i=1}^{n}p(i)$$. Putting the explanation given in the textbook here: "p(i) denotes the precision at rank i of the documents in L, and Rel is the set of all relevant documents in the collection. If Di is not relevant, we would ignore the contribution from this rank by setting p(i) = 0. If Di is relevant, to obtain p(i) we divide the number of relevant documents we’ve seen so far by the current position in the list (which is i)" Makes sense - thank you!
https://campuswire.com/c/G984118D3/feed/905 Conditional Entropy, Mutual Information, and Syntagmatic Relations Does a low conditional entropy and high mutual information between two words indicate that they are syntagmatically related?  What if, when one word is present, the other tends to be absent? Wouldn't there be a low conditional entropy/high mutual information in that case? However, those words should not be syntagmatically related, as they do not commonly appear together, right? If one word is present while the other is absent, they will have no mutual information,  or maybe we can say they are "negatively" syntagmatically related. This is a good question! Indeed, in such a case, you may also have a low conditional entropy and high mutual information, but the two words are unlikely syntagmatically related since they don't tend to co-occur. In reality, such a case is probably rare, but it would be highly interesting to see whether there are such words in natural language text. Maybe you could explore it a bit if you have time? Or perhaps propose a course project on exploring this?  Because of this concern, it's very common to use Pointwise Mutual Information where we only consider the case when their correlation is via co-occurrence, i.e., both are present. https://en.wikipedia.org/wiki/Pointwise_mutual_information
https://campuswire.com/c/G984118D3/feed/143 Questions on deep understandings of NL The professor said that tasks like translation needs a deep understanding of natural language and I do agree because in real life translators need to be good at both the source and target language.  Then, I suddenly came up with a thought that what is the deep understanding of natural language? How can we define the task of deep understanding in terms of what computer can process?  Can a level deep understanding of a sentence be represented by some kind of a numerical value? How do we evaluate if a system have a deep understanding of a natural language?From my understanding, we usually evaluate our language model with a test dataset like other non-language ML models. One of the popular benchmarks is [GLUE (The General Language Understanding Evaluation)](https://gluebenchmark.com/).  >Can a level deep understanding of a sentence be represented by some kind of a numerical value?  E.g., you can see **numerical** scores for language models on [the GLUE leaderboard](https://gluebenchmark.com/leaderboard). For a task such as retrieval of documents based on keywords of a query, we could get the relevant documents by just looking for documents in which those keywords are present.  Such tasks can be completed using simple statistical approaches. However, if you want to translate one language to another, using simple statistical approaches might not work. The machine needs to parse the text in the original text, understand its structure and determine how to translate it to the target language (the syntactic structures and the grammar rules of the two languages can be very different). This problem cannot be solved well by just looking at the words or word counts in the text, but rather, a better understanding of natural language is required (if you remember, natural language can be very ambiguous to machines because we skip a lot of context information assuming that a human reading it would be able to understand).  This is explained in detail on page 44 (Chapter 3) of the textbook if you want to read further. I would suggest looking at the section and also taking a look at the example of Eliza (a dialogue system) where they try to bypass the issues of understanding natural language by using simple rules. Thank you for the reference. I will read that.  Thank you for letting me know the name of the benchmark! Now maybe I should dig into how they actually composed the tests.
https://campuswire.com/c/G984118D3/feed/1157 No logs available yet. Your build may still be starting up. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c12cd790-8dc5-4efb-b1e6-f9b509b19025/image.png) Have set up a new token Have set up a new webhook ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/9ce929d5-a756-4110-b44d-edf77bde091e/image.png) Have repushed it Still having this issue I had this same issue yesterday. I Think your GitHub token may expired. Try remove the link from lab and generate new token from GitHub and relink with the lab Like I said in the post I have already create a new token and new webhook. What do you mean by relink with the lab? Beside a new webhook, did you do anything else? you need to remove the linked account from livedatalab. And relink it with github using your new token. there is instruction in MP1. It has nothing to do with the webhook.  That works, thanks. 
https://campuswire.com/c/G984118D3/feed/153 Just use Google Colab for MPs! Hey everyone, I just wanted everyone to know that you can use Google Colab for working on the Machine Problems. I was able to use it without any issues for MP1. There are no installation issues with metapy and pytoml on Google Colab. And since UIUC gives us an G Suite account, you can use that for making the Colab notebooks, and it won't count against your personal Google Account's storage!  Google Colab will also run much faster than your laptop, especially assuming future MPs might be more computationally demanding. All for free!   As far as pushing it to Github is concerned, I just made sure it is working on Colab correctly, and then copied it onto my local computer and pushed the code from there. This is because Colab doesn't have a direct integration to push to Github.  I would appreciate it if you like this post if you found it helpful.
https://campuswire.com/c/G984118D3/feed/1144 Values for final output coming around [-177000,-176000......] Hello, my autograder on live data is giving me an incorrect implementation error because my values are coming around -177000 for the final array that it returns. I'm not sure what I'm doing wrong here because I think I implemented all the functions correctly. Can someone help with this? Thank you!It would be helpful if you can offer the screenshot of your code. But one potential problem could be your calcuation. Check the part where you calculate the score. Check the probability matrix and see if they are sum to 1 I am facing the same problem, were you able to fix this? I can't figure it out still not sure whats wrong... :( Hi! have you figured out what you're missing? I have the same issue too. Any update? Having same issue...
https://campuswire.com/c/G984118D3/feed/407 Is the digital library down? Hi,  Im not sure if this is a problem effecting others as well, but I can't seem to connect to the digital library. I have previously and currently have half of my links submitted successfully, but upon checking today, I am stuck on a redirecting page.   Is the library down? (May be relevant: I am using the Illinois VPN and am connected to the internet without issue. I have not altered or changed anything (chrome extension, computer updates etc...) from my last visit to the digital library.)  Thanks!  ![Screenshot%20from%202022-09-09%2015-54-36.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f215aff4-3ed7-4908-b297-73e3b9484082/Screenshot%20from%202022-09-09%2015-54-36.png)I'm having the same issue! I have the same issue right now! Same issue for me. I can submit to the DL but I cannot connect to `http://timan.cs.illinois.edu:4000/auth` I too am having the same issue Just curious, how can we be sure submissions are being retained if we can't access http://timan.cs.illinois.edu:4000/auth? We cannot. I hope my submissions today were recorded. The Chrome extension said they were so fingers crossed. Hi everyone, looks like the server was frozen for some reason. I've restarted it now and everything works on my end. Please let me know if it doesn't work for you! It appears to be back up. It looks like it is back up! Thank you! If you click on the link 'My Submissions' link at top right, it will show you the list of all your submissions that were recorded. Also when you are submitting your items, the extension/tools always confirms with a message 'your submission has been recorded and indexed'.
https://campuswire.com/c/G984118D3/feed/594 MP 2.3 LiveDataLab doesn't have Scipy Hello, my code crashes when i try to submit it on livedatalab since it doesn't have scipy library installed. I use this library when computing the Pvalue that I write in the significance.txtYou can just calculate that locally and push significance.txt to the repo If you are using python, you can use this for pvalue calculation https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html You only need the value, you don't need to use the scipy on livedatalab. I had the same problem because I also edited the code to have the libraries. I just commented out the code I added and turned in the values.
https://campuswire.com/c/G984118D3/feed/199 LiveDataLab Issue? Hi,   I am not able to integrate with the LiveDataLab seems like this is not reachable. Is this known issue?  http://livedatalab.centralus.cloudapp.azure.com/api/webhook/triggerWhat error are you getting? http://livedatalab.centralus.cloudapp.azure.com/ seems to be reachable for me  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c9161b48-ad35-4ea4-beeb-f1e14f2fb412/image.png) When I am submitting it is no longer showing me submission history Try again now, should be up and running now Yes. It is up. Now I am seeing a different error.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/428a8eda-9bd1-4061-95cb-49728a437a0f/image.png) are you sure you submitted the right PAT credentials when linking the github account? Make sure they are not expired. The grader is not able to run your code because it can't access it. I would suggest unlinking the account and redoing it Thanks. It worked after relinking
https://campuswire.com/c/G984118D3/feed/863 Document Length Normalization for JM smoothing Please let me confirm. JM smoothing does not incorporate document length normalization, while Dirichlet Prior Smoothing does. Is it correct?Yes. That is the same very reason that p(w|d) remains same even if the term count doubles in the document Week 4 Quiz 5. Thanks, Hitesh!
https://campuswire.com/c/G984118D3/feed/484 MP2.2 I searched for "videos on Gamma Code" on the DL site. I see that even text related to gamma code like papers and posts on Campuswire apart from Youtube popped up. Would these posts and other links be considered relevant or not relevant?As per the instructions, we just have to review the search results (irrespective if they are text, paper, video, campuswire post, blog, etc) and access their relevance and select at least 10 relevant documents (or up to the number of returned results).   It would be similar to what we do when we try to search for something on a search engine. We review the documents returned and determine the ones that can help with our query. For example: with query 'How to compute gamma code', I might be looking for any type of documents versus 'videos on Gamma Code' I might be just interested in only the video type of documents or might have video as a preference but still be okay if I find a blog that is relevant to the computation of the gamma code.   In my opinion it depends on what as a user you find relevant for your query.  
https://campuswire.com/c/G984118D3/feed/1077 PLSA equation for the equation, why would the background treated differently from the each topic? also, why do we have to keep the term of (1-λb) in the equation for each topic? wouldn't the math be more straightforward with prob of each topic θ * P(w|θ) and treat background as a category of topics as well?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cb155ae4-4b14-4608-9004-64b42cf869d4/image.png)> why do we have to keep the term of $$1 -\lambda_b$$ in the equation for each topic?  Because the background topic is treated separately, and since every word might be generated from either the background topic or one of our `k` topics, we need to first detect the probability that the background model didn't generate the word, and then multiply that by the probability that the specific topic generated the word. This is a result of the split between Background and other topics -- because of the "tiered" nature of this split. First, we determine if the word should be from background, then if not determine which topic it should be generated from.  >why would the background treated differently from the each topic?  As far as why we treat the background model differently, i believe this is likely so we can have a single parameter to set to determine the amount of words generated from the background topic. While it would be straightforward to treat the background topic as just another topic (and I also have been wondering the same thing as you -- it should "work" at least to a degree i think as you describe), one benefit of explicitly pulling it out is that now $$\lambda_b$$ can be a tuneable parameter, whereas the $$\vec\pi$$s are estimated via some process like Expectation Maximization. That way we can control to what degree common words are removed, etc.  It's possible there are other (or better) reasons to separate out the background like this. But I am not sure, and I may want to also try to think about it a bit more. For now I will mark your question as unresolved; I also find this very interesting! thanks for helping - but I'm actually questioning why a "tiered" approach was taken... but working on MP3, seems the idea to treat background  differently was involved later, which implies such approach may bring some advantages in computation... perhaps because the background distribution is "known" - just a guess i think that is related to my second paragraph and beyond. i believe that the reason for this is so we can have a tuneable parameter, $$\lambda_b$$ for us to control how aggressive we are in filtering out common words, instead of relying on an approach like EM to find it like with the $$\vec\pi$$ values.  in PLSA, the user sets $$\lambda_b$$ but does not set $$\vec\pi$$, which i think allows us some control. there may be other reasons here as well as you've mentioned (ie efficiency).  i've edited my original answer to make the part where i answer this in more detail more clearly delineated The primary reason I find is that we need be able to control how much the background model contributes to the output. But, if we treat it as just another topic, then that background model will have to be a special topic with its probability controllable. That way, the background model is still considered to be one among the topics, but a special one, thus the parameter.  That's something I think, but may be there's some analytical reason.
https://campuswire.com/c/G984118D3/feed/1329 pre-quiz 11 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e41aaab3-a9d3-4f7c-98e8-14a8bca94813/image.png) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d75e9d88-9fa3-4278-a8f8-96c8acd2885f/image.png) I need help with this question.  I believe it should be calculated following this equation.  confused about c(w,d)here. it should be be c(w,d1) or c(w,d3)?  I calculate the p(w|theta1) and c(w,d1) based on d1 and couldn't match with any options given in Q8. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/b7131be5-8ff6-43a0-adb6-25c66019128e/image.png)  confused!
https://campuswire.com/c/G984118D3/feed/898 Q6 in Practice Quiz 4 Hi,  Can someone help with the below query ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c58be7c6-7a3a-4508-bceb-ece41759a9d2/image.png) My understanding is that the likehood will become 0 (instead of minus infinity) if a term is in the query and not in the document [without using smoothing]. Any idea why is it minus infinity?I suppose log 0 is minus infinity or undefined Ah, yes. Thanks for responding.
https://campuswire.com/c/G984118D3/feed/80 Quiz 1 Question on "better way to measure similarity" For this problem, we are asked among two methods which is a better way to measure similarity within a VSM model. I believe this topic is not covered in the week 1 module but the week 2 (unconfirmed since I haven't watched it yet). This question should appear in Quiz 2 instead right? If this is the case, should the instructors post a note on watching the relavant sections of week 2 before attempting quiz 1?I believe I know the question you're referring to. I'm not sure how much we're allowed to discuss material from the graded quiz, but to my knowledge, I don't believe the information for that question was stated in the week 1 lectures (I just reviewed my week 1 notes and haven't started week 2 yet) - but it is briefly discussed near the very end of week 1's textbook readings (ch. 1-6). In Q9 of the practice quiz, the correct answer is that the L2 distance is affected by document length. Someone also posted this link to me as an explanation: https://stackoverflow.com/questions/19410270/vector-space-model-cosine-similarity-vs-euclidean-distance  This problem should be similar to the one on the practice quiz. even the question was shown in the practice question, there is no a clear answer whether the method affected by document length is superior than those not...  think about it twice - i take my word back LOL
https://campuswire.com/c/G984118D3/feed/961 deadline and tasks of Tech Review and Project Hi all,  Here I summarize the deadline (US Time) and tasks for Tech Review and Project. Please feel free to correct me if I miss anything:  ### Tech Review Info available at Week 1 (Orientation Information) and Week 8 - End of Oct 23, Sunday: select a topic  (possible topics are available at [HERE](https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit?usp=sharing)) and sign up [HERE](https://docs.google.com/spreadsheets/d/1hWAyxd82FcitN9eG3yMW6ckq6l1VASBtYWpaPbywMPc/edit?usp=sharing)  - End of Nov 6, Sunday: submit the Tech Review via Coursera (Week 11: Technology Review Submission)  ### Project Info available at Week 1 (Orientation Info - Course Project Overview) Detail available at [HERE](https://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit?usp=sharing) - End of Oct 16, Sunday: **register for a  Microsoft CMT account** [HERE](https://cmt3.research.microsoft.com), and **sign up the form about the team** [HERE](https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit?usp=sharing) - End of Oct 23, Sunday: submit a proposal via CMT (instruction [HERE](https://docs.google.com/document/d/1XFvippp_jyiUUXpWsps4Geh5KsyZRkJq/edit)) - End of Oct 30, Sunday: peer-review project proposals, everyone will review 1~2 proposals - End of Nov 13, Sunday: submit a progress report - End of Nov 20, Sunday: peer-review project progress reports, everyone will review 2~3 reports - End of Dec 8, **Thursday**: **software code submission with documentation**, and **software usage tutorial presentation**   - End of Dec 16, **Friday**: peer-review project code, documentation, and presentations, everyone will review 2~3 groups  Thanks! Looks good, from what I can tell. Note that the reports per reviewer might be 2~3, rather than 1~2. But the groups that each reviewer will review will be held mostly consistent throughout each review point.  okay, updated!
https://campuswire.com/c/G984118D3/feed/1326 Text Classification Evaluation For Single Document and For Classifier What do you calculate precision and recall for? Each document? Each category? Each classifier? Could you provide some examples? Thank you.  Also, on slides 5-6 of 11.3, what do the columns with "c"s on top mean? How do some documents have a "y" (system says yes) in multiple columns?i'll start with clarification of slides 5-6 of 11.3: the columns with the "c"s on top are "categories". the rows, "d" are documents. let's say you have 3 categories, `{"sports", "science", "art"}`. a single document may be about science and not art, or maybe it is about both science and sports. this is how we can get multiple columns, and each document has an evaluation for that category. there is a ground truth (the `+` and `-`) for that document for that category, and a system evaluation (`y` and `n`)  generally, there are a few different ways to calculate precision and recall, and you've mentioned a couple of them. these are the types of aggregations covered in Week 11:  - **Per-Document Evaluation**: How good are the decisions on document $$d_i$$ -- this can be done on each document, and is done per document. in the slides you reference this would be aggregating precision/recall over a single row (and getting a different measure for each row)  - **Per-Category Evaluation**: How good are the decisions on $$c_i$$, the individual category. here we go over columns instead of rows in *per-document evaluation* (getting a different value for each column)  - **Macro Average over all the Documents**: Once you have your *per-document evaluation*, you can then aggregate (ie via a simple arithmetic mean) over all of the documents to get overall precision, overall recall, overall f-score  - **Macro Average over all the Categories**: Once you have your *per-category evaluation*, you can then aggregate over all the categories to get overall precision, overall recall, overall f-score  - **Micro Averaging of Precision and Recall**: this is similar to classification accuracy, as this simply looks over every entry and calculated precision and recall for every document and every category at once  generally speaking, you would also want to compare different classifiers using these measures to see which ones have better per-document, per-category, or macro/micro averages of these metrics.  please let me know if i answered your question or if i can clarify better
https://campuswire.com/c/G984118D3/feed/1138 livedatalab failure without log solved: my github token expired... I had to delete the linked account from the lab and relink with new generated token. ___ I tried to submit my code but livedatalab keeps failure and no log. does anyone know why?![Screen%20Shot%202022-10-22%20at%201.44.58%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/ef113078-1cd7-4ab2-a26f-af25a3d561ba/Screen%20Shot%202022-10-22%20at%201.44.58%20PM.png) ![Screen%20Shot%202022-10-22%20at%201.45.11%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e3389c4a-2e8d-4144-8696-fb3ba8ff2467/Screen%20Shot%202022-10-22%20at%201.45.11%20PM.png) What do you see in LiveDataLab on the "submission history" screen? I have encountered similar issues like that before. Two steps I've tried and worked:  1. Try logging into the tool again via Coursera (Open Tool link) or 2. Re-do all of the GitHub cloning steps once again to reset the webhook just highlighted in orange like my previous other failures.  Are git add, commit, and push are working for you? If yes, then your repo will show something like "plsa.py updated a few  seconds ago" every time you push.  If git add, commit, and push __are__ working, then you should not need to repeat the git clone process. However, you should try to recreate the link between GitHub and LiveDataLab. You could start by redoing the webhook, as Gabe suggests. Please remember that the link in the instructions beginning "livelab" is NOT correct. Another possible cause is your GitHub personal access token has timed out. (The default token life is only 30 days.) I had such an experience. yes thank you so much I just found my token expired and relinked and it works!
https://campuswire.com/c/G984118D3/feed/425 LiveDataLab: ImportError: cannot import name suppress Hi,  I'm trying to submit my MP1 but I kept getting this error message. My code was successfully committed and pushed to my Github. Does anybody know how to solve the issue? Thank you.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/93cd06cf-1a11-4d95-abea-90b6083cc067/image.png)  Btw I've tested my code in vscode and the result seems to be good. Don't know why LiveDataLab gives me a hard time. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/17adae9f-f1c0-4b35-96a9-5c96e82347b4/image.png)Can you upgrade to python on 3.5. That could help. I am seeing deprecation warning all here. I installed Anaconda and used it for metapy. I'm actually running it on 3.5 thru conda...  The grading part is done in the cloud so i guess it has nothing to do with my local python version? it looks like your code is trying to import something called `suppress` from `contextlib`; not sure if the true stacktrace here is being obfuscated a bit, but i don't have that import (i only import `metapy`). is it possible to remove this import, or is that being imported by `metapy` as well? Thank you Ryan! You found the issue.  I did not even realize that on top of my code there are two extra lines - ``` from contextlib import suppress from curses import meta ```  I swear I never wrote these two lines and i have have no idea where they came from. Guess vscode added them for me when i did not notice? lol. But after deleted these two lines, the submission finally succeed... but with a 50% grade. Smh. Do you happen to know where i can find a more detailed feedback from the grader? Thanks. oh great! yea i know sometimes IDEs like to "help" you by including imports they think are relevant so that could be it  as far as additional feedback, i'm not sure how detailed it is for this assignment. my advice here would just be to double check and make sure you are doing each and every step listed in the readme, and that you are doing it in the right order, namely: 1. tokenize with ICUTokenizer (with argument `suppress_tags=True` 2. lowercases 3. removes words with less than 2 and more than 5 characters 4. performs stemming 5. produces trigrams for an input sentence  let me know if that doesn't help, i'll see if i can find anything additional about the livedatalab autograder output oh also check #268 i just found this campuswire answer that may be relevant since this post also mentioned a score of 0.5   Thank you so much Ryan! Yes i missed the lowercase. much appreciated. i wish the grader could tell us which test the code did not pass lol. no problem SiCheng! very happy to help
https://campuswire.com/c/G984118D3/feed/447 MP2.3 grader is missing Hi TAs and Professor,  Currently, when I submitted my file for grading, I was met with this error. ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e855ab88-9331-4cac-a567-b2c3e5ed1bc2/image.png)  Can someone check this? Thank you  DanhSorry about that, can you try now?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/671d25b8-529e-4bf1-8f84-be03b87cbca5/image.png) Still failed but different message now. It said InL2 is not implemented correctly but a line above it said that there is no log2 in 'module'. I was able to run everything locally so maybe upstream build is missing some dependencies like 'math'. Please check now, you should be able to run it now.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a602feff-6e4d-4193-8490-18ad1319e6b2/image.png) Still the same error, with different log. Looks like the grader is not happy with your InL2 function? I was able to get my submission to pass just now. Got it! log2 is not implemented on whatever version python the azure machine is using. So just need to change math.log2 -> to math.log(w/e, 2). Submission just got graded! Thanks everyone   Thanks Danh for sharing the change math.log2 -> to math.log(w/e, 2) . It helped in my implementation. 
https://campuswire.com/c/G984118D3/feed/1180 Proposal Submission in Coursera When will Coursera reflect the submission via CMT? Also, only one group member is to submit at a time, correct?Yes, that seems to be the case based on this document -  https://docs.google.com/document/d/1XFvippp_jyiUUXpWsps4Geh5KsyZRkJq/edit . Team leader needs to submit the document. Once we click on Submit button, Coursera shows as the assignment is submitted. For the grades, looks like it will be updated by TAs after the peer and Staff review
https://campuswire.com/c/G984118D3/feed/434 submitting different articles from same link  can we submit different articles from the same link ?I'm having some difficulty understanding your question - do you mind sending the link here which has the articles that you'd like to add? To my understanding of your question: You can try to find out the source of each article. In this case, each source is a different link.  Or you can just highlight and submit each of these different articles (one by one) on the same page(link). In this case, all submissions will be under the same link. It seems one link (url) is considered one resource, and you need at least 15 of them. i see  thanks I think its better to use different links either mentioning different topics or same  topics.  The DL will record the qty of your submission.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/bc6f4ba0-6d67-4eb1-a599-daccbb70b4aa/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/23c626f0-f7d1-45f9-993a-b8f8f7bcd67e/image.png) awesome  thanks  If your question is if we can use the same browser window to find different documents/ articles and then submit them using the extension, then the answer is yes.   Remember as long as you are on a webpage that is different than the one you had submitted previously then it should be okay.  In my understanding, there could be one exception to when you would want to submit the same URL more than once. It would make sense if that document talks about two or more important topics for example - Stemming and Lemmatization, then for better tagging you can use the same URL with two query points assuming that the document is relevant enough for the users to be able to retrieve it for both the searches.
https://campuswire.com/c/G984118D3/feed/951 Submitting Team Roster Does anyone know how to submit a team roster?  It is due by Sunday at midnight, correct?  Thanks!!For the October 17th deadline: please fill out [this sheet](https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit?usp=sharing) according to [the instructions](https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit?usp=sharing). You do not need to do anything on Coursera for this deadline.      Thank you! Both those links point to the same thing. Did you mean to post instructions?
https://campuswire.com/c/G984118D3/feed/637 how to find c(t, D)? In the section 'Writing lnL2', im confused how we find the values for c(t, D) and c(t, Q). I know all thre other values come from sd but it doesnt look like sd has those 2 values.  Am I missing something?You can review the list of available attributes in the `sd` object. There is a description of each parameter, which includes `c(t, D)` and  `c(t, Q)` https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html so would corpus_term_count be considered c(t, Q)? I think corpus_term_count would be something like c(t, C).  The total number of times the term occurs in the corpus, no? that makes sense, but then which one would be c(t, Q)? Something like query_term_weight/query_length? I found it where where Jose did.   Check out the link Jose posted: https://meta-toolkit.org/doxygen/structmeta_1_1index_1_1score__data.html  c(t,D) -> count of term in the document c(t,Q) -> count of term in the query  there are attributes with descriptions very close to that.
https://campuswire.com/c/G984118D3/feed/982 Is Coursera down? My Coursera seems bugged, all courses are unenrolled and nothing shows up. iPad version bugged too. Cannot view lecture materials nor do quizzes.same here. exam approaching soon...  I wonder if this is related to the MCS program application deadline (10/15) as for the next round of application the deadline will need to be updated... ^ ^ ^ Also experiencing issues on Coursera Experiencing same here same Me too... Exam is coming... Not a good time :( Same here. https://downdetector.com/status/coursera/ You can still get to the course if  you open a direct link...  https://www.coursera.org/learn/cs-410/home/week/1 coursera is back up.
https://campuswire.com/c/G984118D3/feed/24 Tips for MP1 setup Hi, It is my first time doing bare clone and mirror push with Github. I worked very hard to understand the instructions and finally did it correctly! Here are some tips that may help you: 1. all the commands are in your local terminal; 2. after you did mirror push you may need to enter your github username and password. Please use your access token (used to link your github to LiveDataLab) as your password (not your account password). see:https://stackoverflow.com/questions/17659206/git-push-results-in-authentication-failed 3. Make sure when you link you github account on livedatalab the right information (**github username not your email address**) is used! The URL for the webhook is fixed now. The correct link should be shared by the instructor or TAs. Please see below my config: ![Screen%20Shot%202022-08-22%20at%2011.05.01%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c6919a81-e4f0-4e74-b752-c0a9d65f779e/Screen%20Shot%202022-08-22%20at%2011.05.01%20PM.png) ![Screen%20Shot%202022-08-22%20at%2011.04.38%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a77a5d8d-94a7-4476-a6fb-d5cbae229b6b/Screen%20Shot%202022-08-22%20at%2011.04.38%20PM.png) 4. if you use VS code, it is even easier to commit and push the changes onto github. Make sure you enabled git in your VS code settings.  If you find these tips are helpful, please give a "Like".
https://campuswire.com/c/G984118D3/feed/308 Participation Stats for Extra Credit Professor alluded to referencing participation stats for calculating extra credit points. If you’d like to see how Campuswire scores you in that regard, access your “Reputation”, as follows:  1. In the side panel, choose “(A+) Grades” ![image2022-09-04%2011%3A25%3A26.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e10e7fb5-4e66-461a-a918-abf087270182/image2022-09-04%2011%3A25%3A26.jpg)  2. In the top tab, select “Reputation” ![image2022-09-04%2011%3A25%3A35.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/596a4b2c-c36f-4725-9240-9f48fa88c42a/image2022-09-04%2011%3A25%3A35.jpg)
https://campuswire.com/c/G984118D3/feed/16 MP1 Webhook - bad link I am attempting to add the webhook to my MP1_private repo, but the supplied: http://livelab.centralus.cloudapp.azure.com/api/webhook/trigger  is not valid/able to connect to host.  Do we have an updated url?  Will this url be working in time?    Thanks in advance!try **livedatalab** instead of livelab.  Just tried that, and am getting the same warning.  Are you getting a warning on your end?  Can you paste the url that is working for you?   check this post: https://campuswire.com/c/G984118D3/feed/4 this is not the webhook Please try this one: http://livedatalab.centralus.cloudapp.azure.com/course/join/WZS77P00E570J74 This is what I am seeing with the above and below url's:   ![1.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/14c9ed12-1b59-4c92-b279-caad2640d68c/1.png) is this one working: http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger  haven't tried it yet. Will keep you posted. same For anyone else reading this **** this is the correct url **** http://livedatalab.centralus.cloudapp.azure.com/api/webhook/trigger  The instructor endorsed answer did not work for me.  The best guidance I got was campuswire #24  Thank you so much, it works :)  thanks William, this works
https://campuswire.com/c/G984118D3/feed/67 Metapy Module Not Found Hello, I have looked all over the campuswire and tried many solutions (Windows Machine) yet I have not been able to solve the error I have been experiencing.  ``` $ py search_eval.py Traceback (most recent call last):   File "C:\DEV\MP2.2_private\search_eval.py", line 5, in <module>     import metapy ModuleNotFoundError: No module named 'metapy' ```  I am in a conda environment (Python 3.5/2.7) after seeing other students' suggest it and metapy is installed in it.  ``` $ pip install metapy Requirement already satisfied: metapy in c:\users\kings\.conda\envs\py35_env\lib\site-packages (0.2.13) You are using pip version 10.0.1, however version 20.3.4 is available. You should consider upgrading via the 'python -m pip install --upgrade pip' command. ```  I don't know what to do, I have been at this all morning and have yet to get it working. Any idea as to what I could possibly do?I think you should run `pip install --upgrade pip` before running `pip install metapy pytoml`, as suggested in the setup tutorial from https://github.com/CS410Assignments/MP1 Are you running search_eval.py with the same version number of Python in which you installed metapy? Unfortunately, this did not fix the issue. I've tried it and even uninstalled them both to retry. @Harita Reddy I am using version 3.5 in Python, but I try running with ```py search_eval.py```, should I try ```python3 search_eval.py```? After doing so in the env, it says python is not found.  ```$ python3 serach_eval.py Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases. (py35_env) ```   Can you share the results of running the following commands within the conda environment?  ```bash where py where python3 where pip which py which python3 ``` ``` kings@DESKTOP-6P3S8M5 MINGW64 /c/DEV/MP2.2_private (main) $ where py C:\Windows\py.exe (py35_env)  kings@DESKTOP-6P3S8M5 MINGW64 /c/DEV/MP2.2_private (main) $ where python3 C:\Users\kings\AppData\Local\Microsoft\WindowsApps\python3.exe (py35_env)  kings@DESKTOP-6P3S8M5 MINGW64 /c/DEV/MP2.2_private (main) $ where pip C:\Users\kings\.conda\envs\py35_env\Scripts\pip.exe C:\Users\kings\AppData\Local\Programs\Python\Python37-32\Scripts\pip.exe C:\ProgramData\Anaconda3\Scripts\pip.exe (py35_env)  kings@DESKTOP-6P3S8M5 MINGW64 /c/DEV/MP2.2_private (main) $ which py /c/Windows/py (py35_env)  kings@DESKTOP-6P3S8M5 MINGW64 /c/DEV/MP2.2_private (main) $ which python3 /c/Users/kings/AppData/Local/Microsoft/WindowsApps/python3 (py35_env)  kings@DESKTOP-6P3S8M5 MINGW64 /c/DEV/MP2.2_private (main) $ ```  Here is the output of all the commands. It seems that python3 and py are recognized outside of the env. Can you share the results of `which python` as well. ``` kings@DESKTOP-6P3S8M5 MINGW64 /c/DEV/MP2.2_private (main) $ which python /c/Users/kings/.conda/envs/py35_env/python (py35_env)  ``` Here you go!  And here is the output when running with python (Unsure of what it means currently):  ``` $ python search_eval.py Usage: search_eval.py config.toml (py35_env)  ``` It looks like metapy was resolved, so you are on the right path. Even I was unable to get metapy to install correctly on my computer. To solve this issue, I am simply using Google Colab for MPs, because metapy works natively there. Refer to this post - https://campuswire.com/c/G984118D3/feed/153
https://campuswire.com/c/G984118D3/feed/673 MP2.3 Repository Not Found I have been trying to clone my repository but I am getting the error: remote: Repository not found. fatal: repository 'https://github.com/kduddi2/MP2_private.git/' not found even though when I click the link I am able to see my repository which I populated with everything from the CS410Assignments repo. I have been having this issue for the last MP as well but I am not sure how to fix it. I have tried different git clone commands that I have found.  Would appreciate any ideas anyone has to go about this. Thanks!can you try cloning through the git url, in other words: `git clone git@github.com:kduddi2/MP2.3_private.git` ? also, i think the repo should be named `MP2.3_private` instead of `MP2_private` but that shouldn't make a difference for cloning I think your may make sure you logged in GitHub.  Check your github is correct. There are other ways to upload the code. You have an option on Github gui to import directly from the repo of interest. Or try to download the files to local machine and then you can upload the files directly to your git project. I was able to figure it out. The issue was that I had a different GitHub account and password stored on keychain access on my laptop which impacted my git config. The repo was able to be found after I deleted it from keychain access.Thanks!
https://campuswire.com/c/G984118D3/feed/1031 Things Due Now Other than registering for a Microsoft CMT account and signing up on the Project Sign-Up sheet, do we need to make a post on CMT? Or is that it?If it helps, similar query is answered at #998 & #892 Hmmm it seems like the forking stuff isn't until Oct 23? The next thing would be to submit the proposal before 23rd
https://campuswire.com/c/G984118D3/feed/344 Digital library submissions tracking Is there a way to keep track of how many submissions I've made or if I could view all submissions by user?You can track the number of submissions that you've made by visiting the website and clicking "My Submissions" after logging in. However, there is no way to see the number of submissions made by other users Hi Kevin - I have submitted more than 10 entries, but "My Submissions" only shows 10. Has anyone else had the same issue? The new ones (>10) that I submit did not show up. Yes, it looks like this is happening to me, too. I'll take a look into fixing this today. I've set the limit to 50 for now, so you should be able to see your submissions! Confirm that it's working now! Thanks a lot.  Perhaps it will be useful for someone else too, the website mentioned by Kevin is http://timan.cs.illinois.edu:4000/.
https://campuswire.com/c/G984118D3/feed/941 MP3 Shows as Green in Datalab but no Score Hello, I submitted a complete implementation of mp3. I tested in LiveDataLab and the results are green, but I am unsure if this means I passed the tests for it. Does this mean I passed or that the implementation was able to run for the tests?Green just means the code executed successfully, not that your implementation necessarily passed the scoring tests. Check the "leaderboard" link on LiveDataLab for results. The Leaderboard has been blank since my submission. Does this mean I should repull the entire project again? Haven't finished MP3 myself yet so I'm not sure, but the leaderboard has shown results (0/1) in the past. If you clicked through Coursera at least once before your most recent commit, do you see a grade populated in the Coursera gradebook? No the grade hasn’t been populated in coursera either. I am also experiencing the same thing.. I changed the status to ‘unsolved’ for TA’s attention.  I am seeing your grade on the leaderboard It seems like your tests did pass, not sure why your grade is not showing up on the leaderboard. I've updated your grade on Coursera.  Thank you! @Priyanka This is the leaderboard that I can see and it says 0. Are you saying I didn't pass this assignment yet?  ![Screen%20Shot%202022-10-12%20at%204.47.16%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/6b081c9e-e4fd-4f81-ab62-f2b1c905ae34/Screen%20Shot%202022-10-12%20at%204.47.16%20PM.png) Then, this is very confusing as I am seeing 'success' here. Coursera is still at 0% as well.  ![Screen%20Shot%202022-10-12%20at%204.47.03%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8905e3d9-bf6e-4c33-b952-8cc2c47e22ef/Screen%20Shot%202022-10-12%20at%204.47.03%20PM.png) Hm, is your code running locally? Have you removed the print statements from the expectation and maximization step functions i.e.:  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2973ce2e-d4c9-451f-82d0-66fb92ef2bf8/image.png) Yes I did removed those as it caused issues/delays when I tried to see the logs. My code worked locally..  Should I keep the print statements?  +1 Do we have to remove all the print statements ??
https://campuswire.com/c/G984118D3/feed/33 Orientation & Quiz 1 Due date Hi, I just discovered that the quiz due date for the orientation quiz and quiz 1 are both August 22, 2:59 AM EDT for me. Technically, I wasn't even able to see the class on Coursera until August 22, let along finish both quizzes in 3 hours after 12:00 AM. I just want to check if I already got a zero for both quizzes since the quizzes weigh 25% of the final grade? Thank you very much.The due dates are incorrect, it should the coming Sunday. We are working on fixing this. #38 This is now fixed.
https://campuswire.com/c/G984118D3/feed/531 What is k_3 in the BM25 model Hello, I was working on MP2.3 and had this question: what is k_3 in the BM25 model? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a10324e9-4f23-474a-a4bb-a82043ff7296/image.png)  The equation in the textbook does not have k_3 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/55e15595-06fd-4dcf-aba8-fad3cbc84837/image.png) Thanks,https://www.ccs.neu.edu/home/vip/teach/IRcourse/1_retrieval_models/slides/m03.s03%20-%20TF-IDF%20and%20Okapi%20BM25.pdf  The 7th slide has an BM25 formulation with k1 and k2.  I don't understand it, but seems to be a tf adjustment? I had the same question. From what I could find out, both k1 and k3 are for adjusting the effect of term frequency. k1 pertains to Document term frequency. k3 pertains to Query term frequency. An equation using both k1 and k3 can be found at on the fourth page of this paper: http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/schuthecir14.pdf. Here is that equation:.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dfdb6e13-f3f1-4805-99f0-0a9a13d962a0/image.png) There may be other BM25 equations; I didn't look that far. From what I've found from [this resource](http://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/schuthecir14.pdf), BM25 can be expressed with a $$k_3$$ term as shown below. Here, the $$k_3$$ term is a parameter in an expression for the query term frequency weighting. It can be seen that this expression is independent of the document being scored, so it's the same value for each respective query term for all scored documents.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c7c3681b-86aa-4068-940c-52b7cd8fe46a/image.png)
https://campuswire.com/c/G984118D3/feed/349 Number of Submissions MP2 When I look at "My Submissions" on the CS410 Digital Library page, it seems like it only prints out 10, no matter how many I submit.  Am I doing something wrong?  Thanks!!Looks like TA Kevin is looking into it https://campuswire.com/c/G984118D3/feed/344 Hey Frank,  No, you're not doing anything wrong. The limit was previously set to 10. I've changed it to 50, which should be sufficient for the assignment. 
https://campuswire.com/c/G984118D3/feed/291 Week 2 ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/c64ca861-ec13-4535-8263-baef2ccd49d4/image.png)  Can someone decompose the steps here? How are we getting 101 for 3 in Y-codeAs mentioned in the slide,  Gamma code stores integer in two parts **First part:** 1 + floor(log x) in unary **Second part:** x – 2 power floor(log x) in binary using floor(log x) bits Here log is to **base 2** for both the parts.  If X= 3  **First part:**  1+ floor (log 3) = 1+ floor(1.5849) = 1+1 = 2 , Unary of 2 = 10 **Second part:** 3 - 2 power floor( log 3) = 3 - 2 power 1 = 3 - 2 = 1  So, the gamma representation 	= first part which is 10 & second part which is 1 (to be represented in 1 bit) 								**= 101** Hi,  I had a similar question. I think the answer below helps with the decomposition of what we learnt in the lecture video. It does get a bit tricky when we have a number like 5 or 6 which share the same floor[log] value of 2.  For example: If X = 5,  First Part: Unary of (1 + floor[log 5]) = Unary of (1 + 2) = 110 Second Part: Binary of (5 - 2 ^ floor[log 5]) = Binary of (5 - 4) = 01  I was a bit confused as to why we chose the two bits here and how would we represent when X = 6 since the first part will remain the same while the second part be binary of 6 which in two bits becomes 10 but again why 2 bits?   Another way to calculate the gamma code,  **Approach:**  If X = 5,  **Step 1:** Calculate the binary of 5 = 101 **Step 2:** Remove the first digit from the number: new number - 01 (after removing the first digit). **Step 3**: Prefix the number with unary of 5 i.e. 110. Note: Another way to think about this is to take **01** and add number of 1's based on the length (i.e. since length of 01 is 2, we add 11) followed by a 0.   Gamma Code = 11001  The one thing that messed me up was how many digits to use for the binary (rightmost) part.  Do you use 01 or 001 or 1?  It seems like the answer is that you use floor(log x) bits.  Is this correct?  Thx!! Yes, its floor (log x) bits where log is to the base2 
https://campuswire.com/c/G984118D3/feed/397 Question about Practice Quiz 4 - Question 7 Hi,   I have some confusion regarding the answers of following question:  **Which of the following is NOT correct about the unigram model?** 1. The probability of generating the word A OR B is the sum of the probability of generating A and the probability of generating B. 2. The probability of generating the word sequence "A" "B" "C" is the same as generating "C" "B" "A." 3. The probability of generating the words A AND B is the product of the probability of generating A and the probability of generating B.  I don't understand why option 1 is NOT correct, personally, I feel since the description in Option 1 is suggesting that only one of the words "A" and "B" can be shown at a time, so they should be considered as mutually exclusive, thus: $$p(A\;or\;B) = p(A) + p(B)$$  Could someone please point out where I was wrong?   Thanks I do agree that the question is not clear in this case and might cause confusion. Since this is a practice quiz, it should be okay but we will make changes for the future. This question is not clear. I have already asked TAs and could not get a satisfactory reply. In my opinion, the original meaning of option 1 is "the 'words' A OR B", and it should be (1 - p(A) - p(B)) ^ 2 in this case. We will get this question reviewed. I agree that the question is unclear. I see, thank you so much! May I ask what is "the 'words' A OR B"? I am still confused about why it is 1 - p(A)*p(B) in this case for "the 'words' A OR B". I am still thinking it is p(A)+p(B). Could you provide more details? Thank you very much. Sorry, I was wrong about the probability in the previous answer. In my understanding, "the 'words' A OR B" means a language model generates two words. Then the probability of one of these two words is A or B is (1 - p(A) - p(B)) ^ 2. If the language model just generates one word, the probability is p(A)+p(B). In short, the question is not clear about the number of words the language model generates. I am still confused. Lol. I agree with you "If the language model just generates one word, the probability is p(A)+p(B)". But maybe this question is not so important. Thank you very much for your further explanation! One explanation I have is word A may be the same as word B, then the probability is P(A).
https://campuswire.com/c/G984118D3/feed/405 MP2.1 Content Hi there, I wonder whether the lessons of Week 2 are sufficient for us to do the MP2.1 or not. Thank you.The overview document & digital library instructions shared seem to be sufficient for completing MP2.1.  Yes, MP2.1 does not depend on the topics covered in Week 2.  I cannot access Overview document. It says I dont have access to google doc. can any one provide overview document and DL instructions The content of Week2 is sufficient to finish the MP2.1. You can get it done quickly as you follow the instruction.
https://campuswire.com/c/G984118D3/feed/268 get 0.5 points for MP1 Hi everyone, I believe I followed the instruction steps but only got 0.5 points, does anyone know the problem, I do not know how to see the issue from the log on livedatalabYou might have missed stemming process.  There is a README file in repository. If you hadn't read it carefully, I recommend you to read it and follow every process.  #128 will also help you if you use jupyter notebook. Thanks! I did forget to add the step for stemming. issue fixed
https://campuswire.com/c/G984118D3/feed/740 Stopwords - MP2.4 I am just wondering if we have to apply the stopwords filter somewhere in the assignment and then the ranking function ? The assignment mentions only to change ranker function.You don't have to do that. You just need to edit the ```load_ranker``` function by adjusting its parameters, using another ranking function, or implementing your own ranking function that passes the baseline to get full credit. thank you !!
https://campuswire.com/c/G984118D3/feed/761 Combined slides wk1-wk6 Share Hi all,  I have combined the slides of week 1 to week 6 and uploaded the file online. This will be helpful for reviewing the content for Exam 1.    [cs410%20combined%20slides%20wk1-6.pdf](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dbee30af-ef6a-4d29-a42a-e58c1e7dbd4c/cs410%20combined%20slides%20wk1-6.pdf)
https://campuswire.com/c/G984118D3/feed/190 lecture slides do we have access to slides used in the lecture videos?We can download the slides using below options 1.  Download individual slides for the course  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/918a417c-e070-43f2-9d8e-3819581679d7/image.png)  2. Download all the slides for the course using coursera-dl https://github.com/coursera-dl/coursera-dl 
https://campuswire.com/c/G984118D3/feed/773 Slides for 6.6, 6.8,  6.9 How can I download the slides for lectures 6.6, 6.8, and 6.9 ?If you download the slides from lesson 6.5, it looks like it includes 6.6 as well in the pdf. Further, this appears to repeat for downloading 6.7 (which includes the slides for 6.8 and 6.9.)  It requires that you only download part 1 pdf slides to receive part 2 and part 3 slides where available.  Just as a note, the pdf download for 6.7 says on the first slides "part 1 and 2 (6.8)", but includes part 3's slides (6.9) at the very end as well. Thanks, I have not checked the pdf yet.
https://campuswire.com/c/G984118D3/feed/1269 Practice Quiz 11 - Q2 Confusion or Error Question 2 asks us to select the answer that is not true. I've calculated them multiple times now, and I must be missing something, because I've calculated them all to be true. Where have I gone wrong?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8549a0a3-fe81-4136-ac5d-e37e4649c5c7/image.png)For precision, we want to calculate the rate of true postives y(+) over true posittives and false positives (y+ and y-).  For p(c3) we only have two y(+) and no y(-) which would result in a precision of 2/(2 + 0) or 1. The result with precision(c3) = 2/3 would be incorrect. First note that: - y(+) is a true positives (TP) - y(-) is a false positive (FP) - n(-) is a true negative (TN) - n(+) is false negative (FN)  $$Precision = \frac{\#TP}{\#TP + \#FP}$$ $$Recall= \frac{\#TP}{\#TP + \#FN}$$ $$F_1= \frac{2\cdot Precision\cdot Recall}{Precision + Recall}$$  P(c3) is the precision of the third classifier, so we only have to look at the values in the third column. $$P(c3) = \frac{2}{2+0}=1$$ R(c3) is the recall of the third classifier, so again we only have to look at the values in the third column. $$R(c3) = \frac{2}{2+1}=2/3$$    For the rest:  $$P(c1) = \frac{3}{3+0}=1$$  $$R(c1) = \frac{3}{3+0}=1$$  $$P(c2) = \frac{2}{2+1}=2/3$$  $$R(c2) = \frac{2}{2+0}=1$$  $$F_1(c2) = \frac{2\cdot \frac{2}{3} \cdot 1}{\frac{2}{3} + 1}=4/5$$  $$F_1(c3) = \frac{2\cdot 1 \cdot \frac{2}{3}}{1 + \frac{2}{3}}=4/5$$ Ah, that's why. I was mentally exchanging the meaning of the y(-) and n(+) symbols. Thanks!
https://campuswire.com/c/G984118D3/feed/255 Doubt regarding Quiz 1 Hello,  I had a doubt regarding the Week 1 Quiz where the question was -   In VSM model, which of the following similarity/distance measures would be affected by document length?  1. L2 distance 2. Cosine similarity Hi,  I believe both cosine similarity and Euclidean distance are used to measure the proximity between vectors in a vector space. Cosine similarity is used to measure the similarity between the document irrespective of their size. I would recommend you look at how the two metrics are calculated to get a better understanding of them.  Okay, thanks a lot! In addition to Chandan's answer, #43 may be helpful. Think about how the value of Cosine and L2 distance changes as the vectors get closer or farther apart. Are those equally good measures of “similarity”? Specifically, what is the Cosine of a vector with itself (perfectly self-similar)? What about the L2 distance of a vector with itself? How about both measures with orthogonal vectors? Do the ranges match?
https://campuswire.com/c/G984118D3/feed/186 Not able to open LiveDataLab link I keep seeing this error when I try to open the livedatalab site using this link : http://livedatalab.centralus.cloudapp.azure.com/ As a result I am completely blocked. Please let me know if you have ideas. Thank you ![Screen%20Shot%202022-08-30%20at%208.46.51%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/598ab1a9-f677-4f58-a29c-a5b56a7421c2/Screen%20Shot%202022-08-30%20at%208.46.51%20AM.png)I see the same error. Was able to access it an hour ago (though all the projects were missing then). Probably TA's can help us know if some maintenance/change activity is in progress. Yeah, the site looks down right now - tried multiple browsers and command line apps but getting a server error each time.   Looks like it's hosted on Azure, though, which is having some major issues right now with DNS and container apps - could be related. https://status.azure.com/en-us/status ok at least we know its not a personal issue with laptop now.  Hi all, please refer to https://campuswire.com/c/G984118D3/feed/21. We will update once we resolve issues. oh ok, but this issue is not with just submission. The link itself does not open. Ok np will keep monitoring, hopefully it gets fixed soon :) Thanks The website is opening fine for me but it is really slow as compared to before. Maybe there is some maintenance going on
https://campuswire.com/c/G984118D3/feed/1201 Submission Success? I got result as below in livedatalab. But cannot see score in leaderboard (blank). Is it ok or I missed a step? Thanks.  ![](data:image/png;base64,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)Seems similar to this post: https://campuswire.com/c/G984118D3/feed/1197   This looks like you passed. May be TAs can confirm Thanks Thanks I thought as long as you see a Score 1 on the leaderboard  you should be fine (TAs please confirm). Try adding another commit and push I have tried resubmitting, but still there is no score on the leaderboard. Hmm interesting. Maybe try reaching out to a TA. I saw a Score of 1 on the leaderboard after a couple of minutes of submitting 
https://campuswire.com/c/G984118D3/feed/1288 3 Credit Tech Review Hello! For 3 credit students, do we need to submit that we won't be doing the Tech Review? Or do we simply not submit anything, and our status as a 3-credit student will be handled? Thanks!  I don't think you need to submit anything. Tech Review is only for students who took this course as 4 credits.
https://campuswire.com/c/G984118D3/feed/1141 Cant see other's submission Hello i can only see my own submission in CMT https://cmt3.research.microsoft.com/CS410Expo2022I am not sure if we will be able to see submissions/Github links posted by others. Probably it is not the time for us to review?
https://campuswire.com/c/G984118D3/feed/282 1+log(x) can someone please help how come 1+log(5) is 3?  It is from lecture 2.5 I calculated 1+(log5) the value is  1.69, but in the lecture, it says the value of this is 3. I am confused! please help. Hello Nusrat! Try using log base 2 and applying the floor function, which rounds down to the next whole number.  log base 2 of 5 = 2.322.  floor of log base 2 of 5 = 2 thanks If it helps, can refer to post https://campuswire.com/c/G984118D3/feed/291 As David answered, I realized that it's log base 2.
https://campuswire.com/c/G984118D3/feed/952 1. Form a team (5%,  due beginning of Week 9, Oct 17, 2022) ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/948fdd88-5ea7-4059-8daf-36dc70fbd1c7/image.png)  Where is the form for us to submit? Namely, "1) this form," mentioned above.  Is it just this Google Sheet? https://docs.google.com/spreadsheets/d/1ZmeAR8uMgTGbVHFwzw4UAvkFpW6ldEWO4hZt-RApO9c/edit#gid=0Correct, you only need to fill out the Google sheet according to the instructions for the October 17th team formation deadline.  Do we need to have project topic figured out by October 17th? Or can that be filled out by the proposal deadline? No, a tentative topic/idea is enough and you can change it later. It is required to be done by the proposal deadline. 
https://campuswire.com/c/G984118D3/feed/313 Metapy not recognized by PyCharm Anyone using Pycharm ? It's not able to identify "metapy" package even with conda virtual environment (Python 3.7).  Note that I'm able to use metapy from CLI.  Error : "ModuleNotFoundError: No module named 'metapy'"You might need to have a python3.5 conda virtual env for it. Maybe try Python 3.5 instead? I was able to run everything in PyCharm with 3.5  Also refer to the MP1 Setup section > Please note that students have had issues using metapy with specific Python versions in the past (e.g. Python 3.7 on mac). To avoid issues, please use Python 2.7 or 3.5. Your code will be tested using Python 3.5 Maybe try sublime or vscode instead? I can use metapy with sublime.  It looks PyCharm stopped support for python 3.5 Not sure if it's a good way, I installed python 3.5 through anaconda, I just use Pycharm for editing, but I run the code in the command line.
https://campuswire.com/c/G984118D3/feed/442 MP2.2 how many submission? Hi,  Just a quick reminder that the overview document hasn't been updated yet with MP2.2 and MP2.3 descriptions and requirements.  Is this intended?  ThanksPlease see [Coursera](https://www.coursera.org/learn/cs-410/gradedLti/KGkqH/mp2-2) for the description and requirements for MP2.2.  I've also updated the overview document with the description and requirements.  For full credit, you must submit at least 5 queries, each with at least 10 relevance judgments  Kevin, Just a couple questions about the process for MP 2.2 and beyond: -  How are our judgments tabulated when there are multiple references to the same link, each with its own "relevant/not relevant" radio buttons? For example, if link A appears once and link B appears 8 times in the document set, and I mark all as relevant, will B be more heavily weighted in subsequent searches? - What happens (for subsequent searches) if I mark 4 instances of link B as relevant (e.g., based on their descriptions) and the other 4 as not relevant (e.g., because they have no descriptions)? - How would you like us to think about cases where a seemingly relevant document that is not accessible (e.g., requires a paid subscription to some service) and relevance is questionable based only on the publicly available info, such as the abstract? - How would you like us to handle links that do not work? I tripped over two examples: One led to a 404 error; the other was 'blocked' although its long URL ended with the actual document (http://timan.cs.illinois.edu:4001/redirect?hash=9_none_cf6c6bbe-e5bc-4112-b495-3405f7dd5380&redirect_url=chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://arxiv.org/ftp/arxiv/papers/1708/1708.05148.pdf)    Thanks for the great questions!  1. Search is happening over the highlighted text + explanation, so each submission is treated as its own "result". Moreover, for the purposes of this assignment (and MP2.4), there isn't any "weight calculation". Rather, MP2.4 will be tuning weights of BM25 to maximize the retrieval performance. Outside of the assignment, I haven't thought much about how to weight judged results, but I think favoring B makes some sense, as it a more "popular" link.  2. Then in the assignment, the score will increase for retrieving the four instances marked relevant. Outside of the assignment, I'm not sure. 3. It might depend on the context. For example, if the result points to Netflix with the description of "movie and tv show streaming service", then I'd say it is relevant (even if I don't have a subscription). And generally, if it should be accessible by members of the CS410 class (e.g., though the library, VPN, or proxy), then it should not be marked as not relevant due to access. But if it is completely inaccessible, then it should probably be marked as not relevant.  4. Links that do not work should be marked as not relevant. In the future, I need to add a "Dead Link" feedback mechanism.   
https://campuswire.com/c/G984118D3/feed/792 Exam 1 open book or open note? Hi there, I wonder whether Exam 1 is open book or open note or not. It is very hard to remember all of the formulas and implications or something related.   Also, how should we prepare for Exam 1? Is there any practice exam or something similar?- The exam will be closed notes (i.e., no external material allowed). - You may use two sheets of scratch paper during the exam. - No calculators are allowed.  - No bathroom breaks are allowed.  https://www.coursera.org/learn/cs-410/supplement/Q468O/exam-policies-and-technical-support  The "Lesson 6.10: Summary for Exam 1" lecture video is a summary.  Also, some general info from the course introduction video (from the transcript): > And another thing to mention is also to allocate a sufficient time for the preparation of two proctored exams because they will be given only once. That means, you only have one chance to take each exam, so you want to really prepare well for them. However, those exams are mostly to confirm that you have indeed mastered the materials. So, they will have similar questions to the quiz questions that you have already seen. And in fact, some questions may be exactly the same as the questions in the quizzes. And we do that because this would allow you to have some sense about what the questions might look like in the exams. So, they should not be much surprise if you have actually worked on all the questions in those quizzes and have made sure that you have understood the answers to those questions.
https://campuswire.com/c/G984118D3/feed/633 MP2.4 Faculty Score Hello, It seems like some students have very low faculty score even after the update. Suddenly drop down to 400 times lower. Do you know why this happens? ![Screen%20Shot%202022-09-24%20at%204.01.36%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/cf84b050-8803-469d-b824-1b7ef19e54e0/Screen%20Shot%202022-09-24%20at%204.01.36%20PM.png)  I spent hours in tuning my parameters and still gets lower than 0.001 in faculty score. Please advise, thank you.I was trying to figure out what that was, since I also got that problem. Try deleting anything extra that was generated and wasn't in the original repo, that fixed it for me.  Thank you! It seems like the idx folder needs to be deleted. This was generated by the old dataset. I also had the same issue - couldn't understand why I can't get my scores up until I saw this post - deleted the idx folder and now it started working. Thank you guys! I had the same issue. I made a fresh clone of the assignment repo and it was resolved (originally cloned the repo on Thursday 9/22). Thanks everyone for this! I was having the same issue and it solved it for me
https://campuswire.com/c/G984118D3/feed/432 Lesson 3.2 Question about Document Collection Numbering Hello, I have a question about a slide in Week 3.  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/731063dc-864a-48c4-8ebb-e52bf8d8ee29/image.png)  In the slide above, shouldn't be the number in 'Document Collection' D10 instead of D48, if total number of documents are 10?  Thank you!The total number of relevant docs are 10, but the total number of docs in the whole document collection could be much more. I got it! Thank you! Conceptually, the number of relevant documents and total number of documents are two different things. The former relates to the number of document that are relevant for a query where as the total number of document are documents we have in our collection. We run our query in this collection to get the results.  I would recommend you revise the example provided in Lesson 3.3 at 10:59 into the video. It gives a nice intuition to who we can retrieve the non relevant document from the document collection and can go outside the number of results retrieved.   
https://campuswire.com/c/G984118D3/feed/1218 Calculating Log Likelihood I don't understand what I'm doing wrong when calculating likelihood, I am getting the log of the self.document_topic_prob dot the   self.topic_word_prob then multiplying that with tem_doc_matrix and getting the sum ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/dd0713a8-da1e-4050-95c9-8f41d187e08c/image.png) but with that code I only get two iterations. Any pointers on what I'm doing wrong?What does your exit condition in the `plsa` function look like? You should be checking whether the delta (absolute value in the change in log-likelihood) is less than the threshold value, which is very small by default (like 0.0001). See question 4 on post #988. Make sure the formula you’re using matches the one there
https://campuswire.com/c/G984118D3/feed/1337 Can I make my repo of tech review private? It seems like after forking, the repo is public by default. I wonder if I can make it private and still make it work with livedatalab. I am using my private Github account and I don't want the course-related content to be public on my profile. I'm not sure if this is possible. If you try making it private and still are able to receive a 1 on the leaderboard, then you should be good.  Thank you for your reply and I will try.   I don't mind the course shares our materials on the platform of the course but I feel uncomfortable that I have to make it public on my own "platform" without either being notified before setting up the livedatalab or having an alternative to finish the task. I hope you will understand.
https://campuswire.com/c/G984118D3/feed/1212 MP3 convergence condition Is the MP considered a success if the program converges at (-inf) or have I made some error?You should see a score 1 on leaderboard if it is success.  It does show a 1 on the leaderboard, but when I check the log files I get a warning " RuntimeWarning: divide by zero encountered in log", then it runs for 1 more iteration after that and displays SUCCESS You are probably dividing by zero somewhere in your code (maybe when you are calculating the log likelihood). I think they only check for e and m steps. On my first attempt I forgot to update current_likelihood value and still passed, but it ran 4k iterations. I fixed it and pushed and the number of iterations decreased significantly (~20). How many iterations does it take until the likelihood value converges?  For my code, I didn't notice the epsilon value, so I went with the np.isclose() method where I compared current likelihood value with previous likelihood value. With that I got around 520-570 iterations before it stopped with a SUCCESS.  Once I changed my code to include comparison with epsilon, Every single time my program stooped after 4k iterations with SUCCESS but when I checked my iterations, i encountered the same error around the same 520-570 iterations range.  My best guess is the way you are calculating the log likelihood might be off. See question 4 on post #988. Try the formula there. Your likelihood values should converge at an earlier iteration Just to confirm the formula for one document, log∑j∈topicsP(topic=j∣doc=d)∗P(word=w∣topic=j)) is essentially summation of each topic_prob multipled by the word_topic_prob of each word in that doc.  So for each topic we would multiply all word probs for a topic for a document and then multiply it by the topic prob of that topic for a document. Then we sum the results, and take its log.   Follow the formula on #988. You only apply the log on the result of the inner summation. Not the whole thing
https://campuswire.com/c/G984118D3/feed/416 Lesson 5.8: HITS Algorithm slide We see the following slide on [this Coursera lesson](https://www.coursera.org/learn/cs-410/lecture/d6INf/lesson-5-8-link-analysis-part-3-optional).  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/929588c0-8973-4de8-84e5-839a21c2aa78/image.png)  Is a link from d1 to d4 missing in the graph above?Yes! Thanks for catching the error on this slide; I will need to correct it.  We need to either modify the graph to add a link from d1 to d4 or remove the "1" corresponding to this link in the matrix.   Thank you for your confirmation!
https://campuswire.com/c/G984118D3/feed/106 MP1 Setup Please help! what went wrong here ? how can i fix it   ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/eb864d95-4446-4264-b18a-a3dc7da22391/image.png)Looks like your code did not pass the test cases, you might need to debug your code a bit more.  do i need to install python new version on PC ?  Do we need to commit example.py? yes! there is a place inside example.py where you should "place your code here"  I also see from the output that you're using python 2.7 and not python 3, things should theoretically still work with python 2.7 but I'd keep an eye on that.  Thanks , i am missing code .Does this code available for copy & paste or we have  to write it . Yes, you will need to write the code for the function tokens_lowercase thanks all for helping . 
https://campuswire.com/c/G984118D3/feed/63 LiveDataLab link is down Is it only me or the link is temporarily down ?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2b5f73f1-8713-43bd-9ac4-d5d16634f6f4/image.png)  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/f2d849e4-caaf-40c5-a5c9-e1812929f314/image.png)The link should be http://livedatalab.centralus.cloudapp.azure.com/ Thank you. Thanks. This helped.
https://campuswire.com/c/G984118D3/feed/243 Google Colab wget syntax error For anyone who is having trouble executing the wget command in Google Colab, the Colab requires a “!” in front of the wget command for it to work. Thought this could be helpful.
https://campuswire.com/c/G984118D3/feed/910 Extra Exam 1 Slots Open I noticed a ton of new exam times available on ProctorU **this weekend**, and was able to reschedule at no extra charge.  If you missed the opportunity to schedule the exam on Saturday, now is your chance!  
https://campuswire.com/c/G984118D3/feed/570 why we use ttest_rel and not ttest_ind For the MP2.3 git repo, the link for the python t-test is pointing to the function of ttest_rel.  I'm wondering why we are not using ttest_ind?  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/70f12879-0af4-4486-96f7-bcbd56ade0c7/image.png)We use ttest_rel() instead of ttest_ind(), as we wish to perform a paired t test.  According to Wikipedia, a paired t-test "is used when the samples are dependent; that is, when there is only one sample that has been tested twice (repeated measures) or when there are two samples that have been matched or "paired". "  We are comparing the performance of two ranking functions on the same data, where the ith entry in one list equals the AP from evaluating the results of a query using BM25, and the ith entry in the other list equals the AP from evaluating the results of the same query using InL2. The entries of these lists are therefore paired.  We would use an independent sample of scores, if each sample of values was drawn from some distribution separately, and there was no correspondence between the ith value in the first list and the ith value in the second list. 
https://campuswire.com/c/G984118D3/feed/752 Unable to get webhook submission on to LiveDataLab Hi,   I have added webhook successfully and committed to my repo.   I committed changes AFTER adding the webhook in.   However I do not see any web hook activity or submission history under my LiveDataLab page.   Could someone please help?  ![Screen%20Shot%202022-09-29%20at%209.33.43%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8f3075a4-7214-4a72-bec0-9393c58dcdbd/Screen%20Shot%202022-09-29%20at%209.33.43%20PM.png)   1. Can you try triggering the webhook manually and seeing the result? 2. What events is your webhook triggering off of? Should be `push` events if i recall correctly your webhook seems to connect. Thats why it shows a green Check. If you click on it , you can see the logs. Every commit will trigger a webhook activity to datalabs. ![Screen%20Shot%202022-10-01%20at%207.32.40%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/316f5277-d569-428a-88a9-418e87b007da/Screen%20Shot%202022-10-01%20at%207.32.40%20AM.png)   I see that the webhook is pushing successfully on Github but there is no submission history for LiveDataLab under MP2.4.     ![Screen%20Shot%202022-10-01%20at%207.33.42%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/d83f2642-ec85-4bd2-94d6-06a97d182dc7/Screen%20Shot%202022-10-01%20at%207.33.42%20AM.png)   when I click on leaderboard there are no results (I was able to see results for other assignments) -- Could someone please help what steps I need to make to ensure the submission is received for grading? Thank you!  Thank you! Yes it is able to detect push I see in the logs. However LiveDataLab is not detecting the submission. I have deleted and re-linked my Github account multiple times (lowercase github.com, my username)   but it is not showing up (I added screenshot below to follow up).  Thank you so much! Are there any logs displayed for a submission?  Have you checked if the personal access token you used to link your github account has expired? From github, you can go to settings then developer settings at the bottom left and then click personal access token to check. Hi yes I have done this multiple times. The PAT is not expired. I am still unable to get submission through. Please help. I can share my repo if needed 
https://campuswire.com/c/G984118D3/feed/1216 Terminal Log Error I am getting this error and do not understand what is missing.  I dont have access to the logs. ![Screen%20Shot%202022-10-25%20at%2012.15.07%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/fdc1b020-bb13-4a98-a684-dc343f91e869/Screen%20Shot%202022-10-25%20at%2012.15.07%20PM.png)The error is "need more than 1 value to unpack". It seems like there is an error in your code.  Figured it out. I was reading the file wrong. Strange that it worked on mac previously.
https://campuswire.com/c/G984118D3/feed/356 Digital Library Submission Counter Hi, not sure if the tooling already has this functionality, but I think it'd be awesome if there was a count displayed to the user to view how many submissions they have done on the Digital Library.   This doesn't seem like a high priority, but would generally be helpful for user tracking as the number of submissions gets higher and higher.  
https://campuswire.com/c/G984118D3/feed/1188 CMT - Conflict of interest Hello, I saw this in our CMT submission page: ![Capture.PNG](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/73c5dec6-b39b-4935-887d-c049582dfa72/Capture.PNG)  What does it mean? Do we need to worry about this? Thanks.That form is for when you want to avoid having your submission be reviewed by someone who may know about your submission (e.g., an advisor of a student reviewing the student's paper submission).   For our class, no need to worry about the conflict of interest!
https://campuswire.com/c/G984118D3/feed/436 Lesson 9.1: slide typo? ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/2d59532f-c1ab-4373-aec6-4123546f8208/image.png)  Should $$p(\theta_d) + (\theta_B) = 1$$ be $$p(\theta_d) + p(\theta_B) = 1$$ ?Yeah I think it's a typo, there should be a p since we're talking about probabilities. Thanks! Thanks for catching the error! I'll fix it later.  Thank you, Professor!
https://campuswire.com/c/G984118D3/feed/767 "No logs available yet. Your build may still be starting up." Hi, everyone,  I encoutered this issue.   Submission Details  Submission Number:1 Status:FAILURE Duration:00:00:26.5 (HH:MM:SS.MS)  I've gone through the past similar posts and found we may delete the livedatalav link and relink to solve this problem.  I tried this and but it dosen't work.  Can anyone or TA help me?  Thanks.  Mingqing TengUpdating: This is due to the expired GitHub token. So Re-generate token->delete link account-> reLink. It works.....  Oh... I spent 90% of the total time on setting up ...
https://campuswire.com/c/G984118D3/feed/79 git push --mirror error hi, I git cloned and then git push --mirror. But I have one error:   remote: Repository not found. fatal: repository 'https://github.com/myaccount/MP1_private.git/' not found where"myaccount" is my github account.  Can anyone help me out?  Thanks.You can directly upload the files to git and commit.  I was encountering the same issue and I resolved it by adding my github id in front of github.com. So try git push --mirror https://myaccount@github.com/myaccount/MP1_private.git instead. You may want to consider creating an access token for github as described [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/about-authentication-to-github). If that's not enough info, you can google tutorials for your specific OS.  Then your command would look like: ``` push --mirror git@github.com:your_user_name/MP1_private.git # note the ':' after github.com ```  For a drag-and-drop implementation to this issue you could use [github desktop](https://desktop.github.com/) .   It works, thanks! This should be the best way to access each github project.
https://campuswire.com/c/G984118D3/feed/430 mp 2.1 grade How do we know what our grades are for mp 2.1? I am following the instructions to submit websites to the Digital library, but don't know how to confirm this/ view a grade for it. "And grading for the assignment will be updated on Coursera once the assignment is due."  This is what the overview document mentions. I think we may know our grades on Coursera next week. But what if we did something wrong? Can we rectify it?  I don't think so. We indeed need to be careful. However, this mp just requires us to record 15 relevant web links with detailed instructions, so maybe it is not so bad. I will mark this question unresolved, so we can wait for an answer from TAs. As mentioned, the grades will be uploaded early next week. As long as you have at least 15 submissions (can check "My Submissions" at http://timan.cs.illinois.edu:4000), you should receive full credit.   Edit: it was a "/" instead of a "." I've changed it to be correct. This link did not work, I am getting a ERR_NAME_NOT_RESOLVED issue.  I am using the vpn, not sure what is wrong.   Sorry, not sure why that link didn't work. Try this:  http://timan.cs.illinois.edu:4000/ Yes the second link works, thanks!  Please review the instructions provided for MP2.1.  ![Screen%20Shot%202022-09-11%20at%204.23.39%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/97d2e370-accb-42a4-a704-37d8e6922984/Screen%20Shot%202022-09-11%20at%204.23.39%20PM.png)  From a student perspective, we are good, as long as we  have submitted 15 relevant documents using the extension. We have the ability to verify the submissions using the link: [My Submissions](http://timan.cs.illinois.edu:4000).   
https://campuswire.com/c/G984118D3/feed/352 How to get the green check mark for Week 3? I'm assuming we will not be able to get a full completion status until **after** the due date?   During Week 3 for MP 2.1 it says "*grading for the assignment will be updated on Coursera once the assignment is due.*" I've completed MP 2.1 but just wanted to make sure I get a completion status. Can anyone confirm whether there are additional steps to get the green check mark in Coursera? TYIA! ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e1d0fb4a-3fc9-4c35-a234-4a8bc7b74f6d/image.png)For both MP2.1 and MP2.2, I will periodically batch upload grades to Coursera. As long as you can view 15+ submissions on the DL website and the account uses your NetID and/or university email, you should be good. I will make an announcement when I upload the grades so that you can double-check that you received credit.  I have updated DL with 15 relevant docs. However, I have not received my grade on coursera yet. Just wanted to check if I missed anything in my submission. Thanks  Hey Guru, if this is not yet answered for you: Kevin answered this in the previous comment, that he will make an announcement when he uploads the grades. Got it ! I thought  that the grades will be uploaded prior to the due date. Thanks
https://campuswire.com/c/G984118D3/feed/717 How to prepare for Exam1 Lesson 6.10 with the title "summary for exam 1" doesn't mention exam 1 explicitly.  Are only the contents from lessons 1-6 covered for Exam 1?  When to register for exam 1?  Thank!You can schedule the exam now.  I already have. There are 3 options to select . Which one to select for the exam.![Screen%20Shot%202022-09-27%20at%2010.12.35%20AM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e4ea5500-e685-41fd-8514-7bca57ad2606/Screen%20Shot%202022-09-27%20at%2010.12.35%20AM.png) We are section DSO.  It is fall 2022, so I assume that this is FA22.  It is the first exam, so I would say the first choice.  I don't remember what I picked, but I would assume option 1. This was the response from TA (to my post to TA & Instructor only)  "Please don't sign up for the exam till we release more information."  May be we can wait for TAs to confirm. ok thanks! Usually TA's sends a mail asking to book for the exam and it also mentions which one to select. Not sure if i missed that.  You beat me to it. I was also going to ask if the exam includes calculation questions, as the Coursera test policy in week 6 says we are allowed 2 pieces of scratch paper, but no calculator. Was there a confirmation from TAs regarding the course content for Exam 1. Is it 1-6 as specified in Coursera Lesson 6.10 I wonder why no TA comes up and confirm the sign-up for Exam 1. Based on my experience with other courses, sometimes the link will be changed, then we have to cancel and sign up again.
https://campuswire.com/c/G984118D3/feed/244 Access to Google Tools (Colab) For those wanting to use Google Colab notebooks, etc., here is a link on how to request access to Google Workspace. Students get access to all tools, which is super awesome.  https://help.uillinois.edu/TDClient/42/UIUC/Requests/ServiceDet?ID=135
https://campuswire.com/c/G984118D3/feed/591 MP2.3 grade not visible on Coursera Hi everyone,  I submitted MP2.3 to live data lab and got a score of 1. However I dont see my grade updated on Coursera. I also clicked on the "Open Tool" link from coursera, and logged into live data lab from there as mentioned. Is this expected, when will we see this? ![Screen%20Shot%202022-09-23%20at%201.35.35%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/90e0a754-e68a-48c5-84ba-d7a8ba5d43ff/Screen%20Shot%202022-09-23%20at%201.35.35%20PM.png)May be can wait for sometime and try refreshing coursera. As you have the score of 1, coursera will show 100% for MP2.3 Its been 2 days since I finished the assignment, still grade doesn't show. Send email to the TA. Score should reflect immediately on coursera. Where can I see the list of TA emails? how do I see the score/points I got? All I see is the STATUS (success or fail). Click on the leaderboard from liveDataLab. You should see a score of 1 if you got a 100 percent. Do you see your grade on coursera? Oh thanks I see the score now. No, I do not see any grade on coursera. Create new post, and you can post only to instructors and TA's. Are you using the university id properly with livedatalab as login, if there is a mismatch it won't update your coursera. It doesn't show for me either Did this issue get solved for someone? sadly not for me just yet. I've been waiting for a day, the grade is also not updated for me either I followed #659, and it worked, although it didn't work yesterday.
https://campuswire.com/c/G984118D3/feed/127 Lesson 2.1 Hi, In Lecture 2.1, shouldn't d2 be equal to (1,1,0,0,1) instead of what is mentioned in the lecture. Since two words 'organic food' don't match with the words in the query. So the only match would be news, about, and campaign. This in turn would make f(q, d2) = 2, hence would be at the end of the list. Please let me know if I am not doing this the right way. Thanks![Screen%20Shot%202022-08-27%20at%207.12.27%20PM.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/70d37fbb-5702-4b7f-abf9-0a942cf68b56/Screen%20Shot%202022-08-27%20at%207.12.27%20PM.png)As query is " news about presidential campaign" q=(1,1,1,1,0...) As document2 has only occurrence of  words "news","about", campaign" so d2=(1,1,0,1,...) . Hence, f(q,d2) = 1x1+1x1+1x0+1x1=3. Hope this helps. Thanks for the answer. I was just wondering in the case of 'news about organic food campaign', shouldn't it be (1,1,0,0,1)? Since organic food are two different words within the document  Just as a follow up if anyone is confused, we are matching occurences of individual words in order to the original query only.  ex. query                                =      news[0] about[1] presidential[2] campaign[3] d1                                     =  ... **news about** organic food **campaign** ... original query comparison      1              1                  0                    1 *matches to the original query only in the positional text order of "news", "about", "presidential", "campaign", with each word in the document being counted and then tallied.  So, in the case of Term frequency weighting, we can see that "news about _____ campaign" fills positions 0, 1, and 3 in our 4 text sequence compared to the original query only once. Since our query is only 4 words long (positions 0-3), the values of d2 after positions 0-3 do not have an impact as the query is not searching for any words beyond "news about presidential campaign." The words in between also do not impact the sequence of order for our original query. We can imagine the query is a tally of occurrences in the document and the just tally how many times "news", "about", "presidential", and "campaign" occur. Looking next at D4, we find the word presidential (position 2) occurs twice and therefore tallies to 2.
https://campuswire.com/c/G984118D3/feed/1190 Converging at 20 iterations in LiveDataLab but incorrect implementation My code is converging at 20 iterations and live data lab says success, but it also says, "INCORRECT IMPLEMENTATION". I am not sure how to debug further.   I added many print statements to check whether my probability matrix sums to 1, etc...   Would appreciate any help on how to debug further!  ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/4de8a856-7094-4b44-9bab-5c8dff44e899/image.png)That may not be sufficient. You may still need to monitor the log likelihood in each iteration to ensure it increases monotonically. I printed them out and it is increasing monotonically, but sometimes it goes until the 50th iteration and has a likelihood of ~ 169,000. (1st iteration starts with ~ -178,000)  Interesting. You may also print out topic_word_prob (theta) and document_topic_prob (pi) to check if they make sense according to the labels in the text.txt.  My log likelihood also increased from -179013 to -159718 after 50 iterations.  Thank you! Will print out topic_word_prob (theta) and document_topic_prob (pi). I used 3 loops so maybe that might be causing the problem not sure Sure. BTW, you can round them to 3 decimals. If they are correct, they really make sense.  Have you figure out the problem? My log likelihood range is identical to yours, and the auto grader stopped a little over 20, and my results are also wrong. Haven't figured it out yet :/  Hoping a TA can give some guidance! Is your leaderboard empty? I have a lot of submissions, but none are showing up in the leaderboard as well Mine is empty as well Once I fixed an error in my M step and it passed, it also showed up in the leaderboard! Was able to fix it, my M step normalization was incorrect. thanks!
https://campuswire.com/c/G984118D3/feed/540 Downloading Lecture Slides with coursera-dl on MAC Hi, I realized that it's quite time consuming to manually go through each week's lecture video to have all the slides downloaded so I looked into how I can do that in bulk. It may be helpful for you if you are in similar situations, here's how I did it:  In the terminal, type:  1. pip install coursera-dl 2. coursera-dl -u <user> -p <pass> -ca <cauth_token> --path <download_path> -f "pdf" cs-410  and you should see it start downloading the pdf lecture slides.  <user> is just your Coursera log in email, similarly for <pass>, you don't need quotation marks over them.  <download_path> should be a path to the directory where you'd like your lecture slides stored.  Here is how to find the cauth_token ([reference](https://apple.stackexchange.com/questions/409668/how-to-install-and-run-coursera-dl-on-m1-mac-without-errors)) : 1 First login to coursera.org in your web browser, for example, in chrome 2 Go to web browser settings 3 Advanced 4 Privacy and Security 5 Site Settings 6 Cookies and Site Data 7 See all cookies and site data 8 Find coursera.org, click into it and check for CAUTH 9 Copy this value. It might be kinda big  To find your course name (if you are interested in other courses), just use the name after /learn/ in the url for your course. E.g. https://www.coursera.org/learn/cs-410/home/ would give you the course name: cs-410  More documentations on coursera-dl: https://github.com/coursera-dl/coursera-dl#running-the-script
https://campuswire.com/c/G984118D3/feed/937 Can I use calculator in Exam 1 Can I use calculator in Exam 1?No. You are allowed only two sheets of paper or a whiteboard. So if there are questions related to gamma-code(compress problem), we need to calculate log manually? For gamma encoding, the key calculation is floor(log base 2 of X). The floor function makes this much more simple. Basically, you only need to find a power of 2 that is closest to X, but not greater than X.   Suppose X=10. To find floor(log2(10)): 1. The largest power of 2 that is less than or equal to 10 is 8. 2. 8 = 2^3, so floor(log2(10) = 3.  In fact, floor(log2(X)) = 3 for any integer X from 8 to 15. Does this help? Thanks
https://campuswire.com/c/G984118D3/feed/879 Exam 1 could not be started Dear Professor and TAs, I scheduled Exam1 for 9:40 am CT today (Mon 10/10) but after 70 minutes, Proctor U was not able to start the exam. Here is a Coursera screen shot as evidence: ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/2a7a8d1e-3a24-4db4-919f-2014b58c3ca3/5631ca1a-54e9-4f49-be86-e0c157e36051/image.png) ProctorU claimed that "the institution [UIUC] provided the wrong password" and that they "left email and voice mail for the instructor and institution but received no response". ProctorU declined to identify exact contact information, so I cannot confirm who exactly was contacted. ProctorU maintained that they can do nothing until they receive a response.  Please know that Coursera is configured to allow only one attempt by ProctorU to enter the password (see screen shot below). ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/8b301ad9-d083-4cc2-9c4a-7fb9c2c1456b/image.png) However, Proctor U claims they are not limited in how many attempts they have to get the password right. During the 70-minute session, I watched ProctorU go back numerous times to the above page in Coursera, but they were unable to enter another password. At the moment, I cannot even reschedule the exam because ProctorU wants to restart the previous failed session (see screen shot). ![image.png](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/a082d795-1f3a-453f-91ce-a9482a12ce4e/image.png) Please advise what to do. I was ready to take the exam at this time, so this situation is very disconcerting. David BurrusWe're investigating this right now. Hang tight.  Hey David,  1) just checked the password on Coursera and ProctorU, they do match. I think the proctor just didn't copy it over correctly. 2) the contact for exam issues is JC (jcmorgan@illinois.edu) , JC hasn't received any communication from PorctorU about this issue yet (I just checked with him).  3) what we can do is increase the number of attempts for the Exam Password (you're right, its now only 1 attempt allowed)   David, I just changed the number of attempts for the Password to "Unlimited attempts", this should allow the proctors to attempt to enter the password multiple times until they get it right. Assma, I suspect that you are correct: the proctor did not copy it correctly. He told me that I did not need to look away, as stated in the instructions. When he pasted in the password, I saw that the number of dots was at least 50 or 60 characters -- perhaps not coincidentally, similar in the length to the URL he had pasted just previously into the address bar.  Thank you very much for increasing the number of password attempts. What should be my next step -- can I try to "reconnect" to the exam at any time? I do not know the guidelines for such a situation. David yes David, let's try to "reconnect" first.  If JC receives anything from them I'll update you here.  worst case scenario we get you to re-register for the exam for another slot (sorry about that).  But let's try to reconnect first, since you didn't get your money's worth :)  Assma,  I am online right now with ProctorU. The latest proctor has tried three times to connect, but so far we have not passed the password test. I believe she tried more than one password, but of course I have no information about those passwords. Have other CS410 students been able to start the exam?  David ok this is starting to get weird. Let me recheck the passwords Assma, The proctor advises that copy-and-paste does not work in this case. She has been typing it manually. David checked it again, they match David... I haven't heard of any other student who's running into this too (although you might be the very first student to attempt the Exam)  is there a way I can join the zoom?  are you in? I'm watching you live, I see the password went through. Good luck David this proctor is ridiculous, you should be able to just start your Exam, as long as you see the "start" button, means everything is passed Assma, sorry for not responding until now but I really wanted to start (and finish) get the exam, and my confidence in ProctorU was not high. I'm glad you were able to witness some of my experience.  FYI -- and I share this only in case other students have problems -- when I tried to reconnect, two different proctors in a row (Billy S and Ryalin) disconnected almost immediately, without any explanation. I only can guess that they thought I was late for my 9:40 scheduled time. There definitely were no Internet access issues, at least on my side. So, on my third "reconnect" attempt, I immediately typed in the ProctorU chat box "PLEASE DO NOT HANG UP, THIS IS A SESSION RECONNECT". Maybe the third proctor (Zyrellen C) would have continued the session anyway, but I did not want to take any chances. Zyrellen was very nice, actually, but like this morning's proctor (Bharat M) needed a ProctorU "advocate" (higher level of customer service rep, I think) to help enter the password. It became a little absurd when the advocate, Douglas G, entered the password correctly (100% from Coursera) then proceeded to enter it again, twice. It's possible he was having Zyrellen practice the password: there was no audio at this time. This morning, about an hour into the call, I did ask ProctorU if any of the other 250-ish students in this class had similar password issues, but they did not answer. So maybe you're right: I was their first student to attempt this exam.  In any case, all done! Woo-hoo! 
https://campuswire.com/c/G984118D3/feed/386 F1-score why is it more popular Why is F-1 score more popular than F-2, F-3 score etc? Any reason for it/ use cases?As per my understanding, the F1 score is the harmonic mean of the precision and recall where it gives equal weightage to precision and recall. The more generic F-beta score applies additional weights, valuing one of precision or recall more than the other. Usecases where recall is twice as important as precision the F2 score will be more appropriate. Example of higher recall usecase (as given in practice quiz) - automatic system to filter tweets of the possible communication between terrorists. If you answer questions anonymously, then you won’t get points when I upvote it, I’m afraid. Thanks for the upvote. The points are granted even when answering anonymously :) 
https://campuswire.com/c/G984118D3/feed/719 MP2.4 can’t submit ![image2022-09-27%2011%3A24%3A42.jpg](https://campuspro-uploads.s3.us-west-2.amazonaws.com/984118d3-29f1-4a34-9a3b-14c65608f28c/e7728a87-9711-420b-b51e-558716a0f78b/image2022-09-27%2011%3A24%3A42.jpg) I can’t  submit successfully from the first time. I didn’t change the code, and just pushed. But the status is failure.can you post the error. you can go into the red highlighted link and take a screenshot of the console output. I think that is because your github personal access token is expired.  Please follow the week 1 instructions to regenerate a personal access token in github and then relink the github account in livedatalab.  The screenshot of the error code will be helpful Thanks !That’s the answer I just experienced the same issue, the root cause is because the github personal access token has expired. The steps I follow to solve the issue:  1. Follow the livelab set up instruction to create a new token from github Settings 2. Click "Delete Linked Accounts" from the Live data lab page 3. Delete expired tokens in Github 4. Click "Link new account" in the Live data lab page, input your new token in the "API Key" 4. Try a dummy push from your repo, it will ask for user name and password, for password, paste the token instead
https://www.coursera.org/learn/cs-410/supplement/v3hCA/this-week-is-for-working-on-the-projectdict_values(['List\nCS 410: Text Information Systems\nWeek 13\nThis week is for working on the project...\nPrevious\nNext\nthis is a week for working on the project\nReading:\nReading\nThis week is for working on the project...\n. Duration: 10 minutes\n10 min\nCourse Project\nThis week is for working on the project...\nPlease use this time to work on the project.\nMark as completed\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/lecture/8Ki0H/10-4-text-clustering-generative-probabilistic-models-part-3-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is a continuing discussion of generative probabilistic models for tax classroom.\n0:14\nIn this lecture, we're going to do a finishing discussion of generative probabilistic models for text crossing.\n0:21\nSo this is a slide that you have seen before and here, we show how we define the mixture model for text crossing and what the likelihood function looks like. And we can also compute the maximum likelihood estimate, to estimate the parameters. In this lecture, we're going to do talk more about how exactly we're going to compute the maximum likelihood estimate. As in most cases the Algorithm can be used to solve this problem for mixture models. So here's the detail of this Algorithm for document clustering. Now, if you have understood how Algorithm works for topic models like TRSA, and I think here it would be very similar. And we just need to adapt a little bit to this new mixture model. So as you may recall Algorithm starts with initialization of all the parameters. So this is the same as what happened before for topic models.\n1:28\nAnd then we're going to repeat until the likelihood converges and in each step we'll do E step and M step. In M step, we're going to infer which distribution has been used to generate each document. So I have to introduce a hidden variable Zd for each document and this variable could take a value from the range of 1 through k, representing k different distributions.\n1:59\nMore specifically basically, we're going to apply base rules to infer which distribution is more likely to have generated this document, or computing the posterior probability of the distribution given the document.\n2:17\nAnd we know it's proportional to the probability of selecting this distribution p of Z the i. And the probability of generating this whole document from the distribution which is the product of the probabilities of world for this document as you see here. Now, as you all clear this use for kind of remember,\n2:45\nthe normalizer or the constraint on this probability. So in this case, we know the constraint on this probability in E-Step is that all the probabilities of Z equals i must sum to 1. Because the documented must have been generated from precisely one of these k topics. So the probability of being generated from each of them should sum to 1. And if you know this constraint, then you can easily compute this distribution as long as you know what it is proportional to. So once you compute this product that you see here, then you simply normalize\n3:31\nthese probabilities, to make them sum to 1 over all the topics. So that's E-Step, after E-Step we want to know which distribution is more likely to have generated this document d, and which is unlikely.\n3:45\nAnd then in M-Step we're going to re-estimate all the parameters based on the in further z values or in further knowledge about which distribution has been used to generate which document. So the re-estimation involves two kinds of parameters 1 is p of theta and this is the probability of selecting a particular distribution. Before we observe anything, we don't have any knowledge about which cluster is more likely. But after we have observed that these documents, then we can crack the evidence to infer which cluster is more likely. And so this is proportional to the sum of the probability of Z sub d j is equal to i.\n4:34\nAnd so this gives us all the evidence about using topic i, theta i to generate a document. Pull them together and again, we normalize them into probabilities.\n4:50\nSo this is for key of theta sub i.\n4:54\nNow the other kind of parameters are the probabilities of words in each distribution, in each cluster. And this is very similar to the case piz and here we just report the kinds of words that are in documents that are inferred to have been generated from a particular topic of theta i here. This would allows to then estimate how many words have actually been generated from theta i. And then we'll normalize again these accounts in the probabilities so that the probabilities on all the words would sum to up. Note that it's very important to understand these constraints as they are precisely the normalizing in all these formulas. And it's also important to know that the distribution is over what?\n5:54\nFor example, the probability of theta is over all the k topics, that's why these k probabilities will sum to 1. Whereas the probability of a word given theta is a probability distribution over all the words. So there are many probabilities and they have to sum to 1. So now, let's take a look at a simple example of two clusters. I've two clusters, I've assumed some initialized values for the two distributions. And let's assume we randomly initialize two probability of selecting each cluster as 0.5, so equally likely. And then let's consider one document that you have seen here. There are two occurrences of text and two occurrences of mining. So there are four words together and medical and health did not occur in this document. So let's think about the hidden variable.\n6:50\nNow for each document then we much use a hidden variable. And before in piz, we used one hidden variable for each work because that's the output from one mixture model. So in our case the output from the mixture model or the observation from mixture model is a document, not a word. So now we have one hidden variable attached to the document. Now that hidden variable must tell us which distribution has been used to generate the document. So it's going to take two values, one and two to indicate the two topics.\n7:25\nSo now how do we infer which distribution has been used generally d? Well it's been used base rule, so it looks like this. In order for the first topic theta 1 to generate a document, two things must happen. First, theta sub 1 must have been selected. So it's given by p of theta 1. Second, it must have also be generating the four words in the document. Namely, two occurrences of text and two occurrences of sub mining. And that's why you see the numerator has the product of the probability of selecting theta 1 and the probability of generating the document from theta 1. So the denominator is just the sum of two possibilities of generality in this document. And you can plug in the numerical values to verify indeed in this case, the document is more likely to be generated from theta 1, much more likely than from theta 2. So once we have this probability, we can easily compute the probability of Z equals 2, given this document. How? Well, we can use the constraint. That's going to be 1 minus 100 over 101. So now it's important that you note that in such a computation there is a potential problem of underflow. And that is because if you look at the original numerator and the denominator, it involves the competition of a product of many small probabilities. Imagine if a document has many words and it's going to be a very small value here that can cause the problem of underflow. So to solve the problem, we can use a normalize. So here you see that we take a average of all these two math solutions to compute average at the screen called a theta bar.\n9:24\nAnd this average distribution would be comparable to each of these distributions in terms of the quantities or the magnitude. So we can then divide the numerator and the denominator both by this normalizer. So basically this normalizes the probability of generating this document by using this average word distribution. So you can see the normalizer is here. And since we have used exactly the same normalizer for the numerator and the denominator. The whole value of this expression is not changed but by doing this normalization you can see we can make the numerators and the denominators more manageable in that the overall value is not going to be very small for each. And thus we can avoid the underflow problem.\n10:24\nIn some other times we sometimes also use logarithm of the product to convert this into a sum of log of probabilities. This can help preserve precision as well, but in this case we cannot use algorithm to solve the problem. Because there is a sum in the denominator, but this kind of normalizes can be effective for solving this problem. So it's a technique that's sometimes useful in other situations in other situations as well. Now let's look at the M-Step. So from the E-Step we can see our estimate of which distribution is more likely to have generated a document at d. And you can see d1's more like got it from the first topic, where is d2 is more like from second topic, etc. Now, let's think about what we need to compute in M-step well basically we need to re-estimate all the parameters. First, look at p of theta 1 and p of theta 2. How do we estimate that? Intuitively you can just pool together these z, the probabilities from E-step. So if all of these documents say, well they're more likely from theta 1, then we intuitively would give a higher probability to theta 1. In this case, we can just take an average of these probabilities that you see here and we've obtain a 0.6 for theta 1. So 01 is more likely and then theta 2. So you can see probability of 02 would be natural in 0.4. What about these word of probabilities? Well we do the same, and intuition is the same. So we're going to see, in order to estimate the probabilities of words in theta 1, we're going to look at which documents have been generated from theta 1. And we're going to pull together the words in those documents and normalize them. So this is basically what I just said.\n12:20\nMore specifically, we're going to do for example, use all the kinds of text in these documents for estimating the probability of text given theta 1. But we're not going to use their raw count or total accounts. Instead, we can do that discount them by the probabilities that each document is likely been generated from theta 1. So these gives us some fractional accounts. And then these accounts would be then normalized in order to get the probability. Now, how do we normalize them? Well these probability of these words must assign to 1. So to summarize our discussion of generative models for clustering. Well we show that a slight variation of topic model can be used for clustering documents. And this also shows the power of generating models in general. By changing the generation assumption and changing the model slightly we can achieve different goals, and we can capture different patterns and types of data. So in this case, each cluster is represented by unigram language model word distribution and that is similar to topic model. So here you can see the word distribution actually generates a term cluster as a by-product. A document that is generated by first choosing a unigram language model. And then generating all the words in the document are using just a single language model. And this is very different from again copy model where we can generate the words in the document by using multiple unigram language models.\n13:56\nAnd then the estimated model parameters are given both topic characterization of each cluster and the probabilistic assignment of each document into a cluster.\n14:07\nAnd this probabilistic assignment sometimes is useful for some applications. But if we want to achieve harder clusters mainly to\n14:16\npartition documents into disjointed clusters. Then we can just force a document into the cluster corresponding to the words distribution that's most likely to have generated the document. We've also shown that the Algorithm can be used to compute the maximum likelihood estimate. And in this case, we need to use a special number addition technique to avoid underflow. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/lRm0I/lesson-5-5-web-indexingdict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.5: Web Indexing\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.5: Web Indexing\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:07\nThis lecture is about the Web Indexing.\n0:11\nIn this lecture, we will continue talking about the Web Search and we're going to talk about how to create a Web Scale Index.\n0:24\nSo once we crawl the web, we've got a lot of web pages. The next step is to use the indexer to create the inverted index.\n0:36\nIn general, we can use the same information retrieval techniques for creating an index and that is what we talked about in previous lectures, but there are there are new challenges that we have to solve. For web scale indexing, and the two main challenges are scalability and efficiency. The index would be so large, that it cannot actually fit into any single machine or single disk. So we have to store the data on virtual machines.\n1:06\nAlso, because the data is so large, it's beneficial to process the data in parallel, so that we can produce index quickly. Now to address these challenges, Google has made a number of innovations. One is the Google File System that's a general File system, that can help programmers manage files stored on a cluster of machines.\n1:32\nThe second is MapReduce. This is a general software framework for supporting parallel computation.\n1:38\nHadoop is the most well known open source implementation of MapReduce. Now used in many applications.\n1:50\nSo, this is the architecture of the google file system.\n1:53\nIt uses a very simple centralized management mechanism to manage all the specific locations of. Files, so it maintains the file namespace and look up a table to know where exactly each file is stored.\n2:11\nThe application client will then talk to this GFS master, and that obtains specific locations of the files they want to process.\n2:22\nAnd once the GFS file kind obtained the specific location about the files, then the application client can talk to the specific servers whether data actually sits directly, so you can avoid involving other node. In the network.\n2:46\nSo when this file system stores the files on machines, the system also with great fixed sizes of chunks, so the data files are separated into.\n3:00\nMany chunks. Each chunk is 64 MB, so it's pretty big. And that's appropriate for large data processing. These chunks are replicated to ensure reliability. So this is something that the programmer doesn't have to worry about, and it's all taken care of by this file system. So from the application perspective, the programmer would see this as if it's a normal file. And the programmer doesn't have to know where exactly it is stored and can just invoke high level. Operators to process the file.\n3:39\nAnd another feature is that the data transfer is directly between application and chunk servers. So it's efficient in this sense.\n3:51\nOn top of the Google file system, Google also proposed MapReduce as a general framework for parallel programming. Now, this is very useful to support a task like building inverted index.\n4:06\nAnd so, this framework is,\n4:12\nHiding a lot of low-level features from the program. As a result, the programmer can make a minimum effort to create an application that can be run a large cluster in parallel.\n4:28\nSo some of the low level details are hidden in the framework including the specific and network communications or load balancing or where the task are executed. All these details are hidden from the programmer.\n4:47\nThere is also a nice feature which is the built in fault tolerance. If one server is broken, the server is down, and then some tasks may not be finished. Then the MapReduce mapper will know that the task has not been done. So it automatically dispatches a task on other servers that can do the job. And therefore, again the program doesn't have to worry about that So here's how MapReduce works. The input data would be separated into a number of key value pairs. Now what exactly is in the value would depend on the data and it's actually a fairly general framework to allow you to just partition the data into different parts and each part can be then processed in parallel.\n5:37\nEach key value pair would be and send it to a map function. The program was right the map function, of course.\n5:45\nAnd then the map function will process this Key Value pair and then generate a number of other Key Value pairs. Of course, the new key is usually different from the old key that's given to the map as input. And these key value pairs are the output of the map function and all the outputs of all the map functions would be then collected,\n6:12\nand then there will be for the sorting based on the key. And the result is that, all the values that are associated with the same key will be then grouped together. So now we've got a pair of of a key and separate values attached to this key.\n6:31\nSo this would then be sent to a reduce function.\n6:36\nNow, of course, each reduce function will handle a different key, so we will send these output values to multiple reduce functions each handling a unique key.\n6:52\nA reduce function would then process the input, which is a key in a set of values to produce another set of key values as the output. So these output values would be then corrected together to form the final output.\n7:12\nAnd so, this is the general framework of MapReduce. Now the programmer only needs to write the Map function and the Reduce function. Everything else is actually taken care of by the MapReduce framework. So you can see the program really only needs to do minimum work. And with such a framework, the input data can be partitioned into multiple parts, which is processing parallel first by map, and then being the process after we reach the reduced stage. The much more reduced if I'm [INAUDIBLE] can also further process\n7:55\nthe different keys and their associated values in parallel. So it achieves some,\n8:05\nit achieves the purpose of parallel processing of a large data set. So let's take a look at a simple example. And that's Word Counting.\n8:16\nThe input is containing words, and the output that we want to generate is the number of occurrences of each word. So it's the Word Count.\n8:28\nWe know this kind of counting would be useful to, for example, assess the popularity of a word in a large collection and this is useful for achieving a factor of IDF wading for search.\n8:42\nSo how can we solve this problem? Well, one natural thought is that, well this task can be done in parallel by simply counting different parts of the file in parallel, and then in the end we just combine all the counts. And that's precisely the idea of what we can do with MapReduce.\n9:02\nWe can parallelize on lines in this input file.\n9:07\nSo more specifically, we can assume the input to each map function is\n9:14\na key value pair that represents the line number and the string on that line. So the first line, for example, has a key of one and that is another word by word and just the four words on that line. So this key value pair would be sent to a Map Function. The Map Function then would just count the words in this line.\n9:41\nAnd in this case, of course there are only four words. Each world gets a count of one and these are the output that you see here on this slide from this map function. So the map function is really very simple if you look at what the pseudocode looks like on the right side, you see it simply needs to iterate all the words and this line. And then just collect the function\n10:09\nwhich means it would then send the word and the count to the collector. The collector would then try to sort all these key value pairs from different Map Functions, right? So the function is very simple and the programmer specifies this function as a way to process each part of the data.\n10:31\nOf course, the second line will be handled by a different Map Function which we will produce a single output. Okay, now the output from the map functions will be then and send it to a collector and the collector would do the internal grouping or sorting. So at this stage, you can see, we have collected a match for pairs. Each pair is a word and its count in a line. So, once we see all these pairs. Then we can sort them based on the key, which is the word. So we will collect all the counts of a word, like bye here, together.\n11:09\nAnd similarly, we do that for other words. Like Hadoop, Hello, etc. So each word now is attached to a number of values, a number of counts.\n11:20\nAnd these counts represent the occurrences to solve this word in different lights. So now we have got a new pair of a key and a set of values, and this pair will then be fed into reduce function, so the reduce function now would have to finish the job of counting the total occurrences of this word. Now, it has all ready got all these puzzle accounts, so all it needs to do is simply to add them up. So the reduce function here is very simple, as well. You have a counter, and then iterate all the other words. That you'll see in this array. And that, you just accumulate accounts, right? And then finally, you output the P and the proto account. And that's precisely what we want as the output of this whole program.\n12:12\nSo you can see, this is all ready very similar to. To building an Invert index. And if you think about it, the output here is index. And we have already got a dictionary, basically. We have got the count. But what's missing is the document the specific frequency counts of words in those documents. So we can modify this slightly to actually be able to index in parallel, so here's one way to do that. So in this case, we can assume the input from Map Function is a pair of a key which denotes the document ID, and the value denoting the screen for that document, so it's all the words in that document. And so, the map function would do something very similar to what we have seen in the word campaign example. It simply groups all the counts of this word in this document together. And it would then generate a set of key value pairs. Each key is a word, and the value is the count of this word in this document plus the document ID. Now, you can easily see why we need to add document ID here, because later in inverted index, we would like to keep this formation, so the Map Function should keep track of it, and this can then be sent to the reduce function later. Now similarly another document D2 can be processed in the same way. So in the end, again, there is a sorting mechanism that would group them together. And then we will have just a key, like a java, associated with all the documents that match this key. Or all the documents where java occurred.\n14:04\nAnd the counts, so the counts of java in those documents. And this will be collected together. And this will be, so fed into the reduce function. So now you can see the reduce function has already got input that looks like an inverted index entry. So it's just the word and all the documents that contain the word and the frequencies of the word in those documents. So all you need to do is simply to concatenate them\n14:37\ninto a continuous chunk of data. And this can be done written to a file system. So basically the reduce function is going to do very minimal. Work.\n14:49\nAnd so, this is a pseudo-code for [INAUDIBLE] that's construction. Here we see two functions, procedure Map and procedure Reduce. And a programmer would specify these two functions to program on top of map reduce. And you can see basically they are doing what I just described. In the case of map, it's going to count the occurrences of a word using the AssociativeArray. And it would output all the counts together with the document ID here. So, this is the reduce function, on the other hand, simply concatenates all the input that it has been given, and then put them together as one single entry for this key. So this is a very simple MapReduce function, yet it would allow us to construct an inverted index at very large scale, and the data can be processed by different machines. And program doesn't have to take care of the details.\n16:12\nSo this is how we can do parallel index construction for web search.\n16:20\nSo to summarize, web scale indexing requires some new techniques that go beyond the. Standard traditional indexing techniques. Mainly, we have to store index on multiple machines. And this is usually done by using a filing system, like a Google file system. But this should be through a file system. And secondly, it requires creating an index an parallel, because it's so large and takes long time to create an index for all the documents. So if we can do it in parallel, it will be much faster and this is done by using the MapReduce framework.\n16:57\nNote that both the GFS and MapReduce frameworks are very general, so they can also support many other applications.\n17:07\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/rU7LT/lesson-3-3-evaluation-of-tr-systems-evaluating-ranked-lists-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 3\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\nPrevious\nNext\nWeek 3 Information\nWeek 3 Lessons\nVideo:\nVideo\nLesson 3.1: Evaluation of TR Systems\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.2: Evaluation of TR Systems - Basic Measures\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.6: Evaluation of TR Systems - Practical Issues\n. Duration: 15 minutes\n15 min\nWeek 3 Activities\nProgramming Assignment 2.1\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:07\nThis lecture is about, how we can evaluate a ranked list?\n0:13\nIn this lecture, we will continue the discussion of evaluation. In particular, we are going to look at, how we can evaluate a ranked list of results.\n0:24\nIn the previous lecture, we talked about, precision-recall. These are the two basic measures for, quantitatively measuring the performance of a search result.\n0:40\nBut, as we talked about, ranking, before, we framed that the text of retrieval problem, as a ranking problem.\n0:50\nSo, we also need to evaluate the, the quality of a ranked list.\n0:56\nHow can we use precision-recall to evaluate, a ranked list? Well, naturally, we have to look after the precision-recall at different, cut-offs. Because in the end, the approximation of relevant documents, set, given by a ranked list, is determined by where the user stops browsing. Right? If we assume the user, securely browses, the list of results, the user would, stop at some point, and that point would determine the set. And then, that's the most important, cut-off, that we have to consider, when we compute the precision-recall. Without knowing where exactly user would stop, then we have to consider, all the positions where the user could stop. So, let's look at these positions. Look at this slide, and then, let's look at the, what if the user stops at the, the first document? What's the precision-recall at this point? What do you think?\n1:56\nWell, it's easy to see, that this document is So, the precision is one out of one. We have, got one document, and that's relevent. What about the recall? Well, note that, we're assuming that, there are ten relevant documents, for this query in the collection, so, it's one out of ten.\n2:16\nWhat if the user stops at the second position?\n2:19\nTop two.\n2:21\nWell, the precision is the same, , two out of two. And, the record is two out of ten.\n2:28\nWhat if the user stops at the third position? Well, this is interesting, because in this case, we have not got any, additional relevant document, so, the record does not change.\n2:41\nBut the precision is lower, because we've got number [INAUDIBLE] so, what's exactly the precision?\n2:49\nWell, it's two out of three, right? And, recall is the same, two out of ten. So, when would see another point, where the recall would be different? Now, if you look down the list, well, it won't happen until, we have, seeing another relevant document. In this case D5, at that point, the, the recall is increased through three out of ten, and, the precision is three out of five.\n3:15\nSo, you can see, if we keep doing this, we can also get to D8. And then, we will have a precision of four out of eight, because there are eight documents, and four of them are relevant. And, the recall is a four out of ten.\n3:29\nNow, when can we get, a recall of five out of ten? Well, in this list, we don't have it, so, we have to go down on the list. We don't know, where it is? But, as convenience, we often assume that, the precision is zero,\n3:47\nat all the, the othe, the precision are zero at all the other levels of recall, that are beyond the search results. So, of course, this is a pessimistic assumption, the actual position would be higher, but we make, make this assumption,\n4:05\nin order to, have an easy way to, compute another measure called Average Precision, that we will discuss later.\n4:14\nNow, I should also say, now, here you see, we make these assumptions that are clearly not, accurate.\n4:22\nBut, this is okay, for the purpose of comparing to, text methods. And, this is for the relative comparison, so, it's okay, if the actual measure, or actual, actual number deviates a little bit, from the true number. As long as the deviation, is not biased toward any particular retrieval method, we are okay. We can still, accurately tell which method works better. And, this is important point, to keep in mind. When you compare different algorithms, the key's to avoid any bias toward each method. And, as long as, you can avoid that. It's okay, for you to do transformation of these measures anyway, so, you can preserve the order.\n5:09\nOkay, so, we'll just talk about, we can get a lot of precision-recall numbers at different positions. So, now, you can imagine, we can plot a curve. And, this just shows on the, x-axis, we show the recalls.\n5:23\nAnd, on the y-axis, we show the precision. So, the precision line was marked as .1, .2, .3, and, 1.0. Right? So, this is, the different, levels of recall. And,, the y-axis also has, different amounts, that's for precision.\n5:45\nSo, we plot the, these, precision-recall numbers, that we have got, as points on this picture. Now, we can further, and link these points to form a curve. As you'll see, we assumed all the other, precision as the high-level recalls, be zero. And, that's why, they are down here, so, they are all zero. And this, the actual curve probably will be something like this, but, as we just discussed, it, it doesn't matter that much, for comparing two methods.\n6:20\nbecause this would be, underestimated, for all the method.\n6:25\nOkay, so, now that we, have this precision-recall curve, how can we compare ranked to back list? All right, so, that means, we have to compare two PR curves.\n6:38\nAnd here, we show, two cases. Where system A is showing red, system B is showing blue, there's crosses.\n6:48\nAll right, so, which one is better? I hope you can see, where system A is clearly better. Why? Because, for the same level of recall,\n6:58\nsee same level of recall here, and you can see, the precision point by system A is better, system B. So, there's no question. In here, you can imagine, what does the code look like, for ideal search system? Well, it has to have perfect, precision at all the recall points, so, it has to be this line. That would be the ideal system. In general, the higher the curve is, the better, right? The problem is that, we might see a case like this. This actually happens often. Like, the two curves cross each other.\n7:32\nNow, in this case, which one is better?\n7:35\nWhat do you think?\n7:38\nNow, this is a real problem, that you actually, might have face. Suppose, you build a search engine, and you have a old algorithm, that's shown here in blue, or system B. And, you have come up with a new idea. And, you test it. And, the results are shown in red, curve A.\n7:59\nNow, your question is, is your new method better than the old method?\n8:05\nOr more, practically, do you have to replace the algorithm that you're already using, your, in your search engine, with another, new algorithm? So, should we use system, method A, to replace method B? This is going to be a real decision, that you to have to make. If you make the replacement, the search engine would behave like system A here, whereas, if you don't do that, it will be like a system B. So, what do you do?\n8:36\nNow, if you want to spend more time to think about this, pause the video. And, it's actually very useful to think about that. As I said, it's a real decision that you have to make, if you are building your own search engine, or if you're working, for a company that, cares about the search.\n8:52\nNow, if you have thought about this for a moment, you might realize that, well, in this case, it's hard to say. Now, some users might like a system A, some users might like, like system B. So, what's the difference here? Well, the difference is just that, you know, in the, low level of recall, in this region, system B is better. There's a higher precision. But in high recall region, system A is better.\n9:20\nNow, so, that also means, it depends on whether the user cares about the high recall, or low recall, but high precision. You can imagine, if someone is just going to check out, what's happening today, and want to find out something relevant in the news.\n9:34\nWell, which one is better? What do you think?\n9:38\nIn this case, clearly, system B is better, because the user is unlikely examining a lot of results. The user doesn't care about high recall.\n9:47\nOn the other hand, if you think about a case, where a user is doing you are, starting a problem. You want to find, whether your idea ha, has been started before. In that case, you emphasize high recall. So, you want to see, as many relevant documents as possible. Therefore, you might, favor, system A. So, that means, which one is better? That actually depends on users, and more precisely, users task.\n10:19\nSo, this means, you may not necessarily be able to come up with one number,\n10:25\nthat would accurately depict the performance.\n10:29\nYou have to look at the overall picture. Yet, as I said, when you have a practical decision to make, whether you replace ours with another, then you may have to actually come up with a single number, to quantify each, method. Or, when we compare many different methods in research, ideally, we have one number to compare, them with, so, that we can easily make a lot of comparisons. So, for all these reasons, it is desirable to have one, single number to match it up. So, how do we do that? And, that, needs a number to summarize the range. So, here again it's the precision-recall curve, right? And, one way to summarize this whole ranked, list, for this whole curve, is look at the area underneath the curve.\n11:19\nRight? So, this is one way to measure that. There are other ways to measure that, but, it just turns out that,,\n11:26\nthis particular way of matching it has been very, popular, and has been used, since a long time ago for text And, this is, basically, in this way, and it's called the average precision. Basically, we're going to take a, a look at the, every different, recall point.\n11:47\nAnd then, look out for the precision. So, we know, you know, this is one precision. And, this is another, with, different recall. Now, this, we don't count to this one, because the recall level is the same, and we're going to, look at the, this number, and that's precision at a different recall level et cetera. So, we have all these, you know, added up. These are the precisions at the different points, corresponding to retrieving the first relevant document, the second, and then, the third, that follows, et cetera. Now, we missed the many relevant documents, so, in all of those cases, we just, assume, that they have zero precisions.\n12:33\nAnd then, finally, we take the average. So, we divide it by ten, and which is the total number of relevant documents in the collection.\n12:41\nNote that here, we're not dividing this sum by four. Which is a number retrieved relevant documents. Now, imagine, if I divide by four, what would happen?\n12:54\nNow, think about this, for a moment.\n12:57\nIt's a common mistake that people, sometimes, overlook.\n13:02\nRight, so, if we, we divide this by four, it's actually not very good. In fact, that you are favoring a system, that would retrieve very few random documents, as in that case, the denominator would be very small. So, this would be, not a good matching. So, note that this denomina, denominator is ten, the total number of relevant documents. And, this will basically ,compute the area, and the needs occur. And, this is the standard method, used for evaluating a ranked list.\n13:41\nNote that, it actually combines recall and, precision. But first, you know, we have precision numbers here, but secondly, we also consider recall, because if missed many, there would be many zeros here. All right, so, it combines precision and recall. And furthermore, you can see this measure is sensitive to a small change of a position of a relevant document. Let's say, if I move this relevant document up a little bit, now, it would increase this means, this average precision. Whereas, if I move any relevant document, down, let's say, I move this relevant document down, then it would decrease, uh,the average precision. So, this is a very good, because it's a very sensitive to the ranking of every relevant document. It can tell, small differences between two ranked lists. And, that is what we want, sometimes one algorithm only works slightly better than another. And, we want to see this difference. In contrast, if we look at the precision at the ten documents. If we look at this, this whole set, well, what, what's the precision, what do you think? Well, it's easy to see, that's a four out of ten, right? So, that precision is very meaningful, because it tells us, what user would see? So, that's pretty useful, right? So, it's a meaningful measure, from a users perspective. But, if we use this measure to compare systems, it wouldn't be good, because it wouldn't be sensitive to where these four relevant documents are ranked. If I move them around the precision at ten, still, the same. Right. So, this is not a good measure for comparing different algorithms. In contrast, the average precision is a much better measure. It can tell the difference of, different, a difference in ranked list in, subtle ways. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/QKK7y/lesson-2-6-system-implementation-fast-searchdict_values(["List\nCS 410: Text Information Systems\nWeek 2\nLesson 2.6: System Implementation - Fast Search\nPrevious\nNext\nWeek 2 Information\nWeek 2 Lessons\nVideo:\nVideo\nLesson 2.1: Vector Space Model - Improved Instantiation\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\nLesson 2.2: TF Transformation\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 2.3: Doc Length Normalization\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.4: Implementation of TR Systems\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 2.5: System Implementation - Inverted Index Construction\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.6: System Implementation - Fast Search\n. Duration: 17 minutes\n17 min\nWeek 2 Activities\nProgramming Assignment 1\nLesson 2.6: System Implementation - Fast Search\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about how to do faster search by using invert index.\n0:14\nIn this lecture, we're going to continue the discussion of system implementation. In particular, we're going to talk about how to support a faster search by using invert index.\n0:26\nSo let's think about what a general scoring function might look like.\n0:32\nNow of course, the vector space model is a special case of this, but we can imagine many other retrieval functions of the same form.\n0:42\nSo the form of this function is as follows.\n0:46\nWe see this scoring function of a document D and a query Q is defined as first a function of fa that adjustment a function that would consider two factors. That I'll assume here at the end, f sub d of d and f sub q of q. These are adjustment factors of a document and a query, so they are at the level of a document and the query. So and then inside of this function, we also see there's another function called h. So this is the main part of the scoring function and these as I just said of the scoring factors at the level of the whole document and the query. For example, document [INAUDIBLE] and this aggregate punching would then combine all these. Now inside this h function, there are functions that would compute the weights of the contribution of a matched query term ti.\n2:08\nSo this g, the function g gives us the weight of a matched query term ti in document d.\n2:23\nAnd this h function would then aggregate all these weights. So for example, take a sum of all the matched query terms,\n2:36\nbut it can also be a product or it could be another way of aggregating them.\n2:41\nAnd then finally, this adjustment the functioning would then consider the document level or query level factors to further adjust this score, for example, document [INAUDIBLE]. So, this general form would cover many state of [INAUDIBLE] functions. Let's look at how we can score documents with such a function using virtual index.\n3:07\nSo, here's a general algorithm that works as follows. First this query level and document level factors can be pre-computed in the indexing time. Of course, for the query we have to compute it at the query time but for document, for example, document [INAUDIBLE] can be pre-computed. And then, we maintain a score accumulator for each document d to computer h.\n3:34\nAn h is an aggregation function over all the matching query terms. So how do we do that? For each period term we're going to do fetch the inverted list from the invert index. This will give us all the documents that match this query term\n3:52\nand that includes d1, f1 and so dn fn. So each pair is a document ID and the frequency of the term in the document. Then for each entry d sub j and f sub j are particular match of the term in this particular document d sub j. We'll going to compute the function g that would give us something like weight of this term, so we're computing the weight completion of matching this query term in this document. And then, we're going to update the score accumulator for this document and this would allow us to add this to our accumulator that would incrementally compute function h. So this is basically a general way to allow pseudo computer or functions of this form by using the inbound index. Note that we don't have to attach any of document and that didn't match any query term. Well, this is why it's fast, we only need to process the documents that matched at least one query term. In the end, then we're going to adjust the score the computer this function f sub a and then we can sort. So let's take a look at a specific example. In this case, let's assume the scoring function is a very simple one, it just takes the sum of t f, the role of t f, the count of a term in the document.\n5:25\nThis simplification would help shield the algorithm clearly. It's very easy to extend the computation to include other weights like the transformation of tf, or [INAUDIBLE] or IDF [INAUDIBLE]. So let's take a look at specific example, where the queries information security\n5:48\nand it show some entries of invert index on the right side. Information occurred in four documents and their frequencies are also there, security occurred in three documents. So let's see how the arrows works, so first we iterate overall query terms and we fetch the first query then, what is that? That's information, right? And imagine we have all these score accumulators who score the,\n6:17\nscores for these documents. We can imagine there will be other but then they will only be allocated as needed. So before we do any waiting of terms, we don't even need a score of. That comes actually we have these score accumulators eventually allocating.\n6:38\nSo lets fetch the interest from the entity [INAUDIBLE] for information, that the first one.\n6:46\nSo these four accumulators obviously would be initialize as zeros.\n6:51\nSo, the first entry is d1 and 3, 3 is occurrences of information in this document. Since our scoring function assume that the score is just a sum of these raw counts. We just need to add a 3 to the score accumulator to account for the increase of score due to matching this term information, a document d1. And then, we go to the next entry, that's d2 and 4 and then we add a 4 to the score accumulator of d2. Of course, at this point, that we will allocate the score accumulator as needed. And so at this point, we allocated the d1 and d2, and the next one is d3, and we add one, we allocate another score [INAUDIBLE] d3 and add one to it. And then finally, the d4 gets a 5, because the term information occurred five times in this document. Okay, so this completes the processing of all the entries in the invert index for information. It processed all the contributions of matching information in this four documents.\n8:01\nSo now, our error will go to the next that's security. So, we're going to fetch all the inverted index entries for security.\n8:10\nSo, in this case, there are three entries, and we're going to go through each of them. The first is d2 and 3 and that means security occur three humps in d2 and what do we do? Well, we do exactly the same, as what we did for information. So, this time we're going to change the score [INAUDIBLE] d2 since it's already allocated and what we do is we'll add 3 to the existing value which is a 4, so we now get a 7 for d2.\n8:41\nD2 score is increased because the match that falls the information and the security. Go to the next entry, that's d4 and 1, so we would the score for d4 and again, we add 1 to d4 so d4 goes from 5 to 6. Finally, we process d5 and a 3. Since we have not yet allocated a score accumulated for d5, at this point, we're going to allocate 1 for d5, and we're going to add a 3 to it. So, those scores, of the last rule, are the final scores for these documents.\n9:20\nIf our scoring function is just a simple some of TF values.\n9:27\nNow, what if we, actually, would like to do form addition? Well, we going to do the [INAUDIBLE] at this point, for each document.\n9:36\nSo, to summarize this, all right, so you can see, we first process the information determine query term information and we processed all the entries in what index for this term. Then we process the security, all right, its worst think about what should be the order of processing here when we can see the query terms? It might make a difference especially if we don't want to keep all the score accumulators. Let's say, we only want to keep the most promising score accumulators. What do you think would be a good order to go through? Would you process a common term first or would you process a rare term first?\n10:24\nThe answers is we just go to who should process the rare term first. A rare term would match a few documents, and then the score contribution would be higher, because the ideal value would be higher. And then, it allows us to attach the most diplomacy documents first. So, it helps pruning some non-promising ones, if we don't need so many documents to be returned to the user. So those are all heuristics for further improving the accuracy. Here you can also see how we can incorporate the idea of waiting, right? So they can [INAUDIBLE] when we incorporate [INAUDIBLE] when we process each query time. When we fetch the inverted index we can fetch the document frequency and then we can compute IDF. Or maybe perhaps the IDF value has already been precomputed when we indexed the documents. At that time, we already computed the IDF value that we can just fetch it, so all these can be done at this time. So that would mean when we process all the entries for information, these words would be adjusted by the same IDF, which is IDF for information.\n11:36\nSo this is the basic idea of using inverted index for fast research and it works well for all kinds of formulas that are of the general form. And this generally, the general form covers actually most state of art retrieval functions. So there are some tricks to further improve the efficiency, some general techniques to encode the caching. This is we just store some results of popular queries, so that next time when you see the same query, you simply return the stored results. Similarly, you can also slow the list of inverted index in the memory for a popular term. And if the query term is popular likely, you will soon need to factor the inverted index for the same term again. So keeping it in the memory would help, and these are general techniques for improving efficiency. We can also keep only the most promising accumulators because a user generally doesn't want to examine so many documents. We only need to return high qualities subset of documents that likely are ranked on the top.\n12:47\nFor that purpose, we can then prune the accumulators. We don't have to store all the accumulators. At some point, we just keep the highest value accumulators. Another technique is to do parallel processing and that's needed for really process in such a large data set like the web data set. And you scale up to the Web-scale really to have the special techniques you do parallel processing and to distribute the storage of files on machines. So here is a list of some text retrieval toolkits, it's not a complete list. You can find more information at this URL on the bottom. And here, I listed your four here, Lucene's one of the most popular toolkits that can support a lot of applications and it has very nice support for applications. You can use it to build a search engine application very quickly. The downside is that it's not that easy to extend it, and the algorithms implemented they are also not the most advanced algorithms. Lemur or Indri is another toolkit that does not have such a nice support web application as Lucene but it has many advanced search algorithms and it's also easy to extend. Terrier is yet another toolkit that also has good support for application capability and some advanced algorithms. So that's maybe in between Lemur or Lucene, or maybe rather combining the strands of both, so that's also useful tool kit. MeTA is a toolkit that we will use for the problem assignment and this is a new toolkit that has\n14:47\na combination of both text retrieval algorithms and text mining algorithms. And so talking models are implement they are a number of text analysis algorithms implemented in the toolkit as well as basic search algorithms. So to summarize all the discussion about the System Implementation,\n15:11\nhere are the major takeaway points. Inverted index is the primary data structure for supporting a search engine and that's the key to enable faster response to a user's query.\n15:26\nAnd the basic idea is to preprocess the data as much as we can, and we want to do compression when appropriate. So that we can save disk space and we can speed up IO and processing of inverted index in general. We talked about how to construct the invert index when the data can't fit into the memory. And then we talk about faster search using that index basically, what's we exploit the invective index to accumulate a scores for documents [INAUDIBLE] algorithm. And we exploit the Zipf's law to avoid the touching many documents that don't match any query term and this algorithm can actually support a wide range of ranking algorithms.\n16:13\nSo these basic techniques have great potential for further scaling up using distributed file system, parallel processing, and caching. Here are two additional readings you can take a look, if you have time and you are interested in learning more about this. The first one is a classical textbook on the efficiency o inverted index and the compression techniques. And how to, in general feel that the efficient any inputs of the space, overhead and speed. The second one is a newer textbook that has a nice discussion of implementing and evaluating search engines.\n16:58\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/PK3Gd/7-6-text-representation-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.6 Text Representation: Part 2\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.6 Text Representation: Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]. So, as we explained the different text representation tends to enable different analysis.\n0:16\nIn particular, we can gradually add more and more deeper analysis results to represent text data. And that would open up a more interesting representation\n0:29\nopportunities and also analysis capacities. So, this table summarizes what we have just seen. So the first column shows the text representation. The second visualizes the generality of such a representation. Meaning whether we can do this kind of representation accurately for all the text data or only some of them. And the third column shows the enabled analysis techniques.\n0:56\nAnd the final column shows some examples of application that can be achieved through this level of representation. So let's take a look at them. So as a stream text can only be processed by stream processing algorithms. It's very robust, it's general.\n1:15\nAnd there was still some interesting applications that can be down at this level. For example, compression of text. Doesn't necessarily need to know the word boundaries. Although knowing word boundaries might actually also help.\n1:28\nWord base repetition is a very important level of representation. It's quite general and relatively robust, indicating they were a lot of analysis techniques. Such as word relation analysis, topic analysis and sentiment analysis. And there are many applications that can be enabled by this kind of analysis. For example, thesaurus discovery has to do with discovering related words. And topic and opinion related applications are abounded. And there are, for example, people might be interesting in knowing the major topics covered in the collection of texts. And this can be the case in research literature. And scientists want to know what are the most important research topics today. Or customer service people might want to know all our major complaints from their customers by mining their e-mail messages. And business intelligence people might be interested in understanding consumers' opinions about their products and the competitors' products to figure out what are the winning features of their products.\n2:43\nAnd, in general, there are many applications that can be enabled by the representation at this level.\n2:53\nNow, moving down, we'll see we can gradually add additional representations. By adding syntactical structures, we can enable, of course, syntactical graph analysis. We can use graph mining algorithms to analyze syntactic graphs. And some applications are related to this kind of representation. For example, stylistic analysis generally requires syntactical structure representation.\n3:22\nWe can also generate the structure based features. And those are features that might help us classify the text objects into different categories by looking at the structures sometimes in the classification. It can be more accurate. For example, if you want to classify articles into\n3:45\ndifferent categories corresponding to different authors. You want to figure out which of the k authors has actually written this article, then you generally need to look at the syntactic structures.\n4:03\nWhen we add entities and relations, then we can enable other techniques such as knowledge graph and answers, or information network and answers in general. And this analysis enable applications about entities.\n4:22\nFor example, discovery of all the knowledge and opinions about real world entities.\n4:28\nYou can also use this level representation to integrate everything about anything from scaled resources.\n4:37\nFinally, when we add logical predicates, that would enable large inference, of course. And this can be very useful for integrating analysis of scattered knowledge.\n4:50\nFor example, we can also add ontology on top of the,\n4:54\nextracted the information from text, to make inferences.\n4:59\nA good of example of application in this enabled by this level of representation, is a knowledge assistant for biologists. And this program that can help a biologist manage all the relevant knowledge from literature about a research problem such as understanding functions of genes.\n5:22\nAnd the computer can make inferences about some of the hypothesis that the biologist might be interesting. For example, whether a gene has a certain function, and then the intelligent program can read the literature to extract the relevant facts, doing compiling and information extracting. And then using a logic system to actually track that's the answers to researchers questioning about what genes are related to what functions.\n5:57\nSo in order to support this level of application we need to go as far as logical representation. Now, this course is covering techniques mainly based on word based representation.\n6:12\nAnd these techniques are general and robust and that's more widely used in various applications.\n6:21\nIn fact, in virtually all the text mining applications you need this level of representation and then techniques that support analysis of text in this level.\n6:35\nBut obviously all these other levels can be combined and should be combined in order to support the sophisticated applications. So to summarize, here are the major takeaway points. Text representation determines what kind of mining algorithms can be applied. And there are multiple ways to represent the text, strings, words, syntactic structures, entity-relation graphs, knowledge predicates, etc. And these different representations should in general be combined in real applications to the extent we can. For example, even if we cannot do accurate representations of syntactic structures, we can state that partial structures strictly. And if we can recognize some entities, that would be great. So in general we want to do as much as we can.\n7:34\nAnd when different levels are combined together, we can enable a richer analysis, more powerful analysis.\n7:42\nThis course however focuses on word-based representation. Such techniques have also several advantage, first of they are general and robust, so they are applicable to any natural language. That's a big advantage over other approaches that rely on more fragile natural language processing techniques. Secondly, it does not require much manual effort, or sometimes, it does not require any manual effort. So that's, again, an important benefit, because that means that you can apply it directly to any application.\n8:20\nThird, these techniques are actually surprisingly powerful and effective form in implications.\n8:29\nAlthough not all of course as I just explained.\n8:34\nNow they are very effective partly because the words are invented by humans as basically units for communications.\n8:45\nSo they are actually quite sufficient for representing all kinds of semantics.\n8:53\nSo that makes this kind of word-based representation all so powerful. And finally, such a word-based representation and the techniques enable by such a representation can be combined with many other sophisticated approaches.\n9:14\nSo they're not competing with each other. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/irt/CjO6a/project-proposal-submission-for-gradingdict_values(['List\nCS 410: Text Information Systems\nWeek 9\nProject Proposal Submission for Grading\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nWeek 9 Activities\nPractice Quiz: Week 9 Practice Quiz\n11 questions\nQuiz: Week 9 Quiz\n9 questions\nGraded Assignment: Project Proposal Submission for Grading\nSubmitted\nProject Proposal Submission for Grading\nGraded Assignment\n3 hours\n • 3h\nThe assignment has been updated. You will see the new version of the assignment on the next attempt.\nSubmit your assignment\nDue \nOctober 24, 12:59 AM EDT\nOct 24, 12:59 AM EDT\nReceive feedback\nYour grade\n\nView Feedback\nLike\nDislike\nReport an issue\nClose\nCoursera Honor Code\nWe’re dedicated to protecting the integrity of your work on Coursera.\nAs part of this effort, we’ve created an honor code that we ask everyone to follow. Learn more\nAll learners should:\nSubmit their own original work\nAvoid sharing answers with others\nReport suspected violations\nContinue'])
https://www.coursera.org/learn/cs-410/lecture/wBtIp/7-8-paradigmatic-relation-discovery-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.8 Paradigmatic Relation Discovery Part 1\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.8 Paradigmatic Relation Discovery Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the Paradigmatics Relation Discovery. In this lecture we are going to talk about how to discover a particular kind of word association called a paradigmatical relation.\n0:25\nBy definition, two words are paradigmatically related if they share a similar context. Namely, they occur in similar positions in text. So naturally our idea of discovering such a relation is to look at the context of each word and then try to compute the similarity of those contexts.\n0:50\nSo here is an example of context of a word, cat.\n0:55\nHere I have taken the word cat out of the context and you can see we are seeing some remaining words in the sentences that contain cat.\n1:09\nNow, we can do the same thing for another word like dog.\n1:13\nSo in general we would like to capture such a context and then try to assess the similarity of the context of cat and the context of a word like dog.\n1:24\nSo now the question is how can we formally represent the context and then define the similarity function.\n1:33\nSo first, we note that the context actually contains a lot of words. So, they can be regarded as a pseudo document, a imagine document, but there are also different ways of looking at the context. For example, we can look at the word that occurs before the word cat. We can call this context Left1 context. All right, so in this case you will see words like my, his, or big, a, the, et cetera. These are the words that can occur to left of the word cat. So we say my cat, his cat, big cat, a cat, et cetera. Similarly, we can also collect the words that occur right after the word cat. We can call this context Right1, and here we see words like eats, ate, is, has, et cetera. Or, more generally, we can look at all the words in the window of text around the word cat. Here, let's say we can take a window of 8 words around the word cat. We call this context Window8.\n2:49\nNow, of course, you can see all the words from left or from right, and so we'll have a bag of words in general to represent the context.\n3:01\nNow, such a word based representation would actually give us an interesting way to define the perspective of measuring the similarity. Because if you look at just the similarity of Left1, then we'll see words that share just the words in the left context, and we kind of ignored the other words that are also in the general context. So that gives us one perspective to measure the similarity, and similarly, if we only use the Right1 context, we will capture this narrative from another perspective. Using both the Left1 and Right1 of course would allow us to capture the similarity with even more strict criteria.\n3:49\nSo in general, context may contain adjacent words, like eats and my, that you see here, or non-adjacent words, like Saturday, Tuesday, or some other words in the context.\n4:05\nAnd this flexibility also allows us to match the similarity in somewhat different ways. Sometimes this is useful, as we might want to capture similarity base on general content. That would give us loosely related paradigmatical relations. Whereas if you use only the words immediately to the left and to the right of the word, then you likely will capture words that are very much related by their syntactical categories and semantics.\n4:41\nSo the general idea of discovering paradigmatical relations is to compute the similarity of context of two words. So here, for example, we can measure the similarity of cat and dog based on the similarity of their context. In general, we can combine all kinds of views of the context. And so the similarity function is, in general, a combination of similarities on different context. And of course, we can also assign weights to these different similarities to allow us to focus more on a particular kind of context. And this would be naturally application specific, but again, here the main idea for discovering pardigmatically related words is to computer the similarity of their context. So next let's see how we exactly compute these similarity functions. Now to answer this question, it is useful to think of bag of words representation as vectors in a vector space model.\n5:48\nNow those of you who have been familiar with information retrieval or textual retrieval techniques would realize that vector space model has been used frequently for modeling documents and queries for search. But here we also find it convenient to model the context of a word for paradigmatic relation discovery. So the idea of this approach is to view each word in our vocabulary as defining one dimension in a high dimensional space. So we have N words in total in the vocabulary, then we have N dimensions, as illustrated here. And on the bottom, you can see a frequency vector representing a context, and here we see where eats occurred 5 times in this context, ate occurred 3 times, et cetera. So this vector can then be placed in this vector space model. So in general, we can represent a pseudo document or context of cat as one vector, d1, and another word, dog, might give us a different context, so d2. And then we can measure the similarity of these two vectors. So by viewing context in the vector space model, we convert the problem of paradigmatical relation discovery into the problem of computing the vectors and their similarity.\n7:20\nSo the two questions that we have to address are first, how to compute each vector, and that is how to compute xi or yi.\n7:31\nAnd the other question is how do you compute the similarity.\n7:35\nNow in general, there are many approaches that can be used to solve the problem, and most of them are developed for information retrieval. And they have been shown to work well for matching a query vector and a document vector. But we can adapt many of the ideas to compute a similarity of context documents for our purpose here. So let's first look at the one plausible approach, where we try to match the similarity of context based on the expected overlap of words, and we call this EOWC.\n8:17\nSo the idea here is to represent a context by a word vector where each word has a weight that's equal to the probability that a randomly picked word from this document vector, is this word. So in other words, xi is defined as the normalized account of word wi in the context, and this can be interpreted as the probability that you would actually pick this word from d1 if you randomly picked a word.\n8:56\nNow, of course these xi's would sum to one because they are normalized frequencies,\n9:02\nand this means the vector is actually probability of the distribution over words.\n9:10\nSo, the vector d2 can be also computed in the same way, and this would give us then two probability distributions representing two contexts.\n9:24\nSo, that addresses the problem how to compute the vectors, and next let's see how we can define similarity in this approach. Well, here, we simply define the similarity as a dot product of two vectors, and this is defined as a sum of the products\n9:41\nof the corresponding elements of the two vectors.\n9:46\nNow, it's interesting to see that this similarity function actually has a nice interpretation, and that is this. Dot product, in fact that gives us the probability that two randomly picked words from the two contexts are identical. That means if we try to pick a word from one context and try to pick another word from another context, we can then ask the question, are they identical? If the two contexts are very similar, then we should expect we frequently will see the two words picked from the two contexts are identical. If they are very different, then the chance of seeing identical words being picked from the two contexts would be small. So this intuitively makes sense, right, for measuring similarity of contexts.\n10:41\nNow you might want to also take a look at the exact formulas and see why this can be interpreted as the probability that two randomly picked words are identical.\n10:57\nSo if you just stare at the formula to check what's inside this sum, then you will see basically in each case it gives us the probability that we will see an overlap on a particular word, wi. And where xi gives us a probability that we will pick this particular word from d1, and yi gives us the probability of picking this word from d2. And when we pick the same word from the two contexts, then we have an identical pick, right so. That's one possible approach, EOWC, extracted overlap of words in context. Now as always, we would like to assess whether this approach it would work well. Now of course, ultimately we have to test the approach with real data and see if it gives us really semantically related words.\n11:57\nReally give us paradigmatical relations, but analytically we can also analyze this formula a little bit. So first, as I said, it does make sense, right, because this formula will give a higher score if there is more overlap between the two contexts. So that's exactly what we want. But if you analyze the formula more carefully, then you also see there might be some potential problems, and specifically there are two potential problems. First, it might favor matching one frequent term very well, over matching more distinct terms.\n12:36\nAnd that is because in the dot product, if one element has a high value and this element is shared by both contexts and it contributes a lot to the overall sum,\n12:51\nit might indeed make the score higher than in another case, where the two vectors actually have a lot of overlap in different terms. But each term has a relatively low frequency, so this may not be desirable. Of course, this might be desirable in some other cases. But in our case, we should intuitively prefer a case where we match more different terms in the context, so that we have more confidence in saying that the two words indeed occur in similar context. If you only rely on one term and that's a little bit questionable, it may not be robust.\n13:34\nNow the second problem is that it treats every word equally, right. So if you match a word like the and it will be the same as matching a word like eats, but intuitively we know matching the isn't really surprising because the occurs everywhere. So matching the is not as such strong evidence as matching what a word like eats, which doesn't occur frequently. So this is another problem of this approach.\n14:13\nIn the next chapter we are going to talk about how to address these problems. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/gJTFA/10-2-text-clustering-generative-probabilistic-models-part-1-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about generating probabilistic models for text clustering.\n0:13\nIn this lecture, we're going to continue discussing text clustering, and we're going to introduce generating probabilistic models as a way to do text clustering. So this is the overall plan for covering text clustering. In the previous lecture, we have talked about what is text clustering and why text clustering is interesting. In this lecture, we're going to talk about how to do text clustering. In general, as you see on this slide, there are two kinds of approaches. One is generating probabilistic models, which is the topic of this lecture. And later, we'll also discuss similarity-based approaches.\n0:53\nSo to talk about generating models for text clustering, it would be useful to revisit the topic mining problem using topic models, because the two problems are very similar. This is a slide that you have seen earlier in the lecture on topic model. Here we show that we have input of a text collection C and a number of topics k, and vocabulary V. And we hope to generate as output two things. One is a set of topics denoted by Theta i's, each is awarded distribution and the other is pi i j. These are the probabilities that each document covers each topic. So this is a topic coverage and it's also visualized here on this slide. You can see that this is what we can get by using a topic model. Now, the main difference between this and the text clustering problem is that here, a document is assumed to possibly cover multiple topics. And indeed, in general, a document will be covering more than one topic with nonzero probabilities. In text clustering, however, we only allow a document to cover one topic, if we assume one topic is a cluster.\n2:24\nSo that means if we change the problem definition just slightly by assuming that each document that can only be generated by using precisely one topic.\n2:37\nThen we'll have a definition of the clustering problem as you'll hear. So here the output is changed so that we no longer have the detailed coverage distributions pi i j. But instead, we're going to have a cluster assignment decisions, Ci. And Ci is a decision for the document i. And C sub i is going to take a value from 1 through k to indicate one of the k clusters.\n3:09\nAnd basically tells us that d i is in which cluster. As illustrated here, we no longer have multiple topics covered in each document. It is precisely one topic. Although which topic is still uncertain. There is also a connection with\n3:29\nthe problem of mining one topic that we discussed earlier. So here again, it's a slide that you have seen before and here we hope to estimate a topic model or distribution based on precisely one document. And that's when we assume that this document, it covers precisely one topic.\n3:52\nBut we can also consider some variations of the problem. For example, we can consider there are N documents, each covers a different topic, so that's N documents, and topics. Of course, in this case, these documents are independent, and these topics are also independent. But, we can further allow these documents with share topics, and we can also assume that we are going to assume there are fewer topics than the number of documents, so these documents must share some topics. And if we have N documents that share k topics, then we'll again have precisely the document clustering problem.\n4:34\nSo because of these connections, naturally we can think about how to use a probabilistically generative model to solve the problem of text clustering.\n4:43\nSo the question now is what generative model can be used to do clustering?\n4:49\nAs in all cases of designing a generative model, we hope the generative model would adopt the output that we hope to generate or the structure that we hope to model. So in this case, this is a clustering structure, the topics and each document that covers one topic. And we hope to embed such preferences in the generative model. But, if you think about the main difference between this problem and the topic model that we talked about earlier. And you will see a main requirement is how can we force every document to be generated from precisely one topic, instead of k topics, as in the topic model?\n5:35\nSo let's revisit the topic model again in more detail. So this is a detailed view of a two component mixture model. When we have k components, it looks similar. So here we see that when we generate a document,\n5:53\nwe generate each word independent.\n5:57\nAnd when we generate each word, but first make a choice between these distributions. We decide to use one of them with probability. So p of theta 1 is the probability of choosing the distribution on the top. Now we first make this decision regarding which distribution should be used to generate the word. And then we're going to use this distribution to sample a word. Now note that in such a generative model, the decision on which distribution to use for each word is independent. So that means, for example, the here could have generated from the second distribution, theta 2 whereas text is more likely generated from the first one on the top.\n6:49\nThat means the words in the document that could have been generated in general from multiple distributions.\n6:58\nNow this is not what we want, as we said, for text clustering, for document clustering, where we hoped this document will be generated from precisely one topic.\n7:09\nSo now that means we need to modify the model. But how? Well, let's first think about why this model cannot be used for clustering. And as I just said, the reason is because it has allowed multiple topics to contribute a word to the document.\n7:28\nAnd that causes confusion because we're not going to know which cluster this document is from. And it's, more importantly it's violating our assumption about the partitioning of documents in the clusters. If we really have one topic to correspond it to one cluster of documents, then we would have a document that we generate from precisely one topic. That means all the words in the document must have been generated from precisely one distribution. And this is not true for such a topic model that we're seeing here. And that's why this cannot be used for clustering because it did not ensure that only one distribution has been used to generate all the words in one document.\n8:15\nSo if you realize this problem, then we can naturally design alternative mixture model for doing clustering. So this is what you're seeing here. And we again have to make a decision regarding which distribution to use to generate this document because the document could potentially be generated from any of the k word distributions that we have. But this time, once we have made a decision to choose one of the topics, we're going to stay with this regime to generate all the words in the document.\n8:49\nAnd that means, once we have made a choice of the distribution in generating the first word, we're going to go stay with this distribution in generating all of the other words in the document. So, in other words, we only make the choice once for, basically, we make the decision once for this document and this state was just to generate all the words. Similarly if I had choosing the second distribution, theta sub 2 here, you can see which state was this one. And then generate the entire document of d. Now, if you compare this picture with the previous one, you will see the decision of using a particular distribution is made just once for this document, in the case of document clustering. But in the case of topic model, we have to make as many decisions as the number of words in the document. Because for each word, we can make a potentially different decision. And that's the key difference between the two models.\n9:58\nBut this is obviously also a mixed model so we can just group them together as one box to show that this is the model that will give us a probability of the document. Now, inside of this model, there is also this switch of choosing a different distribution. And we don't observe that so that's a mixture model. And of course a main problem in document clustering is to infer which distribution has been used to generate a document and that would allow us to recover the cluster identity of a document.\n10:37\nSo it will be useful to think about the difference from the topic model as I have also mentioned multiple times.\n10:46\nAnd there are mainly two differences, one is the choice of\n10:56\nusing that particular distribution is made just once for document clustering. Whereas in the topic model, it's made it multiple times for different words. The second is that word distribution, here, is going to be used to regenerate all the words for a document.\n11:19\nBut, in the case of one distribution doesn't have to generate all the words in the document. Multiple distribution could have been used to generate the words in the document.\n11:34\nLet's also think about a special case, when one of the probability of choosing a particular distribution is equal to 1. Now that just means we have no uncertainty now. We just stick with one particular distribution. Now in that case, clearly, we will see this is no longer mixture model, because there's no uncertainty here and we can just use precisely one of the distributions for generating a document. And we're going back to the case of estimating one order distribution based on one document.\n12:12\nSo that's a connection that we discussed earlier. Now you can see it more clearly. So as in all cases of using a generative model to solve a problem, we first look at data and then think about how to design the model. But once we design the model, the next step is to write down the likelihood function. And after that we're going to look at the how to estimate the parameters.\n12:36\nSo in this case, what's the likelihood function? It's going to be very similar to what you have seen before in topic models but it will be also different.\n12:45\nNow if you still recall what the likelihood function looks like in then you will realize that in general, the probability of observing a data point from mixture model is going to be a sum of all the possibilities of generating the data.\n13:00\nIn this case, so it's going to be a sum over these k topics, because every one can be user generated document. And then inside is the sum you can still recall what the formula looks like, and it's going to be the product of two probabilities. One is the probability of choosing the distribution, the other is the probability of observing a particular datapoint from that distribution.\n13:27\nSo if you map this kind of formula to our problem here, you will see the probability of observing a document d\n13:37\nis basically a sum in this case of two different distributions because we have a very simplified situation of just two clusters. And so in this case, you can see it's a sum of two cases. In each case, it's indeed the probability of choosing the distribution either theta 1 or theta 2. And then, the probability is multiplied by the probability of observing this document from this particular distribution.\n14:16\nAnd if you further expanded this probability of observing the whole document, we see that it's a product of observing each word X sub i. And here we made the assumption that each word is generated independently, so the probability of the whole document is just a product of the probability of each word in the document.\n14:40\nSo this form should be very similar to the topic model. But it's also useful to think about the difference and for that purpose, I am also copying the probability of topic model of these two components here. So here you can see the formula looks very similar or in many ways, they are similar.\n15:02\nBut there is also some difference.\n15:06\nAnd in particular, the difference is on the top. You see for the mixture model for document clustering, we first take a product, and then take a sum.\n15:16\nAnd that's corresponding to our assumption of first make a choice of choosing one distribution and then stay with the distribution, it'll generate all the words. And that's why we have the product inside the sum.\n15:30\nThe sum corresponds to the choice. Now, in topic model, we see that the sum is actually inside the product. And that's because we generated each word independently. And that's why we have the product outside, but when we generate each word we have to make a decision regarding which distribution we use so we have a sum there for each word. But in general, these are all mixture models and we can estimate these models by using the Algorithm, as we will discuss more later. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/15dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 15\nWeek 15\n20 min of readings left\n2 graded assignments left\nTake Exam 2, and continue working on your Course Project!\nExam 2\n2 graded assignments left\nHow to Schedule and Take the Exam\nReading•\n. Duration: 10 minutes\n10 min\nExam Policies and Technical Support\nReading•\n. Duration: 10 minutes\n10 min\nProctorU Password - Exam 2\nQuiz•1 question\n•Grade: --\nAvailable November 28, 10:00 AM EST - December 4, 11:00 PM EST\nExam 2\nQuiz•20 questions\n•Grade: --\nAvailable November 28, 10:00 AM EST - December 4, 11:00 PM EST'])
https://www.coursera.org/learn/cs-410/supplement/KdTdY/week-10-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 10\nWeek 10 Overview\nPrevious\nNext\nWeek 10 Information\nReading:\nReading\nWeek 10 Overview\n. Duration: 10 minutes\n10 min\nWeek 10 Lessons\nWeek 10 Activities\nWeek 10 Overview\nDuring this week's lessons, you will learn text clustering, including the basic concepts, main clustering techniques, and how to evaluate text clustering. You will also start learning text categorization, which is related to text clustering, but with pre-defined categories that can be viewed as pre-defining clusters.\nTime\nThis module should take approximately 3 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with required assignments in bold):\nActivity\nEstimated Time Required\nWeek 10 Video Lectures\n2 hours\nWeek 10 Graded Quiz\n1 hour\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain the concept of text clustering and why it is useful.\nExplain how Hierarchical Agglomerative Clustering and k-Means clustering work.\nExplain how to evaluate text clustering.\nExplain the concept of text categorization and why it is useful.\nExplain how Naïve Bayes classifier works.\nGuiding Questions\nDevelop your answers to the following guiding questions while watching the video lectures throughout the week.\nWhat is clustering? What are some applications of clustering in text mining and analysis?\nHow does hierarchical agglomerative clustering work? How do single-link, complete-link, and average-link work for computing group similarity? Which of these three ways of computing group similarity is least sensitive to outliers in the data? \nHow do we evaluate clustering results? \nWhat is text categorization? What are some applications of text categorization? \nWhat does the training data for categorization look like? \nHow does the Naïve Bayes classifier work? \nWhy do we often use logarithm in the scoring function for Naïve Bayes?  \nAdditional Readings and Resources\nThe following readings are optional:\nC. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM and Morgan & Claypool Publishers, 2016. Chapters 14 & 15.\nManning, Chris D., Prabhakar Raghavan, and Hinrich Schütze. Introduction to Information Retrieval. Cambridge: Cambridge University Press, 2007. Chapters 13-16.\nYang, Yiming. An Evaluation of Statistical Approaches to Text Categorization. Inf. Retr. 1, 1-2 (May 1999), 69-90. doi: 10.1023/A:1009982220290\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nClustering, document clustering, and term clustering\nClustering bias\nPerspective of similarity\nHierarchical Agglomerative Clustering, and k-Means\nDirection evaluation (of clustering), indirect evaluation (of clustering)\nText categorization, topic categorization, sentiment categorization, email routing\nSpam filtering\nNaïve Bayes classifier\nSmoothing\nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issue\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/PsyKR/10-5-text-clustering-similarity-based-approachesdict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.5 Text Clustering: Similarity-based Approaches\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.5 Text Clustering: Similarity-based Approaches\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC] This lecture is about the similarity-based approaches to text clustering.\n0:13\nIn this lecture we're going to to continue the discussion of how to do a text clustering.\n0:18\nIn particular, we're going to to cover different kinds of approaches than generative models, and that is similarity-based approaches. So the general idea of similarity-based clustering is to explicitly specify a similarity function to measure the similarity between two text objects. Now this is in contrast with a generative model where we implicitly define the clustering bias by using a particular object to function like a [INAUDIBLE] function.\n0:52\nThe whole process is driven by optimizing the [INAUDIBLE,] but here we explicitly provide a view of what we think are similar. And this is often very useful because then it allows us to inject any particular view of similarity into the clustering program. So once we have a similarity function, we can then aim at optimally partitioning, to partitioning the data into clusters or into different groups. And try to maximize the inter-group similarity and minimize the inter-group similarity. That is to ensure the objects that are put into the same group to be similar, but the objects that are put into different groups to be not similar. And these are the general goals of clustering, and there is often a trade off between achieving both goals. Now there are many different methods for doing similarity based clustering, and in general I think we can distinguish the two strategies at high level. One is to progressively construct the hierarchy of clusters, and so this often leads to hierarchical clustering. And we can further distinguish it two ways, to construct a hierarchy depending on whether we started with the collection to divide the connection. Or started with individual objectives and gradually group them together, so one is bottom-up that can be called agglomerative. Well we gradually group a similar objects into larger and larger clusters. Until we group everything together, the other is top-down or divisive, in this case we gradually partition the whole data set into smaller and smaller clusters. The other general strategy is to start with the initial tentative clustering and then iteratively improve it. And this often leads for a flat clustering, one example is k-Means, so as I just said, there are many different clustering methods available. And a full coverage of all the clustering methods would be beyond the scope of this course. But here we are going to talk about the two representative methods, in some detail\n3:14\none is Hierarchical Agglomerative Clustering or HAC, the other is k-Means. So first of it we'll get the agglomerative hierarchical clustering, in this case, we're given a similarity function to measure similarity between two objects. And then we can gradually group similar objects together in a bottom-up fashion to form larger and larger groups. And they always form a hierarchy, and then we can stop when some stopping criterion is met. It could be either some number of clusters has been achieved or the threshold for similarity has been reached.\n3:52\nThere are different variations here, and they mainly differ in the ways to compute a group similarity. Based on the individual objects similarity, so let's illustrate how again induced a structure based on just similarity. So start with all the text objects and we can then measure the similarity between them. Of course based on the provided similarity function, and then we can see which pair has the highest similarity. And then just group them together, and then we're going to see which pair is the next one to group.\n4:30\nMaybe these two now have the highest similarity, and then we're going to gradually group them together. And then every time we're going to pick the highest similarity, the similarity of pairs to group.\n4:45\nThis will give us a binary tree eventually to group everything together.\n4:50\nNow, depending on our applications, we can use the whole hierarchy as a structure for browsing, for example. Or we can choose a cutoff, let's say cut here to get four clusters, or we can use a threshold to cut. Or we can cut at this high level to get just two clusters, so this is a general idea, now if you think about how to implement this algorithm. You'll realize that we have everything specified except for how to compute group similarity.\n5:24\nWe are only given the similarity function of two objects, but as we group groups together, we also need to assess the similarity between two groups. There are also different ways to do that and there are the three popular methods. Single-link, complete-link, and average-link, so given two groups and the single-link algorithm. Is going to define the group similarity as the similarity of the closest pair of the two groups. Complete-link defines the similarity of the two groups as the similarity of the farthest system pair. Average-link defines the similarity as average of similarity of all the pairs of the two groups. So it's much easier to understand the methods by illustrating them, so here are two groups, g1 and g2 with some objects in each group. And we know how to compute the similarity between two objects, but the question now is, how can we compute the similarity between the two groups?\n6:29\nAnd then we can in general base this on the similarities of the objects in the two groups.\n6:35\nSo, in terms of single-link and we're just looking at the closest pair so in this case, these two paired objects will defined the similarities of the two groups.\n6:47\nAs long as they are very close, we're going to say the two groups are very\n6:51\nclose so it is an optimistic view of similarity.\n6:57\nThe complete link on the other hand were in some sense pessimistic, and by taking the similarity of the two farthest pair as the similarity for the two groups. So we are going to make sure that if the two groups are having a high similarity. Then every pair of the two groups, or the objects in the two groups will have, will be ensured to have high similarity.\n7:29\nNow average link is in between, so it takes the average of all these pairs. Now these different ways of computing group similarities will lead to different clustering algorithms. And they would in general give different results, so it's useful to take a look at their differences and to make a comparison.\n7:53\nFirst, single-link can be expected to generally the loose clusters, the reason is because as long as two objects are very similar in the two groups, it will bring the two groups together.\n8:09\nIf you think about this as similar to having parties with people, then it just means two groups of people would be partying together. As long as in each group there is a person that\n8:27\nis well connected with the other group. So the two leaders of the two groups can have a good relationship with each other and then they will bring together the two groups. In this case, the cluster is loose, because there's no guarantee that other members of the two groups are actually very close to each other. Sometimes they may be very far away, now in this case it's also based on individual decisions, so it could be sensitive to outliers. The complete-link is in the opposite situation, where we can expect the clusters to be tight. And it's also based on individual decision so it can be sensitive to outliers. Again to continue the analogy to having a party of people, then complete-link would mean when two groups come together. They want to ensure that even\n9:21\nthe people that are unlikely to talk to each other would be comfortable. Always talking to each other, so ensure the whole class to be coherent. The average link of clusters in between and as group decision, so it's\n9:37\ngoing to be insensitive to outliers, now in practice which one is the best. Well, this would depend on the application and sometimes you need a lose clusters. And aggressively cluster objects together that maybe single-link is good. But other times you might need a tight clusters and a complete-link might be better. But in general, you have to empirically evaluate these methods for your application to know which one is better.\n10:07\nNow, next let's look at another example of a method for similarity-based clustering. In this case, which is called k-Means clustering, we will represent each text object as a term vector. And then assume a similarity function defined on two objects, now we're going to start with some tentative clustering results by just selecting k randomly. selected vectors as centroids of k clusters and treat them as centers as if they represent, they each represent a cluster. So this gives us the initial tentative cluster, then we're going to iteratively improve it. And the process goes like this, and once we have these centroids Decide. We're going to assign a vector to the cluster whose centroid is closest to the current vector. So basically we're going to measure the distance between this vector, and each of the centroids, and see which one is the closest to this one. And then just put this object into that cluster, this is to have tentative assignment of objects into clusters. And we're going to partition all the objects into k clusters based on our tentative clustering and centroids.\n11:28\nThen we can do re-compute the centroid based on the locate the object in each cluster. And this is to adjust the centroid, and then we can repeat this process until the similarity-based objective function. In this case, it's within cluster sum of squares converges, and theoretically we can show that. This process actually is going to minimize the within cluster sum of squares where define object and function. Given k clusters, so it can be also shown, this process will converge to a local minimum. I think about this process for a moment, it might remind you the Algorithm for mixture model.\n12:13\nIndeed this algorithm is very similar to the Algorithm for the mixture model for clustering. More specifically we also initialize these parameters in the Algorithm so the random initialization is similar.\n12:34\nAnd then in the Algorithm, you may recall that, we're going to repeat E-step and M-step to improve our parameter estimation. In this case, we're going to improve the clustering result iteratively by also doing two steps. And in fact that the two steps are very similar to Algorithm, in that when we locate the vector into one of the clusters based on our tentative clustering. It's very similar to inferring the distribution that has been used to generate the document, the mixture model. So it is essentially similar to E-step, so what's the difference, well the difference is here. We don't make a probabilistic allocation as in the case of E-step, the brother will make a choice. We're going to make a call if this, there upon this closest to cluster two, then we're going to say you are in cluster two. So there's no choice, and we're not going to say, you assume the set is belonging to a cluster two. And so we're not going to have a probability, but we're just going to put one object into precisely one cluster. In the E-step however, we do a probability location, so we split in counts. And we're not going to say exactly which distribution has been used to generate a data point. Now next, we're going to adjust the centroid, and this is very similar to M-step where we re-estimate the parameters. That's when we'll have a better estimate of the parameter, so here we'll have a better clustering result by adjusting the centroid. And note that centroid is based on the average of the vectors in the cluster. So this is also similar to the M-step where we do counts,pull together counts and then normalize them. The difference of course is also because of the difference in the E-step, and we're not going to consider probabilities when we count the points. In this case, k-Means we're going to all make count of the objects as allocated to this cluster. And this is only a subset of data points, but in the Algorithm, we in principle consider all the data points based on probabilistic allocations.\n14:56\nBut in nature they are very similar and that's why it's also maximizing well defined object of functions. And it's guaranteed to convert local minimum, so to summarize our discussion of clustering methods. We first discussed model based approaches, mainly the mixture model. Here we use the implicit similarity function to define the clustering bias. There is no explicit define similarity function, the model defines clustering bias and the clustering structure is built into a generative model. That's why we can use potentially a different model to recover different structure.\n15:40\nComplex generative models can be used to discover complex clustering structures. We do not talk about in full, but we can easily design, generate a model to generate a hierarchical clusters. We can also use prior to further customize the clustering algorithm to for example control the topic of one cluster or multiple clusters. However one disadvantage of this approach is that there is no easy way to directly control the similarity measure.\n16:11\nSometimes we want to that, but it's very hard to inject such a special definition of similarity into such a model.\n16:20\nWe also talked about similarity-based approaches, these approaches are more flexible to actually specify similarity functions.\n16:29\nBut one major disadvantage is that their objective function is not always very clear.\n16:35\nThe k-Means algorithm has clearly defined the objective function, but it's also very similar to a model based approach. The hierarchical clustering algorithm on the other hand is harder to specify the objective function. So it's not clear what exactly is being optimized,\n17:00\nboth approaches can generate term clusters. And document clusters, and term clusters can be in general, generated by representing each term with some text content. For example, take the context of each term as a representation of each term, as we have done in semantic relation learning. And then we can certainly cluster terms, based on actual text [INAUDIBLE]. Of course, term clusters can be generated by using generative models as well, as we've seen. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/vtHNZ/course-project-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 1\nCourse Project Overview\nPrevious\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nCourse Project Overview\nIntroduction\nThe course project is to give the students hands-on experience on developing some novel information retrieval and/or text mining tools. It would allow the students to potentially apply all the knowledge and skills learned in the course to solve a real-world problem. Group work is encouraged, but not required (i.e., you can have a one-person team). The maximum size of a team is 5 members to avoid challenges in efficient coordination of the work by too many team members. However, a team of a larger size is also possible subject to the approval of the instructor. A typical reason for a larger team is because the project has a very natural task division among the team members so that the need for frequent interactions and coordination of team members may be minimum despite the large size of the team. Whenever possible, collaboration of multiple project teams is strongly encouraged to minimize the amount of work of each team via expertise or resource sharing, as well as to generate “combined impact” (e.g., one team may develop a crawler that can be used by another team that develops a search engine).\nMore details (what to submit at each step) can be found here: \nhttps://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit?usp=sharing\nGrading criteria\nYour project will be graded based on the following weighting scheme, corresponding to three stages of work including 1) team  search and formation; 2) proposal development; 3) project result submission. All team members will receive the same grade provided that every member has made sufficient effort (at least 20 hours of quality time to work on the project).\nTeam formation (): Every student is required to form a team by the beginning of Week 9 (Oct 17, 2022). The team shall need to designate one of the team members as the team leader who will be responsible for submitting and/or uploading the course project deliverables.\nProject proposal (): Each project team is required to submit a roughly one-page project proposal by the end of Week 9 (Oct 23, 2022), and graded based on completion. Specifically, the following grades will be assigned, based on the answering of questions from the Google Doc:\n100: Answers all questions\n90: Missing one or two questions\n75: Missing more than two questions, significantly incomplete, cannot understand\n0: No proposal in repository\nProgress report (): Each project team is required to submit a short progress report by the beginning of Week 13 (Nov 14, 2022) listing the progress made, remaining tasks, any challenges/issues being faced, etc. It will be graded based on completion with respect to addressing the following points: (1) Progress made thus far, (2) Remaining tasks, (3)Any challenges/issues being faced. Grades will be assigned as follows:\n100: Addresses all points\n90: Missing one point\n75: Missing two points\n0: Missing all points or no submission\nSoftware code submission with documentation (): Due during  Week 16 (Dec 08, 2022), each project team will be asked to submit the produced source code with reasonable documentation. The documentation should cover both how to use the software and how the software is implemented. The  of the grade would be distributed as follows:  for source code submission;  for documentation submission. Both would be graded based on completion. \nSoftware usage tutorial presentation (): Due during Week 15 (Dec 08, 2022).  of the grading is based on completion and the remaining  is based on result of testing the software by graders\nThus, your course project work would be graded primarily based on your effort. If you have followed our guidelines and completed all the required tasks, you should receive at least  of the total points for the project. This is to encourage the students to pay attention to time management and set realistic goals that can actually be completed by the end of the semester. The remaining  is based on how well your software works; a fully functioning software would be given the whole , whereas a buggy software or a software with missing functions would result in losing some of the  of the grade. Completion of a functioning software is emphasized also due to the potential dependency between multiple projects when they are all contributing to a larger project (e.g., one team may produce a crawler to crawl data for use by another team to build a search engine).\nThe proposal, progress report and final submission will be peer-graded by you. The TAs will review all peer reviews and make sure that (1) the reviews are fair, and (2) the submissions satisfy all requirements. \nPeer-Grading Instructions\nWe'll be using peer-grading for grading project proposals, progress reports and final submissions. Later in the semester, you will receive an email from CMT inviting you as a reviewer, and we will make an announcement when the emails are sent. \nPeer grading will begin soon after the submission deadlines have passed. \nThe proposal peer-grading should be done between Oct 24 (12:00 am CST)-Oct 30 (11:59pm CST). \nThe progress report peer-grading should be done between Nov 15 (12:00 am CST)-Nov 20 (11:59 pm CST). \nThe final submissions peer-grading should be done between Dec 08 (12:00 am CST)-Dec 16 (11:59 pm CST). \nPlease finish the gradings on time.\nEach project will be reviewed by at least 2 students and each student will review ~2-3 projects. We will be using your comments and scores as a guide to assign the final scores to the projects. Please grade them carefully and honestly. We appreciate your help with managing the grading of such a large class. We hope the process would also be a good learning experience for you. More specific instructions will be announced and sent later. \nInstructions\n1. Form a team (,  due beginning of Week 9, Oct 17, 2022)\nBy the beginning of Week 9, you should form teams.  Groups of more than 5 members are highly discouraged. The team also needs to designate a member of the team as team leader. Your team leader will be responsible for submitting and/or uploading the course project deliverables such as: 1) this form, 2) the proposal, 3) the presentation, 4) the code, etc. \n2. Pick a topic & Write a proposal (, due end of Week 9, Oct 23 2022)\nPicking a project topic\nStudents will be able to choose their own topic or select a topic from a list of suggested topics which we will provide by Week 4. \nWriting a project proposal\nEach project team is required to write a one-page proposal before you actually go in depth on a topic. The proposal is due end of Week 9.\nIn the proposal, you should include the names and email addresses of all the team members. One member must be designated as the project coordinator/leader for the team, so please make sure to indicate that. The project coordinator would be responsible for the coordination of the project work by the team and also communication with the instructor or TA when the team needs help. \nDetailed instructions about the required content of the proposal will be provided by Week 4, including a short set of questions that need to be answered in the proposal. As long as those questions are addressed (wherever applicable), the proposal does not have to be very long. A couple of sentences for each question would be sufficient.\n3. Peer-review project proposals (due Middle of Week 10, Oct 30 2022)\nEach student will review the progress reports of ~1-2 groups and provide feedback/suggestions.  TAs will go through the peer assessments and make sure that the proposals satisfies all requirements listed in the Google Doc.\n4. Work on the project\nYou should try to reuse any existing tools as much as possible so as to minimize the amount of work without sacrificing your goal. Discuss any problems or issues with your teammates or classmates on Campuswire and leverage Campuswire to collaborate with each other. Consistent with our course policy, we strongly encourage you to help each other in all the course work so as to maximize your gain of new knowledge and skills while minimizing your work as much as possible. We will do our best to help you as well. Consider documenting your work regularly. This way, you will already have a lot of things written down by the end of the semester.\n5. Submit progress report (, due beginning of Week 13, Nov 14 2022)\nEach team must submit a short report detailing 1) Progress made thus far, 2) Remaining tasks, 3) Any challenges/issues being faced. The graders, TAs and instructor may provide help and suggestions based on the report. Continue working on the project until the final submission.\n6. Peer-review progress reports (due end of Week 13, Nov 20 2022)\nEach student will review the progress reports of ~1-2 groups and provide feedback/suggestions. \n7. Software code submission with documentation (, due middle of Week 16, Dec 08 2022)\nEach team must submit the software code produced for the project along with a written documentation. The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. 4) Brief description of contribution of each team member in case of a multi-person team. Note that if you are in a team, it is your responsibility to figure out how to contribute to your group project, so you will need to act proactively and in a timely manner if your group coordinator has not assigned a task to you. There will be no opportunity to make up for any task that you failed to accomplish. In general, all the members of a team will get the same grade for the project unless the documentation submission indicates that some member(s) only superficially participated in the project without doing much actual work; in that case, we will discount the grade. Everyone is expected to spend at least 20 hours to seriously work on your course project as a minimum, not including the time spent for preparing the documentation.\nThe  of the grade would be distributed as follows:  for source code submission;  for documentation submission. The  for the documentation submission includes  for overview of functions,  for implementation documentation,  for usage documentation. There is no strict length requirement for the documentation.\n8. Software usage tutorial presentation (, due middle of Week 16, Dec 08 2022)\nAt the end of the semester, every project team will be asked to submit a short tutorial presentation (e.g., a voiced ppt presentation) to explain how the developed software is to be used. The presentation must include (1) sufficient instructions on how to install the software if applicable, (2) sufficient instructions on how to use the software, and (3) at least one example of use case so as to allow a grader to use the provided use case to test the software. There is no strict length requirement for this video submission, but you should target at 5~10 minutes. A presentation shorter than 5 minutes is unlikely detailed enough to help users understand how to use the software, whereas a longer video than 10 minutes might be too long for impatient users. However, feel free to produce a longer presentation if needed.\nThe tutorial presentation would be graded based on\n1) completion of the presentation (); and\n2) result of testing the software by graders ().\nIf the software passes the test (i.e., is working as expected), full points will be given; otherwise, points will be deducted from the  allocated to the “result of testing the software by graders.”\n9. Peer-review project code, documentation, and presentations (due Dec 16 2022)\nEach student will review the final project submissions of ~2-3 groups and provide feedback/suggestions. \nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/home/week/11dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 11\nWeek 11\n2h of videos left\n10 min of readings left\nAll graded assignments completed\nDuring this module, you will continue learning about various methods for text categorization, particularly discriminative qualifiers, and you will also learn sentiment analysis and opinion mining, including a detailed introduction to a particular technique for sentiment classification (i.e., ordinal regression).\nWeek 11 Information\nWeek 11 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 11 Lessons\n11.1 Text Categorization: Discriminative Classifier Part 1\nVideo•\n. Duration: 20 minutes\n20 min\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\nVideo•\n. Duration: 31 minutes\n31 min\n11.3 Text Categorization: Evaluation Part 1\nVideo•\n. Duration: 14 minutes\n14 min\n11.4 Text Categorization: Evaluation Part 2\nVideo•\n. Duration: 10 minutes\n10 min\n11.5 Opinion Mining and Sentiment Analysis: Motivation\nVideo•\n. Duration: 17 minutes\n17 min\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\nVideo•\n. Duration: 11 minutes\n11 min\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\nVideo•\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\nComplete\nWeek 11 Practice Quiz\nPractice Quiz•11 questions\nWeek 11 Quiz\nGraded\nQuiz•8 questions\n•Grade: 87.\nTechnology Review Submission\nGraded\nGraded External Tool•Submitted\n•Grade: '])
https://www.coursera.org/learn/cs-410/lecture/b1ZFI/8-3-syntagmatic-relation-discovery-mutual-information-part-1dict_values(['List\nCS 410: Text Information Systems\nWeek 8\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]. This lecture is about the syntagmatic relation discovery and mutual information.\n0:13\nIn this lecture we are going to continue discussing syntagmatic relation discovery. In particular, we are going to talk about another the concept in the information series, we called it mutual information and how it can be used to discover syntagmatic relations. Before we talked about the problem of conditional entropy and that is the conditional entropy computed different pairs of words. It is not really comparable, so that makes it harder with this cover, strong synagmatic relations globally from corpus. So now we are going to introduce mutual information, which is another concept in the information series that allows us to, sometimes, normalize the conditional entropy to make it more comparable across different pairs.\n1:04\nIn particular, mutual information in order to find I(X:Y), matches the entropy reduction of X obtained from knowing Y. More specifically the question we are interested in here is how much of an entropy of X can we obtain by knowing Y.\n1:27\nSo mathematically it can be defined as the difference between the original entropy of X, and the condition of Y of X given Y.\n1:37\nAnd you might see, as you can see here it can also be defined as reduction of entropy of Y because of knowing X.\n1:48\nNow normally the two conditional interface H of X given Y and the entropy of Y given X are not equal, but interestingly, the reduction of entropy by knowing one of them, is actually equal. So, this quantity is called a Mutual Information in order to buy I here. And this function has some interesting properties, first it is also non-negative. This is easy to understand because the original entropy is always\n2:22\nnot going to be lower than the possibility reduced conditional entropy. In other words, the conditional entropy will never exceed the original entropy. Knowing some information can always help us potentially, but will not hurt us in predicting x.\n2:41\nThe signal property is that it is symmetric like additional entropy is not symmetrical, mutual information is, and the third property is that It reaches its minimum, zero, if and only if the two random variables are completely independent. That means knowing one of them does not tell us anything about the other and this last property can be verified by simply looking at the equation above and it reaches 0 if and only the conditional entropy of X [INAUDIBLE] Y is exactly the same as original entropy of X. So that means knowing why it did not help at all and that is when X and a Y are completely independent.\n3:32\nNow when we fix X to rank different Ys using conditional entropy would give the same order as ranking based on mutual information because in the function here, H(X) is fixed because X is fixed. So ranking based on mutual entropy is exactly the same as ranking based on the conditional entropy of X given Y, but the mutual information allows us to compare different pairs of x and y. So, that is why mutual information is more general and in general, more useful.\n4:10\nSo, let us examine the intuition of using mutual information for Syntagmatical Relation Mining.\n4:17\nNow, the question we ask forcing that relation mining is, whenever "eats" occurs, what other words also tend to occur?\n4:25\nSo this question can be framed as a mutual information question, that is, which words have high mutual information was eats, so computer the missing information between eats and other words.\n4:39\nAnd if we do that, and it is basically a base on the same as conditional we will see that words that are strongly associated with eats, will have a high point. Whereas words that are not related will have lower mutual information. For this, I will give some example here. The mutual information between "eats" and "meats", which is the same as between "meats" and "eats," because the information is symmetrical is expected to be higher than the mutual information between eats and the, because knowing the does not really help us as a predictor. It is similar, and knowing eats does not help us predicting, the as well. And you also can easily see that the mutual information between a word and itself is the largest, which is equal to the entropy of this word and so, because in this case the reduction is maximum because knowing one allows us to predict the other completely. So the conditional entropy is zero, therefore the mutual information reaches its maximum. It is going to be larger, then are equal to the machine volume eats in other words. In other words picking any other word and the computer picking between eats and that word. You will not get any information larger the computation from eats and itself.\n6:16\nSo now let us look at how to compute the mute information. Now in order to do that, we often\n6:25\nuse a different form of mutual information, and we can mathematically rewrite the mutual information into the form shown on this slide. Where we essentially see a formula that computes what is called a KL-divergence or divergence. This is another term in information theory. It measures the divergence between two distributions.\n6:50\nNow, if you look at the formula, it is also sum over many combinations of different values of the two random variables but inside the sum, mainly we are doing a comparison between two joint distributions. The numerator has the joint, actual observed the joint distribution of the two random variables.\n7:12\nThe bottom part or the denominator can be interpreted as the expected joint distribution of the two random variables, if they were independent because when two random variables are independent, they are joined distribution is equal to the product of the two probabilities.\n7:35\nSo this comparison will tell us whether the two variables are indeed independent. If they are indeed independent then we would expect that the two are the same,\n7:44\nbut if the numerator is different from the denominator, that would mean the two variables are not independent and that helps measure the association.\n7:56\nThe sum is simply to take into consideration of all of the combinations of the values of these two random variables. In our case, each random variable can choose one of the two values, zero or one, so we have four combinations here. If we look at this form of mutual information, it shows that the mutual information matches the divergence of the actual joint distribution from the expected distribution under the independence assumption. The larger this divergence is, the higher the mutual information would be.\n8:33\nSo now let us further look at what are exactly the probabilities, involved in this formula of mutual information.\n8:41\nAnd here, this is all the probabilities involve, and it is easy for you to verify that. Basically, we have first to [INAUDIBLE] probabilities corresponding to the presence or absence of each word. So, for w1, we have two probabilities shown here.\n9:02\nThey should sum to one, because a word can either be present or absent. In the segment, and similarly for the second word, we also have two probabilities representing presence or absences of this word, and there is some to y as well.\n9:21\nAnd finally, we have a lot of joined probabilities that represent the scenarios of co-occurrences of the two words, and they are shown here.\n9:34\nAnd they sum to one because the two words can only have these four possible scenarios. Either they both occur, so in that case both variables will have a value of one, or one of them occurs. There are two scenarios.\n9:51\nIn these two cases one of the random variables will be equal to one and the other will be zero and finally we have the scenario when none of them occurs. This is when the two variables taking a value of zero.\n10:07\nSo these are the probabilities involved in the calculation of mutual information, over here.\n10:16\nOnce we know how to calculate these probabilities, we can easily calculate the mutual information.\n10:24\nIt is also interesting to know that there are actually some relations or constraint among these probabilities, and we already saw two of them, right? So in the previous slide, that you have seen that the marginal probabilities of these words sum to one and we also have seen this constraint, that says the two words have these four scenarios of co-occurrency, but we also have some additional constraints listed in the bottom.\n10:58\nFor example, this one means if we add up the probabilities that we observe the two words occur together and the probabilities when the first word occurs and the second word does not occur. We get exactly the probability that the first word is observed. In other words, when the word is observed. When the first word is observed, and there are only two scenarios, depending on whether the second word is also observed. So, this probability captures the first scenario when the second word actually is also observed, and this captures the second scenario when the second word is not observed. So, we only see the first word, and it is easy to see the other equations also follow the same reasoning.\n11:46\nNow these equations allow us to compute some probabilities based on other probabilities, and this can simplify the computation.\n11:55\nSo more specifically, if we know the probability that a word is present, like in this case, so if we know this, and if we know the probability of the presence of the second word, then we can easily compute the absence probability, right? It is very easy to use this equation to do that, and so we take care of the computation of these probabilities of presence and absence of each word. Now let\'s look at the [INAUDIBLE] distribution. Let us assume that we also have available the probability that they occurred together. Now it is easy to see that we can actually compute all the rest of these probabilities based on these.\n12:46\nSpecifically for example using this equation we can compute the probability that the first word occurred and the second word did not, because we know these probabilities in the boxes, and similarly using this equation we can compute the probability that we observe only the second word. Word. And then finally, this probability can be calculated by using this equation because now this is known, and this is also known, and this is already known, right. So this can be easier to calculate. So now this can be calculated.\n13:26\nSo this slide shows that we only need to know how to compute these three probabilities that are shown in the boxes, naming the presence of each word and the co-occurence of both words, in a segment. [MUSIC]\nLike\nDislike\nReport an issue\nShare'])
https://www.coursera.org/learn/cs-410/lecture/EbbsO/9-1-probabilistic-topic-models-mixture-of-unigram-language-modelsdict_values(['List\nCS 410: Text Information Systems\nWeek 9\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:06\nThis lecture is about the mixture of unigram language models.\n0:11\nIn this lecture we will continue discussing probabilistic topic models. In particular, what we introduce a mixture of unigram language models. This is a slide that you have seen earlier. Where we talked about how to get rid of the background words that we have on top of for one document.\n0:36\nSo if you want to solve the problem, it would be useful to think about why we end up having this problem. Well, this obviously because these words are very frequent in our data and we are using a maximum likelihood to estimate. Then the estimate obviously would have to assign high probability for these words in order to maximize the likelihood. So, in order to get rid of them that would mean we\'d have to do something differently here.\n1:05\nIn particular we\'ll have to say this distribution doesn\'t have to explain all the words in the tax data. What were going to say is that, these common words should not be explained by this distribution. So one natural way to solve the problem is to think about using another distribution to account for just these common words. This way, the two distributions can be mixed together to generate the text data. And we\'ll let the other model which we\'ll call background topic model to generate the common words. This way our target topic theta here will be only generating the common handle words that are characterised the content of the document.\n1:52\nSo, how does this work? Well, it is just a small modification of the previous setup where we have just one distribution. Since we now have two distributions, we have to decide which distribution to use when we generate the word. Each word will still be a sample from one of the two distributions.\n2:13\nText data is still generating the same way. Namely, look at the generating of the one word at each time and eventually we generate a lot of words. When we generate the word, however, we\'re going to first decide which of the two distributions to use. And this is controlled by another probability, the probability of theta sub d and the probability of theta sub B here.\n2:41\nSo this is a probability of enacting the topic word of distribution. This is the probability of enacting the background word\n2:52\nof distribution denoted by theta sub B.\n2:55\nOn this case I just give example where we can set both to 0.5. So you\'re going to basically flip a coin, a fair coin, to decide what you want to use. But in general these probabilities don\'t have to be equal. So you might bias toward using one topic more than the other. So now the process of generating a word would be to first we flip a coin. Based on these probabilities choosing each model and if let\'s say the coin shows up as head, which means we\'re going to use the topic two word distribution. Then we\'re going to use this word distribution to generate a word. Otherwise we might be going slow this path.\n3:41\nAnd we\'re going to use the background word distribution to generate a word.\n3:46\nSo in such a case, we have a model that has some uncertainty associated with the use of a word distribution. But we can still think of this as a model for generating text data. And such a model is called a mixture model.\n4:02\nSo now let\'s see. In this case, what\'s the probability of observing a word w? Now here I showed some words. like "the" and "text". So as in all cases, once we setup a model we are interested in computing the likelihood function. The basic question is, so what\'s the probability of observing a specific word here? Now we know that the word can be observed from each of the two distributions, so we have to consider two cases. Therefore it\'s a sum over these two cases.\n4:34\nThe first case is to use the topic for the distribution to generate the word. And in such a case then the probably would be theta sub d, which is the probability of choosing the model multiplied by the probability of actually observing the word from that model. Both events must happen in order to observe. We first must have choosing the topic theta sub d and then, we also have to actually have sampled the word the from the distribution. And similarly, the second part accounts for a different way of generally the word from the background.\n5:15\nNow obviously the probability of text the same is all similar, right? So we also can see the two ways of generating the text. And in each case, it\'s a product of the probability of choosing a particular word is multiplied by the probability of observing the word from that distribution.\n5:35\nNow whether you will see, this is actually a general form. So might want to make sure that you have really understood this expression here. And you should convince yourself that this is indeed the probability of obsolete text. So to summarize what we observed here. The probability of a word from a mixture model is a general sum of different ways of generating the word.\n6:00\nIn each case, it\'s a product of the probability of selecting that component model. Multiplied by the probability of actually observing the data point from that component of the model. And this is something quite general and you will see this occurring often later. So the basic idea of a mixture model is just to retrieve thesetwo distributions together as one model. So I used a box to bring all these components together. So if you view this whole box as one model, it\'s just like any other generative model. It would just give us the probability of a word.\n6:42\nBut the way that determines this probability is quite the different from when we have just one distribution.\n6:50\nAnd this is basically a more complicated mixture model. So the more complicated is more than just one distribution. And it\'s called a mixture model.\n7:00\nSo as I just said we can treat this as a generative model. And it\'s often useful to think of just as a likelihood function. The illustration that you have seen before, which is dimmer now, is just the illustration of this generated model. So mathematically, this model is nothing but to just define the following generative model. Where the probability of a word is assumed to be a sum over two cases\n7:26\nof generating the word. And the form you are seeing now is a more general form that what you have seen in the calculation earlier. Well I just use the symbol w to denote any water but you can still see this is basically first a sum. Right? And this sum is due to the fact that the water can be generated in much more ways, two ways in this case. And inside a sum, each term is a product of two terms. And the two terms are first the probability of selecting a component like of D Second, the probability of actually observing the word from this component of the model. So this is a very general description of all the mixture models. I just want to make sure that you understand this because this is really the basis for understanding all kinds of on top models.\n8:28\nSo now once we setup model. We can write down that like functioning as we see here. The next question is, how can we estimate the parameter, or what to do with the parameters. Given the data. Well, in general, we can use some of the text data to estimate the model parameters. And this estimation would allow us to discover the interesting knowledge about the text. So you, in this case, what do we discover? Well, these are presented by our parameters and we will have two kinds of parameters. One is the two worded distributions, that result in topics, and the other is the coverage of each topic in each.\n9:12\nThe coverage of each topic. And this is determined by probability of C less of D and probability of theta, so this is to one. Now, what\'s interesting is also to think about special cases like when we send one of them to want what would happen? Well with the other, with the zero right? And if you look at the likelihood function,\n9:36\nit will then degenerate to the special case of just one distribution. Okay so you can easily verify that by assuming one of these two is 1.0 and the other is Zero.\n9:49\nSo in this sense, the mixture model is more general than the previous model where we have just one distribution. It can cover that as a special case.\n9:59\nSo to summarize, we talked about the mixture of two Unigram Language Models and the data we\'re considering here is just One document. And the model is a mixture model with two components, two unigram LM models, specifically theta sub d, which is intended to denote the topic of document d, and theta sub B, which is representing a background topic that we can set to attract the common words because common words would be assigned a high probability in this model.\n10:33\nSo the parameters can be collectively called Lambda which I show here you can again\n10:41\nthink about the question about how many parameters are we talking about exactly. This is usually a good exercise to do because it allows you to see the model in depth and to have a complete understanding of what\'s going on this model. And we have mixing weights, of course, also.\n10:59\nSo what does a likelihood function look like? Well, it looks very similar to what we had before. So for the document, first it\'s a product over all the words in the document exactly the same as before. The only difference is that inside here now it\'s a sum instead of just one. So you might have recalled before we just had this one there.\n11:25\nBut now we have this sum because of the mixture model. And because of the mixture model we also have to introduce a probability of choosing that particular component of distribution.\n11:39\nAnd so this is just another way of writing, and by using a product over all the unique words in our vocabulary instead of having that product over all the positions in the document. And this form where we look at the different and unique words is a commutative that formed for computing the maximum likelihood estimate later. And the maximum likelihood estimator is, as usual, just to find the parameters that would maximize the likelihood function. And the constraints here are of course two kinds. One is what are probabilities in each [INAUDIBLE] must sum to 1 the other is the choice of each [INAUDIBLE] must sum to 1. [MUSIC]\nLike\nDislike\nReport an issue\nShare'])
https://www.coursera.org/learn/cs-410/supplement/MLJKl/technology-review-informationdict_values(['List\nCS 410: Text Information Systems\nWeek 8\nTechnology Review Information\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nWeek 8 Activities\nTechnology Review (4-credit students only)\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nTechnology Review Information\nCS410 Technology Review  (4-credit students only)\n  CS410 Technology Review\nThe Technology Review assignment is designed to provide students with an opportunity to go beyond the materials covered in the course lectures to learn about an interesting course-related cutting-edge technology topic not covered in any lecture. In this assignment, a student is required to write a short review article on a chosen topic by the student from a list of suggested topics by the instructor and TAs. A student can also propose a topic not on the list subject to the approval of the instructor.\nThe topic of a technology review can be one of the three broad topic categories related to the general topic of “text data retrieval and analysis”:   \n1) Useful software toolkits for processing text data or building text data applications. \n2) Emerging new applications of text retrieval or analysis. \n3) New techniques for text retrieval or analysis. \nA list of specific topics will be provided for students to choose, but students may also propose additional topics interesting to them subject to the approval of the instructor. A review may cover one toolkit/application/technique in-depth or compare multiple toolkits/applications/techniques. The former is only allowed if the toolkit/application/technique is sufficiently complex to justify devoting an entire review to it. In any case, your review must have novel content that does not exist in any existing literature or webpages so that it would offer unique information/knowledge that others can learn from your review. So please make sure to check whether there is already a review on the topic before you devote time to complete a review. If you find an existing review on the topic, you may still write about the topic, but just need to make sure that you take a somewhat different perspective than the existing review or add new content on top of the existing review (i.e., extending it in some way).\nThe Technology Review should be completed individually. It will be graded based on completion of the following two tasks\n1. Topic proposal: Every student is required to select a topic from a provided topic list or propose a topic by the end of Week 9 (Oct 23, 2021) in the signup sheet (access with illinois.edu email address): \nhttps://docs.google.com/spreadsheets/d/1hWAyxd82FcitN9eG3yMW6ckq6l1VASBtYWpaPbywMPc/edit?usp=sharing\nSome sample topics are provided here: https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit?usp=sharing   \n2. Review submission: Each student is required to submit a complete Technology Review by the end of Week 11 (Nov 6, 2021).  The review must have a coherent storyline (Intro, Body, Conclusion) and cite relevant references. It must be at least ~2 pages   \nThe deadline is set to an earlier time than that of course project code submission so as to give the students an opportunity to read some relevant reviews (especially those on toolkits) before finishing their projects if they want to. \n      Completed\nGo to next item\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/home/week/14dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 14\nWeek 14: Fall Break (11/19 - 11/27)\n10 min of readings left\nWeek 14 Information\nNo Office Hours\nReading•\n. Duration: 10 minutes\n10 min'])
https://www.coursera.org/learn/cs-410/lecture/taJgZ/9-10-latent-dirichlet-allocation-lda-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 9\n9.10 Latent Dirichlet Allocation (LDA): Part 2\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.10 Latent Dirichlet Allocation (LDA): Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] So now let's talk about the exchanging of PLSA to of LDA and to motivate that, we need to talk about some deficiencies of PLSA. First, it's not really a generative model because we can compute the probability of a new document. You can see why, and that's because the pis are needed to generate the document, but the pis are tied to the document that we have in the training data. So we can't compute the pis for future document.\n0:34\nAnd there's some heuristic workaround, though. Secondly, it has many parameters, and I've asked you to compute how many parameters exactly there are in PLSA, and you will see there are many parameters. That means that model is very complex. And this also means that there are many local maxima and it's prone to overfitting. And that means it's very hard to also find a good local maximum.\n1:02\nAnd that we are representing global maximum. And in terms of explaining future data, we might find that it will overfit the training data because of the complexity of the model. The model is so flexible to fit precisely what the training data looks like. And then it doesn't allow us to generalize the model for using other data.\n1:23\nThis however is not a necessary problem for text mining because here we're often only interested in hitting the training documents that we have. We are not always interested in modern future data, but in other cases, or if we would care about the generality, we would worry about this overfitting.\n1:42\nSo LDA is proposing to improve that, and basically to make PLSA a generative model by imposing a Dirichlet prior on the model parameters. Dirichlet is just a special distribution that we can use to specify product. So in this sense, LDA is just a Bayesian version of PLSA, and the parameters are now much more regularized. You will see there are many few parameters and you can achieve the same goal as PLSA for text mining. It means it can compute the top coverage and topic word distributions as in PLSA. However, there's no. Why are the parameters for PLSA here are much fewer, there are fewer parameters and in order to compute a topic coverage and word distributions, we again face a problem of influence of these variables because they are not parameters of the model. So the influence part again face the local maximum problem. So essentially they are doing something very similar, but theoretically, LDA is a more elegant way of looking at the top and bottom problem. So let's see how we can generalize the PLSA to LDA or a standard PLSA to have LDA. Now a full treatment of LDA is beyond the scope of this course and we just don't have time to go in depth on that talking about that. But here, I just want to give you a brief idea about what's extending and what it enables, all right. So this is the picture of LDA. Now, I remove the background of model just for simplicity.\n3:15\nNow, in this model, all these parameters are free to change and we do not impose any prior. So these word distributions are now represented as theta vectors. So these are word distributions, so here. And the other set of parameters are pis. And we would present it as a vector also. And this is more convenient to introduce LDA. And we have one vector for each document. And in this case, in theta, we have one vector for each topic.\n3:50\nNow, the difference between LDA and PLSA is that in LDA, we're not going to allow them to free the chain. Instead, we're going to force them to be drawn from another distribution.\n4:03\nSo more specifically, they will be drawn from two Dirichlet distributions respectively, but the Dirichlet distribution is a distribution over vectors. So it gives us a probability of four particular choice of a vector. Take, for example, pis, right. So this Dirichlet distribution tells us which vectors of pi is more likely. And this distribution in itself is controlled by another vector of parameters of alphas.\n4:31\nDepending on the alphas, we can characterize the distribution in different ways but with full certain choices of pis to be more likely than others. For example, you might favor the choice of a relatively uniform distribution of all the topics. Or you might favor generating a skewed coverage of topics, and this is controlled by alpha. And similarly here, the topic or word distributions are drawn from another Dirichlet distribution with beta parameters. And note that here, alpha has k parameters, corresponding to our inference on the k values of pis for our document. Whereas here, beta has n values corresponding to controlling the m words in our vocabulary.\n5:17\nNow once we impose this price, then the generation process will be different. And we start with joined pis from the Dirichlet distribution and this pi will tell us these probabilities.\n5:35\nAnd then, we're going to use the pi to further choose which topic to use, and this is of course very similar to the PLSA model.\n5:47\nAnd similar here, we're not going to have these distributions free. Instead, we're going to draw one from the Dirichlet distribution. And then from this, then we're going to further sample a word. And the rest is very similar to the. The likelihood function now is more complicated for LDA. But there's a close connection between the likelihood function of LDA and the PLSA. So I'm going to illustrate the difference here. So in the top, you see PLSA likelihood function that you have already seen before. It's copied from previous slide. Only that I dropped the background for simplicity.\n6:27\nSo in the LDA formulas you see very similar things. You see the first equation is essentially the same. And this is the probability of generating a word from multiple word distributions.\n6:40\nAnd this formula is a sum of all the possibilities of generating a word. Inside a sum is a product of the probability of choosing a topic multiplied by the probability of observing the word from that topic.\n6:55\nSo this is a very important formula, as I've stressed multiple times. And this is actually the core assumption in all the topic models. And you might see other topic models that are extensions of LDA or PLSA. And they all rely on this. So it's very important to understand this. And this gives us a probability of getting a word from a mixture model. Now, next in the probability of a document, we see there is a PLSA component in the LDA formula, but the LDA formula will add a sum integral here. And that's to account for the fact that the pis are not fixed. So they are drawn from the original distribution, and that's shown here. That's why we have to take an integral, to consider all the possible pis that we could possibly draw from this Dirichlet distribution. And similarly in the likelihood for the whole collection, we also see further components added, another integral here.\n7:58\nRight? So basically in the area we're just adding this integrals to account for the uncertainties and we added of course the Dirichlet distributions to cover the choice of this parameters, pis, and theta.\n8:12\nSo this is a likelihood function for LDA. Now, next to this, let's talk about the parameter as estimation and inferences. Now the parameters can be now estimated using exactly the same approach maximum likelihood estimate for LDA. Now you might think about how many parameters are there in LDA versus PLSA. You'll see there're a fewer parameters in LDA because in this case the only parameters are alphas and the betas. So we can use the maximum likelihood estimator to compute that. Of course, it's more complicated because the form of likelihood function is more complicated. But what's also important is notice that now these parameters that we are interested in name and topics, and the coverage are no longer parameters in LDA. In this case we have to use basic inference or posterior inference to compute them based on the parameters of alpha and the beta. Unfortunately, this computation is intractable. So we generally have to resort to approximate inference.\n9:18\nAnd there are many methods available for that and I'm sure you will see them when you use different tool kits for LDA, or when you read papers about\n9:30\nthese different extensions of LDA. Now here we, of course, can't give in-depth instruction to that, but just know that they are computed based in inference by using the parameters alphas and betas. But our math [INAUDIBLE], actually, in the end, in some of our math list, it's very similar to PLSA. And, especially when we use algorithm called class assembly, then the algorithm looks very similar to the Algorithm. So in the end, they are doing something very similar.\n10:10\nSo to summarize our discussion of properties of topic models, these models provide a general principle or way of mining and analyzing topics in text with many applications. The best basic task setup is to take test data as input and we're going to output the k topics. Each topic is characterized by word distribution. And we're going to also output proportions of these topics covered in each document.\n10:38\nAnd PLSA is the basic topic model, and in fact the most basic of the topic model. And this is often adequate for most applications. That's why we spend a lot of time to explain PLSA in detail.\n10:53\nNow LDA improves over PLSA by imposing priors. This has led to theoretically more appealing models. However, in practice, LDA and PLSA tend to give similar performance, so in practice PLSA and LDA would work equally well for most of the tasks.\n11:12\nNow here are some suggested readings if you want to know more about the topic. First is a nice review of probabilistic topic models.\n11:20\nThe second has a discussion about how to automatically label a topic model. Now I've shown you some distributions and they intuitively suggest a topic. But what exactly is a topic? Can we use phrases to label the topic? To make it the more easy to understand and this paper is about the techniques for doing that. The third one is empirical comparison of LDA and the PLSA for various tasks. The conclusion is that they tend to perform similarly. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/16dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 16\nWeek 16\n1 graded assignment left\nBe sure to submit your Course Project presentation and report by Dec 13, 2020.\nShow Learning Objectives\nWeek 16 Activities\n1 graded assignment left\nProject Code, Documentation and Presentation\nDue, Dec 8, 12:59 PM EST\nGraded Assignment•\n. Duration: 3 hours\n3h\n•Grade: --\nEnd of Course Survey\nHow was the course?\nResume\n. Click to resume'])
https://www.coursera.org/learn/cs-410/lecture/kM78U/lesson-6-4-future-of-web-searchdict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.4: Future of Web Search\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.4: Future of Web Search\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND].\n0:07\nThis lecture is about the future of web search.\n0:12\nIn this lecture, we're going to talk about some possible future trends of web search and intelligent information retrieval systems in general.\n0:24\nIn order to further improve the accuracy of a search engine, it's important that to consider special cases of information need. So one particular trend could be to have more and more specialized than customized search engines, and they can be called vertical search engines.\n0:46\nThese vertical search engines can be expected to be more effective than the current general search engines because they could assume that users are a special group of users that might have a common information need, and then the search engine can be customized with this ser, so, such users.\n1:07\nAnd because of the customization, it's also possible to do personalization. So the search can be personalized,\n1:15\nbecause we have a better understanding of the users.\n1:20\nBecause of the restrictions with domain, we also have some advantages in handling the documents, because we can have better understanding of documents. For example, particular words may not be ambiguous in such a domain. So we can bypass the problem of ambiguity.\n1:38\nAnother trend we can expect to see, is the search engine will be able to learn over time. It's like a lifetime learning or lifelong learning, and this is, of course, very attractive because that means the search engine will self-improve itself. As more people are using it, the search engine will become better and better, and this is already happening, because the search engines can learn from the [INAUDIBLE] of feedback. More users use it, and the quality of the search engine allows for the popular queries that are typed in by many users allow it to become better, so this is sort of another feature that we will see.\n2:21\nThe third trend might be to the integration of bottles of information access. So search, navigation, and recommendation or filtering might be combined to form a full-fledged information management system. And in the beginning of this course, we talked about push versus pull. These are different modes of information access, but these modes can be combined.\n2:48\nAnd similarly, in the pull mode, querying and the browsing could also be combined. And in fact we're doing that basically, today, is the [INAUDIBLE] search endings. We are querying, sometimes browsing, clicking on links. Sometimes we've got some information recommended. Although most of the cases the information recommended is because of advertising. But in the future, you can imagine seamlessly integrate the system with multi-mode for information access, and that would be convenient for people.\n3:23\nAnother trend is that we might see systems that try to go beyond the searches to support the user tasks. After all, the reason why people want to search is to solve a problem or to make a decision or perform a task. For example consumers might search for opinions about products in order to purchase a product, choose a good product by, so in this case it would be beneficial to support the whole workflow of purchasing a product, or choosing a product.\n3:56\nIn this era, after the common search engines already provide a good support. For example, you can sometimes look at the reviews, and then if you want to buy it, you can just click on the button to go the shopping site and directly get it done. But it does not provide a, a good task support for many other tasks. For example, for researchers, you might want to find the realm in the literature or site of the literature. And then, there's no, not much support for finishing a task such as writing a paper. So, in general, I think, there are many opportunities in the wait. So in the following few slides, I'll be talking a little bit more about some specific ideas or thoughts that hopefully, can help you in imagining new application possibilities. Some of them might be already relevant to what you are currently working on. In general, we can think about any intelligent system, especially intelligent information system, as we specified by these these three nodes. And so if we connect these three into a triangle, then we'll able to specify an information system. And I call this Data-User-Service Triangle. So basically the three questions you ask would be who are you serving and what kind of data are you are managing and what kind of service you provide.\n5:24\nRight there, this would help us basically specify in your system.\n5:30\nAnd there are many different ways to connect them depending on how you connect them, you will have a different kind of systems. So let me give you some examples. On the top, you can see different kinds of users. On the left side, you can see different types of data or information, and on the bottom, you can see different service functions. Now imagine you can connect all these in different ways. So, for example, you can connect everyone with web pages, and the support search and browsing, what do you get? Well, that's web search, right?\n6:02\nWhat if we connect UIUC employees with organization documents or enterprise documents to support the search and browsing, but that's enterprise search. If you connect the scientist with literature information to provide all kinds of service, including search, browsing, or alert of new random documents or mining analyzing research trends, or provide the task with support or decision support. For example, we might be, might be able to provide a support for automatically generating related work section for a research paper, and this would be closer to task support. Right? So then we can imagine this would be a literature assistant. If we connect the online shoppers with blog articles or product reviews\n6:53\nthen we can help these people to improve shopping experience. So we can provide, for example data mining capabilities to analyze the reviews, to compare products, compare sentiment of products and to provide task support or decision support to have them choose what product to buy. Or we can connect customer service people with emails from the customers,\n7:22\nand, and we can imagine a system that can provide a analysis of these emails to find that the major complaints of the customers. We can imagine a system we could provide task support by automatically generating a response to a customer email. Maybe intelligently attach also a promotion message if appropriate, if they detect that that's a positive message, not a complaint, and then you might take this opportunity to attach some promotion information. Whereas if it's a complaint, then you might be able to\n7:59\nautomatically generate some generic response first and tell the customer that he or she can expect a detailed response later, etc. All of these are trying to help people to improve the productivity.\n8:15\nSo this shows that the opportunities are really a lot. It's just only restricted by our imagination. So this picture shows the trend of the technology, and also, it characterizes the, intelligent information system in three angles. You can see in the center, there's a triangle that connects keyword queries to search a bag of words representation. That means the current search engines basically provides search support to users and mostly model users based on keyword queries and sees the data through bag of words representation. So it's a very simple approximation of the actual information in the documents. But that's what the current system does. It connects these three nodes in such a simple way, or it only provides a basic search function and doesn't really understand the user, and it doesn't really understand that much information in the documents. Now, I showed some trends to push each node toward a more advanced function. So think about the user node here, right? So we can go beyond the keyword queries, look at the user search history, and then further model the user completely to understand the, the user's task environment, task need context or other information. Okay, so this is pushing for personalization and complete user model. And this is a major direction in research in, in order to build intelligent information systems. On the document side, we can also see, we can go beyond bag of words implementation to have entity relation representation. This means we'll recognize people's names, their relations, locations, etc. And this is already feasible with today's natural processing technique. And Google is the reason the initiative on the knowledge graph. If you haven't heard of it, it is a good step toward this direction. And once we can get to that level without initiating robust manner at larger scale, it can enable the search engine to provide a much better service. In the future we would like to have knowledge representation where we can add perhaps inference rules, and then the search engine would become more intelligent.\n10:49\nSo this calls for large-scale semantic analysis, and perhaps this is more feasible for vertical search engines. It's easier to make progress in the particular domain. Now on the service side, we see we need to go beyond the search of support information access in general.\n11:07\nSo search is only one way to get access to information as well recommender systems and push and pull so different ways to get access to random information. But going beyond access, we also need to help people digest the information once the information is found, and this step has to do with analysis of information or data mining. We have to find patterns or convert the text information into real knowledge that can be used in application or actionable knowledge that can be used for decision making. And furthermore the knowledge will be used to help a user to improve productivity in finishing a task, for example, a decision-making task. Right, so this is a trend. And, and, and so basically, in this dimension, we anticipate in the future intelligent information systems will provide intelligent and interactive task support. Now I should also emphasize interactive here, because it's important to optimize the combined intelligence of the users and the system. So we, we can get some help from users in some natural way. And we don't have to assume the system has to do everything when the human, user, and the machine can collaborate in an intelligent way, an efficient way, then the combined intelligence will be high and in general, we can minimize the user's overall effort in solving problem.\n12:42\nSo this is the big picture of future intelligent information systems, and this hopefully can provide us with some insights about how to make further innovations on top of what we handled today. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/6dict_values(["Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 6\nWeek 6\nAll videos completed\n20 min of readings left\nAll graded assignments completed\nIn this week's lessons, you will learn how machine learning can be used to combine multiple scoring factors to optimize ranking of documents in web search (i.e., learning to rank), and learn techniques used in recommender systems (also called...\nShow more\nWeek 6 Information\nWeek 6 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 6 Lessons\nComplete\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\nVideo•\n. Duration: 5 minutes\n5 min\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\nVideo•\n. Duration: 10 minutes\n10 min\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\nVideo•\n. Duration: 4 minutes\n4 min\nLesson 6.4: Future of Web Search\nVideo•\n. Duration: 13 minutes\n13 min\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\nVideo•\n. Duration: 12 minutes\n12 min\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\nVideo•\n. Duration: 10 minutes\n10 min\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\nVideo•\n. Duration: 6 minutes\n6 min\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\nVideo•\n. Duration: 12 minutes\n12 min\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\nVideo•\n. Duration: 4 minutes\n4 min\nLesson 6.10: Summary for Exam 1\nVideo•\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nComplete\nWeek 6 Practice Quiz\nPractice Quiz•5 questions\nWeek 6 Quiz\nGraded\nQuiz•9 questions\n•Grade: \nProject Information\nCourse Project Overview\nReading•\n. Duration: 10 minutes\n10 min\nProgramming Assignment 2.4\nComplete\nMP2.4\nGraded\nGraded External Tool•Submitted\n•Grade: "])
https://www.coursera.org/learn/cs-410/lecture/3d9fD/lesson-6-2-learning-to-rank-part-2-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:06\nSo now let's take a look at the specific method that's based on regression. Now, this is one of the many different methods, and in fact, it's one of the simplest methods. And I choose this to explain the idea because it's simple.\n0:26\nSo in this approach, we simply assume that the relevance of document with respect to a query is related to a linear combination of all the features. Here I used Xi to denote the feature. So Xi of Q and D is a feature. And we can have as many features as we would like.\n0:55\nAnd we assume that these features can be combined in a linear manner.\n1:03\nAnd each feature is controlled by a parameter here, and this beta i is a parameter. That's a weighting parameter. A larger value would mean the feature would have a higher weight, and it would contribute more to the scoring function. This specific form of the function actually also involves a transformation of the probability of relevance. So this is the probability of relevance. And we know that the probability of relevance is within the range from 0 to 1.\n1:36\nAnd we could have just assumed that the scoring function is related to this linear combination. So we can do a linear regression. But then, the value of this linear combination could easily go beyond 1. So this transformation here would map the 0 to 1 range to the whole range of real values, you can verify it by yourself.\n2:10\nSo this allows us then to connect to the probability of variance which is between 0 and 1 to a linear combination of arbitrary features. And if we rewrite this into a probability function, we would get the next one. So on this equation, now we'll have the probability of relevance.\n2:35\nAnd on the right hand side, we'll have this form. Now, this form is clearly nonnegative, and it still involves a linear combination of features. And it's also clear that if this value is, this is actually negative of the linear combination in the equation above. If this value here is large, then it would mean this value is small. And therefore, this whole probability would be large. And that's we expect, that basically, it would mean if this combination gives us a high value, then the document's more likely irrelevant. So this is our hypothesis. Again, this is not necessarily the best hypothesis, but this is a simple way to connect these features with the probability of relevance.\n3:40\nSo now we have this combination function. The next task is to estimate the parameters so that the function cache will be applied. But without knowing the beta values, it's harder to apply this function.\n3:58\nSo let's see how can estimate our beta values. All right, let's take a look at a simple example.\n4:08\nIn this example, we have three features. One is the BM25 score of the document and the query. One is the PageRank score of the document, which might or might not depend on the query. We might have a topic-sensitive PageRank, that would depend on the query. Otherwise, the general PageRank doesn't really depend on the query. And then we have BM25 score on the anchor test of the document. Now, these are then the feature values for a particular document query pair.\n4:41\nAnd in this case, the document is D1 and the judgment says that it's relevant.\n4:48\nHere's another training instance and it's these feature values, but in this case, it's not relevant. This is an oversimplified case where we just have two instances, but it's sufficient to illustrate the point. So what we can do is we use the maximum likelihood estimator to actually estimate the parameters.\n5:13\nBasically, we're going to predict the relevance status of the document based on the feature values. That is, given that we observed these feature values here.\n5:28\nCan we predict the relevance here? Now, of course, the prediction would be using this function that you see here. And we hypothesize that the probability of relevance is related to features in this way. So we are going to see, for what values of beta we can predict the relevance well. What do we mean by predicting the relevance well? Well, we just mean, in the first case, for D1 this expression right here should give high values. In fact, we'll hope this to gave a value close to 1. Why? Because this is a relevant document.\n6:14\nOn the other hand, in the second case, for D2, we hope this value will be small, right. Why? Because it's a non-relevant document. So now let's see how this can be mathematically expressed. And this is similar to expressing the probability of document, only that we are not talking about the probability of words, but talking about the probability of relevance, 1 or 0. So what's the probability of this document being relevant if it has these feature values?\n6:54\nWell, this is just this expression. We just need to plug in the Xi's. So that's what we will get. It's exactly like what we have seen above, only that we replaced these Xi's with now specific values. So for example, this 0.7 goes to here and this 0.11 goes to here. And these are different feature values, and we combine them in this particular way. The beta values are still unknown. But this gives us the probability that this document is relevant, if we assume such a model. Okay? And we want to maximize this probability, since this is a relevant document. What do we do for the second document? Well, we want to compute the probability that the prediction is non-relevant. So this would mean we have to compute 1 minus this expression, since this expression is actually the probability of relevance. So to compute the non-relevance from relevance, we just do 1 minus the probability of relevance. Okay? So this whole expression then just is our probability of predicting these two relevance values. One is 1 here, one is 0. And this whole equation is our probability of observing a 1 here and observing a 0 here.\n8:44\nOf course, this probability depends on the beta values.\n8:50\nSo then our goal is to adjust the beta values to make this whole thing reach its maximum, make it as large as possible. So that means we're going to compute this. The beta is just the parameter values that would maximize this whole likelihood expression. And what it means is, if you look at the function, is, we're going to choose betas to make this as large as possible and make this also as large as possible, which is equivalent to say, make this part as small as possible.\n9:30\nAnd this is precisely what we want.\n9:34\nSo once we do the training, now we will know the beta values. So then this function would be well-defined. Once beta values are known, both this and this would be completely specified. So for any new query and new document, we can simply compute the features for that pair. And then we just use this formula to generate the ranking score. And this scoring function can be used to rank documents for a particular query. So that's the basic idea of learning to rank. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/B6GP3/office-hoursdict_values(['List\nCS 410: Text Information Systems\nWeek 1\nOffice Hours\nPrevious\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nOffice Hours\nOffice hours will be held by the instructor and TAs each week using Zoom. All times are given in US Central Time and using a 24-hour clock (Time Zone Converter). See below for specific days and times.\nInstructor: Professor Cheng Zhai\nTuesdays: 8-9 pm (CT)\nWeekly TA Office Hours:\nTA\nRole\nDay\nTime (CT)\nStart Week\nEnd Week\nAssma\nHead TA\nWednesday\nNoon - 1 pm\nWeek 1\nWeek 15\nYuxiang\nMP1-3 Lead\nMonday\n7 - 8 pm\nWeek 1\nWeek 8\nThursday\n7 - 8 pm\nWeek 1\nWeek 8\nDaniel\nMP4 Lead\nTBD\nTBD\nTBD\nTBD\nKevin\nProject Lead\nTBD\nTBD\nTBD\nWeek 15\nOffice Hours Recordings:\nYou can find the recordings here: https://www.coursera.org/learn/cs-410/resources/nOxcB \nMark as completed\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/lecture/M7ylk/lesson-5-3-feedback-in-text-retrieval-feedback-in-lmdict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the feedback in the language modeling approach.\n0:12\nIn this lecture, we will continue the discussion of feedback in text retrieval. In particular, we're going to talk about the feedback in language modeling approaches.\n0:23\nSo we derive the query likelihood ranking function by making various assumptions.\n0:30\nAs a basic retrieval function, all those formulas worked well. But if we think about the feedback information, it's a little bit awkward to use query likelihood to perform feedback, because a lot of times the feedback information is additional information about the query. But we assume the query has generated it by assembling words from a language model in the query likelihood method. It's kind of unnatural to sample words that form feedback documents. As a result, researchers proposed a way to generalize query likelihood function, and it's called Kullback-Leibler divergence retrieval model.\n1:15\nAnd this model is actually going to make the query likelihood retrieval function much closer to vector space model. Yet this form of the language model can be regarded as a generalization of query likelihood, in the sense that it can cover query likelihood as a special case.\n1:38\nAnd in this case, then feedback can be achieved through simply query model estimation or updating. This is very similar to Rocchio, which updates the query vector.\n1:50\nSo let's see what is this KL-divergence retrieval model. So on the top, what you see is a query likelihood retrieval function, this one.\n2:05\nAnd then KL-divergence, or also called cross entropy, retrieval model is basically to generalize the frequency part here into a language model. So basically it's the difference given by the probabilistic model here to characterize what the user is looking for, versus the count of query words there.\n2:35\nAnd this difference allows us to plug in various different ways to estimate this. So this can be estimated in many different ways, including using feedback information. But this is called a KL-divergence, because this can be interpreted as matching the KL-divergence of two distributions. One is the query model, denoted by this distribution. One is the document language model here and smooth them with a collection language model, of course. And we are not going to talk about the detail of that, and you'll find it in some references. It's also called cross entropy because, in fact, we ignore some terms in the KL-divergence function and we will end up having actually cross entropy. And both are terms of information theory.\n3:34\nBut anyway, for our purposes here, you can just see the two formulas look almost identical, except that here we have a probability of a word given by a query language model.\n3:52\nAnd here the sum is over all the words that are in the document and also with the nonzero probability for the query model. So it's kind of, again, a generalization of sum over all the matching query words.\n4:09\nNow you can also easily see we can recover the query likelihood retrieval function by simply setting this query model to the relative frequency of a word in the query.\n4:23\nThis is very easy to see once you plug this into here you can eliminate this query length as a constant. And then you will get exactly like that. So you can see the equivalence. And that's also why this KL-divergence model can be regarded as a generalization of query likelihood, because we can cover query likelihood as a special case. But it would also allow us to do much more than that.\n4:50\nSo this is how we can use the KL-divergence model to then do feedback. The picture shows that we first estimate a document language model, then we estimate a query language model, and we compute the KL-divergence. This is often denoted by a D here.\n5:09\nBut this basically means this is exactly like the vector space model, because we compute a vector for the document, then compute another vector for the query, and then we compute the distance. Only that these vectors are of special forms, they are probability distributions.\n5:27\nAnd then we get the results and we can find some feedback documents. Let's assume they are mostly positive documents, although we could also consider both kinds of documents. So what we could do is, like in Rocchio, we're going to compute another language model called the feedback language model here. Again, this is going to be another vector just like the computing centroid of vector in Rocchio. And then this model can be combined with the original query model using a linear interpolation, and this would then give us an update model, just like, again, in Rocchio. So here we can see the parameter alpha can control the amount of feedback. If it's set to zero, then essentially there is no feedback. If it's set to one, we get full feedback and we ignore the original query. And this is generally not desirable, right? So unless you are absolutely sure you have seen a lot of relevant documents, then the query terms are not important.\n6:31\nSo of course, the main question here is, how do you compute this theta F? This is the big question here, and once you can do that, the rest is easy. So here we will talk about one of the approaches, and there are many approaches, of course. This approach is based on generative model, and I'm going to show you how it works. This will use a generative mixture model. So this picture shows that we have this model here, the feedback model that we want to estimate.\n6:58\nAnd the basis is the feedback documents. Let's say we are observing the positive documents. These are the clicked documents by users or random documents judged by users, or are simply top ranked documents that we assume to be relevant.\n7:14\nNow imagine how we can compute a centroid for these documents by using language model. One approach is simply to assume these documents are generated from this language model. As we did before, what we could do is just normalize the word frequency here to here and then we will get this word distribution.\n7:36\nNow the question is whether this distribution is good for feedback. Well, you can imagine the top ranked word would be what? What do you think?\n7:48\nWell, those words would be common words. As we always see in a language model, the top ranked words are actually common words like the, a, etc. So it's not very good for feedback, because we would be adding a lot of such words to our query when we interpolate this with the original query model.\n8:08\nSo this was not good, so we need to do something. In particular, we are trying to get rid of those common words. And we have seen actually one way to do that by using background language model in the case of learning the associations of words, the words that are related to the word computer. We could do that and that would be another way to do this, but here we are going to talk about another approach which is a more principled approach. In this case, we're going to say well, you said that there are common words here in these documents that should not belong to this topic model, right?\n8:50\nSo now what we can do is to assume that, well, those words are generated from background language model, so they will generate those words like the, for example. And if we use maximum likelihood estimate, note that if all the words here must be generated from this model, then this model is forced to assign high probabilities to a word like the, because it occurs so frequently here. Note that in order to reduce its probability in this model, we have to have another model, which is this one, to help explain the word the here. And in this case, it's not appropriate to use the background language model to achieve this goal because this model would assign high probabilities to these common words.\n9:43\nSo in this approach, then, we assume this machine that was generating these words would work as follows. We have a source control up here. Imagine we flip a coin here to decide what distribution to use. With probability of lambda, the coin shows up as head and we're going to use the background language model. And we're going to do that in sample word from that model. With probability of 1 minus lambda, we're going to decide to use a known topic model, here, that we would like to estimate. And we're going to then generate a word here. If we make this assumption and this whole thing will be just one model, and we call this a mixture model because there are two distributions that are mixed together. And we actually don't know when each distribution is used.\n10:35\nSo again, think of this whole thing as one model,\n10:42\nand we can still ask for words and it will still give us a word in a random manner. And of course, which word will show up will depend on both this distribution and that distribution. In addition, it would also depend on this lambda, because if you say lambda is very high and it's going to always use the background distribution, you will get different words. Then if you say, well, lambda is very small, we're going to use this. So all of these are parameters in this model. And then if you're thinking this way, basically we can do exactly the same as what we did before. We're going to use maximum likelihood estimator to adjust this model, to estimate the parameters. Basically we're going to adjust this parameter so that we can best explain all the data. The difference now is that we are not asking this model a known to explain this. But rather we are going to ask this whole model, mixture model, to explain the data. Because it has got some help from the background model, it doesn't have to assign high probabilities to words like the. As a result, it will then assign higher probabilities to other words that are common here but not having high probability here. So those would be common here.\n12:11\nAnd if they're common, they would have to have high probabilities, according to a maximum likelihood estimate method. And if they are rare here, then you don't get much help from this background model. As a result, this topic model must assign high probabilities. So the high probability words, according to the topic model, would be those that are common here but rare in the background.\n12:43\nSo this is basically a little bit like an idea of weighting here. But this would allow us to achieve the effect of removing these topic words that are meaningless in the feedback.\n12:56\nSo mathematically, what we have is to compute the likelihood, again, local likelihood, of the feedback documents.\n13:06\nAnd note that we also have another parameter, lambda here, but we assume that the lambda denotes the noise in the feedback document. So we are going to, let's say set this to a parameter. Let's say  of the words are noise or  are noise. And this can then be assumed it will be fixed. If we assume this is fixed, then we only have these probabilities as parameters, just like in the simple unigram language model. We have n parameters, n is the number of words. And then the likelihood function would look like this.\n13:42\nIt's very similar to the global likelihood function we see before, except that inside the logarithm there's a sum here. And this sum is because we consider two distributions. And which one is used would depend on lambda, and that's why we have this form.\n14:02\nBut mathematically, this is the function with theta as unknown variables. So this is just a function. All the other values are known except for this guy.\n14:15\nSo we can then choose this probability distribution to maximize this log likelihood, the same idea as the maximum likelihood estimate as a mathematical problem. We just have to solve this optimization problem. We essentially would try all the theta values until we find one that gives this whole thing the maximum probability. So it's a well-defined math problem.\n14:40\nOnce we have done that, we obtain this theta F that can then be interpolated with original query model to the feedback.\n14:50\nSo here are some examples of the feedback model learned from a web document collection. And we do pseudo-feedback we just use the top ten documents and we use this mixture model. So the query is airport security. What we do is we first retrieve ten documents from the web database and this is of course pseudo-feedback. And then we're going to feed that mixture model to this ten document set.\n15:21\nAnd these are the words learned using this approach. This is the probability of a word given by the feedback model in both cases.\n15:31\nSo in both cases you can see the highest probability words include the very relevant words to the query. So airport security, for example, these query words still show up as high probabilities in each case naturally, because they occur frequently in the top ranked documents. But we also see beverage, alcohol, bomb, terrorist, etc. So these are relevant to this topic, and they, if combined with original query, can help us much more accurately on documents. And also they can help us bring up documents that only mention some of these other words, maybe, for example, just airport and then bomb, for example.\n16:18\nSo this is how pseudo-feedback works. It shows that this model really works and picks up some related words to the query. What's also interesting is that if you look at the two tables here and you compare them, then you'll see, in this case, when lambda is set to a small value, then we'll see some common words here. And that means, well, we don't use the background model often. Remember, lambda confuses the probability of using background model to generate the text. If we don't rely much on background model, we still have to use this topic model to account for the common words. Whereas if we set lambda to a very high value, we will use the background model very often to explain these words. Then there's no burden on expanding those common words in the feedback documents by the topic model. So as a result, the topic model here is very discriminative. It contains all the relevant words without common words.\n17:21\nSo this can be added to the original query to achieve feedback.\n17:28\nSo to summarize, in this lecture we have talked about the feedback in language model approach. In general, feedback is to learn from examples. These examples can be assumed examples, can be pseudo-examples, like assume the top ten documents that are assumed to be relevant. They could be based on user interactions, like feedback based on clickthroughs or implicit feedback. We talked about the three major feedback scenarios, relevance feedback, pseudo feedback, and implicit feedback. We talked about how to use Rocchio to do feedback in vector space model and how to use query model estimation for feedback in language model. And we briefly talked about the mixture model and the basic idea.\n18:19\nThere are many other methods. For example, the relevance model is a very effective model for estimating query model. So you can read more about these methods in the references that\n18:32\nare listed at the end of this lecture. So there are two additional readings here. The first one is a book that has a systematic review and discussion of language models for information retrieval. And the second one is a important research paper that's about relevance based language models, and it's a very effective way of computing query model. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/GJyGG/9-8-probabilistic-latent-semantic-analysis-plsa-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 9\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:08\nWe can compute this maximum estimate by using the EM algorithm. So in the e step, we now have to introduce more hidden variables because we have more topics, so our hidden variable z now, which is a topic indicator can take more than two values. So specifically will take a k plus one values, with b in the noting the background. And once locate, to denote other k topics, right.\n0:36\nSo, now the e step, as you can recall is your augmented data, and by predicting the values of the hidden variable. So we're going to predict for a word, whether the word has come from one of these k plus one distributions. This equation allows us to predict the probability that the word w in document d is generated from topic zero sub j.\n1:03\nAnd the bottom one is the predicted probability that this word has been generated from the background. Note that we use document d here to index the word. Why? Because whether a word is from a particular topic actually depends on the document. Can you see why? Well, it's through the pi's. The pi's are tied to each document. Each document can have potentially different pi's, right. The pi's will then affect our prediction. So, the pi's are here. And this depends on the document.\n1:38\nAnd that might give a different guess for a word in different documents, and that's desirable.\n1:46\nIn both cases we are using the Baye's Rule, as I explained, basically assessing the likelihood of generating word from each of this division and there's normalize.\n1:57\nWhat about the m step? Well, we may recall the m step is we take advantage of the inferred z values. To split the counts. And then collected the right counts to re-estimate the parameters. So in this case, we can re-estimate our coverage of probability. And this is re-estimated based on collecting all the words in the document.\n2:22\nAnd that's why we have the count of the word in document. And sum over all the words. And then we're going to look at to what extent this word belongs to\n2:34\nthe topic theta sub j. And this part is our guess from each step.\n2:40\nThis tells us how likely this word is actually from theta sub j. And when we multiply them together, we get the discounted count that's located for topic theta sub j. And when we normalize this over all the topics, we get the distribution of all the topics to indicate the coverage. And similarly, the bottom one is the estimated probability of word for a topic. And in this case we are using exact the same count, you can see this is the same discounted account, ] it tells us to what extend we should allocate this word [INAUDIBLE] but then normalization is different. Because in this case we are interested in the word distribution, so we simply normalize this over all the words. This is different, in contrast here we normalize the amount all the topics. It would be useful to take a comparison between the two.\n3:37\nThis give us different distributions. And these tells us how to improve the parameters.\n3:48\nAnd as I just explained, in both the formula is we have a maximum estimate based on allocated word counts [INAUDIBLE]. Now this phenomena is actually general phenomena in all the EM algorithms. In the m-step, you general with the computer expect an account of the event based on the e-step result, and then you just and then count to four, particular normalize it, typically. So, in terms of computation of this EM algorithm, we can actually just keep accounting various events and then normalize them. And when we thinking this way, we also have a more concise way of presenting the EM Algorithm. It actually helps us better understand the formulas. So I'm going to go over this in some detail. So as a algorithm we first initialize all the unknown perimeters randomly, all right. So, in our case, we are interested in all of those coverage perimeters, pi's and awarded distributions [INAUDIBLE], and we just randomly normalize them. This is the initialization step and then we will repeat until likelihood converges. Now how do we know whether likelihood converges? We can do compute likelihood at each step and compare the current likelihood with the previous likelihood. If it doesn't change much and we're going to say it stopped, right.\n5:19\nSo, in each step we're going to do e-step and m-step. In the e-step we're going to do augment the data by predicting the hidden variables. In this case, the hidden variable, z sub d, w, indicates whether the word w in d is from a topic or background. And if it's from a topic, which topic. So if you look at the e-step formulas, essentially we're actually normalizing these counts, sorry, these probabilities of observing the word from each distribution. So you can see, basically the prediction of word from topic zero sub j is based on the probability of selecting that theta sub j as a word distribution to generate the word. Multiply by the probability of observing the word from that distribution.\n6:17\nAnd I said it's proportional to this because in the implementation of EM algorithm you can keep counter for this quantity, and in the end it just normalizes it. So the normalization here is over all the topics and then you would get a probability.\n6:36\nNow, in the m-step, we do the same, and we are going to collect these.\n6:43\nAllocated account for each topic.\n6:47\nAnd we split words among the topics.\n6:50\nAnd then we're going to normalize them in different ways to obtain the real estimate. So for example, we can normalize among all the topics to get the re-estimate of pi, the coverage. Or we can re-normalize based on all the words. And that would give us a word distribution.\n7:10\nSo it's useful to think algorithm in this way because when implemented, you can just use variables, but keep track of these quantities in each case.\n7:23\nAnd then you just normalize these variables to make them distribution.\n7:32\nNow I did not put the constraint for this one. And I intentionally leave this as an exercise for you. And you can see, what's the normalizer for this one? It's of a slightly different form but it's essentially the same as the one that you have seen here in this one. So in general in the envisioning of EM algorithms you will see you accumulate the counts, various counts and then you normalize them.\n8:01\nSo to summarize, we introduced the PLSA model. Which is a mixture model with k unigram language models representing k topics.\n8:11\nAnd we also added a pre-determined background language model to help discover discriminative topics, because this background language model can help attract the common terms.\n8:23\nAnd we select the maximum estimate that we cant discover topical knowledge from text data. In this case PLSA allows us to discover two things, one is k worded distributions, each one representing a topic and the other is the proportion of each topic in each document.\n8:41\nAnd such detailed characterization of coverage of topics in documents can enable a lot of photo analysis. For example, we can aggregate the documents in the particular pan period to assess the coverage of a particular topic in a time period. That would allow us to generate the temporal chains of topics. We can also aggregate topics covered in documents associated with a particular author and then we can categorize the topics written by this author, etc. And in addition to this, we can also cluster terms and cluster documents. In fact, each topic can be regarded as a cluster. So we already have the term clusters. In the higher probability, the words can be regarded as\n9:29\nbelonging to one cluster represented by the topic. Similarly, documents can be clustered in the same way. We can assign a document to the topic cluster that's covered most in the document. So remember, pi's indicate to what extent each topic is covered in the document, we can assign the document to the topical cluster that has the highest pi.\n9:57\nAnd in general there are many useful applications of this technique.\n10:03\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/gxXq6/lesson-1-4-overview-of-text-retrieval-methodsdict_values(["List\nCS 410: Text Information Systems\nWeek 1\nLesson 1.4: Overview of Text Retrieval Methods\nPrevious\nNext\nOrientation Information\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nVideo:\nVideo\nLesson 1.1: Natural Language Content Analysis\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 1.2: Text Access\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.3: Text Retrieval Problem\n. Duration: 26 minutes\n26 min\nVideo:\nVideo\nLesson 1.4: Overview of Text Retrieval Methods\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 1.5: Vector Space Model - Basic Idea\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\n. Duration: 17 minutes\n17 min\nWeek 1 Activities\nLesson 1.4: Overview of Text Retrieval Methods\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is a overview of text retrieval methods.\n0:13\nIn the previous lecture, we introduced the problem of text retrieval. We explained that the main problem is the design of ranking function to rank documents for a query. In this lecture, we will give an overview of different ways of designing this ranking function.\n0:33\nSo the problem is the following. We have a query that has a sequence of words and the document that's also a sequence of words. And we hope to define a function f\n0:45\nthat can compute a score based on the query and document. So the main challenge you hear is with design a good ranking function that can rank all the relevant documents on top of all the non-relevant ones. Clearly, this means our function must be able to measure the likelihood that a document d is relevant to a query q. That also means we have to have some way to define relevance. In particular, in order to implement the program to do that, we have to have a computational definition of relevance. And we achieve this goal by designing a retrieval model, which gives us a formalization of relevance.\n1:32\nNow, over many decades, researchers have designed many different kinds of retrieval models. And they fall into different categories.\n1:42\nFirst, one family of the models are based on the similarity idea.\n1:50\nBasically, we assume that if a document is more similar to the query than another document is, then we will say the first document is more relevant than the second one. So in this case, the ranking function is defined as the similarity between the query and the document. One well known example in this case is vector space model, which we will cover more in detail later in the lecture.\n2:20\nA second kind of models are called probabilistic models. In this family of models, we follow a very different strategy, where we assume that queries and documents are all observations from random variables.\n2:36\nAnd we assume there is a binary random variable called R here\n2:42\nto indicate whether a document is relevant to a query.\n2:46\nWe then define the score of document with respect to a query as a probability that this random variable R is equal to 1, given a particular document query. There are different cases of such a general idea. One is classic probabilistic model, another is language model, yet another is divergence from randomness model.\n3:12\nIn a later lecture, we will talk more about one case, which is language model. A third kind of model are based on probabilistic inference. So here the idea is to associate uncertainty to inference rules, and we can then quantify the probability that we can show that the query follows from the document.\n3:37\nFinally, there is also a family of models that are using axiomatic thinking. Here, an idea is to define a set of constraints that we hope a good retrieval function to satisfy.\n3:55\nSo in this case, the problem is to seek a good ranking function that can satisfy all the desired constraints.\n4:05\nInterestingly, although these different models are based on different thinking, in the end, the retrieval function tends to be very similar. And these functions tend to also involve similar variables. So now let's take a look at the common form of a state of the art retrieval model and to examine some of the common ideas used in all these models.\n4:33\nFirst, these models are all based on the assumption of using bag of words to represent text, and we explained this in the natural language processing lecture. Bag of words representation remains the main representation used in all the search engines.\n4:53\nSo with this assumption, the score of a query, like a presidential campaign news with respect to a document of d here, would be based on scores computed based on each individual word.\n5:09\nAnd that means the score would depend on the score of each word, such as presidential, campaign, and news. Here, we can see there are three different components, each corresponding to how well the document matches each of the query words.\n5:31\nInside of these functions, we see a number of heuristics used.\n5:38\nSo for example, one factor that affects the function d here is how many times does the word presidential occur in the document? This is called a term frequency, or TF.\n5:51\nWe might also denote as c of presidential and d. In general, if the word occurs more frequently in the document, then the value of this function would be larger. Another factor is, how long is the document? And this is to use the document length for scoring. In general, if a term occurs in a long document many times, it's not as significant as if it occurred the same number of times in a short document. Because in a long document, any term is expected to occur more frequently.\n6:38\nFinally, there is this factor called document frequency. That is, we also want to look at how often presidential occurs in the entire collection, and we call this document frequency, or df of presidential. And in some other models, we might also use a probability to characterize this information.\n7:05\nSo here, I show the probability of presidential in the collection.\n7:10\nSo all these are trying to characterize the popularity of the term in the collection. In general, matching a rare term in the collection is contributing more to the overall score than matching up common term.\n7:25\nSo this captures some of the main ideas used in pretty much older state of the art original models.\n7:34\nSo now, a natural question is, which model works the best?\n7:39\nNow it turns out that many models work equally well. So here are a list of the four major models that are generally regarded as a state of the art original models, pivoted length normalization, BM25, query likelihood, PL2. When optimized, these models tend to perform similarly. And this was discussed in detail in this reference at the end of this lecture. Among all these, BM25 is probably the most popular. It's most likely that this has been used in virtually all the search engines, and you will also often see this method discussed in research papers.\n8:22\nAnd we'll talk more about this method later in some other lectures.\n8:30\nSo, to summarize, the main points made in this lecture are first the design of a good ranking function pre-requires a computational definition of relevance, and we achieve this goal by designing appropriate retrieval model.\n8:47\nSecond, many models are equally effective, but we don't have a single winner yet. Researchers are still active and working on this problem, trying to find a truly optimal retrieval model.\n9:00\nFinally, the state of the art ranking functions tend to rely on the following ideas. First, bag of words representation. Second, TF and document frequency of words. Such information is used in the weighting function to determine the overall contribution of matching a word and document length. These are often combined in interesting ways, and we'll discuss how exactly they are combined to rank documents in the lectures later.\n9:36\nThere are two suggested additional readings if you have time.\n9:41\nThe first is a paper where you can find the detailed discussion and comparison of multiple state of the art models.\n9:49\nThe second is a book with a chapter that gives a broad review of different retrieval models. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/N5cBh/9-6-probabilistic-topic-models-expectation-maximization-algorithm-part-3dict_values(["List\nCS 410: Text Information Systems\nWeek 9\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:07\nSo, I just showed you that empirically the likelihood will converge, but theoretically it can also be proved that EM algorithm will converge to a local maximum. So here's just an illustration of what happened and a detailed explanation. This required more knowledge about that, some of that inequalities, that we haven't really covered yet.\n0:39\nSo here what you see is on the X dimension, we have a c0 value. This is a parameter that we have. On the y axis we see the likelihood function. So this curve is the original likelihood function, and this is the one that we hope to maximize. And we hope to find a c0 value at this point to maximize this. But in the case of Mitsumoto we can not easily find an analytic solution to the problem. So, we have to resolve the numerical errors, and the EM algorithm is such an algorithm. It's a Hill-Climb algorithm. That would mean you start with some random guess. Let's say you start from here, that's your starting point. And then you try to improve this by moving this to another point where you can have a higher likelihood. So that's the ideal hill climbing. And in the EM algorithm, the way we achieve this is to do two things. First, we'll fix a lower bound of likelihood function. So this is the lower bound. See here.\n1:51\nAnd once we fit the lower bound, we can then maximize the lower bound. And of course, the reason why this works, is because the lower bound is much easier to optimize. So we know our current guess is here. And by maximizing the lower bound, we'll move this point to the top. To here.\n2:13\nRight? And we can then map to the original likelihood function, we find this point. Because it's a lower bound, we are guaranteed to improve this guess, right? Because we improve our lower bound and then the original likelihood curve which is above this lower bound will definitely be improved as well.\n2:36\nSo we already know it's improving the lower bound. So we definitely improve this original likelihood function, which is above this lower bound. So, in our example, the current guess is parameter value given by the current generation. And then the next guess is the re-estimated parameter values. From this illustration you can see the next guess is always better than the current guess. Unless it has reached the maximum, where it will be stuck there. So the two would be equal. So, the E-step is basically to compute this lower bound. We don't directly just compute this likelihood function but we compute the length of the variable values and these are basically a part of this lower bound. This helps determine the lower bound. The M-step on the other hand is to maximize the lower bound. It allows us to move parameters to a new point. And that's why EM algorithm is guaranteed to converge to a local maximum.\n3:42\nNow, as you can imagine, when we have many local maxima, we also have to repeat the EM algorithm multiple times. In order to figure out which one is the actual global maximum. And this actually in general is a difficult problem in numeral optimization. So here for example had we started from here, then we gradually just climb up to this top. So, that's not optimal, and we'd like to climb up all the way to here, so the only way to climb up to this gear is to start from somewhere here or here. So, in the EM algorithm, we generally would have to start from different points or have some other way to determine a good initial starting point.\n4:29\nTo summarize in this lecture we introduced the EM algorithm. This is a general algorithm for computing maximum maximum likelihood estimate of all kinds of models, so not just for our simple model. And it's a hill-climbing algorithm, so it can only converge to a local maximum and it will depend on initial points.\n4:49\nThe general idea is that we will have two steps to improve the estimate of. In the E-step we roughly [INAUDIBLE] how many there are by predicting values of useful hidden variables that we would use to simplify the estimation. In our case, this is the distribution that has been used to generate the word. In the M-step then we would exploit such augmented data which would make it easier to estimate the distribution, to improve the estimate of parameters. Here improve is guaranteed in terms of the likelihood function. Note that it's not necessary that we will have a stable convergence of parameter value even though the likelihood function is ensured to increase. There are some properties that have to be satisfied in order for the parameters also to convert into some stable value.\n5:47\nNow here data augmentation is done probabilistically. That means, we're not going to just say exactly what's the value of a hidden variable. But we're going to have a probability distribution over the possible values of these hidden variables. So this causes a split of counts of events probabilistically.\n6:07\nAnd in our case we'll split the word counts between the two distributions. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/cMSgR/9-3-probabilistic-topic-models-mixture-model-estimation-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 9\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] Now lets look at another behaviour of the Mixed Model and in this case lets look at the response to data frequencies. So what you are seeing now is basically the likelihood of function for the two word document and we now in this case the solution is text. A probability of 0.9 and the a probability of 0.1. Now it's interesting to think about a scenario where we start adding more words to the document. So what would happen if we add many the's to the document?\n0:41\nNow this would change the game, right? So, how? Well, picture, what would the likelihood function look like now? Well, it start with the likelihood function for the two words, right? As we add more words, we know that. But we have to just multiply the likelihood function by additional terms to account for the additional. occurrences of that. Since in this case, all the additional terms are the, we're going to just multiply by this term. Right? For the probability of the.\n1:12\nAnd if we have another occurrence of the, we'd multiply again by the same term, and so on and forth. Add as many terms as the number of the's that we add to the document, d'. Now this obviously changes the likelihood function. So what's interesting is now to think about how would that change our solution? So what's the optimal solution now?\n1:38\nNow, intuitively you'd know the original solution, pulling the 9 versus pulling the ,will no longer be optimal for this new function. Right?\n1:48\nBut, the question is how should we change it. What general is to sum to one. So he know we must take away some probability the mass from one word and add the probability mass to the other word. The question is which word to have reduce the probability and which word to have a larger probability. And in particular, let's think about the probability of the. Should it be increased to be more than 0.1? Or should we decrease it to less than 0.1? What do you think?\n2:19\nNow you might want to pause the video a moment to think more about. This question. Because this has to do with understanding of important behavior of a mixture model. And indeed, other maximum likelihood estimator. Now if you look at the formula for a moment, then you will see it seems like another object Function is more influenced by the than text. Before, each computer. So now as you can imagine, it would make sense to actually assign a smaller probability for text and lock it. To make room for a larger probability for the. Why? Because the is repeated many times. If we increase it a little bit, it will have more positive impact. Whereas a slight decrease of text will have relatively small impact because it occurred just one, right? So this means there is another behavior that we observe here. That is high frequency words generated with high probabilities from all the distributions. And, this is no surprise at all, because after all, we are maximizing the likelihood of the data. So the more a word occurs, then it makes more sense to give such a word a higher probability because the impact would be more on the likelihood function. This is in fact a very general phenomenon of all the maximum likelihood estimator. But in this case, we can see as we see more occurrences of a term, it also encourages the unknown distribution theta sub d to assign a somewhat higher probability to this word.\n4:07\nNow it's also interesting to think about the impact of probability of Theta sub B. The probability of choosing one of the two component models. Now we've been so far assuming that each model is equally likely. And that gives us 0.5. But you can again look at this likelihood function and try to picture what would happen if we increase the probability of choosing a background model. Now you will see these terms for the, we have a different form where the probability that would be\n4:40\neven larger because the background has a high probability for the word and the coefficient in front of 0.9 which is now 0.5 would be even larger. When this is larger, the overall result would be larger. And that also makes this the less important for theta sub d to increase the probability before the. Because it's already very large. So the impact here of increasing the probability of the is somewhat regulated by this coefficient, the point of i. If it's larger on the background, then it becomes less important to increase the value. So this means the behavior here, which is high frequency words tend to get the high probabilities, are effected or regularized somewhat by the probability of choosing each component. The more likely a component is being chosen. It's more important that to have higher values for these frequent words. If you have a various small probability of being chosen, then the incentive is less. So to summarize, we have just discussed the mixture model. And we discussed that the estimation problem of the mixture model and particular with this discussed some general behavior of the estimator and that means we can expect our estimator to capture these infusions. First every component model attempts to assign high probabilities to high frequent their words in the data. And this is to collaboratively maximize likelihood. Second, different component models tend to bet high probabilities on different words. And this is to avoid a competition or waste of probability. And this would allow them to collaborate more efficiently to maximize the likelihood.\n6:33\nSo, the probability of choosing each component regulates the collaboration and the competition between component models. It would allow some component models to respond more to the change, for example, of frequency of the theta point in the data.\n6:53\nWe also talked about the special case of fixing one component to a background word distribution, right? And this distribution can be estimated by using a collection of documents, a large collection of English documents, by using just one distribution and then we'll just have normalized frequencies of terms to give us the probabilities of all these words. Now when we use such a specialized mixture model, we show that we can effectively get rid of that one word in the other component.\n7:23\nAnd that would make this cover topic more discriminative.\n7:27\nThis is also an example of imposing a prior on the model parameter and the prior here basically means one model must be exactly the same as the background language model and if you recall what we talked about in Bayesian estimation, and this prior will allow us to favor a model that is consistent with our prior. In fact, if it's not consistent we're going to say the model is impossible. So it has a zero prior probability. That effectively excludes such a scenario. This is also issue that we'll talk more later. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/kM6Ie/lesson-4-6-smoothing-methods-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 4\nLesson 4.6: Smoothing Methods - Part 1\nPrevious\nNext\nWeek 4 Information\nWeek 4 Lessons\nVideo:\nVideo\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.2: Statistical Language Model\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 4.3: Query Likelihood Retrieval Function\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.4: Statistical Language Model - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.5: Statistical Language Model - Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.6: Smoothing Methods - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.7: Smoothing Methods - Part 2\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nProgramming Assignment 2.2\nLesson 4.6: Smoothing Methods - Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:07\nThis lecture is about the specific smoothing methods for language models used in probabilistic retrieval model.\n0:16\nIn this lecture, we will continue the discussion of language models for information retrieval, particularly the query likelihood retrieval method. And we're going to talk about specifically the smoothing methods used for such a retrieval function.\n0:33\nSo this is a slide from a previous lecture where we show that with a query likelihood ranking and smoothing with the collection language model, we add up having a retrieval function that looks like the following. So this is the retrieval function based on these assumptions that we have discussed. You can see it's a sum of all the matching query terms, here. And inside its sum is the count of the term in the query and some weight for the term in the document.\n1:12\nWe have t of i, the f weight here, and then we have another constant here in n.\n1:20\nSo clearly if we want to implement this function using programming language, we still need to figure out a few variables. In particular, we're going to need to know how to estimate the probability of a word exactly and how do we set alpha.\n1:40\nSo in order to answer this question, we have to think about very specific smoothing methods, and that is main topic of this lecture.\n1:48\nWe're going to talk about two smoothing methods. The first is simple linear interpolation with a fixed coefficient. And this is also called a Jelinek-Mercer smoothing.\n2:01\nSo the idea is actually very simple. This picture shows how we estimate a document language model by using maximum likelihood estimate. That gives us word counts normalized by the total number of words in the text. The idea of using this method\n2:22\nis to maximize the probability of the observed text. As a result, if a word like network is not observed in the text, it's going to get 0 probability, as shown here.\n2:37\nSo the idea of smoothing, then, is to rely on collection language model where this word is not going to have a zero probability to help us decide what nonzero probability should be assigned to such a word. So we can note that network has a nonzero probability here. So in this approach what we do is we do a linear interpolation between the maximum likelihood placement here and the collection language model, and this is computed by the smoothing parameter lambda, which is between 0 and 1. So this is a smoothing parameter. The larger lambda is, the more smoothing we will have. So by mixing them together, we achieve the goal of assigning nonzero probabilities to a word like network. So let's see how it works for some of the words here.\n3:32\nFor example, if we compute the smooth probability for text.\n3:37\nNow the maximum likelihood estimated gives us 10 over 100, and that's going to be here.\n3:44\nBut the collection probability is this. So we'll just combine them together with this simple formula.\n3:53\nWe can also see the word network, which used to have a zero probability, now is getting a non-zero probability of this value. And that's because the count is going to be zero for network here. But this part is nonzero, and that's basically how this method works. Now if you think about this and you can easily see now the alpha sub d in this smoothing method is basically lambda. Because that's remember the coefficient in front of the probability of the word given by the collection language model here. Okay, so this is the first smoothing method. The second one is similar but it has a tie-in into the coefficient for linear interpolation. It's often called Dirichlet Prior, or Bayesian, Smoothing.\n4:54\nSo again here we face problem of zero probability for an unseen word like network.\n5:03\nAgain we will use the collection language model, but in this case, we're going to combine them in somewhat different ways. The formula first can be seen as a interpolation of the maximum likelihood estimate and the collection language model as before, as in the J-M smoothing method. Only that the coefficient now is not lambda, a fixed number, but a dynamic coefficient in this form, where mu is a parameter, it's a non-negative value. And you can see if we set mu to a constant, the effect is that a long document would actually get a smaller coefficient here.\n5:46\nBecause a long document will have longer lengths, therefore the coefficient is actually smaller. And so a long document would have less smoothing, as we would expect. So this seems to make more sense than a fixed coefficient smoothing. Of course, this part would be of this form so that the two coefficients would sum to 1. Now this is one way to understand this smoothing. Basically, it means it's a dynamic coefficient interpolation.\n6:22\nThere is another way to understand this formula which is even easier to remember, and that's on this side.\n6:33\nSo it's easier to see how we can rewrite the smoothing method in this form. Now in this form we can easily see what change we have made to the maximum likelihood estimate, which would be this part. So normalize the count by the document length. So in this form we can see what we did is we add this to the count of every word.\n7:01\nSo what does this mean? Well, this is basically something related to the probability of the word in the collection.\n7:10\nAnd we multiply that by the parameter mu.\n7:14\nAnd when we combine this with the count here, essentially we are adding pseudocounts to the observed text. We pretend every word has got this many pseudocount. So the total count would be the sum of these pseudocounts and the actual count of the word in the document.\n7:39\nAs a result, in total we would have added this many pseudocounts. Why? Because if you take somewhat this one\n7:50\nover all the words, then we'll see the probability of the words would sum to 1, and that gives us just mu. So this is the total number of pseudocounts that we added.\n8:01\nAnd so these probabilities would still sum to 1. So in this case, we can easily see the method is essentially to\n8:13\nadd this as a pseudocount to this data. Pretend we actually augment the data by including some pseudo data defined by the collection language model. As a result, we have more counts is that the total counts for a word would be like this. And as a result, even if a word has zero count here, let's say if we have zero count here, then it would still have nonzero count because of this part. So this is how this method works. Let's also take a look at some specific example here. So for text again we will have 10 as the original count that we actually observe, but we also add some pseudocount. And so the probability of text would be of this form. Naturally, the probability of network would be just this part. And so here you can also see what's alpha sub d here.\n9:15\nCan you see it? If you want to think about it, you can pause the video.\n9:20\nBut you'll notice that this part is basically alpha sub d. So we can see, in this case, alpha sub d does depend on the document, because this length depends on the document, whereas in the linear interpolation, the J-M smoothing method, this is a constant. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/Yxm53/11-4-text-categorization-evaluation-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 11\n11.4 Text Categorization: Evaluation Part 2\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nVideo:\nVideo\n11.1 Text Categorization: Discriminative Classifier Part 1\n. Duration: 20 minutes\n20 min\nVideo:\nVideo\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\n. Duration: 31 minutes\n31 min\nVideo:\nVideo\n11.3 Text Categorization: Evaluation Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n11.4 Text Categorization: Evaluation Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n11.5 Opinion Mining and Sentiment Analysis: Motivation\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\n11.4 Text Categorization: Evaluation Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is a continued discussion of evaluation of text categorization. Earlier we have introduced measures that can be used with computer provision and recall. For each category and each document now in this lecture we're going to\n0:27\nfurther examine how to combine the performance of the different categories of different documents how to aggregate them, how do we take average? You see on the title here I indicated it's called a macro average and this is in contrast to micro average that we'll talk more about later.\n0:47\nSo, again, for each category we're going to compute the precision require an f1 so for example category c1 we have precision p1, recall r1 and F value f1. And similarly we can do that for category 2 and and all the other categories. Now once we compute that and we can aggregate them, so for example we can aggregate all the precision values. For all the categories, for computing overall precision. And this is often very useful to summarize what we have seen in the whole data set. And aggregation can be done many different ways. Again as I said, in a case when you need to aggregate different values, it's always good to think about what's the best way of doing the aggregation. For example, we can consider arithmetic mean, which is very commonly used, or you can use geometric mean, which would have different behavior. Depending on the way you aggregate, you might have got different conclusions. in terms of which method works better, so it's important to consider these differences and choosing the right one or a more suitable one for your task. So the difference fore example between arithmetically and geometrically is that the arithmetically would be dominated by high values whereas geometrically would be more affected by low values. Base and so whether you are want to emphasis low values or high values would be a question relate with all you And similar we can do that for recal and F score. So that's how we can generate the overall precision, recall and F score.\n2:31\nNow we can do the same for aggregation of other all the document All right. So it's exactly the same situation for each document on our computer. Precision, recall, and F. And then after we have completed the computation for all these documents, we're going to aggregate them to generate the overall precision, overall recall, and overall F score.\n2:53\nThese are, again, examining the results from different angles. Which one's more useful will depend on your application. In general, it's beneficial to look at the results from all these perspectives. And especially if you compare different methods in different dimensions, it might reveal which method Is better in which measure or in what situations and this provides insightful. Understanding the strands of a method or a weakness and this provides further insight for improving them.\n3:28\nSo as I mentioned, there is also micro-average in contrast to the macro average that we talked about earlier. In this case, what we do is you pool together all the decisions, and then compute the precision and recall.\n3:45\nSo we can compute the overall precision and recall by just counting how many cases are in true positive, how many cases in false positive, etc, it's computing the values in the contingency table, and then we can compute the precision and recall just once.\n4:06\nIn contrast, in macro-averaging, we're going to do that for each category first. And then aggregate over these categories or we do that for each document and then aggregate all the documents but here we pooled them together.\n4:21\nNow this would be very similar to the classification accuracy that we used earlier, and one problem here of course to treat all the instances, all the decisions equally.\n4:32\nAnd this may not be desirable.\n4:36\nBut it may be a property for some applications, especially if we associate the, for example, the cost for each combination. Then we can actually compute for example, weighted classification accuracy. Where you associate the different cost or utility for each specific decision,\n4:56\nso there could be variations of these methods that would be more useful. But in general macro average tends to be more information than micro average, just because it might reflect the need for understanding performance\n5:14\non each category or performance on each document which are needed in applications. But macro averaging and micro averaging, they are both very common, and you might see both reported in research papers on Categorization. Also sometimes categorization results might actually be evaluated from ranking prospective.\n5:40\nAnd this is because categorization results are sometimes or often indeed passed it to a human for various purposes. For example, it might be passed to humans for further editing. For example, news articles can be tempted to be categorized by using a system and then human editors would then correct them.\n6:02\nAnd all the email messages might be throughout to the right person for handling in the help desk. And in such a case the categorizations will help prioritizing the task for particular customer service person.\n6:19\nSo, in this case the results have to be prioritized\n6:26\nand if the system can't give a score to the categorization decision for confidence then we can use the scores to rank these decisions and then evaluate the results as a rank list, just as in a search engine. Evaluation where you rank the documents in responsible query.\n6:49\nSo for example a discovery of spam emails can be evaluated\n6:55\nbased on ranking emails for the spam category. And this is useful if you want people to to verify whether this is really spam, right? The person would then take the rank To check one by one and then verify whether this is indeed a spam. So to reflect the utility for humans in such a task, it's better to evaluate Ranking Chris and this is basically similar to a search again.\n7:25\nAnd in such a case often the problem can be better formulated as a ranking problem instead of a categorization problem. So for example, ranking documents in a search engine can also be framed as a binary categorization problem, distinguish the relevant documents that are useful to users from those that are not useful, but typically we frame this as a ranking problem, and we evaluate it as a rank list. That's because people tend to examine the results so\n7:52\nranking evaluation more reflects utility from user's perspective.\n7:58\nSo to summarize categorization evaluation, first evaluation is always very important for all these tasks. So get it right.\n8:07\nIf you don't get it right, you might get misleading results. And you might be misled to believe one method is better than the other, which is in fact not true. So it's very important to get it right.\n8:18\nMeasures must also reflect the intended use of the results for a particular application. For example, in spam filtering and news categorization the results are used in maybe different ways.\n8:30\nSo then we would need to consider the difference and design measures appropriately.\n8:36\nWe generally need to consider how will the results be further processed by the user and think from a user's perspective. What quality is important? What aspect of quality is important?\n8:49\nSometimes there are trade offs between multiple aspects like precision and recall and so we need to know for this application is high recall more important, or high precision is more important.\n8:59\nIdeally we associate the different cost with each different decision arrow. And this of course has to be designed in an application specific way.\n9:08\nSome commonly used measures for relative comparison methods are the following. Classification accuracy, it's very commonly used for especially balance. [INAUDIBLE] preceding [INAUDIBLE] Scores are common and report characterizing performances, given angles and give us some [INAUDIBLE] like a [INAUDIBLE] Per document basis [INAUDIBLE] And then take a average of all of them, different ways micro versus macro [INAUDIBLE]. In general, you want to look at the results from multiple perspectives and for particular applications some perspectives would be more important than others but diagnoses and analysis of categorization methods. It's generally useful to look at as many perspectives as possible to see subtle differences between methods or tow see where a method might be weak from which you can obtain sight for improving a method.\n10:04\nFinally sometimes ranking may be more appropriate so be careful sometimes categorization has got may be better frame as a ranking tasks and there're machine running methods for optimizing ranking measures as well.\n10:17\nSo here are two suggested readings. One is some chapters of this book where you can find more discussion about evaluation measures. The second is a paper about comparison of different approaches to text categorization and it also has an excellent discussion of how to evaluate textual categorization. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/3bVa3/12-5-contextual-text-mining-contextual-probabilistic-latent-semantic-analysisdict_values(["List\nCS 410: Text Information Systems\nWeek 12\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\nPrevious\nNext\nWeek 12 Information\nWeek 12 Lessons\nVideo:\nVideo\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.3 Text-Based Prediction\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n12.4 Contextual Text Mining: Motivation\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\n12.8 Summary for Exam 2\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC] This lecture is about a specific technique for Contextual Text Mining called Contextual Probabilistic Latent Semantic Analysis.\n0:19\nIn this lecture, we're going to continue discussing Contextual Text Mining. And we're going to introduce Contextual Probablitistic Latent Semantic Analysis as exchanging of POS for doing contextual text mining.\n0:34\nRecall that in contextual text mining we hope to analyze topics in text, in consideration of the context so that we can associate the topics with a property of the context were interesting.\n0:48\nSo in this approach, contextual probabilistic latent semantic analysis, or CPLSA, the main idea is to express to the add interesting context variables into a generating model.\n1:03\nRecall that before when we generate the text we generally assume we'll start wIth some topics, and then assemble words from some topics. But here, we're going to add context variables, so that the coverage of topics, and also the content of topics would be tied in context. Or in other words, we're going to let the context Influence both coverage and the content of a topic.\n1:31\nThe consequences that this will enable us to discover contextualized topics. Make the topics more interesting, more meaningful. Because we can then have topics that can be interpreted as specifically to a particular context that we are interested in. For example, a particular time period.\n1:52\nAs an extension of PLSA model, CPLSA does the following changes. Firstly it would model the conditional likelihood of text given context.\n2:07\nThat clearly suggests that the generation of text would then depend on context, and that allows us to bring context into the generative model.\n2:18\nSecondly, it makes two specific assumptions about the dependency of topics on context. One is to assume that depending on the context, depending on different time periods or different locations, we assume that there are different views of a topic or different versions of word descriptions that characterize a topic.\n2:38\nAnd this assumption allows us to discover different variations of the same topic in different contexts.\n2:46\nThe other is that we assume the topic coverage also depends on the context.\n2:55\nThat means depending on the time or location, we might cover topics differently.\n3:00\nAgain, this dependency would then allow us to capture the association of topics with specific contexts. We can still use the EM algorithm to solve the problem of parameter estimation.\n3:16\nAnd in this case, the estimated parameters would naturally contain context variables. And in particular, a lot of conditional probabilities of topics given certain context. And this is what allows you to do contextual text mining. So this is the basic idea.\n3:35\nNow, we don't have time to introduce this model in detail, but there are references here that you can look into to know more detail. Here I just want to explain the high level ideas in more detail. Particularly I want to explain the generation process. Of text data that has context associated in such a model.\n4:01\nSo as you see here, we can assume there are still multiple topics. For example, some topics might represent a themes like a government response, donation Or the city of New Orleans. Now this example is in the context of Hurricane Katrina and that hit New Orleans.\n4:22\nNow as you can see we assume there are different views associated with each of the topics. And these are shown as View 1, View 2, View 3. Each view is a different version of word distributions. And these views are tied to some context variables. For example, tied to the location Texas, or the time July 2005, or the occupation of the author being a sociologist.\n4:56\nNow, on the right side, now we assume the document has context information. So the time is known to be July 2005. The location is Texas, etc. And such context information is what we hope to model as well. So we're not going to just model the text.\n5:15\nAnd so one idea here is to model the variations of top content and various content. And this gives us different views of the water distributions.\n5:27\nNow on the bottom you will see the theme coverage of top Coverage might also vary according to these context because in the case of a location like Texas, people might want to cover the red topics more. That's New Orleans. That's visualized here. But in a certain time period, maybe Particular topic and will be covered more. So this variation is also considered in CPLSA. So to generate the searcher document With context, with first also choose a view.\n6:08\nAnd this view of course now could be from any of these contexts. Let's say, we have taken this view that depends on the time. In the middle. So now, we will have a specific version of word distributions. Now, you can see some probabilities of words for each topic.\n6:26\nNow, once we have chosen a view, now the situation will be very similar to what happened in standard ((PRSA)) We assume we have got word distribution associated with each topic, right?\n6:39\nAnd then next, we will also choose a coverage from the bottom, so we're going to choose a particular coverage, and that coverage, before is fixed in PLSA, and assigned to a particular document. Each document has just one coverage distribution.\n6:58\nNow here, because we consider context, so the distribution of topics or the coverage of Topics can vary depending on the context that has influenced the coverage.\n7:10\nSo, for example, we might pick a particular coverage. Let's say in this case we picked a document specific coverage.\n7:20\nNow with the coverage and these word distributions we can generate a document in exactly the same way as in PLSA. So what it means, we're going to use the coverage to choose a topic, to choose one of these three topics. Let's say we have picked the yellow topic. Then we'll draw a word from this particular topic on the top.\n7:44\nOkay, so we might get a word like government. And then next time we might choose a different topic, and we'll get donate, etc. Until we generate all the words. And this is basically the same process as in PLSA.\n8:00\nSo the main difference is when we obtain the coverage. And the word distribution, we let the context influence our choice So in other words we have extra switches that are tied to these contacts that will control the choices of different views of topics and the choices of coverage.\n8:22\nAnd naturally the model we have more parameters to estimate. But once we can estimate those parameters that involve the context, then we will be able to understand the context specific views of topics, or context specific coverages of topics. And this is precisely what we want in contextual text mining.\n8:40\nSo here are some simple results. From using such a model. Not necessary exactly the same model, but similar models. So on this slide you see some sample results of comparing news articles about Iraq War and Afghanistan War.\n8:56\nNow we have about 30 articles on Iraq wa,r and 26 articles on Afghanistan war. And in this case, the goal is to review the common topic. It's covered in both sets of articles and the differences of variations of the topic in each of the two collections.\n9:18\nSo in this case the context is explicitly specified by the topic or collection.\n9:25\nAnd we see the results here show that there is a common theme that's corresponding to Cluster 1 here in this column. And there is a common theme indicting that United Nations is involved in both Wars. It's a common topic covered in both sets of articles. And that's indicated by the high probability words shown here, united and nations.\n9:51\nNow if you know the background, of course this is not surprising and this topic is indeed very relevant to both wars. If you look at the column further and then what's interesting's that the next two cells of word distributions actually tell us collection specific variations of the topic of United Nations. So it indicates that the Iraq War, United Nations was more involved in weapons factions, whereas in the Afghanistan War it was more involved in maybe aid to Northern Alliance. It's a different variation of the topic of United Nations.\n10:30\nSo this shows that by bringing the context. In this case different the walls or different the collection of texts. We can have topical variations tied to these contexts, to review the differences of coverage of the United Nations in the two wars.\n10:46\nNow similarly if you look at the second cluster Class two, it has to do with the killing of people, and, again, it's not surprising if you know the background about wars. All the wars involve killing of people, but imagine if you are not familiar with the text collections. We have a lot of text articles, and such a technique can reveal the common topics covered in both sets of articles. It can be used to review common topics in multiple sets of articles as well. If you look at of course in that column of cluster two, you see variations of killing of people and that corresponds to different contexts\n11:28\nAnd here is another example of results obtained from blog articles about Hurricane Katrina.\n11:37\nIn this case, what you see here is visualization of the trends of topics over time.\n11:47\nAnd the top one shows just the temporal trends of two topics. One is oil price, and one is about the flooding of the city of New Orleans.\n12:00\nNow these topics are obtained from blog articles about Hurricane Katrina.\n12:07\nAnd people talk about these topics. And end up teaching to some other topics. But the visualisation shows that with this technique, we can have conditional distribution of time. Given a topic. So this allows us to plot this conditional probability the curve is like what you're seeing here. We see that, initially, the two curves tracked each other very well. But later we see the topic of New Orleans was mentioned again but oil price was not. And this turns out to be the time period when another hurricane, hurricane Rita hit the region. And that apparently triggered more discussion about the flooding of the city.\n12:54\nThe bottom curve shows the coverage of this topic about flooding of the city by block articles in different locations. And it also shows some shift of coverage that might be related to people's migrating from the state of Louisiana to Texas for example.\n13:20\nSo in this case we can see the time can be used as context to review trends of topics.\n13:27\nThese are some additional results on spacial patterns. In this case it was about the topic of government response. And there was some criticism about the slow response of government in the case of Hurricane Katrina.\n13:44\nAnd the discussion now is covered in different locations. And these visualizations show the coverage in different weeks of the event. And initially it's covered mostly in the victim states, in the South, but then gradually spread into other locations. But in week four, which is shown on the bottom left, we see a pattern that's very similar to the first week on the top left. And that's when again Hurricane Rita hit in the region. So such a technique would allow us to use location as context to examine their issues of topics. And of course the moral is completely general so you can apply this to any other connections of text. To review spatial temporal patterns.\n14:34\nHis view found another application of this kind of model, where we look at the use of the model for event impact analysis.\n14:43\nSo here we're looking at the research articles information retrieval. IR, particularly SIGIR papers. And the topic we are focusing on is about the retrieval models. And you can see the top words with high probability about this model on the left.\n14:59\nAnd then we hope to examine the impact of two events. One is a start of TREC, for Text and Retrieval Conference. This is a major evaluation sponsored by U.S. government, and was launched in 1992 or around that time. And that is known to have made a impact on the topics of research information retrieval.\n15:23\nThe other is the publication of a seminal paper, by Croft and Porte. This is about a language model approach to information retrieval. It's also known to have made a high impact on information retrieval research. So we hope to use this kind of model to understand impact. The idea here is simply to use the time as context. And use these events to divide the time periods into a period before. For the event and another after this event. And then we can compare the differences of the topics. The and the variations, etc. So in this case, the results show before track the study of retrieval models was mostly a vector space model, Boolean model etc. But the after Trec, apparently the study of retrieval models have involved a lot of other words. That seems to suggest some different retrieval tasks, so for example, email was used in the enterprise search tasks and subtopical retrieval was another task later introduced by Trec.\n16:28\nOn the bottom, we see the variations that are correlated with the propagation of the language model paper. Before, we have those classic probability risk model, logic model, Boolean etc., but after 1998, we see clear dominance of language model as probabilistic models. And we see words like language model, estimation of parameters, etc. So this technique here can use events as context to understand the impact of event. Again the technique is generals so you can use this to analyze the impact of any event. Here are some suggested readings.\n17:11\nThe first is paper about simple staging of psi to label cross-collection comparison.\n17:21\nIt's to perform comparative text mining to allow us to extract common topics shared by multiple collections. And there are variations in each collection.\n17:31\nThe second one is the main paper about the CPLSA model. Was a discussion of a lot of applications. The third one has a lot of details about the special temporal patterns for the Hurricane Katrina example. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/rLpwp/lesson-1-1-natural-language-content-analysisdict_values(['List\nCS 410: Text Information Systems\nWeek 1\nLesson 1.1: Natural Language Content Analysis\nPrevious\nNext\nOrientation Information\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nVideo:\nVideo\nLesson 1.1: Natural Language Content Analysis\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 1.2: Text Access\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.3: Text Retrieval Problem\n. Duration: 26 minutes\n26 min\nVideo:\nVideo\nLesson 1.4: Overview of Text Retrieval Methods\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 1.5: Vector Space Model - Basic Idea\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\n. Duration: 17 minutes\n17 min\nWeek 1 Activities\nLesson 1.1: Natural Language Content Analysis\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:09\n>> This lecture is about Natural Language of Content Analysis. As you see from this picture, this is really the first step to process any text data. Text data are in natural languages. So computers have to understand natural languages to some extent, in order to make use of the data. So that\'s the topic of this lecture. We\'re going to cover three things. First, what is natural language processing, which is the main technique for processing natural language to obtain understanding.\n0:43\nThe second is the state of the art of NLP which stands for natural language processing.\n0:49\nFinally we\'re going to cover the relation between natural language processing and text retrieval. First, what is NLP? Well the best way to explain it is to think about if you see a text in a foreign language that you can understand.\n1:06\nNow what do you have to do in order to understand that text? This is basically what computers are facing. So looking at the simple sentence like a dog is chasing a boy on the playground.\n1:18\nWe don\'t have any problems understanding this sentence. But imagine what the computer would have to do in order to understand it. Well in general, it would have to do the following. First, it would have to know dog is a noun, chasing\'s a verb, etc. So this is called lexical analysis, or part-of-speech tagging, and we need to figure out the syntactic categories of those words. So that\'s the first step. After that, we\'re going to figure out the structure of the sentence. So for example, here it shows that A and the dog would go together to form a noun phrase.\n1:55\nAnd we won\'t have dog and is to go first. And there are some structures that are not just right.\n2:04\nBut this structure shows what we might get if we look at the sentence and try to interpret the sentence. Some words would go together first, and then they will go together with other words.\n2:16\nSo here we show we have noun phrases as intermediate components, and then verbal phrases. Finally we have a sentence. And you get this structure. We need to do something called a semantic analysis, or parsing. And we may have a parser accompanying the program, and that would automatically created this structure. At this point you would know the structure of this sentence, but still you don\'t know the meaning of the sentence. So we have to go further to semantic analysis. In our mind we usually can map such a sentence to what we already know in our knowledge base. For example, you might imagine a dog that looks like that. There\'s a boy and there\'s some activity here. But for a computer would have to use symbols to denote that.\n3:00\nWe\'d use a symbol (d1) to denote a dog. And (b)1 can denote a boy and then (p)1 can denote a playground.\n3:12\nNow there is also a chasing activity that\'s happening here so we have a relationship chasing that connects all these symbols. So this is how a computer would obtain some understanding of this sentence.\n3:25\nNow from this representation we could also further infer some other things, and we might indeed naturally think of something else when we read a text and this is called inference. So for example, if you believe that if someone\'s being chased and this person might be scared, but with this rule, you can see computers could also infer that this boy maybe scared. So this is some extra knowledge that you\'d infer based on some understanding of the text. You can even go further to understand why the person say at this sentence. So this has to do as a use of language. This is called pragmatic analysis. In order to understand the speak actor of a sentence, right? We say something to basically achieve some goal. There\'s some purpose there. And this has to do with the use of language. In this case the person who said this sentence might be reminding another person to bring back the dog. That could be one possible intent.\n4:33\nTo reach this level of understanding would require all of these steps and a computer would have to go through all these steps in order to completely understand this sentence. Yet we humans have no trouble with understanding that, we instantly would get everything.\n4:52\nThere is a reason for that. That\'s because we have a large knowledge base in our brain and we can use common sense knowledge to help interpret the sentence. Computers unfortunately are hard to obtain such understanding. They don\'t have such a knowledge base. They are still incapable of doing reasoning and uncertainties,\n5:14\nso that makes natural language processing difficult for computers. But the fundamental reason why natural language processing is difficult for computers is simply because natural language has not been designed for computers. Natural languages are designed for us to communicate. There are other languages designed for computers. For example, programming languages. Those are harder for us, right? So natural languages is designed to make our communication efficient. As a result, we omit a lot of common sense knowledge because we assume everyone knows about that. We also keep a lot of ambiguities because we assume the receiver or the hearer could know how to decipher an ambiguous word based on the knowledge or the context. There\'s no need to demand different words for different meanings. We could overload the same word with different meanings without the problem.\n6:10\nBecause of these reasons this makes every step in natural language of processing difficult for computers, ambiguity is the main difficulty.\n6:18\nAnd common sense and reasoning is often required, that\'s also hard.\n6:23\nSo let me give you some examples of challenges here.\n6:27\nConsider the word level ambiguity.\n6:30\nThe same word can have different syntactic categories. For example design can be a noun or a verb.\n6:39\nThe word of root may have multiple meanings. So square root in math sense or the root of a plant.\n6:46\nYou might be able to think about it\'s meanings. There are also syntactical ambiguities. For example, the main topic of this lecture, natural language processing, can actually be interpreted in two ways in terms of the structure. Think for a moment and see if you can figure that out. We usually think of this as processing of natural language, but you could also think of this as do say, language processing is natural.\n7:16\nSo this is an example of synaptic ambiguity. What we have different is structures that can be\n7:24\napplied to the same sequence of words. Another common example of an ambiguous sentence is the following. A man saw a boy with a telescope. Now in this case the question is, who had a telescope.\n7:38\nThis is called a prepositional phrase attachment ambiguity or PP attachment ambiguity. Now we generally don\'t have a problem with these ambiguities because we have a lot of background knowledge to help us disambiguate the ambiguity.\n7:55\nAnother example of difficulty is anaphora resolution. So think about the sentence John persuaded Bill to buy a TV for himself. The question here is does himself refer to John or Bill? So again this is something that you have to use some background or the context to figure out. Finally, presupposition is another problem. Consider the sentence, he has quit smoking. Now this obviously implies that he smoked before.\n8:22\nSo imagine a computer wants to understand all these subtle differences and meanings. It would have to use a lot of knowledge to figure that out. It also would have to maintain a large knowledge base of all the meanings of words and how they are connected to our common sense knowledge of the world. So this is why it\'s very difficult.\n8:45\nSo as a result, we are steep not perfect, in fact far from perfect in understanding natural language using computers. So this slide sort of gains a simplified view of state of the art technologies.\n9:01\nWe can do part of speech tagging pretty well, so I showed  accuracy here. Now this number is obviously based on a certain dataset, so don\'t take this literally. This just shows that we can do it pretty well. But it\'s still not perfect. In terms of parsing, we can do partial parsing pretty well. That means we can get noun phrase structures, or verb phrase structure, or some segment of the sentence, and this dude correct them in terms of the structure.\n9:34\nAnd in some evaluation results, we have seen above  accuracy in terms of partial parsing of sentences. Again, I have to say these numbers are relative to the dataset. In some other datasets, the numbers might be lower. Most of the existing work has been evaluated using news dataset. And so a lot of these numbers are more or less biased toward news data. Think about social media data, the accuracy likely is lower.\n10:05\nIn terms of a semantical analysis, we are far from being able to do a complete understanding of a sentence. But we have some techniques that would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that can allow us to extract the entities and relations mentioned in text articles. For example, recognizing dimensions of people, locations, organizations, etc in text. So this is called entity extraction. We may be able to recognize the relations. For example, this person visited that place or this person met that person or this company acquired another company. Such relations can be extracted by using the computer current Natural Language Processing techniques. They\'re not perfect but they can do well for some entities. Some entities are harder than others.\n11:03\nWe can also do word sense disintegration to some extend. We have to figure out whether this word in this sentence would have certain meaning in another context the computer could figure out, it has a different meaning. Again, it\'s not perfect, but you can do something in that direction.\n11:19\nWe can also do sentiment analysis, meaning, to figure out whether a sentence is positive or negative. This is especially useful for review analysis, for example.\n11:30\nSo these are examples of semantic analysis. And they help us to obtain partial understanding of the sentences.\n11:38\nIt\'s not giving us a complete understanding, as I showed it before, for this sentence. But it would still help us gain understanding of the content. And these can be useful.\n11:51\nIn terms of inference, we are not there yet, probably because of the general difficulty of inference and uncertainties. This is a general challenge in artificial intelligence. Now that\'s probably also because we don\'t have complete semantical representation for natural [INAUDIBLE] text. So this is hard. Yet in some domains perhaps, in limited domains when you have a lot of restrictions on the word uses, you may be able to perform inference to some extent. But in general we can not really do that reliably. Speech act analysis is also far from being done and we can only do that analysis for very special cases. So this roughly gives you some idea about the state of the art. And then we also talk a little bit about what we can\'t do, and so we can\'t even do  part of speech tagging. Now this looks like a simple task, but think about the example here, the two uses of off may have different syntactic categories if you try to make a fine grained distinctions. It\'s not that easy to figure out such differences.\n13:10\nIt\'s also hard to do general complete parsing. And again, the same sentence that you saw before is example.\n13:18\nThis ambiguity can be very hard to disambiguate and you can imagine example where you have to use a lot of knowledge in the context of the sentence or from the background, in order to figure out who actually had the telescope. So although the sentence looks very simple, it actually is pretty hard. And in cases when the sentence is very long, imagine it has four or five prepositional phrases, and there are even more possibilities to figure out.\n13:48\nIt\'s also harder to do precise deep semantic analysis. So here\'s an example. In the sentence "John owns a restaurant." How do we define owns exactly? The word own, it is something that we can understand but it\'s very hard to precisely describe the meaning of own for computers.\n14:11\nSo as a result we have a robust and a general Natural Language Processing techniques that can process a lot of text data.\n14:22\nIn a shallow way, meaning we only do superficial analysis. For example, parts of speech tagging or a partial parsing or recognizing sentiment. And those are not deep understanding, because we\'re not really understanding the exact meaning of the sentence.\n14:41\nOn the other hand of the deep understanding techniques tend not to scale up well, meaning that they would fill only some restricted text. And if you don\'t restrict the text domain or the use of words, then these techniques tend not to work well. They may work well based on machine learning techniques on the data that are similar to the training data that the program has been trained on. But they generally wouldn\'t work well on the data that are very different from the training data. So this pretty much summarizes the state of the art of Natural Language Processing. Of course, within such a short amount of time we can\'t really give you a complete view of NLP, which is a big field. And I\'d expect to see multiple courses on Natural Language Processing topic itself. But because of its relevance to the topic that we talk about, it\'s useful for you to know the background in case you happen to be exposed to that. So what does that mean for Text Retrieval?\n15:48\nWell, in Text Retrieval we are dealing with all kinds of text. It\'s very hard to restrict text to a certain domain. And we also are often dealing with a lot of text data. So that means The NLP techniques must be general, robust, and efficient. And that just implies today we can only use fairly shallow NLP techniques for text retrieval. In fact, most search engines today use something called a bag of words representation.\n16:20\nNow, this is probably the simplest representation you can possibly think of. That is to turn text data into simply a bag of words. Meaning we\'ll keep individual words, but we\'ll ignore all the orders of words. And we\'ll keep duplicated occurrences of words. So this is called a bag of words representation. When you represent text in this way, you ignore a lot of valid information. That just makes it harder to understand the exact meaning of a sentence because we\'ve lost the order.\n16:53\nBut yet this representation tends to actually work pretty well for most search tasks. And this was partly because the search task is not all that difficult. If you see matching of some of the query words in a text document, chances are that that document is about the topic, although there are exceptions.\n17:13\nSo in comparison of some other tasks, for example, machine translation would require you to understand the language accurately. Otherwise the translation would be wrong. So in comparison such tasks are all relatively easy. Such a representation is often sufficient and that\'s also the representation that the major search engines today, like a Google or Bing are using.\n17:35\nOf course, I put in parentheses but not all, of course there are many queries that are not answered well by the current search engines, and they do require the replantation that would go beyond bag of words replantation. That would require more natural language processing to be done.\n17:52\nThere was another reason why we have not used the sophisticated NLP techniques in modern search engines. And that\'s because some retrieval techniques actually, naturally solved the problem of NLP. So one example is word sense disintegration. Think about a word like Java. It could mean coffee or it could mean program language.\n18:15\nIf you look at the word anome, it would be ambiguous, but when the user uses the word in the query, usually there are other words. For example, I\'m looking for usage of Java applet. When I have applet there, that implies Java means program language. And that contest can help us naturally prefer documents which Java is referring to program languages. Because those documents would probably match applet as well. If Java occurs in that documents where it means coffee then you would never match applet or with very small probability. So this is the case when some retrieval techniques naturally achieve the goal of word.\n19:01\nAnother example is some technique called feedback which we will talk about later in some of the lectures. This technique would allow us to add additional words to the query and those additional words could be related to the query words. And these words can help matching documents where the original query words have not occurred. So this achieves, to some extent, semantic matching of terms. So those techniques also helped us bypass some of the difficulties in natural language processing.\n19:40\nHowever, in the long run we still need a deeper natural language processing techniques in order to improve the accuracy of the current search engines. And it\'s particularly needed for complex search tasks.\n19:52\nOr for question and answering.\n19:55\nGoogle has recently launched a knowledge graph, and this is one step toward that goal, because knowledge graph would contain entities and their relations. And this goes beyond the simple bag of words replantation. And such technique should help us improve the search engine utility\n20:14\nsignificantly, although this is the open topic for research and exploration. In sum, in this lecture we talked about what is NLP and we\'ve talked about the state of that techniques. What we can do, what we cannot do. And finally, we also explain why the bag of words replantation remains the dominant replantation used in modern search engines, even though deeper NLP would be needed for future search engines. If you want to know more, you can take a look at some additional readings. I only cited one here and that\'s a good starting point. Thanks. [MUSIC]\nLike\nDislike\nReport an issue\nShare'])
https://www.coursera.org/learn/cs-410/lecture/gw3fo/lesson-5-1-feedback-in-text-retrievaldict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.1: Feedback in Text Retrieval\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.1: Feedback in Text Retrieval\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the feedback in text retrieval.\n0:12\nSo in this lecture, we will continue with the discussion of text retrieval methods.\n0:18\nIn particular, we're going to talk about the feedback in text retrieval.\n0:24\nThis is a diagram that shows the retrieval process.\n0:30\nWe can see the user would type in a query.\n0:37\nAnd then, the query would be sent to a retrieval engine or search engine, and the engine would return results. These results would be issued to the user.\n0:49\nNow, after the user has seen these results, the user can actually make judgements. So for example, the user says, well, this is good and this document is not very useful and this is good again, etc. Now, this is called a relevance judgment or relevance feedback because we've got some feedback information from the user based on the judgements. And this can be very useful to the system, knowing what exactly is interesting to the user. So the feedback module would then take this as input and also use the document collection to try to improve ranking. Typically it would involve updating the query so the system can now render the results more accurately for the user. So this is called relevance feedback. The feedback is based on relevance judgements made by the users. Now, these judgements are reliable but the users generally don't want to make extra effort unless they have to. So the down side is that it involves some extra effort by the user.\n1:57\nThere's another form of feedback called pseudo relevance feedback, or blind feedback, also called automatic feedback. In this case, we can see once the user has gotten [INAUDIBLE] or in fact we don't have to invoke users. So you can see there's no user involved here.\n2:14\nAnd we simply assume that the top rank documents to be relevant. Let's say we have assumed top 10 as relevant.\n2:25\nAnd then, we will then use this assume the documents to learn and to improve the query.\n2:34\nNow, you might wonder, how could this help if we simply assume the top rank of documents? Well, you can imagine these top rank of documents are actually similar to relevant documents even if they are not relevant. They look like relevant documents. So it's possible to learn some related terms to the query from this set. In fact, you may recall that we talked about using language model to analyze what association, to learn related words to the word of computer.\n3:09\nAnd there, what we did is we first use computer to retrieve all the documents that contain computer. So imagine now the query here is a computer. And then, the result will be those documents that contain computer. And what we can do then is to take the top n results. They can match computer very well. And we're going to count the terms in this set. And then, we're going to then use the background language model to choose the terms that are frequent in this set but not frequent in the whole collection. So if we make a contrast between these two what we can find is that related to terms to the word computer. As we have seen before. And these related words can then be added to the original query to expand the query. And this would help us bring the documents that don't necessarily match computer but match other words like program and software. So this is very effective for improving the search result.\n4:18\nBut of course, pseudo-relevancy values are completely unreliable. We have to arbitrarily set a cut off. So there's also something in between called implicit feedback. In this case, what we do is we do involve users, but we don't have to ask users to make judgments. Instead, we're going to observe how the user interacts with the search results. So in this case we'll look at the clickthroughs. So the user clicked on this one. And the user viewed this one. And the user skipped this one. And the user viewed this one again.\n4:50\nNow, this also is a clue about whether the document is useful to the user. And we can even assume that we're going to use only the snippet here in this document, the text that's actually seen by the user instead of the actual document of this entry. The link they are saying web search may be broken but it doesn't matter. If the user tries to fetch this document because of the displayed text we can assume these displayed text is probably relevant is interesting to you so we can learn from such information. And this is called interesting feedback. And we can, again, use the information to update the query. This is a very important technique used in modern. Now, think about the Google and Bing and they can collect a lot of user activities while they are serving us. So they would observe what documents we click on, what documents we skip. And this information is very valuable. And they can use this to improve the search engine.\n5:59\nSo to summarize, we talked about the three kinds of feedback here. Relevant feedback where the user makes explicit judgements. It takes some user effort, but the judgment information is reliable. We talk about the pseudo feedback where we seem to assume top brand marking will be relevant. We don't have to involve the user therefore we could do that, actually before we return the results to the user.\n6:24\nAnd the third is implicit feedback where we use clickthroughs.\n6:29\nWhere we involve the users, but the user doesn't have to make it explicitly their fault. Make judgement. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/5dict_values(["Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 5\nWeek 5\nAll videos completed\n10 min of readings left\nAll graded assignments completed\nIn this week's lessons, you will learn feedback techniques in information retrieval, including the Rocchio feedback method for the vector space model, and a mixture model for feedback with language models. You will also learn how web search engines...\nShow more\nWeek 5 Information\nWeek 5 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 5 Lessons\nComplete\nLesson 5.1: Feedback in Text Retrieval\nVideo•\n. Duration: 6 minutes\n6 min\nLesson 5.2: Feedback in Vector Space Model - Rocchio\nVideo•\n. Duration: 12 minutes\n12 min\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\nVideo•\n. Duration: 19 minutes\n19 min\nLesson 5.4: Web Search: Introduction & Web Crawler\nVideo•\n. Duration: 11 minutes\n11 min\nLesson 5.5: Web Indexing\nVideo•\n. Duration: 17 minutes\n17 min\nLesson 5.6: Link Analysis - Part 1\nVideo•\n. Duration: 9 minutes\n9 min\nLesson 5.7: Link Analysis - Part 2\nVideo•\n. Duration: 17 minutes\n17 min\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\nVideo•\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nComplete\nWeek 5 Practice Quiz\nPractice Quiz•10 questions\nWeek 5 Quiz\nGraded\nQuiz•10 questions\n•Grade: \nProgramming Assignment 2.3\nComplete\nMP2.3\nGraded\nGraded External Tool•Submitted\n•Grade: "])
https://www.coursera.org/learn/cs-410/office-hoursdict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\n Live Events\nUpcoming Live Events\nInstructor Office Hour -ChengXiang Zhai\nTuesday | November 15 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nInstructor Office Hour -ChengXiang Zhai\nTuesday | November 22 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nInstructor Office Hour -ChengXiang Zhai\nTuesday | November 29 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nInstructor Office Hour -ChengXiang Zhai\nTuesday | December 6 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nPast Live Events\nTA Office Hour - Yuxiang\nMonday | August 22 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | August 23 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | August 24 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | August 25 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nTA Office Hour - Yuxiang\nMonday | August 29 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | August 30 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | August 31 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | September 1 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | September 6 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | September 7 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | September 8 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nTA Office Hour - Yuxiang\nMonday | September 12 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | September 13 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | September 14 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | September 15 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nTA Office Hour - Yuxiang\nMonday | September 19 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | September 20 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | September 21 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | September 22 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nTA Office Hour - Yuxiang\nMonday | September 26 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | September 27 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | September 28 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | September 29 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nTA Office Hour - Yuxiang\nMonday | October 3 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | October 4 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | October 5 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | October 6 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nTA Office Hour - Yuxiang\nMonday | October 10 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nInstructor Office Hour -ChengXiang Zhai\nTuesday | October 11 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | October 12 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang\nThursday | October 13 | 8:00 pm – 9:00 pm EST\nFor MP1-3\nhttps://illinois.zoom.us/j/9989980512?pwd=b2Q0V3FsY2o1UERuaisyNDVKbmhMZz09\nExtra Instructor Office Hour for Exam 1\nFriday | October 14 | 9:00 pm – 10:00 pm EST\nExtra Instructor Office Hour on Friday (Oct. 14), at 8pm-9pm Central Time,  to accommodate students who were not able to make my office hour on Tuesday. \n\nTopic: CS410 Instructor Extra Office Hour for Exam 1\nTime: Oct 14, 2022 08:00 PM Central Time (US and Canada)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nInstructor Office Hour -ChengXiang Zhai\nTuesday | October 18 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | October 19 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nTA Office Hour - Yuxiang Liu\nThursday | October 20 | 9:00 pm – 10:00 pm EST\nMP3 session.\nInstructor Office Hour -ChengXiang Zhai\nTuesday | October 25 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | October 26 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nInstructor Office Hour -ChengXiang Zhai\nTuesday | November 1 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791\nTA Office Hour - Assma\nWednesday | November 2 | 1:00 pm – 2:00 pm EST\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/86182070253?pwd=UjViZGRIcmtMMzQrRXZ3S2w2V2g1Zz09\n\nMeeting ID: 861 8207 0253\nPassword: 115831\nInstructor Office Hour -ChengXiang Zhai\nTuesday | November 8 | 9:00 pm – 10:00 pm EST\nTopic: CS410 Instructor Office Hour on Zoom (Fall 2022)\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\nTime: Aug 23, 2022 08:00 PM Central Time (US and Canada)\n        Every week on Tue, until Dec 6, 2022, 16 occurrence(s)\n        Aug 23, 2022 08:00 PM\n        Aug 30, 2022 08:00 PM\n        Sep 6, 2022 08:00 PM\n        Sep 13, 2022 08:00 PM\n        Sep 20, 2022 08:00 PM\n        Sep 27, 2022 08:00 PM\n        Oct 4, 2022 08:00 PM\n        Oct 11, 2022 08:00 PM\n        Oct 18, 2022 08:00 PM\n        Oct 25, 2022 08:00 PM\n        Nov 1, 2022 08:00 PM\n        Nov 8, 2022 08:00 PM\n        Nov 15, 2022 08:00 PM\n        Nov 22, 2022 08:00 PM\n        Nov 29, 2022 08:00 PM\n        Dec 6, 2022 08:00 PM\nPlease download and import the following iCalendar (.ics) files to your calendar system.\nWeekly: https://illinois.zoom.us/meeting/tZ0qceqrqjktE9XNR9BCOWvVcFUoYIzGPm_0/ics?icsToken=98tyKuGhqT0pGdCXtxCGRpx5B4_ob-nwiHpaj_plsi28IQN8VRXANcR3PYtWCv_g\n\nJoin Zoom Meeting\nhttps://illinois.zoom.us/j/89763342791?pwd=alpNZk9kU2FmVi9tdVluNkNMekhOQT09\n\nMeeting ID: 897 6334 2791\nPassword: cs410\n\nOne tap mobile\n+13126266799,,89763342791# US (Chicago)\n+13017158592,,89763342791# US (Washington DC)\n\nDial by your location\n        +1 312 626 6799 US (Chicago)\n        +1 301 715 8592 US (Washington DC)\n        +1 470 250 9358 US (Atlanta)\n        +1 470 381 2552 US (Atlanta)\n        +1 646 518 9805 US (New York)\n        +1 651 372 8299 US (Minnesota)\n        +1 786 635 1003 US (Miami)\n        +1 929 205 6099 US (New York)\n        +1 267 831 0333 US (Philadelphia)\n        +1 253 215 8782 US (Tacoma)\n        +1 346 248 7799 US (Houston)\n        +1 602 753 0140 US (Phoenix)\n        +1 669 219 2599 US (San Jose)\n        +1 669 900 6833 US (San Jose)\n        +1 720 928 9299 US (Denver)\n        +1 971 247 1195 US (Portland)\n        +1 213 338 8477 US (Los Angeles)\n        +1 778 907 2071 Canada\n        +1 438 809 7799 Canada\n        +1 587 328 1099 Canada\n        +1 647 374 4685 Canada\n        +1 647 558 0588 Canada\n        +49 69 7104 9922 Germany\n        +49 695 050 2596 Germany\n        +44 203 481 5237 United Kingdom\n        +44 203 481 5240 United Kingdom\n        +44 131 460 1196 United Kingdom\n        +81 3 4578 1488 Japan\n        +61 3 7018 2005 Australia\n        +61 8 7150 1149 Australia\n        +61 2 8015 6011 Australia\n        +52 554 161 4288 Mexico\nMeeting ID: 897 6334 2791\nPassword: 814961\nFind your local number: https://illinois.zoom.us/u/kcvtuGrnG\n\nJoin by SIP\n89763342791@zoomcrc.com\n\nJoin by H.323\n162.255.37.11 (US West)\n162.255.36.11 (US East)\n221.122.88.195 (China)\n115.114.131.7 (India Mumbai)\n115.114.115.7 (India Hyderabad)\n213.19.144.110 (Amsterdam Netherlands)\n213.244.140.110 (Germany)\n103.122.166.55 (Australia Sydney)\n103.122.167.55 (Australia Melbourne)\n209.9.211.110 (Hong Kong SAR)\n64.211.144.160 (Brazil)\n69.174.57.160 (Canada Toronto)\n65.39.152.160 (Canada Vancouver)\n207.226.132.110 (Japan Tokyo)\n149.137.24.110 (Japan Osaka)\nMeeting ID: 897 6334 2791\nPassword: 814961\n\nJoin by Skype for Business\nhttps://illinois.zoom.us/skype/89763342791'])
https://www.coursera.org/learn/cs-410/lecture/6T38K/7-5-text-representation-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.5 Text Representation: Part 1\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.5 Text Representation: Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:06\nThis lecture is about the textual representation. In this lecture, we are going to discuss textual representation, and discuss how natural language processing can allow us to represent text in many different ways. Let's take a look at this example sentence again. We can represent this sentence in many different ways. First, we can always represent such a sentence as a string of characters. This is true for all the languages when we store them in the computer. When we store a natural language sentence as a string of characters, we have perhaps the most general way of representing text since we always use this approach to represent any text data. But unfortunately, using such a representation will not help us to do semantic analysis, which is often needed for many applications of text mining. The reason is because we're not even recognizing words. So as a string, we're going to keep all the spaces and these ASCII symbols. We can perhaps count what's the most frequent character in English text, or the correlation between those characters, but we can't really analyze semantics. Yet, this is the most general way of representing text because we can use this to represent any natural language text. If we try to do a little bit more natural language processing by doing word segmentation, then we can obtain a representation of the same text, but in the form of a sequence of words. So here we see that we can identify words like a dog is chasing etc. Now with this level of representation, we certainly can do a lot of things, and this is mainly because words are the basic units of human communication in natural language, so they are very powerful. By identifying words, we can for example easily count what are the most frequent words in this document or in the whole collection etc. These words can be used to form topics when we combine related words together, and some words are positive, some words negative, so we can also do sentiment analysis. So representing text data as a sequence of words opens up a lot of interesting analysis possibilities. However, this level of representation is slightly less general than string of characters because in some languages such as Chinese, it's actually not that easy to identify all the word boundaries because in such a language, you see text as a sequence of characters with no space in between. So you'll have to rely on some special techniques to identify words. In such a language, of course then, we might make mistakes in segmenting words. So the sequence of words representation is not as robust as string of characters. But in English, it's very easy to obtain this level of representation, so we can do that all the time.\n4:00\nNow, if we go further to do naturally language processing, we can add a part of speech tags. Now once we do that, we can count, for example, the most frequent nouns or what kind of nouns are associated with what kind of verbs etc. So this opens up a little bit more interesting opportunities for further analysis. Note that I use a plus sign here because by representing text as a sequence of part of speech tags, we don't necessarily replace the original word sequence written. Instead, we add this as an additional way of representing text data, so that now the data is represented as both a sequence of words and a sequence of part of speech tags. This enriches the representation of text data, and thus also enables more interesting analysis.\n5:00\nIf we go further, then we'll be pausing the sentence often to obtain a syntactic structure. Now this of course, further open up a more interesting analysis of, for example, the writing styles or correcting grammar mistakes. If we go further for semantic analysis, then we might be able to recognize dog as an animal, and we also can recognize a boy as a person, and playground as a location. We can further analyze their relations, for example, dog is chasing the boy and the boy is on the playground. Now this will add more entities and relations through entity relation recreation. At this level, then we can do even more interesting things. For example, now we can count easily the most frequent person that's mentioning this whole collection of news articles, or whenever you mention this person, you also tend to see mentioning of another person etc. So this is a very useful representation, and it's also related to the knowledge graph that some of you may have heard of that Google is doing as a more semantic way of representing text data. However, it's also less robust than sequence of words or even syntactical analysis because it's not always easy to identify all the entities with the right types, and we might make mistakes, and relations are even harder to find, and we might make mistakes. So this makes this level of representation less robust, yet it's very useful. Now if we move further to logical condition, then we can have predicates and even inference rules. With inference rules, we can infer interesting derived facts from the text, so that's very useful. But unfortunately, at this level of representation is even less robust and we can make mistakes and we can't do that all the time for all kinds of sentences. Finally, speech acts would add a yet another level of repetition of the intent of saying this sentence. So in this case, it might be a request. So knowing that would allow us to analyze even more interesting things about this observer or the author of this sentence. What's the intention of saying that? What's scenarios? What kind of actions would be made? So this is another level of analysis that would be very interesting. So this picture shows that if we move down, we generally see more sophisticated natural language processing techniques to be used. Unfortunately, such techniques would require more human effort, and they are less accurate. That means there are mistakes. So if we add an texts that are at the levels that are representing deeper analysis of language, then we have to tolerate the errors. So that also means it's still necessary to combine such deep analysis with shallow analysis based on, for example, sequence of words. On the right side, you'll see the arrow points down to indicate that. As we go down, we are representation of text is closer to knowledge representation in our mind, and need for solving a lot of problems. Now this is desirable because as we can represent text at the level of knowledge, we can easily extract the knowledge. That's the purpose of text-mining. So there is a trade-off here between doing a deeper analysis that might have errors but would give us direct knowledge that can be extracted from text. Doing shallow analysis, which is more robust but wouldn't actually give us the necessary deeper representation of knowledge. I should also say that text data are generated by humans and are meant to be consumed by humans. So as a result, in text data analysis, text-mining humans play a very important role, they are always in the loop. Meaning that we should optimize the collaboration of humans and computers. So in that sense, it's okay that computers may not be able to have compute accurately representation of text data, and the patterns that are extracted from text data can be interpreted by humans, and humans can guide the computers to do more accurate analysis by annotating more data, by providing features to guide a machine learning programs to make them work more effectively.\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/V93Td/10-3-text-clustering-generative-probabilistic-models-part-2-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nChinese (Simplified)\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is a continuing discussion of Generative Probabilistic Models for text clustering.\n0:13\nIn this lecture, we are going to continue talking about the text clustering, particularly, the Generative Probabilistic Models.\n0:23\nSo this is a slide that you have seen earlier where we have written down the likelihood function for a document with two distributions, being a two component mixed model for document clustering.\n0:39\nNow in this lecture, we're going to generalize this to include the k clusters. Now if you look at the formula and think about the question, how to generalize it, you'll realize that all we need is to add more terms, like what you have seen here.\n0:57\nSo you can just add more thetas and the probabilities of thetas and the probabilities of generating d from those thetas. So this is precisely what we are going to use and this is the general presentation of the mixture model for document clustering.\n1:19\nSo as more cases would follow these steps in using a generating model first, think about our data. And so in this case our data is a collection of documents, end documents denoted by d sub i, and then we talk about the other models, think of other modelling. In this case, we design a mixture of k unigram language models. It's a little bit different from the topic model, but we have similar parameters. We have a set of theta i's that denote that our distributions corresponding to the k unigram language models. We have p of each theta i as a probability of selecting each of the k distributions we generate the document. Now note that although our goal is to find the clusters and we actually have used a more general notion of a probability of each cluster and this as you will see later, will allow us to assign a document to the cluster that has the highest probability of being able to generate the document.\n2:31\nSo as a result, we can also recover some other interesting\n2:36\nproperties, as you will see later.\n2:42\nSo the model basically would make the following assumption about the generation of a document. We first choose a theta i according to probability of theta i, and then generate all the words in the document using this distribution. Note that it's important that we use this distribution all the words in the document. This is very different from topic model. So the likelihood function would be like what you are seeing here.\n3:10\nSo you can take a look at the formula here, we have used the different notation here in the second line of this equation. You are going to see now the notation has been changed to use unique word in the vocabulary, in the product instead of particular position in the document. So from X subject to W, is a change of notation and this change allows us to show the estimation formulas more easily. And you have seen this change also in the topic model presentation, but it's basically still just a product of the probabilities of all the words.\n4:10\nAnd so with the likelihood function, now we can talk about how to do parameter estimation. Here we can simply use the maximum likelihood estimator. So that's just a standard way of doing things. So all should be familiar to you now. It's just a different model. So after we have estimated parameters, how can we then allocate clusters to the documents? Well, let's take a look at the this situation more closely. So we just repeated the parameters here for this mixture model.\n4:43\nNow if you think about what we can get by estimating such a model, we can actually get more information than what we need for doing clustering, right? So theta i for example, represents the content of cluster i, this is actually a by-product, it can help us summarize what the cluster is about. If you look at the top terms in this cluster or in this word distribution and they will tell us what the cluster is about.\n5:11\np of theta i can be interpreted as indicating the size of cluster because it tells us how likely the cluster would be used to generate the document. The more likely a cluster is used to generate a document, we can assume the larger the cluster size is.\n5:30\nNote that unlike in PLSA and this probability of theta i is not dependent on d.\n5:37\nNow you may recall that the topic you chose at each document actually depends on d. That means each document can have a potentially different choice of topics, but here we have a generic choice probability for all the documents. But of course, even a particular document that we still have to infer which topic is more likely to generate the document. So in that sense, we can still have a document dependent probability of clusters.\n6:10\nSo now let's look at the key problem of assigning documents to clusters or assigning clusters to documents.\n6:17\nSo that's the computer c sub d here and this will take one of the values in the range of one through k to indicate which cluster should be assigned to d.\n6:28\nNow first you might think about a way to use likelihood on it and that is to assign d to the cluster corresponding to the topic of theta i, that most likely has been used to generate d.\n6:42\nSo that means we're going to choose one of those distributions that gives d the highest probability. In other words, we see which distribution has the content that matches our d at the [INAUDIBLE]. Intuitively that makes sense, however, this approach does not consider the size of clusters, which is also a available to us and so a better way is to use the likelihood together with the prior, in this case the prior is p of theta i. And together, that is, we're going to use the base formula to compute the posterior probability of theta, given d.\n7:25\nAnd if we choose theta .based on this posterior probability, we would have the following formula that you see here on the bottom of this slide. And in this case, we're going to choose the theta that has a large P of theta i, that means a large cluster and also a high probability of generating d. So we're going to favor a cluster that's large and also consistent with the document. And that intuitively makes sense because the chance of a document being a large cluster is generally higher than in a small cluster.\n8:07\nSo this means once we can estimate the parameters of the model, then we can easily solve the problem of document clustering. So next, we'll have to discuss how to actually compute the estimate of the model. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/N2zx1/no-office-hoursdict_values(['List\nCS 410: Text Information Systems\nWeek 14\nNo Office Hours\nPrevious\nNext\nWeek 14 Information\nReading:\nReading\nNo Office Hours\n. Duration: 10 minutes\n10 min\nNo Office Hours\nNo office hour this week due to Fall Break\nMark as completed\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/home/week/2dict_values(["Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 2\nWeek 2\nComplete\nAll videos completed\nAll readings completed\nAll graded assignments completed\nIn this week's lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e.,...\nShow more\nWeek 2 Information\nComplete\nWeek 2 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 2 Lessons\nComplete\nLesson 2.1: Vector Space Model - Improved Instantiation\nVideo•\n. Duration: 16 minutes\n16 min\nLesson 2.2: TF Transformation\nVideo•\n. Duration: 9 minutes\n9 min\nLesson 2.3: Doc Length Normalization\nVideo•\n. Duration: 18 minutes\n18 min\nLesson 2.4: Implementation of TR Systems\nVideo•\n. Duration: 21 minutes\n21 min\nLesson 2.5: System Implementation - Inverted Index Construction\nVideo•\n. Duration: 18 minutes\n18 min\nLesson 2.6: System Implementation - Fast Search\nVideo•\n. Duration: 17 minutes\n17 min\nWeek 2 Activities\nComplete\nWeek 2 Practice Quiz\nPractice Quiz•11 questions\nWeek 2 Quiz\nGraded\nQuiz•10 questions\n•Grade: 96.\nProgramming Assignment 1\nComplete\nMP1\nGraded\nGraded External Tool•Submitted\n•Grade: "])
https://www.coursera.org/learn/cs-410/lecture/tfXZ4/lesson-6-9-recommender-systems-collaborative-filtering-part-3dict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] So to summarize our discussion of recommender systems, in some sense, the filtering task for recommender task is easy, and in some other sense, the task is actually difficult. So it's easy because the user's expectation is low. In this case the system takes initiative to push information to the user. The user doesn't really make any effort, so any recommendation is better than nothing. All right. So, unless you recommend the noise items or useless documents. If you can recommend some useful information users generally will appreciate it, so that's, in that sense that's easy. However, filtering is actually much harder task than retrieval because you have to make a binary decision and you can't afford waiting for a lot of items and then you're going to see whether one item is better than others. You have to make a decision when you see this item. Think about news filtering. As soon as you see the news enough to decide whether the news would be interesting to the user. If you wait for a few days, well, even if you can make accurate recommendation of the most relevant news, the utility is going to be significantly decreased.\n1:28\nAnother reason why it's hard is because of data sparseness if you think of this as a learning problem. Collaborative filtering, for example, is purely based on learning from the past ratings. So if you don't have many ratings there's really not that much you can do, right? And yeah I just mentioned this cold start problem. This is actually a very serious, serious problem. But of course there are strategies that have been proposed for the soft problem,\n2:00\nand there are different strategies that you can use to alleviate the problem. You can use, for example, more user information to asses their similarity, instead of using the preferences of these users on these items give me additional information available about the user, etc.\n2:21\nAnd we also talk about two strategies for filtering task. One is content-based where we look at items there is collaborative filtering where we look at Use a similarity. And they obviously can be combined in a practical system. You can imagine they generally would have to be combined. So that would give us a hybrid strategy for filtering. And we also could recall that we talked about push versus pull as two strategies for getting access to the text data. And recommender system easy to help users in the push mode, and search engines are serving users in the pull mode. Obviously the two should be combined, and they can be combined. The two have a system that can support user with multiple mode information access. So in the future we could anticipate such a system to be more useful the user. And either, this is an active research area so there are a lot of new algorithms being proposed all the time. In particular those new algorithms tend to use a lot of context information. Now the context here could be the context of the user and could also be the context of the user. Items. The items are not the isolated. They're connected in many ways. The users might form social network as well, so there's a rich context there that we can leverage in order to\n3:59\nreally solve the problem well and then that's active research area where also machine learning algorithms have been applied. Here are some additional readings in the handbook called Recommender Systems and has a collection of a lot of good articles that can give you an overview of a number of specific approaches through recommender systems. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/7jqJI/lesson-2-1-vector-space-model-improved-instantiationdict_values(["List\nCS 410: Text Information Systems\nWeek 2\nLesson 2.1: Vector Space Model - Improved Instantiation\nPrevious\nNext\nWeek 2 Information\nWeek 2 Lessons\nVideo:\nVideo\nLesson 2.1: Vector Space Model - Improved Instantiation\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\nLesson 2.2: TF Transformation\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 2.3: Doc Length Normalization\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.4: Implementation of TR Systems\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 2.5: System Implementation - Inverted Index Construction\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.6: System Implementation - Fast Search\n. Duration: 17 minutes\n17 min\nWeek 2 Activities\nProgramming Assignment 1\nLesson 2.1: Vector Space Model - Improved Instantiation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] In this lecture, we are going to talk about how to improve the instantiation of the vector space model.\n0:17\nThis is a continued discussion of the vector space model. We're going to focus on how to improve the instantiation of this model.\n0:30\nIn the previous lecture, you have seen that with simple instantiations of the vector space model, we can come up with a simple scoring function that would give us basically an account of how many unique query terms are matched in the document.\n0:50\nWe also have seen that this function has a problem, as shown on this slide. In particular, if you look at these three documents, they will all get the same score because they match the three unique query words.\n1:06\nBut intuitively we would like d4 to be ranked above d3, and d2 is really not relevant.\n1:14\nSo the problem here is that this function couldn't capture the following heuristics. First, we would like to give more credit to d4 because it matched presidential more times than d3.\n1:32\nSecond, intuitively, matching presidential should be more important than matching about, because about is a very common word that occurs everywhere. It doesn't really carry that much content.\n1:47\nSo in this lecture, let's see how we can improve the model to solve these two problems. It's worth thinking at this point about why do we have these problems?\n2:01\nIf we look back at assumptions we have made while instantiating the vector space model, we'll realize that the problem is really coming from some of the assumptions. In particular, it has to do with how we placed the vectors in the vector space.\n2:22\nSo then naturally, in order to fix these problems, we have to revisit those assumptions. Perhaps we will have to use different ways to instantiate the vector space model. In particular, we have to place the vectors in a different way.\n2:41\nSo let's see how we can improve this. One natural thought is in order to consider multiple times of a term in the document, we should consider the term frequency instead of just the absence or presence. In order to consider the difference between a document where a query term occurred multiple times and one where the query term occurred just once, we have to consider the term frequency, the count of a term in the document.\n3:13\nIn the simplest model, we only modeled the presence and absence of a term. We ignored the actual number of times that a term occurs in a document. So let's add this back. So we're going to then represent a document by a vector with term frequency as element. So that is to say, now the elements of both the query vector and the document vector will not be 0 or 1s, but instead they will be the counts of a word in the query or the document.\n3:52\nSo this would bring in additional information about the document, so this can be seen as more accurate representation of our documents. So now let's see what the formula would look like if we change this representation. So as you'll see on this slide, we still use dot product.\n4:10\nAnd so the formula looks very similar in the form. In fact, it looks identical. But inside the sum, of course, x i and y i are now different. They are now the counts of word i in the query and in the document. Now at this point I also suggest you to pause the lecture for a moment and just to think about how we can interpret the score of this new function. It's doing something very similar to what the simplest VSM is doing. But because of the change of the vector, now the new score has a different interpretation. Can you see the difference? And it has to do with the consideration of multiple occurrences of the same term in a document. More importantly, we would like to know whether this would fix the problems of the simplest vector space model. So let's look at this example again. So suppose we change the vector representation to term frequency vectors. Now let's look at these three documents again. The query vector is the same because all these words occurred exactly once in the query. So the vector is still a 01 vector. And in fact, d2 is also essentially representing the same way because none of these words has been repeated many times. As a result, the score is also the same, still 3.\n5:45\nThe same is true for d3, and we still have a 3.\n5:51\nBut d4 would be different, because now presidential occurred twice here. So the ending for presidential in the document vector would be 2 instead of 1.\n6:04\nAs a result, now the score for d4 is higher. It's a 4 now.\n6:10\nSo this means by using term frequency, we can now rank d4 above d2 and d3, as we hoped to.\n6:19\nSo this solved the problem with d4.\n6:26\nBut we can also see that d2 and d3 are still filtering the same way. They still have identical scores, so it did not fix the problem here.\n6:40\nSo how can we fix this problem? Intuitively, we would like to give more credit for matching presidential than matching about. But how can we solve the problem in a general way? Is there any way to determine which word should be treated more importantly and which word can be basically ignored? About is such a word which does not really carry that much content. We can essentially ignore that. We sometimes call such a word a stock word. Those are generally very frequent and they occur everywhere. Matching it doesn't really mean anything. But computationally how can we capture that?\n7:24\nSo again, I encourage you to think a little bit about this.\n7:29\nCan you came up with any statistical approaches to somehow distinguish presidential from about?\n7:37\nNow if you think about it for a moment, you'll realize that one difference is that a word like above occurs everywhere. So if you count the occurrence of the word in the whole collection, then we will see that about has much higher frequency than presidential, which tends to occur only in some documents.\n8:01\nSo this idea suggests that we could somehow use the global statistics of terms or some other information to trying to down-weight the element of about in a vector representation of d2. At the same time, we hope to somehow increase the weight of presidential in the vector of d3. If we can do that, then we can expect that d2 will get the overall score to be less than 3 while d3 will get the score above 3. Then we would be able to rank d3 on top of d2.\n8:45\nSo how can we do this systematically?\n8:48\nAgain, we can rely on some statistical count. And in this case, the particular idea is called inverse document frequency. Now we have seen document frequency as one signal used in the modern retrieval functions.\n9:05\nWe discussed this in a previous lecture. So here is the specific way of using it. Document frequency is the count of documents that contain a particular term. Here we say inverse document frequency because we actually want to reward a word that doesn't occur in many documents.\n9:24\nAnd so the way to incorporate this into our vector representation is then to modify the frequency count by multiplying it by the IDF of the corresponding word, as shown here. If we can do that, then we can penalize common words, which generally have a lower IDF, and reward rare words, which will have a higher IDF. So more specifically, the IDF can be defined as the logarithm of M+1 divided by k, where M is the total number of documents in the collection, k is the DF or document frequency, the total number of documents containing the word W. Now if you plot this function by varying k, then you would see the curve would look like this. In general, you can see it would give a higher value for a low DF word, a rare word.\n10:34\nYou can also see the maximum value of this function is log of M+1.\n10:40\nIt would be interesting for you to think about what's the minimum value for this function. This could be an interesting exercise.\n10:50\nNow the specific function may not be as important as the heuristic to simply penalize popular terms.\n11:01\nBut it turns out that this particular function form has also worked very well.\n11:07\nNow whether there's a better form of function here is the open research question. But it's also clear that if we use a linear penalization, like what's shown here with this line, then it may not be as reasonable as the standard IDF.\n11:29\nIn particular, you can see the difference in the standard IDF,\n11:35\nand we somehow have a turning point of here.\n11:41\nAfter this point, we're going to say these terms are essentially not very useful. They can be essentially ignored. And this makes sense when the term occurs so frequently and let's say a term occurs in more than  of the documents, then the term is unlikely very important and it's basically a common term.\n12:03\nIt's not very important to match this word. So with the standard IDF you can see it's basically assumed that they all have low weights. There's no difference. But if you look at the linear penalization, at this point that there is still some difference. So intuitively we'd want to focus more on the discrimination of low DF words rather than these common words.\n12:32\nWell, of course, which one works better still has to be validated by using the empirically correlated dataset. And we have to use users to judge which results are better.\n12:48\nSo now let's see how this can solve problem 2. So now let's look at the two documents again.\n12:56\nNow without the IDF weighting before, we just have term frequency vectors. But with IDF weighting we now can adjust the TF weight by multiplying with the IDF value. For example, here we can see is adjustment and in particular for about there's adjustment by using the IDF value of about, which is smaller than the IDF value of presidential. So if you look at these, the IDF will distinguish these two words. As a result, adjustment here would be larger, would make this weight larger.\n13:37\nSo if we score with these new vectors, then what would happen is that, of course, they share the same weights for news and campaign, but the matching of about will discriminate them. So now as a result of IDF weighting, we will have d3 to be ranked above d2 because it matched a rare word, whereas d2 matched a common word. So this shows that the IDF weighting can solve problem 2.\n14:12\nSo how effective is this model in general when we used TF-IDF weighting? Well, let's look at all these documents that we have seen before. These are the new scores of the new documents. But how effective is this new weighting method and new scoring function point?\n14:33\nSo now let's see overall how effective is this new ranking function with TF-IDF weighting.\n14:40\nHere we show all the five documents that we have seen before, and these are their scores.\n14:47\nNow we can see the scores for the first four documents here seem to be quite reasonable. They are as we expected.\n14:58\nHowever, we also see a new problem because now d5 here, which did not have a very high score with our simplest vector space model, now actually has a very high score. In fact, it has the highest score here.\n15:16\nSo this creates a new problem. This is actually a common phenomenon in designing retrieval functions. Basically, when you try to fix one problem, you tend to introduce other problems. And that's why it's very tricky how to design effective ranking function. And what's the best ranking function is their open research question. Researchers are still working on that.\n15:42\nBut in the next few lectures we're going to also talk about some additional ideas to further improve this model and try to fix this problem.\n15:55\nSo to summarize this lecture, we've talked about how to improve the vector space model, and we've got to improve the instantiation of the vector space model based on TD-IDF weighting. So the improvement is mostly on the placement of the vector where we give high weight to a term that occurred many times in a document but infrequently in the whole collection.\n16:23\nAnd we have seen that this improved model indeed looks better than the simplest vector space model. But it also still has some problems. In the next lecture we're going to look at how to address these additional problems. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/yp69J/week-4-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 4\nWeek 4 Overview\nPrevious\nNext\nWeek 4 Information\nReading:\nReading\nWeek 4 Overview\n. Duration: 10 minutes\n10 min\nWeek 4 Lessons\nWeek 4 Activities\nProgramming Assignment 2.2\nWeek 4 Overview\nDuring this week's lessons, you will learn probabilistic retrieval models and statistical language models, particularly the detail of the query likelihood retrieval function with two specific smoothing methods, and how the query likelihood retrieval function is connected with the retrieval heuristics used in the vector space model.     \nTime\nThis module should take approximately 6 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with assignments in bold):\nActivity\nEstimated Time Required\nWeek 4 Video Lectures\n2 hours\nWeek 4 Graded Quiz\n1 hour\nProgramming Assignment 2.2\n3 hours\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain how to interpret p(R=1|q,d) and estimate it based on a large set of collected relevance judgments (or clickthrough information) about query q and document d.\nExplain how to interpret the conditional probability p(q|d) used for scoring documents in the query likelihood retrieval function.\nExplain what a statistical language model and a unigram language model are.\nExplain how to compute the maximum likelihood estimate of a unigram language model.\nExplain how to use unigram language models to discover semantically related words.\nCompute p(q|d) based on a given document language model p(w|d).\nExplain what smoothing does.\nShow that query likelihood retrieval function implements TF-IDF weighting if we smooth the document language model p(w|d) using the collection language model p(w|C) as a reference language model.\nCompute the estimate of p(w|d) using Jelinek-Mercer (JM) smoothing and Dirichlet Prior smoothing, respectively.  \nGuiding Questions\nDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\nGiven a table of relevance judgments in the form of three columns (query, document, and binary relevance judgments), how can we estimate p(R=1|q,d)?\nHow should we interpret the query likelihood conditional probability p(q|d)?\nWhat is a statistical language model? What is a unigram language model? How many parameters are there in a unigram language model?\nHow do we compute the maximum likelihood estimate of the unigram language model (based on a text sample)?\nWhat is a background language model? What is a collection language model? What is a document language model? \nWhy do we need to smooth a document language model in the query likelihood retrieval model? What would happen if we don’t do smoothing?\nWhen we smooth a document language model using a collection language model as a reference language model, what is the probability assigned to an unseen word in a document?\nHow can we prove that the query likelihood retrieval function implements TF-IDF weighting if we use a collection language model smoothing?\nHow does linear interpolation (Jelinek-Mercer) smoothing work? What is the formula?\nHow does Dirichlet prior smoothing work? What is the formula?\nWhat are the similarities and differences between Jelinek-Mercer smoothing and Dirichlet prior smoothing?  \nAdditional Readings and Resources\nC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapter 6 - Section 6.4 \nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\np(R=1|q,d) ; query likelihood, p(q|d) \nStatistical and unigram language models\nMaximum likelihood estimate \nBackground, collection, and document language models\nSmoothing of unigram language models\nRelation between query likelihood and TF-IDF weighting\nLinear interpolation (i.e., Jelinek-Mercer) smoothing \nDirichlet Prior smoothing \nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/tr0ir/10-6-text-clustering-evaluationdict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.6 Text Clustering: Evaluation\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.6 Text Clustering: Evaluation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC] This lecture is about evaluation of text clustering.\n0:12\nSo far we have talked about multiple ways of doing text clustering but how do we know which method works the best?\n0:22\nSo this has to do with evaluation. Now to talk about evaluation one must go back to the clustering bias that we introduced at the beginning.\n0:32\nBecause two objects can be similar depending on how you look at them,\n0:37\nwe must clearly specify the perspective of similarity. Without that, the problem of clustering is not well defined. So this perspective is also very important for evaluation. If you look at this slide, and you can see we have two different ways to cluster these shapes, and if you ask a question, which one is the best, or which one is better? You actually see, there's no way to answer this question without knowing whether we'd like to cluster based on shapes, or cluster based on sizes. And that's precisely why the perspective on clustering bias is crucial for evaluation.\n1:19\nIn general, we can evaluate text clusters in two ways, one is direct evaluation, and the other indirect evaluation. So in direct evaluation, we want to answer the following questions, how close are the system-generated clusters to the ideal clusters that are generated by humans?\n1:38\nSo the closeness here can be assessed\n1:44\nfrom multiple perspectives and that will help us characterize the quality of cluster result in multiple angles, and this is sometimes desirable.\n1:56\nNow we also want to quantify the closeness because this would allow us to easily compare different measures based on their performance figures.\n2:09\nAnd finally, you can see, in this case, we essentially inject the clustering bias\n2:15\nby using humans, basically humans would bring in the the need or desire to clustering bias. Now, how do we do that exactly? Well, the general procedure would look like this.\n2:28\nGiven a test set which consists of a lot of text objects, we can have humans to create the ideal clustering result, that is, we're going to ask humans to partition the objects to create the gold standard. And they will use their judgments based on the need of a particular application to generate what they think are the best clustering results, and this would be then used to compare with the system generated clusters from the same test set.\n3:01\nAnd ideally, we want the system results to be the same as the human generated results, but in general, they are not going to be the same. So we would like to then quantify the similarity between the system-generated clusters and the gold standard clusters. And this similarity can also be measure from multiple perspectives and this will give us various meshes to quantitatively evaluate a cluster, a clustering result. And some of the commonly used measures include the purity, which measures whether a cluster has a similar object from the same cluster, in the gold standard. And normalized mutual information is a commonly used measure which basically measures based on the identity of cluster of object in the system generally. How well can you predict the cluster of the object in the gold standard or vice versa? And mutual information captures, the correlation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose, F measure is another possible measure.\n4:21\nNow again a thorough discussion of this evaluation and these evaluation issues would be beyond the scope of this course.\n4:29\nI've suggested some reading in the end that you can take a look at to know more about that.\n4:36\nSo here I just want to discuss some high level ideas that would allow you to think about how to do evaluation in your applications. The second way to evaluate text clusters is to do indirect evaluation. So in this case the question to answer is, how useful are the clustering results for the intended applications? Now this of course is application specific question, so usefulness is going to depend on specific applications.\n5:07\nIn this case, the clustering bias is imposed by the independent application as well, so what counts as a best cluster result would be dependent on the application.\n5:19\nNow procedure wise we also would create a test set with text objects for the intended application to quantify the performance of the system.\n5:32\nIn this case, what we care about is the contribution of clustering to some application so we often have a baseline system to compare with.\n5:45\nThis could be the current system for doing something, and then you hope to add a clustering to improve it, or the baseline system could be using a different clustering method. And then what you are trying to experiment with, and you hope to have better idea of word clustering. So in any case you have a baseline system work with, and then you add a clustering algorithm to the baseline system to produce a clustering system.\n6:11\nAnd then we have to compare the performance of your clustering system and the baseline system in terms of the performance measure for that particular application.\n6:21\nSo in this case we call it indirect evaluation of clusters because there's no explicit assessment of the quality of clusters, but rather it's to assess the contribution of clusters to a particular application.\n6:37\nSo, to summarize text clustering,\n6:41\nit's a very useful unsupervised general text mining technique, and it's particularly useful for obtaining an overall picture of the text content. And this is often needed to explore text data, and this is often the first step when you deal with a lot of text data.\n7:01\nThe second application or second kind of applications is through discover interesting clustering structures in text data and these structures can be very meaningful.\n7:13\nThere are many approaches that can be used to form text clustering and we discussed model based approaches and some narrative based approaches. In general, strong clusters tend to show up no matter what method is used. Also the effectiveness of a method highly depends on whether the desired clustering bias is captured appropriately, and this can be done either through using the right generating model, the model design appropriate for the clustering, or the right similarity function expressly define the bias. Deciding the optimal number of customers is a very difficult problem for order cluster methods, and that's because it's unsupervised algorithm, and there's no training there how to guide us to select the best number of clusters.\n8:05\nNow sometimes you may see some methods that can automatically determine the number of clusters, but in general that has some implied application of clustering bias there and that's just not specified. Without clearly defining a clustering bias, it's just impossible to say the optimal number of cluster is what, so this important to keep in mind.\n8:31\nAnd I should also say sometimes we can also use application to determine the number of clusters, for example, if you're clustering search results, then obviously you don't want to generate the 100 clusters, so the number can be dictated by the interface design.\n8:46\nIn other situations, we might be able to use the fitness to data to assess whether we've got a good number of clusters to explain our data well. And to do that, you can vary the number of clusters and watch how well you can fit the data.\n9:07\nIn general when you add a more components to a mixed model you should fit the data better because you, you don't, you can always set the probability of using the new component as zero. So you can't in general fit the data worse than before, but the question is as you add more components would you be able to significantly improve the fitness of the data and that can be used to determine the right number of clusters. And finally evaluation of clustering results, this kind can be done both directly and indirectly, and we often would like to do both in order to get a good sense about how well our method works. So here's some suggested reading and this is particularly useful to better understand how the matches are calculated and clustering in general [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/classmatesdict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nLoading...'])
https://www.coursera.org/learn/cs-410/lecture/thRNy/lesson-3-6-evaluation-of-tr-systems-practical-issuesdict_values(["List\nCS 410: Text Information Systems\nWeek 3\nLesson 3.6: Evaluation of TR Systems - Practical Issues\nPrevious\nNext\nWeek 3 Information\nWeek 3 Lessons\nVideo:\nVideo\nLesson 3.1: Evaluation of TR Systems\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.2: Evaluation of TR Systems - Basic Measures\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.6: Evaluation of TR Systems - Practical Issues\n. Duration: 15 minutes\n15 min\nWeek 3 Activities\nProgramming Assignment 2.1\nLesson 3.6: Evaluation of TR Systems - Practical Issues\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nMongolian\nPortuguese (European)\nRussian\nSlovak\nSpanish\nTelugu\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]. This lecture is about some practical issues that you would have to address in evaluation of text retrieval systems.\n0:14\nIn this lecture, we will continue the discussion of evaluation. We'll cover some practical issues that you have to solve in actual evaluation of text retrieval systems.\n0:25\nSo, in order to create the test collection, we have to create a set of queries. A set of documents and a set of relevance judgments.\n0:35\nIt turns out that each is actually challenging to create. First, the documents and queries must be representative. They must represent the real queries and real documents that the users handle.\n0:48\nAnd we also have to use many queries and many documents in order to avoid a bias of conclusions.\n0:56\nFor the matching of relevant documents with the queries. We also need to ensure that there exists a lot of relevant documents for each query. If a query has only one, that's a relevant option we can actually then. It's not very informative to compare different methods using such a query because there's not that much room for us to see difference. So ideally, there should be more relevant documents in the clatch but yet the queries also should represent the real queries that we care about.\n1:31\nIn terms of relevance judgments, the challenge is to ensure complete judgments of all the documents for all the queries. Yet, minimizing human and fault, because we have to use human labor to label these documents. It's very labor intensive. And as a result, it's impossible to actually label all the documents for all the queries, especially considering a giant data set like the web.\n1:58\nSo this is actually a major challenge, it's a very difficult challenge. For measures, it's also challenging, because we want measures that would accurately reflect the perceived utility of users. We have to consider carefully what the users care about. And then design measures to measure that. If your measure is not measuring the right thing, then your conclusion would be misled. So it's very important.\n2:26\nSo we're going to talk about a couple of issues here. One is the statistical significance test. And this also is a reason why we have to use a lot of queries. And the question here is how sure can you be that observe the difference doesn't simply result from the particular queries you choose. So here are some sample results of average position for System A and System B into different experiments. And you can see in the bottom, we have mean average of position. So the mean, if you look at the mean average of position, the mean average of positions are exactly the same in both experiments, right? So you can see this is 0.20, this is 0.40 for System B. And again here it's also 0.20 and 0.40, so they are identical. Yet, if you look at these exact average positions for different queries. If you look at these numbers in detail, you would realize that in one case, you would feel that you can trust the conclusion here given by the average.\n3:36\nIn the another case, in the other case, you will feel that, well, I'm not sure. So, why don't you take a look at all these numbers for a moment, pause the media. So, if you look at the average, the mean average of position, we can easily, say that well, System B is better, right? So, after all it's 0.40 and this is twice as much as 0.20, so that's a better performance. But if you look at these two experiments, look at the detailed results.\n4:11\nYou will see that, we've been more confident to say that, in the case one, in experiment one. In this case. Because these numbers seem to be consistently better for System B.\n4:25\nWhereas in Experiment 2, we're not sure because looking at some results like this, after System A is better and this is another case System A is better.\n4:39\nBut yet if we look at only average, System B is better.\n4:45\nSo, what do you think?\n4:49\nHow reliable is our conclusion, if we only look at the average?\n4:55\nNow in this case, intuitively, we feel Experiment 1 is more reliable.\n5:01\nBut how can we quantitate the answer to this question? And this is why we need to do statistical significance test.\n5:09\nSo, the idea of the statistical significance test is basically to assess the variants across these different queries. If there is a big variance, that means the results could fluctuate a lot according to different queries. Then we should believe that, unless you have used a lot of queries, the results might change if we use another set of queries. Right, so this is then not so if you have c high variance then it's not very reliable.\n5:43\nSo let's look at these results again in the second case. So, here we show two different ways to compare them. One is a sign test where we just look at the sign. If System B is better than System A, we have a plus sign. When System A is better we have a minus sign, etc. Using this case, if you see this, well, there are seven cases. We actually have four cases where System B is better. But three cases of System A is better, intuitively, this is almost like a random results, right? So if you just take a random sample of you flip seven coins and if you use plus to denote the head and minus to denote the tail and that could easily be the results of just randomly flipping these seven coins. So, the fact that the average is larger doesn't tell us anything. We can't reliably conclude that. And this can be quantitatively measured by a p value. And that basically means\n6:49\nthe probability that this result is in fact from a random fluctuation. In this case, probability is 1.0. It means it surely is a random fluctuation.\n7:01\nNow in Willcoxan test, it's a non-parametric test, and we would be not only looking at the signs, we'll be also looking at the magnitude of the difference. But we can draw a similar conclusion, where you say it's very likely to be from random. To illustrate this, let's think about that such a distribution. And this is called a now distribution. We assume that the mean is zero here. Lets say we started with assumption that there's no difference between the two systems. But we assume that because of random fluctuations depending on the queries, we might observe a difference. So the actual difference might be on the left side here or on the right side here, right?\n7:43\nSo, and this curve kind of shows the probability that we will actually observe values that are deviating from zero here.\n7:53\nNow, so if we look at this picture then, we see that\n8:01\nif a difference is observed here, then the chance is very high that this is in fact a random observation, right? We can define a region of likely observation because of random fluctuation and this is that  of all the outcomes. And in this then the observed may still be from random fluctuation.\n8:28\nBut if you observe a value in this region or a difference on this side, then the difference is unlikely from random fluctuation. All right, so there's a very small probability that you are observe such a difference just because of random fluctuation.\n8:48\nSo in that case, we can then conclude the difference must be real. So System B is indeed better.\n8:56\nSo this is the idea of Statical Significance Test. The takeaway message here is that you have to use many queries to avoid jumping into a conclusion. As in this case, to say System B is better.\n9:09\nThere are many different ways of doing this Statistical Significance Test.\n9:15\nSo now, let's talk about the other problem of making judgments and, as we said earlier, it's very hard to judge all the documents completely unless it's a very small data set. So the question is, if we can afford judging all the documents in the collection, which is subset should we judge?\n9:35\nAnd the solution here is Pooling. And this is a strategy that has been used in many cases to solve this problem.\n9:46\nSo the idea of Pooling is the following. We would first choose a diverse set of ranking methods. These are Text Retrieval systems.\n9:57\nAnd we hope these methods can help us nominate like the relevant documents. So the goal is to pick out the relevant documents. We want to make judgements on relevant documents because those are the most useful documents from users perspectives. So then we're going to have each to return top-K documents.\n10:17\nThe K can vary from systems. But the point is to ask them to suggest the most likely relevant documents.\n10:25\nAnd then we simply combine all these top-K sets to form a pool of documents for human assessors. To judge, so imagine you have many systems each were ten k documents. We take the top-K documents, and we form a union. Now, of course, there are many documents that are duplicated because many systems might have retrieved the same random documents. So there will be some duplicate documents.\n10:56\nAnd there are also unique documents that are only returned by one system. So the idea of having diverse set of ranking methods is to ensure the pool is broad. And can include as many possible relevant documents as possible.\n11:12\nAnd then, the users would, human assessors would make complete the judgments on this data set, this pool. And the other unjudged the documents are usually just assumed to be non relevant. Now if the pool is large enough, this assumption is okay.\n11:32\nBut if the pool is not very large, this actually has to be reconsidered. And we might use other strategies to deal with them and there are indeed other methods to handle such cases. And such a strategy is generally okay for comparing systems that contribute to the pool. That means if you participate in contributing to the pool, then it's unlikely that it would penalize your system because the problematic documents have all been judged.\n12:04\nHowever, this is problematic for evaluating a new system that may have not contributed to the pool. In this case, a new system might be penalized because it might have nominated some read only documents that have not been judged. So those documents might be assumed to be non relevant. That's unfair. So to summarize the whole part of textual evaluation, it's extremely important. Because the problem is the empirically defined problem, if we\n12:38\ndon't rely on users, there's no way to tell whether one method works better.\n12:43\nIf we have in the property experiment design, we might misguide our research or applications. And we might just draw wrong conclusions. And we have seen this is in some of our discussions. So make sure to get it right for your research or application.\n13:00\nThe main methodology is the Cranfield evaluation methodology. And they are the main paradigm used in all kinds of empirical evaluation tasks, not just a search engine variation. Map and nDCG are the two main measures that you should definitely know about and they are appropriate for comparing ranking algorithms. You will see them often in research papers. Precision at 10 documents is easier to interpret from user's perspective. So that's also often useful.\n13:30\nWhat's not covered is some other evaluation strategy like A-B Test. Where the system would mix two, the results of two methods, randomly. And then would show the mixed results to users. Of course, the users don't see which result, from which method. The users would judge those results or click on those documents in a search engine application. In this case then, the search engine can check or click the documents and see if one method has contributed more through the click the documents. If the user tends to click on one, the results from one method,\n14:13\nthen it suggests that message may be better. So this is what leverages the real users of a search engine to do evaluation. It's called A-B Test and it's a strategy that is often used by the modern search engines or commercial search engines. Another way to evaluate IR or textual retrieval is user studies and we haven't covered that. I've put some references here that you can look at if you want to know more about that.\n14:41\nSo, there are three additional readings here. These are three mini books about evaluation and they are all excellent in covering a broad review of Information Retrieval Evaluation. And it covers some of the things that we discussed, but they also have a lot of others to offer.\n15:02\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/CXoWB/lesson-1-3-text-retrieval-problemdict_values(["List\nCS 410: Text Information Systems\nWeek 1\nLesson 1.3: Text Retrieval Problem\nPrevious\nNext\nOrientation Information\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nVideo:\nVideo\nLesson 1.1: Natural Language Content Analysis\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 1.2: Text Access\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.3: Text Retrieval Problem\n. Duration: 26 minutes\n26 min\nVideo:\nVideo\nLesson 1.4: Overview of Text Retrieval Methods\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 1.5: Vector Space Model - Basic Idea\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\n. Duration: 17 minutes\n17 min\nWeek 1 Activities\nLesson 1.3: Text Retrieval Problem\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC] This lecture is about the text retrieval problem.\n0:12\nThis picture shows our overall plan for lectures.\n0:16\nIn the last lecture, we talked about the high level strategies for text access. We talked about push versus pull.\n0:25\nSuch engines are the main tools for supporting the pull mode. Starting from this lecture, we're going to talk about the how search engines work in detail.\n0:38\nSo first it's about the text retrieval problem.\n0:42\nWe're going to talk about the three things in this lecture. First, we define Text Retrieval. Second we're going to make a comparison between Text Retrieval and the related task Database Retrieval.\n0:58\nFinally, we're going to talk about the Document Selection versus Document Ranking as two strategies for responding to a user's query.\n1:09\nSo what is Text Retrieval?\n1:12\nIt should be a task that's familiar for the most of us because we're using web search engines all the time.\n1:19\nSo text retrieval is basically a task where the system would respond to a user's query With relevant documents. Basically, it's for supporting a query\n1:32\nas one way to implement the poll mode of information access.\n1:39\nSo the situation is the following. You have a collection of text retrieval documents. These documents could be all the webpages on the web, or all the literature articles in the digital library. Or maybe all the text files in your computer.\n1:58\nA user will typically give a query to the system to express information need. And then, the system would return relevant documents to users. Relevant documents refer to those documents that are useful to the user who typed in the query.\n2:16\nAll this task is a phone call that information retrieval.\n2:21\nBut literally information retrieval would broadly include the retrieval of other non-textual information as well, for example audio, video, etc. It's worth noting that Text Retrieval is at the core of information retrieval in the sense that other medias such as video can be retrieved by exploiting the companion text data. So for example, current the image search engines actually match a user's query was the companion text data of the image.\n2:59\nThis problem is also called search problem.\n3:05\nAnd the technology is often called the search technology industry.\n3:11\nIf you ever take a course in databases it will be useful to pause the lecture at this point and think about the differences between text retrieval and database retrieval. Now these two tasks are similar in many ways.\n3:29\nBut, there are some important differences.\n3:33\nSo, spend a moment to think about the differences between the two. Think about the data, and the information managed by a search engine versus those that are managed by a database system.\n3:47\nThink about the different between the queries that you typically specify for database system versus queries that are typed in by users in a search engine.\n3:59\nAnd then finally think about the answers.\n4:02\nWhat's the difference between the two? Okay, so if we think about the information or data managed by the two systems, we will see that in text retrieval. The data is unstructured, it's free text. But in databases, they are structured data where there is a clear defined schema to tell you this column is the names of people and that column is ages, etc.\n4:31\nThe unstructured text is not obvious what are the names of people mentioned in the text.\n4:40\nBecause of this difference, we also see that text information tends to be more ambiguous and we talk about that in the processing chapter, whereas in databases. But they don't tend to have where to find the semantics.\n4:58\nThe results important difference in the queries, and this is partly due to the difference in the information or data.\n5:07\nSo test queries tend to be ambiguous. Whereas in their research, the queries are typically well-defined. Think about a SQL query that would clearly specify what records to be returned. So it has very well-defined semantics.\n5:27\nKeyword queries or electronic queries tend to be incomplete, also in that it doesn't really specify what documents should be retrieved. Whereas complete specification for what should be returned.\n5:47\nAnd because of these differences, the answers would be also different. Being the case of text retrieval, we're looking for it rather than the documents.\n5:58\nIn the database search, we are retrieving records or match records with the sequel query more precisely.\n6:09\nNow in the case of text retrieval, what should be the right answers to the query is not very well specified, as we just discussed.\n6:21\nSo it's unclear what should be the right answers to a query. And this has very important consequences, and that is, textual retrieval is an empirically defined problem.\n6:38\nSo this is a problem because if it's empirically defined, then we can not mathematically prove one method is better than another method.\n6:52\nThat also means we must rely on empirical evaluation involving users to know which method works better.\n7:02\nAnd that's why we have. You need more than one lectures to cover the issue of evaluation. Because this is very important topic for Sir Jennings.\n7:13\nWithout knowing how to evaluate heroism properly, there's no way to tell whether we have got the better or whether one system is better than another.\n7:28\nSo now let's look at the problem in a formal way.\n7:32\nSo, this slide shows a formal formulation of the text retrieval problem.\n7:37\nFirst, we have our vocabulary set, which is just a set of words in a language.\n7:44\nNow here, we are considering only one language, but in reality, on the web, there might be multiple natural languages. We have texts that are in all kinds of languages.\n7:57\nBut here for simplicity, we just assume that is one kind of language. As the techniques used for retrieving data from multiple languages Are more or less similar to the techniques used for retrieving documents in one end, which although there is important difference, the principle methods are very similar.\n8:21\nNext, we have the query, which is a sequence of words.\n8:26\nAnd so here, you can see\n8:31\nthe query is defined as a sequence of words. Each q sub i is a word in the vocabulary.\n8:42\nA document is defined in the same way, so it's also a sequence of words. And here, d sub ij is also a word in the vocabulary.\n8:52\nNow typically, the documents are much longer than queries.\n8:57\nBut there are also cases where the documents may be very short.\n9:04\nSo you can think about what might be a example of that case.\n9:09\nI hope you can think of Twitter search. Tweets are very short.\n9:16\nBut in general, documents are longer than the queries.\n9:22\nNow, then we have a collection of documents, and this collection can be very large. So think about the web. It could be very large.\n9:36\nAnd then the goal of text retrieval is you'll find the set of relevant in the documents, which we denote by R'(q), because it depends on the query. And this in general, a subset of all the documents in the collection.\n9:52\nUnfortunately, this set of relevant documents is generally unknown, and user-dependent in the sense that, for the same query typed in by different users, they expect the relevant documents may be different.\n10:09\nThe query given to us by the user is only a hint on which document should be in this set.\n10:17\nAnd indeed, the user is generally unable to specify what exactly should be in this set, especially in the case of web search, where the connection's so large, the user doesn't have complete knowledge about the whole production.\n10:34\nSo the best search system can do is to compute an approximation of this relevant document set. So we denote it by R'(q). So formerly, we can see the task is to compute this R'(q) approximation of the relevant documents. So how can we do that? Now imagine if you are now asked to write a program to do this.\n11:08\nWhat would you do? Now think for a moment. Right, so these are your input. The query, the documents.\n11:20\nAnd then you are to compute the answers to this query, which is a set of documents that would be useful to the user.\n11:29\nSo, how would you solve the problem? Now in general, there are two strategies that we can use.\n11:39\nThe first strategy is we do a document selection, and that is, we're going to have a binary classification function, or binary classifier.\n11:49\nThat's a function that would take a document and query as input, and then give a zero or one as output to indicate whether this document is relevant to the query or not.\n12:02\nSo in this case, you can see the document.\n12:08\nThe relevant document is set, is defined as follows. It basically, all the documents that have a value of 1 by this function.\n12:25\nSo in this case, you can see the system must have decide if the document is relevant or not. Basically, it has to say whether it's one or zero. And this is called absolute relevance. Basically, it needs to know exactly whether it's going to be useful to the user.\n12:41\nAlternatively, there's another strategy called document ranking.\n12:46\nNow in this case, the system is not going to make a call whether a document is random or not. But rather the system is going to use a real value function, f here.\n12:58\nThat would simply give us a value that would indicate which document is more likely relevant.\n13:05\nSo it's not going to make a call whether this document is relevant or not. But rather it would say which document is more likely relevant. So this function then can be used to random documents, and then we're going to let the user decide where to stop, when the user looks at the document. So we have a threshold theta here to determine what documents should be in this approximation set. And we're going to assume that all the documents that are ranked above the threshold are in this set, because in effect, these are the documents that we deliver to the user. And theta is a cutoff determined by the user.\n13:56\nSo here we've got some collaboration from the user in some sense, because we don't really make a cutoff. And the user kind of helped the system make a cutoff.\n14:08\nSo in this case, the system only needs to decide if one document is more likely relevant than another. And that is, it only needs to determine relative relevance,\n14:19\nas opposed to absolute relevance.\n14:22\nNow you can probably already sense that relative relevance would be easier to determine than absolute relevance. Because in the first case, we have to say exactly whether a document is relevant or not.\n14:37\nAnd it turns out that ranking is indeed generally preferred to document selection.\n14:46\nSo let's look at these two strategies in more detail. So this picture shows how it works. So on the left side, we see these documents, and we use the pluses to indicate the relevant documents. So we can see the true relevant documents here consists this set of true relevant documents, consists of these process, these documents.\n15:17\nAnd with the document selection function, we're going to basically classify them into two groups, relevant documents, and non-relevant ones. Of course, the classified will not be perfect so it will make mistakes. So here we can see, in the approximation of the relevant documents, we have got some number in the documents.\n15:43\nAnd similarly, there is a relevant document that's misclassified as non-relevant. In the case of document ranking, we can see the system seems like, simply ranks all the documents in the descending order of the scores. And then, we're going to let the user stop wherever the user wants to stop. If the user wants to examine more documents, then the user will scroll down some more and then stop [INAUDIBLE]. But if the user only wants to read a few random documents, the user might stop at the top position. So in this case, the user stops at d4. So in fact, we have delivered these four documents to our user.\n16:33\nSo as I said ranking is generally preferred, and one of the reasons is because the classifier in the case of document selection is unlikely accurate. Why? Because the only clue is usually the query. But the query may not be accurate in the sense that it could be overly constrained.\n16:57\nFor example, you might expect relevant documents to talk about all these\n17:04\ntopics by using specific vocabulary. And as a result, you might match no relevant documents. Because in the collection, no others have discussed the topic using these vocabularies, right? So in this case, we'll see there is this problem of\n17:25\nno relevant documents to return in the case of over-constrained query.\n17:33\nOn the other hand, if the query is under-constrained, for example, if the query does not have sufficient descriptive words to find the random documents. You may actually end up having of over delivery, and this when you thought these words my be sufficient to help you find the right documents. But, it turns out they are not sufficient and there are many distractions, documents using similar words. And so, this is a case of over delivery.\n18:08\nUnfortunately, it's very hard to find the right position between these two extremes.\n18:15\nWhy? Because whether users looking for the information in general the user does not have a good knowledge about the information to be found. And in that case, the user does not have a good knowledge about what\n18:30\nvocabularies will be used in those relevent documents. So it's very hard for a user to pre-specify the right level of constraints.\n18:44\nEven if the classifier is accurate, we also still want to rend these relevant documents, because they are generally not equally relevant.\n18:56\nRelevance is often a matter of degree.\n18:59\nSo we must prioritize these documents for a user to examine.\n19:06\nAnd note that this prioritization is very important\n19:12\nbecause a user cannot digest all the content the user generally would have to look at each document sequentially.\n19:21\nAnd therefore, it would make sense to users with the most relevant documents. And that's what ranking is doing. So for these reasons, ranking is generally preferred.\n19:36\nNow this preference also has a theoretical justification and this is given by the probability ranking principle.\n19:44\nIn the end of this lecture, there is reference for this.\n19:49\nThis principle says, returning a ranked list of documents in descending order of probability that a document is relevant to the query is the optimal strategy under the following two assumptions.\n20:02\nFirst, the utility of a document (to a user) Is independent of the utility of any other document.\n20:10\nSecond, a user would be assumed to browse the results sequentially. Now it's easy to understand why these assumptions are needed in order to justify Site for the ranking strategy. Because if the documents are independent, then we can evaluate the utility of each document that's separate.\n20:36\nAnd this would allow the computer score for each document independently. And then, we are going to rank these documents based on the scrolls.\n20:45\nThe second assumption is to say that the user would indeed follow the rank list. If the user is not going to follow the ranked list, is not going to examine the documents sequentially, then obviously the ordering would not be optimal.\n21:00\nSo under these two assumptions, we can theoretically justify the ranking strategy is, in fact, the best that you could do. Now, I've put one question here. Do these two assumptions hold?\n21:18\nI suggest you to pause the lecture, for a moment, to think about this.\n21:27\nNow, can you think of some examples that would suggest these assumptions aren't necessarily true.\n21:44\nNow, if you think for a moment, you may realize none of the assumptions Is actually true.\n21:53\nFor example, in the case of independence assumption we might have documents that have similar or exactly the same content. If we look at each of them alone, each is relevant.\n22:07\nBut if the user has already seen one of them, we can assume it's generally not very useful for the user to see another similar or duplicated one.\n22:19\nSo clearly the utility on the document that is dependent on other documents that the user has seen.\n22:27\nIn some other cases you might see a scenario where one document that may not be useful to the user, but when three particular documents are put together. They provide answers to the user's question.\n22:42\nSo this is a collective relevance and that also suggests that the value of the document might depend on other documents.\n22:53\nSequential browsing generally would make sense if you have a ranked list there.\n22:59\nBut even if you have a rank list, there is evidence showing that users don't always just go strictly sequentially through the entire list. They sometimes will look at the bottom for example, or skip some. And if you think about the more complicated interfaces that we could possibly use like two dimensional in the phase. Where you can put that additional information on the screen then sequential browsing is a very restricted assumption.\n23:32\nSo the point here is that\n23:35\nnone of these assumptions is really true but less than that.\n23:41\nBut probability ranking principle establishes some solid foundation for\n23:46\nranking as a primary pattern for search engines. And this has actually been the basis for a lot of research work in information retrieval. And many hours have been designed based on this assumption,\n24:01\ndespite that the assumptions aren't necessarily true. And we can address this problem by doing post processing Of a ranked list, for example, to remove redundancy.\n24:20\nSo to summarize this lecture, the main points that you can take away are the following. First, text retrieval is an empirically defined Problem. And that means which algorithm is better must be judged by the users. Second, document ranking is generally preferred. And this will help users prioritize examination of search results.\n24:47\nAnd this is also to bypass the difficulty in determining absolute relevance Because we can get some help from users in determining where to make the cut off, it's more flexible.\n25:01\nSo, this further suggests that the main technical challenge in designing a search engine is the design effective ranking function.\n25:10\nIn other words, we need to define what is the value of this function F on the query and document pair.\n25:21\nHow we design such a function is the main topic in the following lectures.\n25:29\nThere are two suggested additional readings. The first is the classical paper on the probability ranking principle.\n25:37\nThe second one is a must-read for anyone doing research on information retrieval. It's a classic IR book, which has excellent coverage of the main research and results in early days up to the time when the book was written. Chapter six of this book has an in-depth discussion of the Probability Ranking Principle and Probably for retrieval models in general. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/cIFsU/lesson-6-7-recommender-systems-collaborative-filtering-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:07\nThis lecture is about collaborative filtering.\n0:11\nIn this lecture we're going to continue the discussion of recommended systems. In particular, we're going to look at the approach of collaborative filtering. You have seen this slide before when we talked about the two strategies to answer the basic question, will user U like item X? In the previous lecture, we looked at the item similarity, that's content-based filtering. In this lecture, we're going to look at the user similarity. This is a different strategy, called a collaborative filtering.\n0:44\nSo first, what is collaborative filtering?\n0:47\nIt is to make filtering decisions for individual user based on the judgements of other uses.\n0:54\nAnd that is to say we will infer individual's interest or preferences from that of other similar users. So the general idea is the following. Given a user u, we're going to first find the similar users, U1 through. And then we're going to predict the use preferences based on the preferences of these similar users, U1 through.\n1:22\nNow, the user similarity here can be judged based their similarity, the preferences on a common set of items.\n1:31\nNow here you can see the exact content of item doesn't really matter. We're going to look at the only the relation between the users and the items.\n1:41\nSo this means this approach is very general. It can be applied to any items, not just the text of objects. So this approach would work well under the following assumptions. First, users with the same interest will have similar preferences. Second, the users with similar preferences probably share the same interest. So for example, if the interest of the user is in information retrieval, then we can infer the user probably favor SIGIR papers.\n2:14\nSo those who are interested in information retrieval researching, probably all favor SIGIR papers. That's an assumption that we make. And if this assumption is true, then it would help collaborative filtering to work well. We can also assume that if we see people favor See SIGIR papers, then we can infer their interest is probably information retrieval. So in these simple examples, it seems to make sense, and in many cases such assumption actually does make sense. So another assumption we have to make is that there are sufficiently large number of user preferences available to us. So for example, if you see a lot of ratings of users for movies and those indicate their preferences on movies. And if you have a lot of such data, then cluttered and filtering can be very effective.\n3:09\nIf not, there will be a problem, and that's often called a cold start problem. That means you don't have many preferences available, so the system could not fully take advantage of collaborative filtering yet. So let's look at the filtering problem in a more formal way.\n3:30\nSo this picture shows that we are, in general, considering a lot of users and we're showing m users here, so U1 through. And we're also considering a number of objects. Let's say n objects in order to O1 through On. And then we will assume that the users will be able to judge those objects and the user could for example give ratings to those items. For example, those items could be movies, could be products and then the users would give ratings 1 through 5 and see. So what you see here is that we have shown some ratings available for some combinations. So some users have watched some movies, they have rated those movies, they obviously won't be able to watch all the movies and some users may actually only watch a few movies. So this is in general a small symmetrics. So many items and many entries have unknown values.\n4:39\nAnd what's interesting here is we could potentially infer the value of an element in this matrix based on other values. And that's after the essential question in collaborative filtering, and that is, we assume there's an unknown function here, f. That would map a pair of user and object to a rating. And we have observed the sum values of this function.\n5:08\nAnd we want to infer the value of this function for other pairs that don't have that as available here. So this is very similar to other machinery problems where we'd know the values of the function on some training data set. And we hope to predict the values of this function on some test data so this is a function approximation. And how can we pick out the function based on the observed ratings. So this is the setup. Now there are many approaches to solving this problem. In fact, this is a very active research area or reason that there are special conferences dedicated to the problem,\n6:10\nmajor conference devoted to the problem. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/hgSh4/7-2-overview-text-mining-and-analytics-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.2 Overview Text Mining and Analytics: Part 2\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.2 Overview Text Mining and Analytics: Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] So, looking at the text mining problem more closely, we see that the problem is similar to general data mining, except that we'll be focusing more on text data.\n0:21\nAnd we're going to have text mining algorithms to help us to turn text data into actionable knowledge that we can use in real world, especially for decision making, or for completing whatever tasks that require text data to support. Because, in general, in many real world problems of data mining we also tend to have other kinds of data that are non-textual. So a more general picture would be to include non-text data as well.\n0:56\nAnd for this reason we might be concerned with joint mining of text and non-text data. And so in this course we're going to focus more on text mining, but we're also going to also touch how do to joint analysis of both text data and non-text data. With this problem definition we can now look at the landscape of the topics in text mining and analytics.\n1:21\nNow this slide shows the process of generating text data in more detail.\n1:27\nMore specifically, a human sensor or human observer would look at the word from some perspective.\n1:34\nDifferent people would be looking at the world from different angles and they'll pay attention to different things. The same person at different times might also pay attention to different aspects of the observed world. And so the humans are able to perceive the world from some perspective. And that human, the sensor, would then form a view of the world. And that can be called the Observed World. Of course, this would be different from the Real World because of the perspective that the person has taken can often be biased also.\n2:16\nNow the Observed World can be represented as, for example, entity-relation graphs or in a more general way, using knowledge representation language. But in general, this is basically what a person has in mind about the world. And we don't really know what exactly it looks like, of course. But then the human would express what the person has observed using a natural language, such as English. And the result is text data.\n2:55\nOf course a person could have used a different language to express what he or she has observed. In that case we might have text data of mixed languages or different languages.\n3:10\nThe main goal of text mining Is actually to revert this process of generating text data. We hope to be able to uncover some aspect in this process.\n3:28\nSpecifically, we can think about mining, for example, knowledge about the language.\n3:35\nAnd that means by looking at text data in English, we may be able to discover something about English, some usage of English, some patterns of English.\n3:47\nSo this is one type of mining problems, where the result is some knowledge about language which may be useful in various ways.\n3:58\nIf you look at the picture, we can also then mine knowledge about the observed world. And so this has much to do with mining the content of text data.\n4:11\nWe're going to look at what the text data are about, and then try to get the essence of it or extracting high quality information about a particular aspect of the world that we're interested in.\n4:26\nFor example, everything that has been said about a particular person or a particular entity. And this can be regarded as mining content to describe the observed world in the user's mind or the person's mind.\n4:45\nIf you look further, then you can also imagine we can mine knowledge about this observer, himself or herself. So this has also to do with using text data to infer some properties of this person.\n5:03\nAnd these properties could include the mood of the person or sentiment of the person.\n5:10\nAnd note that we distinguish the observed word from the person because text data can't describe what the person has observed in an objective way. But the description can be also subjected with sentiment and so, in general, you can imagine the text data would contain some factual descriptions of the world plus some subjective comments. So that's why it's also possible to do text mining to mine knowledge about the observer. Finally, if you look at the picture to the left side of this picture, then you can see we can certainly also say something about the real world. Right? So indeed we can do text mining to infer other real world variables. And this is often called a predictive analytics.\n6:00\nAnd we want to predict the value of certain interesting variable. So, this picture basically covered multiple types of knowledge that we can mine from text in general.\n6:14\nWhen we infer other real world variables we could also use some of the results from mining text data as intermediate results to help the prediction. For example, after we mine the content of text data we might generate some summary of content. And that summary could be then used to help us predict the variables of the real world. Now of course this is still generated from the original text data, but I want to emphasize here that often the processing of text data to generate some features that can help with the prediction is very important.\n7:04\nAnd that's why here we show the results of some other mining tasks, including mining the content of text data and mining knowledge about the observer, can all be very helpful for prediction.\n7:21\nIn fact, when we have non-text data, we could also use the non-text data to help prediction, and of course it depends on the problem. In general, non-text data can be very important for such prediction tasks. For example, if you want to predict stock prices or changes of stock prices based on discussion in the news articles or in social media, then this is an example of using text data to predict some other real world variables. But in this case, obviously, the historical stock price data would be very important for this prediction. And so that's an example of non-text data that would be very useful for the prediction. And we're going to combine both kinds of data to make the prediction. Now non-text data can be also used for analyzing text by supplying context.\n8:25\nWhen we look at the text data alone, we'll be mostly looking at the content and/or opinions expressed in the text.\n8:32\nBut text data generally also has context associated.\n8:37\nFor example, the time and the location that associated are with the text data. And these are useful context information.\n8:48\nAnd the context can provide interesting angles for analyzing text data. For example, we might partition text data into different time periods because of the availability of the time. Now we can analyze text data in each time period and then make a comparison. Similarly we can partition text data based on locations or any meta data that's associated to form interesting comparisons in areas. So, in this sense, non-text data can actually provide interesting angles or perspectives for text data analysis. And it can help us make context-sensitive analysis of content or the language usage or\n9:36\nthe opinions about the observer or the authors of text data. We could analyze the sentiment in different contexts. So this is a fairly general landscape of the topics in text mining and analytics. In this course we're going to selectively cover some of those topics. We actually hope to cover most of these general topics.\n10:06\nFirst we're going to cover natural language processing very briefly because this has to do with understanding text data and this determines how we can represent text data for text mining. Second, we're going to talk about how to mine word associations from text data. And word associations is a form of use for lexical knowledge about a language. Third, we're going to talk about topic mining and analysis. And this is only one way to analyze content of text, but it's a very useful ways of analyzing content. It's also one of the most useful techniques in text mining.\n10:53\nThen we're going to talk about opinion mining and sentiment analysis. So this can be regarded as one example of mining knowledge about the observer.\n11:07\nAnd finally we're going to cover text-based prediction problems where we try to predict some real world variable based on text data.\n11:17\nSo this slide also serves as a road map for this course. And we're going to use this as an outline for the topics that we'll cover in the rest of this course. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/qudy6/11-3-text-categorization-evaluation-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 11\n11.3 Text Categorization: Evaluation Part 1\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nVideo:\nVideo\n11.1 Text Categorization: Discriminative Classifier Part 1\n. Duration: 20 minutes\n20 min\nVideo:\nVideo\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\n. Duration: 31 minutes\n31 min\nVideo:\nVideo\n11.3 Text Categorization: Evaluation Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n11.4 Text Categorization: Evaluation Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n11.5 Opinion Mining and Sentiment Analysis: Motivation\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\n11.3 Text Categorization: Evaluation Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the Evaluation of Text Categorization. So we've talked about many different methods for text categorization. But how do you know which method works better?\n0:19\nAnd for a particular application, how do you know this is the best way of solving your problem? To understand these, we have to\n0:29\nhow to we have to know how to evaluate categorization results. So first some general thoughts about the evaluation.\n0:38\nIn general, for evaluation of this kind of empirical tasks such as categorization, we use methodology that was developed in 1960s by information retrieval researchers. Called a Cranfield Evaluation Methodology. The basic idea is to have humans create test correction,\n0:59\nwhere, we already know, every document is tagged with the desired categories. Or, in the case of search, for which query, which documents that should have been retrieved, and this is called, a ground truth. Now, with this ground truth test correction, we can then reuse the collection to test the many different systems and then compare different systems. We can also turn off some components in the system to see what's going to happen. Basically it provides a way to do control experiments to compare different methods.\n1:36\nSo this methodology has been virtually used for all the tasks that involve empirically defined problems.\n1:45\nSo in our case, then, we are going to compare our systems categorization results with the categorization, ground truth, created by humans.\n1:56\nAnd we're going to compare our systems decisions,\n2:00\nwhich documents should get which category with what\n2:06\ncategories have been assigned to those documents by humans. And we want to quantify the similarity of these decisions or equivalently, to measure the difference between the system output and the desired ideal output generated by the humans.\n2:25\nSo obviously, the highest similarity is the better results are.\n2:30\nThe similarity could be measured in different ways. And that would lead to different measures. And sometimes it's desirable also to match the similarity from different perspectives just to have a better understanding of the results in detail. For example, we might be also interested in knowing which category performs better and which which category is easy to categorize, etc. In general, different categorization mistakes however, have different costs for specific applications. So some areas might be more serious than others. So ideally, we would like to model such differences, but if you read many papers in categorization you will see that they don't generally do that. Instead, they will use a simplified measure and that's because it's often okay not to consider such a cost variation when we compare methods and when we are interested in knowing the relative difference of these methods. So it's okay to introduce some bias, as long as the bias is not already with a particular method and then we should expect the more effective method to perform better than a less effective one, even though the measure is not perfect.\n3:53\nSo the first measure that we'll introduce is called classification accuracy and this is a basic into measure the percentage of correct decisions. So here you see that there are categories denoted by c1 through ck and there are n documents, denoted by d1 through d N. And for each pair of category and the document, we can then look at the situation.\n4:16\nAnd see if the system has said yes to this pair, basically has assigned this category to this document. Or no, so this is denoted by Y or M, that's the systems of the decision. And similarly, we can look at the human's decisions also, if the human has assigned a category to the document of that there will be a plus sign here. That just means that a human. We think of this assignment is correct and incorrect then it's a minus. So we'll see all combinations of this Ns, yes and nos, minus and pluses. There are four combinations in total. And two of them are correct, and that's when we have y(+) or n(-), and then there are also two kinds of errors. So the measure of classification accuracy is simply to count how many of these decisions are correct. And normalize that by the total number of decisions we have made. So, we know that the total number of decisions is n, multiplied by k.\n5:20\nAnd, the number of correct decisions are basically of two kinds. One is y plusses. And the other is n minus this n. We just put together the count. Now, this is a very convenient measure that will give us one number to characterize performance of a method. And the higher, the better, of course.\n5:41\nBut the method also has some problems. First it has treated all the decisions equally. But in reality, some decision errors are more serious than others. For example, it may be more important to get the decisions right on some documents, than others.\n5:58\nOr maybe, more important to get the decisions right on some categories, than others, and this would call for some detailed evaluation of this results to understand the strands and\n6:12\nof different methods, and to understand the performance of these methods. In detail in a per category or per document basis. One example that shows clearly the decision errors are having different causes is spam filtering that could be retrieved as two category categorization problem.\n6:36\nMissing a legitimate email result, is one type of error. But letting spam to come into your folder is another type of error. The two types of errors are clearly very different, because it's very important not to miss a legitimate email. It's okay to occasionally let a spam email to come into your inbox. So the error of the first, missing a legitimate email is very, is of high cost. It's a very serious mistake and classification error, classification accuracy does not address this issue.\n7:14\nThere's also another problem with imbalance to test set. Imagine there's a skew to test set where most instances are category one and  of instances are category one. Only  are in category two. In such a case, we can have a very simple baseline that accurately performs very well and that baseline. Sign with similar, I put all instances in the major category.\n7:36\nThat will get us  accuracy in this case. It's going to be appearing to be very effective, but in reality, this is obviously not a good result.\n7:47\nAnd so, in general, when we use classification accuracy as a measure, we want to ensure that the causes of balance.\n7:54\nAnd one above equal number of instances, for example in each class the minority categories or causes tend to be overlooked in the evaluation of classification accuracy. So, to address these problems, we of course would like to also evaluate the results in other ways and in different ways. As I said, it's beneficial to look at after multiple perspectives. So for example, we can look at the perspective from each document as a perspective based on each document. So the question here is, how good are the decisions on this document?\n8:29\nNow, as in the general cases of all decisions, we can think about four combinations of possibilities, depending on whether the system has said yes and depending on whether the human has said it correct or incorrect or said yes or no. And so the four combinations are first when both the human systems said yes, and that's the true positives, when the system says, yes, and it's after the positive. So, when the system says, yes, it's a positive. But, when the human confirm that it is indeed correct, that becomes a true positive.\n9:07\nWhen the system says, yes, but the human says, no, that's incorrect, that's a false positive, have FP.\n9:15\nAnd when the system says no, but the human says yes, then it's a false negative. We missed one assignment. When both the system and human says no, then it's also correctly to assume that's true negatives. All right, so then we can have some measures to just better characterize the performance by using these four numbers and so two popular measures are precision and recall. And these were also proposed by information retrieval researchers 1960s for evaluating search results, but now they have become standard measures, use it everywhere. So when the system says yes, we can ask the question, how many are correct? What's the percent of correct decisions when the system says yes? That's called precision. It's true positive divided by all the cases when the system says yes, all the positives. The other measure is called recall, and this measures\n10:14\nwhether the document has all the categories it should have. So in this case it's divide the true positive by true positives and the false negatives. So these are all the cases where this human Says the document should have this category. So this represents both categories that it should have got, and so recall tells us whether the system has actually indeed assigned all the categories that it should have to this document.\n10:46\nThis gives us a detailed view of the document, then we can aggregate them later.\n10:52\nAnd if we're interested in some documents, and this will tell us how well we did on those documents, the subsets of them. It might be more interesting than others, for example. And this allows us to analyze errors in more detail as well. We can separate the documents of certain characteristics from others, and then look at the errors. You might see a pattern A for this kind of document, this long document. It doesn't as well for shock documents.\n11:18\nAnd this gives you some insight for inputting the method. Similarly, we can look at the per-category evaluation. In this case, we're going to look at the how good are the decisions on a particular category. As in the previous case we can define precision and recall. And it would just basically answer the questions from a different perspective.\n11:39\nSo when the system says yes, how many are correct? That means looking at this category to see if all the documents that are assigned with this category are indeed in this category, right? And recall, would tell us, has the category been actually assigned to all the documents That should have this category.\n12:00\nIt's sometimes also useful to combine precision and recall as one measure, and this is often done by using f measure. And this is just a harmonic mean of precision. Precision and recall defined on this slide. And it's also controlled by a parameter beta to\n12:20\nindicate whether precision is more important or recall is more. When beta is set to 1, we have measure called F1, and in this case, we just take equal weight upon both procedure and recall.\n12:34\nF1 is very often used as a measure for categorization.\n12:39\nNow, as in all cases, when we combine results, you always should think about the best way of combining them, so in this case I don't know if you have thought about it and we could have combined them just with arithmetic mean, right. So that would still give us the same range of values, but obviously there's a reason why we didn't do that and why f1 is more popular, and it's actually useful to think about difference. And we think about that, you'll see that there is indeed some difference and some undesirable property of this arithmatic. Basically, it will be obvious to you if you think about a case when the system says yes for all the category and document pairs. And then try the compute the precision and recall in that case. And see what would happen.\n13:28\nAnd basically, this kind of measure, the arithmetic mean, is not going to be as reasonable as F1 minus one [INAUDIBLE] trade off, so that the two values are equal. There is an extreme case where you have 0 for one letter and one for the other. Then F1 will be low, but the mean would still be reasonably high.\n14:01\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/7zA4L/7-1-overview-text-mining-and-analytics-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.1 Overview Text Mining and Analytics: Part 1\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.1 Overview Text Mining and Analytics: Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] In this lecture we give an overview of Text Mining and Analytics.\n0:13\nFirst, let's define the term text mining, and the term text analytics. The title of this course is called Text Mining and Analytics.\n0:25\nBut the two terms text mining, and text analytics are actually roughly the same.\n0:32\nSo we are not really going to really distinguish them, and we're going to use them interchangeably. But the reason that we have chosen to use both terms in the title is because there is also some subtle difference, if you look at the two phrases literally.\n0:52\nMining emphasizes more on the process. So it gives us a error rate medical view of the problem. Analytics, on the other hand emphasizes more on the result,\n1:07\nor having a problem in mind. We are going to look at text data to help us solve a problem.\n1:16\nBut again as I said, we can treat these two terms roughly the same.\n1:21\nAnd I think in the literature you probably will find the same. So we're not going to really distinguish that in the course.\n1:29\nBoth text mining and text analytics mean that we want to turn text data into high quality information, or actionable knowledge.\n1:42\nSo in both cases, we\n1:45\nhave the problem of dealing with a lot of text data and we hope to. Turn these text data into something more useful to us than the raw text data.\n1:57\nAnd here we distinguish two different results. One is high-quality information, the other is actionable knowledge.\n2:05\nSometimes the boundary between the two is not so clear.\n2:09\nBut I also want to say a little bit about\n2:12\nthese two different angles of the result of text field mining.\n2:19\nIn the case of high quality information, we refer to more concise information about the topic.\n2:28\nWhich might be much easier for humans to digest than the raw text data. For example, you might face a lot of reviews of a product.\n2:38\nA more concise form of information would be a very concise summary of the major opinions about the features of the product. Positive about, let's say battery life of a laptop.\n2:53\nNow this kind of results are very useful to help people digest the text data.\n2:59\nAnd so this is to minimize a human effort in consuming text data in some sense.\n3:06\nThe other kind of output is actually more knowledge. Here we emphasize the utility of the information or knowledge we discover from text data.\n3:18\nIt's actionable knowledge for some decision problem, or some actions to take.\n3:24\nFor example, we might be able to determine which product is more appealing to us, or a better choice for a shocking decision.\n3:38\nNow, such an outcome could be called actionable knowledge, because a consumer can take the knowledge and make a decision, and act on it. So, in this case text mining supplies knowledge for optimal decision making. But again, the two are not so clearly distinguished, so we don't necessarily have to make a distinction.\n4:06\nText mining is also related to text retrieval, which is a essential component in many text mining systems.\n4:15\nNow, text retrieval refers to finding relevant information from a large amount of text data.\n4:24\nSo I've taught another separate MOOC on text retrieval and search engines.\n4:31\nWhere we discussed various techniques for text retrieval.\n4:36\nIf you have taken that MOOC, and you will find some overlap.\n4:42\nAnd it will be useful To know the background of text retrieval of understanding some of the topics in text mining.\n4:51\nBut, if you have not taken that MOOC, it's also fine because in this MOOC on text mining and analytics, we're going to repeat some of the key concepts that are relevant for text mining. But they're at the high level and they also explain the relation between text retrieval and text mining.\n5:12\nText retrieval is very useful for text mining in two ways. First, text retrieval can be a preprocessor for text mining. Meaning that it can help us turn big text data into a relatively small amount of most relevant text data. Which is often what's needed for solving a particular problem.\n5:36\nAnd in this sense, text retrieval also helps minimize human effort.\n5:43\nText retrieval is also needed for knowledge provenance. And this roughly corresponds to the interpretation of text mining as turning text data into actionable knowledge. Once we find the patterns in text data, or actionable knowledge, we generally would have to verify the knowledge. By looking at the original text data. So the users would have to have some text retrieval support, go back to the original text data to interpret the pattern or to better understand an analogy or to verify whether a pattern is really reliable. So this is a high level introduction to the concept of text mining, and the relationship between text mining and retrieval.\n6:32\nNext, let's talk about text data as a special kind of data.\n6:39\nNow it's interesting to view text data as data generated by humans as subjective sensors.\n6:53\nSo, this slide shows an analogy between text data and non-text data. And between humans as subjective sensors and physical sensors, such as a network sensor or a thermometer.\n7:16\nSo in general a sensor would monitor the real world in some way. It would sense some signal from the real world, and then would report the signal as data, in various forms. For example, a thermometer would watch the temperature of real world and then we report the temperature being a particular format.\n7:44\nSimilarly, a geo sensor would sense the location and then report. The location specification, for example, in the form of longitude value and latitude value. A network sends over the monitor network traffic, or activities in the network and are reported. Some digital format of data. Similarly we can think of humans as subjective sensors. That will observe the real world and from some perspective. And then humans will express what they have observed in the form of text data. So, in this sense, human is actually a subjective sensor that would also sense what's happening in the world and then express what's observed in the form of data, in this case, text data. Now, looking at the text data in this way has an advantage of being able to integrate all types of data together. And that's indeed needed in most data mining problems.\n8:56\nSo here we are looking at the general problem of data mining.\n9:02\nAnd in general we would Be dealing with a lot of data about our world that are related to a problem. And in general it will be dealing with both non-text data and text data. And of course the non-text data are usually produced by physical senses. And those non-text data can be also of different formats.\n9:27\nNumerical data, categorical, or relational data, or multi-media data like video or speech.\n9:36\nSo, these non text data are often very important in some problems. But text data is also very important, mostly because they contain a lot of symmetrical content. And they often contain knowledge about the users, especially preferences and opinions of users.\n10:01\nSo, but by treating text data as the data observed from human sensors, we can treat all this data together in the same framework. So the data mining problem is basically to turn such data, turn all the data in your actionable knowledge to that we can take advantage of it to change the real world of course for better. So this means the data mining problem is basically taking a lot of data as input and giving actionable knowledge as output. Inside of the data mining module, you can also see we have a number of different kind of mining algorithms. And this is because, for different kinds of data, we generally need different algorithms for mining the data.\n10:56\nFor example, video data might require computer vision to understand video content. And that would facilitate the more effective mining. And we also have a lot of general algorithms that are applicable to all kinds of data and those algorithms, of course, are very useful. Although, for a particular kind of data, we generally want to also develop a special algorithm. So this course will cover specialized algorithms that are particularly useful for mining text data. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/dM6kh/lesson-1-6-vector-space-retrieval-model-simplest-instantiationdict_values(["List\nCS 410: Text Information Systems\nWeek 1\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\nPrevious\nNext\nOrientation Information\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nVideo:\nVideo\nLesson 1.1: Natural Language Content Analysis\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 1.2: Text Access\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.3: Text Retrieval Problem\n. Duration: 26 minutes\n26 min\nVideo:\nVideo\nLesson 1.4: Overview of Text Retrieval Methods\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 1.5: Vector Space Model - Basic Idea\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\n. Duration: 17 minutes\n17 min\nWeek 1 Activities\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\nIn this lecture we're going to talk about how to instantiate vector space model so that we can get very specific ranking function.\n0:22\nSo this is to continue the discussion of the vector space model, which is one particular approach to design a ranking function.\n0:34\nAnd we're going to talk about how we use the general framework of the the vector space model as a guidance to instantiate the framework to derive a specific ranking function. And we're going to cover the symbolist instantiation of the framework.\n0:55\nSo as we discussed in the previous lecture, the vector space model is really a framework. And this didn't say.\n1:05\nAs we discussed in the previous lecture, vector space model is really a framework. It does not say many things.\n1:14\nSo, for example, here it shows that it did not say how we should define the dimension.\n1:20\nIt also did not say how we place a document vector in this space.\n1:27\nIt did not say how we place a query vector in this vector space.\n1:32\nAnd, finally, it did not say how we should measure the similarity between the query vector and the document vector.\n1:40\nSo you can imagine, in order to implement this model,\n1:46\nwe have to say specifically how we compute these vectors. What is exactly xi? And what is exactly yi?\n1:58\nThis will determine where we place a document vector, where we place a query vector. And, of course, we also need to say exactly what should be the similarity function.\n2:11\nSo if we can provide a definition of the concepts that would define the dimensions and these xi's or yi's and namely weights of terms for queries and document, then we will be able to place document vectors and query vectors in this well defined space. And then, if we also specify similarity function, then we'll have a well defined ranking function.\n2:41\nSo let's see how we can do that and think about the instantiation. Actually, I would suggest you to pause the lecture at this point, spend a couple minutes to think about. Suppose you are asked to implement this idea.\n2:59\nYou have come up with the idea of vector space model, but you still haven't figured out how to compute these vectors exactly, how to define the similarity function. What would you do?\n3:12\nSo, think for a couple of minutes, and then proceed.\n3:20\nSo, let's think about some simplest ways of instantiating this vector space model. First, how do we define the dimension? Well, the obvious choice is to use each word in our vocabulary to define the dimension. And show that there are N words in our vocabulary. Therefore, there are N dimensions. Each word defines one dimension. And this is basically the bag of words with\n3:48\nNow let's look at how we place vectors in this space.\n3:54\nAgain here, the simplest strategy is to\n3:58\nuse a Bit Vector to represent both the query and a document.\n4:04\nAnd that means each element, xi and yi will be taking a value of either zero or 1.\n4:13\nWhen it's 1, it means the corresponding word is present in the document or in the query. When it's 0, it's going to mean that it's absent.\n4:27\nSo you can imagine if the user types in a few words in the query, then the query vector will only have a few 1's, many, many zeros.\n4:37\nThe document vector, generally we have more 1's, of course. But it will also have many zeros since the vocabulary is generally very large. Many words don't really occur in any document.\n4:52\nMany words will only occasionally occur in a document.\n4:58\nA lot of words will be absent in a particular document.\n5:04\nSo now we have placed the documents and the query in the vector space.\n5:11\nLet's look at how we measure the similarity.\n5:15\nSo, a commonly used similarity measure here is Dot Product.\n5:20\nThe Dot Product of two vectors is simply defined as the sum of the products of the corresponding elements of the two vectors. So, here we see that it's the product of x1 and y1. So, here. And then, x2 multiplied by y2. And then, finally, xn multiplied by yn. And then, we take a sum here.\n5:50\nSo that's a Dot Product. Now, we can represent this in a more general way using a sum here.\n5:58\nSo this is only one of the many different ways of measuring the similarity. So, now we see that we have defined the dimensions, we have defined the vectors, and we have also defined the similarity function. So now we finally have the simplest vector space model, which is based on the bit vector [INAUDIBLE] dot product similarity and bag of words [INAUDIBLE]. And the formula looks like this. So this is our formula. And that's actually a particular retrieval function, a ranking function right? Now we can finally implement this function using a program language, and then rank the documents for query. Now, at this point you should again pause the lecture to think about how we can interpreted this score. So, we have gone through the process of modeling the retrieval problem using a vector space model. And then, we make assumptions about how we place vectors in the vector space, and how do we define the similarity. So in the end, we've got a specific retrieval function shown here.\n7:15\nNow, the next step is to think about whether this retrieval function actually makes sense, right? Can we expect this function to actually perform well when we used it to rank documents for user's queries?\n7:28\nSo it's worth thinking about what is this value that we are calculating. So, in the end, we'll get a number. But what does this number mean? Is it meaningful?\n7:42\nSo, spend a couple minutes to sort of think about that.\n7:45\nAnd, of course, the general question here is do you believe this is a good ranking function? Would it actually work well? So, again, think about how to interpret this value. Is it actually meaningful?\n8:01\nDoes it mean something? This is related to how well the document matched the query.\n8:08\nSo, in order to assess whether this simplest vector space model actually works well, let's look at the example.\n8:17\nSo, here I show some sample documents and a sample query. The query is news about the presidential campaign. And we have five documents here. They cover different terms in the query.\n8:34\nAnd if you look at these documents for a moment, you may realize that\n8:41\nsome documents are probably relevant, and some others are probably not relevant.\n8:48\nNow, if I asked you to rank these documents, how would you rank them? This is basically our ideal ranking. When humans can examine the documents, and then try to rank them.\n9:03\nNow, so think for a moment, and take a look at this slide. And perhaps by pausing the lecture.\n9:12\nSo I think most of you would agree that d4 and d3 are probably better than others because they really cover the query well. They match news, presidential and campaign.\n9:27\nSo, it looks like these documents are probably better than the others. They should be ranked on top. And the other three d2, d1, and d5 are really not relevant. So we can also say d4 and d3 are relevant documents, and d1, d2 and d5 are non-relevant. So now let's see if our simplest vector space model could do the same, or could do something closer. So, let's first think about how we actually use this model to score documents. All right. Here I show two documents, d1 and d3. And we have the query also here. In the vector space model, of course we want to first compute the vectors for these documents and the query. Now, I showed the vocabulary here as well. So these are the end dimensions that we'll be thinking about. So what do you think is the vector for the query?\n10:27\nNote that we're assuming that we only use zero and 1 to indicate whether a term is absent or present in the query or in the document. So these are zero,1 bit vectors.\n10:43\nSo what do you think is the query vector?\n10:47\nWell, the query has four words here. So for these four words, there will be a 1. And for the rest, there will be zeros.\n10:57\nNow, what about the documents? It's the same. So d1 has two rows, news and about. So, there are two 1's here, and the rest are zeroes. Similarly, so now that we have the two vectors, let's compute the similarity.\n11:17\nAnd we're going to use Do Product. So you can see when we use Dot Product, we just multiply the corresponding elements, right? So these two will be formal product, and these two will generate another product, and these two will generate yet another product and so on, so forth.\n11:40\nNow you can easily see if we do that, we actually don't have to care about\n11:48\nthese zeroes because whenever we have a zero the product will be zero. So when we take a sum over all these pairs, then the zero entries will be gone.\n12:04\nAs long as you have one zero, then the product would be zero. So, in the fact, we're just counting how many pairs of 1 and 1. In this case, we have seen two, so the result will be 2. So what does that mean? Well, that means this number, or the value of this scoring function, is simply the count of how many unique query terms are matched in the document. Because if a term is matched in the document, then there will be two one's.\n12:41\nIf it's not, then there will be zero on the document side.\n12:46\nSimilarly, if the document has a term but the term is not in the query, there will be a zero in the query vector. So those don't count. So, as a result, this scoring function basically measures how many unique query terms are matched in a document. This is how we interpret this score.\n13:07\nNow, we can also take a look at d3. In this case, you can see the result is 3 because d3 matched to the three distinctive query words news, presidential campaign, whereas d1 only matched the two. Now in this case, this seems reasonable to rank d3 on top of d1.\n13:29\nAnd this simplest vector space model indeed does that. So that looks pretty good. However, if we examine this model in detail, we likely will find some problems. So, here I'm going to show all the scores for these five documents. And you can easily verify they're correct because we're basically counting the number of unique query terms matched in each document.\n13:56\nNow note that this measure actually makes sense, right? It basically means if a document matches more unique query terms, then the document will be assumed to be more relevant. And that seems to make sense. The only problem is here we can note that there are three documents, d2, d3 and d4. And they tied with a 3 as a score.\n14:25\nSo, that's a problem because if you look at them carefully, it seems that the d4 should be ranked above d3 because d3 only mentions the presidential once, but d4 mentioned it multiple times. In the case of d3, presidential could be an dimension. But d4 is clearly above the presidential campaign. Another problem is that d2 and d3 also have the same score. But if you look at the three words that are matched, in the case of d2, it matched the news, about and campaign. But in the case of d3, it matched news, presidential and campaign.\n15:12\nSo intuitively this reads better because matching presidential is more important than matching about, even though about and the presidential are both in the query.\n15:26\nSo intuitively, we would like d3 to be ranked above d2. But this model doesn't do that.\n15:33\nSo that means this model is still not good enough. We have to solve these problems.\n15:41\nTo summarize, in this lecture we talked about how to instantiate a vector space model.\n15:47\nWe mainly need to do three things. One is to define the dimension. The second is to decide how to place documents as vectors in the vector space, and to also place a query in the vector space as a vector.\n16:07\nAnd third is to define the similarity between two vectors, particularly the query vector and the document vector.\n16:17\nWe also talked about various simple way to instantiate the vector space model. Indeed, that's probably the simplest vector space model that we can derive. In this case, we use each word to define the dimension. We use a zero, 1 bit vector to represent a document or a query. In this case, we basically only care about word presence or absence. We ignore the frequency.\n16:45\nAnd we use the Dot Product as the similarity function.\n16:50\nAnd with such a instantiation, we showed that the scoring function is basically to score a document based on the number of distinct query words matched in the document.\n17:04\nWe also showed that such a simple vector space model still doesn't work well, and we need to improve it.\n17:12\nAnd this is a topic that we're going to cover in the next lecture. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/ungradedWidget/Kk0Rv/how-was-the-coursedict_values(['List\nCS 410: Text Information Systems\nWeek 16\nHow was the course?\nPrevious\nWeek 16 Activities\nEnd of Course Survey\nHow was the course?\n. Duration: 15 minutes\n15 min\nHow was the course?\nIframes are not supported in your browser.\nExpand\nMark as completed\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/supplement/BlYwq/week-7-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 7\nWeek 7 Overview\nPrevious\nNext\nWeek 7 Information\nReading:\nReading\nWeek 7 Overview\n. Duration: 10 minutes\n10 min\nWeek 7 Lessons\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\nWeek 7 Overview\nFrom Week 7 to Week 12, the lectures are based on the Text Mining and Analytics MOOC. During this week's lessons, you will receive an overview of natural language processing techniques and text representation, which are the foundation for all kinds of text-mining applications, and word association mining with a particular focus on mining one of the two basic forms of word associations (i.e., paradigmatic relations).\nTime\nThis module should take approximately 11 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with required assignments in bold):\nActivity\nEstimated Time Required\nWeek 7 Video Lectures\n2 hours\nWeek 7 Graded Quiz\n1 hour\nExam 1\n2 hours\nProgramming Assignment 3\n6 hours\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain some basic concepts in natural language processing.\nExplain different ways to represent text data.\nExplain the two basic types of word associations and how to mine paradigmatic relations from text data.\nGuiding Questions\nDevelop your answers to the following guiding questions while watching the video lectures throughout the week.\nWhat does a computer have to do in order to understand a natural language sentence? \nWhat is ambiguity? \nWhy is natural language processing (NLP) difficult for computers? \nWhat is bag-of-words representation? \nWhy is this word-based representation more robust than representations derived from syntactic and semantic analysis of text? \nWhat is a paradigmatic relation?\nWhat is a syntagmatic relation? \nWhat is the general idea for discovering paradigmatic relations from text?\nWhat is the general idea for discovering syntagmatic relations from text? \nWhy do we want to do Term Frequency Transformation when computing similarity of context? \nHow does BM25 Term Frequency transformation work? \nWhy do we want to do Inverse Document Frequency (IDF) weighting when computing similarity of context?  \nAdditional Readings and Resources\nThe following readings are optional:\nC. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM and Morgan & Claypool Publishers, 2016. Chapters 1-4, Chapter 13.\nChris Manning and Hinrich Schütze, Foundations of Statistical Natural Language Processing. MIT Press. Cambridge, MA: May 1999. Chapter 5 on collocations.\nChengxiang Zhai, Exploiting context to identify lexical atoms: A statistical view of linguistic context. Proceedings of the International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97), Rio de Janeiro, Brazil, Feb. 4-6, 1997, pp. 119-129.\nShan Jiang and ChengXiang Zhai, Random walks on adjacency graphs for mining lexical relations from big text data. Proceedings of IEEE BigData Conference 2014, pp. 549-554.\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nPart of speech tagging\nSyntactic analysis\nSemantic analysis\nAmbiguity\nText representation, especially bag-of-words representation\nContext of a word; context similarity\nParadigmatic relation\nSyntagmatic relation\nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/u8NLS/12-3-text-based-predictiondict_values(["List\nCS 410: Text Information Systems\nWeek 12\n12.3 Text-Based Prediction\nPrevious\nNext\nWeek 12 Information\nWeek 12 Lessons\nVideo:\nVideo\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.3 Text-Based Prediction\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n12.4 Contextual Text Mining: Motivation\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\n12.8 Summary for Exam 2\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n12.3 Text-Based Prediction\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the Text-Based Prediction. In this lecture, we're going to start talking about the mining a different kind of knowledge, as you can see here on this slide. Namely we're going to use text data to infer values of some other variables in the real world that may not be directly related to the text. Or only remotely related to text data. So this is very different from content analysis or topic mining where we directly characterize the content of text. It's also different from opinion mining or sentiment analysis, which still have to do is characterizing mostly the content. Only that we focus more on the subject of content which reflects what we know about the opinion holder.\n1:05\nBut this only provides limited review of what we can predict.\n1:10\nIn this lecture and the following lectures, we're going to talk more about how we can predict more Information about the world. How can we get the sophisticated patterns of text together with other kind of data?\n1:28\nIt would be useful first to take a look at the big picture of prediction, and data mining in general, and I call this data mining loop. So the picture that you are seeing right now is that there are multiple sensors, including human sensors, to report what we have seen in the real world in the form of data. Of course the data in the form of non-text data, and text data.\n1:51\nAnd our goal is to see if we can predict some values of important real world variables that matter to us. For example, someone's house condition, or the weather, or etc. And so these variables would be important because we might want to act on that. We might want to make decisions based on that. So how can we get from the data to these predicted values? Well in general we'll first have to do data mining and analysis of the data.\n2:23\nBecause we, in general, should treat all the data that we collected\n2:30\nin such a prediction problem set up. We are very much interested in joint mining of non-text and text data, which should combine all the data together.\n2:41\nAnd then, through analysis, generally there are multiple predictors of this interesting variable to us. And we call these features. And these features can then be put into a predictive model, to actually predict the value of any interesting variable.\n3:02\nSo this then allows us to change the world. And so this basically is the general process for making a prediction based on data, including the test data.\n3:17\nNow it's important to emphasize that a human actually plays a very important role in this process.\n3:24\nEspecially because of the involvement of text data. So human first would be involved in the mining of the data. It would control the generation of these features. And it would also help us understand the text data, because text data are created to be consumed by humans. Humans are the best in consuming or interpreting text data.\n3:48\nBut when there are, of course, a lot of text data then machines have to help and that's why we need to do text data mining.\n3:55\nSometimes machines can see patterns in a lot of data that humans may not see. But in general human would play an important role in analyzing some text data, or applications. Next, human also must be involved in predictive model building and adjusting or testing. So in particular, we will have a lot of domain knowledge about the problem of prediction that we can build into this predictive model. And then next, of course, when we have predictive values for the variables, then humans would be involved in taking actions to change a word or make decisions based on these particular values.\n4:36\nAnd finally it's interesting that a human could be involved in controlling the sensors.\n4:43\nAnd this is so that we can adjust to the sensors to collect the most useful data for prediction.\n4:52\nSo that's why I call this data mining loop. Because as we perturb the sensors, it'll collect the new data and more useful data then we will obtain more data for prediction. And this data generally will help us improve the predicting accuracy. And in this loop, humans will recognize what additional data will need to be collected. And machines, of course, help humans identify what data should be collected next. In general, we want to collect data that is most useful for learning. And there was actually a subarea in machine learning called active learning that has to do with this. How do you identify data\n5:32\npoints that would be most helpful in machine learning programs? If you can label them, right?\n5:38\nSo, in general, you can see there is a loop here from data acquisition to data analysis. Or data mining to prediction of values. And to take actions to change the word, and then observe what happens. And then you can then decide what additional data have to be collected by adjusting the sensors. Or from the prediction arrows, you can also note what additional data we need to acquire in order to improve the accuracy of prediction. And this big picture is actually very general and it's reflecting a lot of important applications of big data.\n6:16\nSo, it's useful to keep that in mind while we are looking at some text mining techniques.\n6:22\nSo from text mining perspective and we're interested in text based prediction. Of course, sometimes texts alone can make predictions. And this is most useful for prediction about human behavior or human preferences or opinions. But in general text data will be put together as non-text data. So the interesting questions here would be, first, how can we design effective predictors? And how do we generate such effective predictors from text?\n6:53\nAnd this question has been addressed to some extent in some previous lectures where we talked about what kind of features we can design for text data. And it has also been addressed to some extent by talking about the other knowledge that we can mine from text. So, for example, topic mining can be very useful to generate the patterns or topic based indicators or predictors that can be further fed into a predictive model. So topics can be intermediate recognition of text. That would allow us to do design high level features or predictors that are useful for prediction of some other variable. It may be also generated from original text data, it provides a much better implementation of the problem and it serves as more effective predictors.\n7:46\nAnd similarly similar analysis can lead to such predictors, as well. So, those other data mining or text mining algorithms can be used to generate predictors.\n7:58\nThe other question is, how can we join the mine text and non-text data together? Now, this is a question that we have not addressed yet. So, in this lecture, and in the following lectures, we're going to address this problem. Because this is where we can generate much more enriched features for prediction. And allows us to review a lot of interesting knowledge about the world. These patterns that are generated from text and non-text data themselves can sometimes, already be useful for prediction. But, when they are put together with many other predictors they can really help improving the prediction.\n8:39\nBasically, you can see text-based prediction can actually serve as a unified framework to combine many text mining and analysis techniques. Including topic mining and any content mining techniques or segment analysis.\n8:55\nThe goal here is mainly to evoke values of real-world variables. But in order to achieve the goal we can do some other preparations. And these are subtasks. So one subtask could mine the content of text data, like topic mining. And the other could be to mine knowledge about the observer. So sentiment analysis, opinion.\n9:21\nAnd both can help provide predictors for the prediction problem.\n9:27\nAnd of course we can also add non-text data directly to the predicted model, but then non-text data also helps provide a context for text analyst. And that further improves the topic mining and the opinion analysis. And such improvement often leads to more effective predictors for our problems. It would enlarge the space of patterns of opinions of topics that we can mine from text and that we'll discuss more later. So the joint analysis of text and non-text data can be actually understood from two perspectives.\n10:05\nOne perspective, we have non-text can help with testimony.\n10:11\nBecause non-text data can provide a context for mining text data provide a way to partition data in different ways. And this leads to a number of type of techniques for contextual types of mining. And that's the mine text in the context defined by non-text data. And you see this reference here, for a large body of work, in this direction. And I will need to highlight some of them, in the next lectures.\n10:39\nNow, the other perspective is text data can help with non-text data mining as well. And this is because text data can help interpret patterns discovered from non-text data. Let's say you discover some frequent patterns from non-text data. Now we can use the text data associated with instances where the pattern occurs as well as text data that is associated with instances where the pattern doesn't look up. And this gives us two sets of text data. And then we can see what's the difference. And this difference in text data is interpretable because text content is easy to digest. And that difference might suggest some meaning for this pattern that we found from non-text data. So, it helps interpret such patterns. And this technique is called pattern annotation.\n11:32\nAnd you can see this reference listed here for more detail.\n11:38\nSo here are the references that I just mentioned. The first is reference for pattern annotation. The second is, Qiaozhu Mei's dissertation on contextual text mining. It contains a large body of work on contextual text mining techniques.\n11:56\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/nE8nq/lesson-5-6-link-analysis-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.6: Link Analysis - Part 1\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.6: Link Analysis - Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKazakh\nKorean\nMongolian\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about link analysis for web search. In this lecture, we're going to talk about the web search and particularly, focusing on how to do link analysis and use the results to improve search. The main topic of this lecture is to look at the ranking algorithms for Web Search.\n0:32\nIn the previous lecture we talked about how to create index. Now that we have index, we want to see how we can improve ranking of Pages. The web. Now standard IR models, can be also applied here. In fact, they are important building blocks, for, improve, for supporting web search. But they aren't sufficient. And mainly for the following reasons. First, on the web, we tend to have very different information needs, for example, people might search for a webpage, or an entry page. And this is different from the traditional library search, where people are primarily interested in collecting literature Information. So this kind of query is often called a navigational queries. The purpose is to navigate into a particular type of the page. So for such queries we might benefit from using link information. Secondly, documents have additional information and on the web pages, are web format, there are a lot of other clues, such as the layout, the title, or link information again. So this has provided opportunity to use extra context information of the document to improve the scoring. And finally, information quality varies a lot. That means we have to consider many factors to improve the range in the algorithm. This would give us a more robust way to rank pages, making it harder for any spammer to just manipulate the one signal to improve the ranking of a page.\n2:10\nSo as a result, people have made a number of major extensions to the ranking algorithms.\n2:16\nOne line is to exploit links to improve scoring.\n2:23\nAnd that's the main topic of this lecture.\n2:26\nPeople have also proposed algorithms to exploit the loudest, they are implicit. Feedback information the form of click throughs and that's of course in the category of feedback techniques and machine all is often used there. In general in web search the ranking algorithms are based on machine learning algorithms to combine all kinds of features. Many of them are based on the standard of virtual models such as BM25 that we talked about [INAUDIBLE] to score different parts of documents or to provide additional features based on content matching, but link information is also very useful so they provide additional scoring signals. So let's look at links in more detail on the web. So this is a snapshot of some part of the web, let's say. So we can see there are many links that link the different pages together. And in this case, you can also look at the center here, there is a description of a link that's pointing to the document on the right side. Now, this description text is called anchor text.\n3:44\nNow if you think about this text, it's actually quite useful because it provides some extra description of that page be points with. So for example, if someone wants to bookmark Amazon.com front page the person might say the biggest online bookstore and then the link to Amazon, right? So, the description here after is very similar to what the user would type in the query box when they are looking for or such a page. And that's why it's very useful for managing pages. Suppose someone types in the query like online bookstore or biggest online bookstore. All right the query would match this anchor text in the page here. And then this actually provides evidence for matching the page that's being pointed to that is the Amazon. a entry page. So if you match anchor text that describes an anchor to a page, actually that provides good evidence for the elements of the page being pointed to. So anchor text is very useful. If you look at the bottom part of this picture you can also see there are some patterns of some links and these links might indicate the utility of a document. So for example, on the right side you'll see this page has received the many inlinks. Now that means many other pages are pointing to this page. This shows that this page is quite useful.\n5:21\nOn the left side you can see this is another page that points to many other pages. So this is a director page that would allow you to actually see a lot of other pages.\n5:32\nSo we can call the first case authority page and the second case half page, but this means the link information can help intuit. One is to provide extra text for matching. The other is to provide some additional scores for the webpage to characterize how likely a page is a hub, how likely a page is a authority.\n5:55\nSo people then of course and proposed ideas to leverage this link information. Google's PageRank which was the main technique that they used in early days is a good example and that is an algorithm to capture page and popularity, basically to score authority. So the intuitions here are links are just like citations in literature. Now think about one page pointing you to another page, this is very similar to one paper citing another paper. So, of course then, if a page is cited often, then we can assume this page to be more useful in general.\n6:35\nSo that's a very good intuition.\n6:38\nNow PageRank is essentially to take advantage of this Intuition to implement with the principal approach. Intuitively, it is essentially doing citation counting or in link counting. It just improves the simple idea in two ways. One it will consider indirect citations. So that means you don't just look at how many in links you have. You also look at what are those pages that are pointing to you. If those pages themselves have a lot of in-links, that means a lot. In some sense, you will get some credit from that. But if those pages that are pointing to you are not being pointed to by other pages they themselves don't have many in-links, then well, you don't get that much. So that's the idea of getting indirect citation. All right, so you can also understand this idea by looking at again the research papers. If you're cited by let's say ten papers, and those ten papers are just workshop papers or some papers that are not very influential, right?\n7:49\nSo although you've got ten in-links, and that's not as good as if you are cited by ten papers that themselves have attracted a lot of other citations.\n8:01\nAnd so in this case where we would like to consider indirect links and page does that. The other idea is it's good to pseudo citations.\n8:13\nAssume that basically every page is having a number zero pseudo citation count. Essentially you are trying to imagine there are many virtual links that will link all the pages together so that you actually get the pseudo citations from everyone.\n8:34\nThe reason why they want to do that. Is this will allow them to solve the problem elegantly with linear algebra technique. So, I think maybe the best way to understand the PageRank is to think of this as through computer probability of random surfer visiting every webpage. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/YSvkh/lesson-3-1-evaluation-of-tr-systemsdict_values(["List\nCS 410: Text Information Systems\nWeek 3\nLesson 3.1: Evaluation of TR Systems\nPrevious\nNext\nWeek 3 Information\nWeek 3 Lessons\nVideo:\nVideo\nLesson 3.1: Evaluation of TR Systems\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.2: Evaluation of TR Systems - Basic Measures\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.6: Evaluation of TR Systems - Practical Issues\n. Duration: 15 minutes\n15 min\nWeek 3 Activities\nProgramming Assignment 2.1\nLesson 3.1: Evaluation of TR Systems\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:07\nThis lecture is about Evaluation of Text Retrieval Systems In the previous lectures, we have talked about the a number of Text Retrieval Methods, different kinds of ranking functions.\n0:23\nBut how do we know which one works the best? In order to answer this question, we have to compare them and that means we have to evaluate these retrieval methods.\n0:34\nSo this is the main topic of this lecture.\n0:40\nFirst, lets think about why do we have to do evaluation? I already give one reason. That is, we have to use evaluation to figure out which retrieval method works better. Now this is very important for advancing our knowledge. Otherwise, we wouldn't know whether a new idea works better than an old idea. In the beginning of this course, we talked about the problem of text retrieval. We compare it with data base retrieval.\n1:08\nThere we mentioned that text retrieval is an empirically defined problem. So evaluation must rely on users. Which system works better, would have to be judged by our users.\n1:25\nSo, this becomes a very challenging problem because how can we get users involved in the evaluation? How can we do a fair comparison of different method?\n1:37\nSo just go back to the reasons for evaluation.\n1:41\nI listed two reasons here. The second reason, is basically what I just said, but there is also another reason which is to assess the actual utility of a Text Regional system. Imagine you're building your own such annual applications, it would be interesting knowing how well your search engine works for your users. So in this case, matches must reflect the utility to the actual users in real occasion. And typically, this has to be done by using user starters and using the real search engine.\n2:16\nIn the second case, or the second reason,\n2:19\nthe measures actually all need to collated with the utility to actually use this. Thus, they don't have to accurately reflect the exact utility to users.\n2:31\nSo the measure only needs to be good enough to tell which method works better.\n2:38\nAnd this is usually done through a test collection. And this is the main idea that we'll be talking about in this course. This has been very important for comparing different algorithms and for improving search engine system in general.\n2:58\nSo let's talk about what to measure. There are many aspects of searching that we can measure, we can evaluate. And here, I listed the three major aspects. One, is effectiveness or accuracy. How accurate are the search results? In this case, we're measuring a system's capability of ranking relevant documents on top of non relevant ones. The second, is efficiency. How quickly can you get the results? How much computing resources are needed to answer a query? In this case, we need to measure the space and time overhead of the system.\n3:32\nThe third aspect is usability. Basically the question is, how useful is a system for new user tasks. Here, obviously, interfaces and many other things also important and would typically have to do user studies.\n3:47\nNow in this course, we're going to talk mostly about effectiveness and accuracy measures. Because the efficiency and usability dimensions are not really unique to search engines. And so they are needed for without any other software systems. And there is also good coverage of such and other causes. But how to evaluate search engine's quality or accuracy is something unique to text retrieval and we're going to talk a lot about this. The main idea that people have proposed before using a test set to evaluate the text retrieval algorithm is called the Cranfield Evaluation Methodology. This one actually was developed a long time ago, developed in 1960s. It's a methodology for laboratory test of system components.\n4:45\nIts sampling methodology that has been very useful, not just for search engine evaluation. But also for evaluating virtually all kinds of empirical tasks, and for example in natural language processing or in other fields where the problem is empirical to find, we typically would need to use such a methodology. And today with the big data challenging with the use of machine learning everywhere. This methodology has been very popular, but it was first developed for a search engine application in the 1960s. So the basic idea of this approach is to build a reusable test collection and define measures.\n5:27\nOnce such a test collection is built, it can be used again and again to test different algorithms. And we're going to define measures that allow you to quantify performance of a system and algorithm.\n5:41\nSo how exactly will this work? Well we can do have a sample collection of documents and this is adjusted to simulate the real document collection in the search application. We're going to also have a sample set of queries, or topics. This is a little simulator that uses queries.\n5:56\nThen, we'll have to have those relevance judgments. These are judgments of which documents should be returned for which queries. Ideally, they have to be made by users who formulated the queries. Because those are the people that know exactly what documents would be used for. And finally, we have to have matches for quantify how well our system's result matches the ideal ranked list. That would be constructed base on user's relevance judgements. So this methodology is very useful for starting retrieval algorithms, because the test can be reused many times. And it will also provide a fair comparison for all the methods. We have the same criteria or same dataset to be used to compare different algorithms. This allows us to compare a new algorithm with an old algorithm that was divided many years ago, by using the same standard. So this is the illustration of this works, so as I said, we need our queries that are showing here. We have Q1, Q2 etc. We also need the documents and that's called the document caching and on the right side you will see we need relevance judgments. These are basically the binary judgments of documents with respect to a query. So for example, D1 is judged as being relevant to Q1, D2 is judged as being relevant as well, and D3 is judged as not relevant. And the Q1 etc. These will be created by users.\n7:34\nOnce we have these, and we basically have a test collection. And then if you have two systems, you want to compare them, then you can just run each system on these queries and the documents and each system would then return results. Let's say if the queries Q1 and then we would have the results here. Here I show R sub A as the results from system A. So this is, remember we talked about task of computing approximation of the relevant document set. R sub A is system A's approximation here.\n8:14\nAnd R sub B is system B's approximation of relevant documents.\n8:21\nNow, let's take a look at these results. So which is better? Now imagine if a user, which one would you like? Now let's take a look at the both results. And there are some differences and there are some documents that are returned by both systems. But if you look at the results, you will feel that maybe A is better in the sense that we don't have many number element documents. And among the three documents returned, the two of them are relevant. So that's good, it's precise. On the other hand one council say maybe B is better, because we've got all of them in the documents. We've got three instead of two. So which one is better and how do we quantify this?\n9:08\nWell, obviously this question highly depends on a user's task. It depends on users as well. You might even imagine for some users may be system A is better. If the user is not interested in getting all the random documents. Right, in this case the user doesn't have to read a million users will see most of the relevant documents. On the other hand, one can also imagine the user might need to have as many random documents as possible. For example, if you're doing a literature survey you might be in the sigma category, and you might find that system B is better. So in the case, we will have to also define measures that will quantify them. And we might need it to define multiple measures because users have different perspectives of looking at the results.\n9:58\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/dkntE/12-1-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 12\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\nPrevious\nNext\nWeek 12 Information\nWeek 12 Lessons\nVideo:\nVideo\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.3 Text-Based Prediction\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n12.4 Contextual Text Mining: Motivation\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\n12.8 Summary for Exam 2\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:01\n[MUSIC] This lecture is about the Latent Aspect Rating Analysis for Opinion Mining and Sentiment Analysis.\n0:14\nIn this lecture, we're going to continue discussing Opinion Mining and Sentiment Analysis.\n0:19\nIn particular, we're going to introduce Latent Aspect Rating Analysis which allows us to perform detailed analysis of reviews with overall ratings.\n0:34\nSo, first is motivation.\n0:37\nHere are two reviews that you often see in the net about the hotel. And you see some overall ratings. In this case, both reviewers have given five stars. And, of course, there are also reviews that are in text.\n0:53\nNow, if you just look at these reviews, it's not very clear whether the hotel is good for its location or for its service. It's also unclear why a reviewer liked this hotel.\n1:06\nWhat we want to do is to decompose this overall rating into ratings on different aspects such as value, rooms, location, and service.\n1:18\nSo, if we can decompose the overall ratings, the ratings on these different aspects, then, we can obtain a more detailed understanding of the reviewer's opinionsabout the hotel.\n1:30\nAnd this would also allow us to rank hotels along different dimensions such as value or rooms. But, in general, such detailed understanding will reveal more information about the user's preferences, reviewer's preferences. And also, we can understand better how the reviewers view this hotel from different perspectives. Now, not only do we want to infer these aspect ratings, we also want to infer the aspect weights. So, some reviewers may care more about values as opposed to the service. And that would be a case. like what's shown on the left for the weight distribution, where you can see a lot of weight is places on value.\n2:18\nBut others care more for service. And therefore, they might place more weight on service than value.\n2:25\nThe reason why this is also important is because, do you think about a five star on value, it might still be very expensive if the reviewer cares a lot about service, right? For this kind of service, this price is good, so the reviewer might give it a five star. But if a reviewer really cares about the value of the hotel, then the five star, most likely, would mean really cheap prices. So, in order to interpret the ratings on different aspects accurately, we also need to know these aspect weights. When they're combined together, we can have a more detailed understanding of the opinion. So the task here is to get these reviews and their overall ratings as input, and then, generate both the aspect ratings, the compose aspect ratings, and the aspect rates as output. And this is a problem called Latent Aspect Rating Analysis.\n3:31\nSo the task, in general, is given a set of review articles about the topic with overall ratings, and we hope to generate three things. One is the major aspects commented on in the reviews. Second is ratings on each aspect, such as value and room service.\n3:53\nAnd third is the relative weights placed on different aspects by the reviewers. And this task has a lot of applications, and if you can do this, and it will enable a lot of applications. I just listed some here. And later, I will show you some results. And, for example, we can do opinion based entity ranking. We can generate an aspect-level opinion summary. We can also analyze reviewers preferences, compare them or compare their preferences on different hotels. And we can do personalized recommendations of products.\n4:29\nSo, of course, the question is how can we solve this problem? Now, as in other cases of these advanced topics, we won’t have time to really cover the technique in detail. But I’m going to give you a brisk, basic introduction to the technique development for this problem. So, first step, we’re going to talk about how to solve the problem in two stages. Later, we’re going to also mention that we can do this in the unified model. Now, take this review with the overall rating as input. What we want to do is, first, we're going to segment the aspects. So we're going to pick out what words are talking about location, and what words are talking about room condition, etc.\n5:13\nSo with this, we would be able to obtain aspect segments. In particular, we're going to obtain the counts of all the words in each segment, and this is denoted by C sub I of W and D. Now this can be done by using seed words like location and room or price to retrieve the [INAUDIBLE] in the segments. And then, from those segments, we can further mine correlated words with these seed words and that would allow us to segmented the text into segments, discussing different aspects. But, of course, later, as we will see, we can also use [INAUDIBLE] models to do the segmentation. But anyway, that's the first stage, where the obtain the council of words in each segment. In the second stage, which is called Latent Rating Regression, we're going to use these words and their frequencies in different aspects to predict the overall rate. And this predicting happens in two stages.\n6:17\nIn the first stage, we're going to use the [INAUDIBLE] and the weights of these words in each aspect to predict the aspect rating. So, for example, if in your discussion of location, you see a word like, amazing, mentioned many times, and it has a high weight. For example, here, 3.9. Then, it will increase the Aspect Rating for location. But, another word like, far, which is an acted weight, if it's mentioned many times, and it will decrease the rating. So the aspect ratings, assume that it will be a weighted combination of these word frequencies where the weights are the sentiment weights of the words. Of course, these sentimental weights might be different for different aspects. So we have, for each aspect, a set of term sentiment weights as shown here. And that's in order by beta sub I and W.\n7:18\nIn the second stage or second step, we're going to assume that the overall rating is simply a weighted combination of these aspect ratings. So we're going to assume we have aspect weights to the [INAUDIBLE] sub i of d, and this will be used to take a weighted average of the aspect ratings, which are denoted by r sub i of d.\n7:42\nAnd we're going to assume the overall rating is simply a weighted average of these aspect ratings. So this set up allows us to predict the overall rating based on the observable frequencies. So on the left side, you will see all these observed information, the r sub d and the count.\n8:03\nBut on the right side, you see all the information in that range is actually latent.\n8:09\nSo, we hope to discover that. Now, this is a typical case of a generating model where would embed the interesting variables in the generated model. And then, we're going to set up a generation probability for the overall rating given the observed words. And then, of course, we can adjust these parameter values including betas Rs and alpha Is in order to maximize the probability of the data. In this case, the conditional probability of the observed rating given the document. So we have seen such cases before in, for example, PISA, where we predict a text data. But here, we're predicting the rating, and the parameters, of course, are very different. But we can see, if we can uncover these parameters, it would be nice, because r sub i of d is precise as the ratings that we want to get. And these are the composer ratings on different aspects. [INAUDIBLE] sub I D is precisely the aspect weights that we hope to get as a byproduct, that we also get the beta factor, and these are the [INAUDIBLE] factor, the sentiment weights of words.\n9:31\nSo more formally,\n9:33\nthe data we are modeling here is a set of review documents with overall ratings. And each review document denote by a d, and the overall ratings denote by r sub d. And d pre-segments turn into k aspect segments. And we're going to use ci(w,d) to denote the count of word w in aspect segment i. Of course, it's zero if the word doesn't occur in the segment.\n10:01\nNow, the model is going to predict the rating based on d. So, we're interested in the provisional problem of r sub-d given d. And this model is set up as follows. So r sub-d is assumed the two follow a normal distribution doesn't mean that denotes actually await the average of the aspect of ratings r Sub I of d as shown here. This normal distribution is a variance of data squared. Now, of course, this is just our assumption. The actual rating is not necessarily anything thing this way. But as always, when we make this assumption, we have a formal way to model the problem and that allows us to compute the interest in quantities. In this case, the aspect ratings and the aspect weights.\n10:52\nNow, the aspect rating as you see on the [INAUDIBLE] is assuming that will be a weight of sum of these weights. Where the weight is just the [INAUDIBLE] of the weight.\n11:04\nSo as I said, the overall rating is assumed to be a weighted average of aspect ratings.\n11:15\nNow, these other values, r for sub I of D, or denoted together by other vector that depends on D is that the token of specific weights. And we’re going to assume that this vector itself is drawn from another Multivariate Gaussian distribution, with mean denoted by a Mu factor, and covariance metrics sigma here.\n11:43\nNow, so this means, when we generate our overall rating, we're going to first draw\n11:49\na set of other values from this Multivariate Gaussian Prior distribution. And once we get these other values, we're going to use then the weighted average of aspect ratings as the mean here to use the normal distribution to generate the overall rating.\n12:13\nNow, the aspect rating, as I just said, is the sum of the sentiment weights of words in aspect, note that here the sentiment weights are specific to aspect. So, beta is indexed by i, and that's for aspect. And that gives us a way to model different segment of a word.\n12:36\nThis is neither because the same word might have positive sentiment for another aspect. It's also used for see what parameters we have here beta sub i and w gives us the aspect-specific sentiment of w. So, obviously, that's one of the important parameters. But, in general, we can see we have these parameters, beta values, the delta, and the Mu, and sigma.\n13:12\nSo, next, the question is, how can we estimate these parameters and, so we collectively denote all the parameters by lambda here. Now, we can, as usual, use the maximum likelihood estimate, and this will give us the settings of these parameters, that with a maximized observed ratings condition of their respective reviews. And of, course, this would then give us all the useful variables that we are interested in computing.\n13:45\nSo, more specifically, we can now, once we estimate the parameters, we can easily compute the aspect rating, for aspect the i or sub i of d. And that's simply to take all of the words that occurred in the segment, i, and then take their counts and then multiply that by the center of the weight of each word and take a sum. So, of course, this time would be zero for words that are not occurring in and that's why were going to take the sum of all the words in the vocabulary.\n14:17\nNow what about the s factor weights? Alpha sub i of d, well, it's not part of our parameter. Right? So we have to use that to compute it. And in this case, we can use the Maximum a Posteriori to compute this alpha value. Basically, we're going to maximize the product of the prior of alpha according to our assumed Multivariate Gaussian Distribution and the likelihood. In this case, the likelihood rate is the probability of generating this observed overall rating given this particular alpha value and some other parameters, as you see here. So for more details about this model, you can read this paper cited here.\n15:05\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/lCSNo/8-9-probabilistic-topic-models-overview-of-statistical-language-models-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC] So now let's talk about the problem a little bit more, and specifically let's talk about the two different ways of estimating the parameters. One is called the Maximum Likelihood estimate that I already just mentioned. The other is Bayesian estimation. So in maximum likelihood estimation, we define best as meaning the data likelihood has reached the maximum. So formally it's given by this expression here, where we define the estimate as a arg max of the probability of x given theta.\n0:46\nSo, arg max here just means its actually a function that will turn. The argument that gives the function maximum value, adds the value. So the value of arg max is not the value of this function. But rather, the argument that has made it the function reaches maximum. So in this case the value of arg max is theta. It's the theta that makes the probability of X, given theta, reach it's maximum. So this estimate that in due it also makes sense and it's often very useful, and it seeks the premise that best explains the data. But it has a problem, when the data is too small because when the data points are too small, there are very few data points. The sample is small, then if we trust data in entirely and try to fit the data and then we'll be biased. So in the case of text data, let's say, all observed 100 words did not contain another word related to text mining. Now, our maximum likelihood estimator will give that word a zero probability. Because giving the non-zero probability would take away probability mass from some observer word. Which obviously is not optimal in terms of maximizing the likelihood of the observer data.\n2:11\nBut this zero probability for all the unseen words may not be reasonable sometimes. Especially, if we want the distribution to characterize the topic of text mining. So one way to address this problem is actually to use Bayesian estimation, where we actually would look at the both the data, and our prior knowledge about the parameters. We assume that we have some prior belief about the parameters. Now in this case of course, so we are not\n2:47\ngoing to look at just the data, but also look at the prior.\n2:54\nSo the prior here is defined by P of theta, and this means, we will impose some preference on certain theta's of others.\n3:06\nAnd by using Bayes Rule, that I have shown here,\n3:12\nwe can then combine the likelihood function. With the prior to give us this\n3:23\nposterior probability of the parameter. Now, a full explanation of Bayes rule, and some of these things related to Bayesian reasoning, would be outside the scope of this course. But I just gave a brief introduction because this is general knowledge that might be useful to you. The Bayes Rule is basically defined here, and allows us to write down one conditional probability of X given Y in terms of the conditional probability of Y given X. And you can see the two probabilities are different in the order of the two variables.\n4:09\nBut often the rule is used for making inferences of the variable, so let's take a look at it again. We can assume that p(X) Encodes our prior belief about X. That means before we observe any other data, that's our belief about X, what we believe some X values have higher probability than others.\n4:40\nAnd this probability of X given Y is a conditional probability, and this is our posterior belief about X. Because this is our belief about X values after we have observed the Y. Given that we have observed the Y, now what do we believe about X? Now, do we believe some values have higher probabilities than others?\n5:09\nNow the two probabilities are related through this one, this can be regarded as the probability of\n5:19\nthe observed evidence Y, given a particular X. So you can think about X as our hypothesis, and we have some prior belief about which hypothesis to choose. And after we have observed Y, we will update our belief and this updating formula is based on the combination of our prior.\n5:48\nAnd the likelihood of observing this Y if X is indeed true,\n5:57\nso much for detour about Bayes Rule. In our case, what we are interested in is inferring the theta values. So, we have a prior here that includes our prior knowledge about the parameters.\n6:15\nAnd then we have the data likelihood here, that would tell us which parameter value can explain the data well. The posterior probability combines both of them,\n6:30\nso it represents a compromise of the the two preferences. And in such a case, we can maximize this posterior probability. To find this theta that would maximize this posterior probability, and this estimator is called a Maximum a Posteriori, or MAP estimate.\n6:55\nAnd this estimator is a more general estimator than the maximum likelihood estimator. Because if we define our prior as a noninformative prior, meaning that it's uniform over all the theta values. No preference, then we basically would go back to the maximum likelihood estimated. Because in such a case, it's mainly going to be determined by this likelihood value, the same as here.\n7:28\nBut if we have some not informative prior, some bias towards the different values then map estimator can allow us to incorporate that. But the problem here of course, is how to define the prior.\n7:44\nThere is no free lunch and if you want to solve the problem with more knowledge, we have to have that knowledge. And that knowledge, ideally, should be reliable. Otherwise, your estimate may not necessarily be more accurate than that maximum likelihood estimate.\n8:01\nSo, now let's look at the Bayesian estimation in more detail.\n8:08\nSo, I show the theta values as just a one dimension value and that's a simplification of course. And so, we're interested in which variable of theta is optimal. So now, first we have the Prior. The Prior tells us that some of the variables are more likely the others would believe. For example, these values are more likely than the values over here, or here, or other places.\n8:42\nSo this is our Prior, and then we have our theta likelihood. And in this case, the theta also tells us which values of theta are more likely. And that just means loose syllables can best expand our theta.\n9:01\nAnd then when we combine the two we get the posterior distribution, and that's just a compromise of the two. It would say that it's somewhere in-between. So, we can now look at some interesting point that is made of. This point represents the mode of prior, that means the most likely parameter value according to our prior, before we observe any data.\n9:25\nThis point is the maximum likelihood estimator, it represents the theta that gives the theta of maximum probability.\n9:32\nNow this point is interesting, it's the posterior mode.\n9:38\nIt's the most likely value of the theta given by the posterior of this. And it represents a good compromise of the prior mode and the maximum likelihood estimate.\n9:51\nNow in general in Bayesian inference, we are interested in the distribution of all these parameter additives as you see here. If there's a distribution over see how values that you can see. Here, P of theta given X.\n10:09\nSo the problem of Bayesian inference is\n10:14\nto infer this posterior, this regime, and also to infer other interesting quantities that might depend on theta. So, I show f of theta here as an interesting variable that we want to compute. But in order to compute this value, we need to know the value of theta. In Bayesian inference, we treat theta as an uncertain variable. So we think about all the possible variables of theta. Therefore, we can estimate the value of this function f as extracted value of f, according to the posterior distribution of theta, given the observed evidence X.\n10:58\nAs a special case, we can assume f of theta is just equal to theta. In this case, we get the expected value of the theta, that's basically the posterior mean. That gives us also one point of theta, and it's sometimes the same as posterior mode, but it's not always the same. So, it gives us another way to estimate the parameter.\n11:24\nSo, this is a general illustration of Bayesian estimation and its an influence. And later, you will see this can be useful for topic mining where we want to inject the sum prior knowledge about the topics. So to summarize, we've used the language model which is basically probability distribution over text. It's also called a generative model for text data. The simplest language model is Unigram Language Model, it's basically a word distribution.\n11:54\nWe introduced the concept of likelihood function, which is the probability of the a data given some model.\n12:02\nAnd this function is very important,\n12:05\ngiven a particular set of parameter values this function can tell us which X, which data point has a higher likelihood, higher probability.\n12:16\nGiven a data sample X, we can use this function to determine which parameter values would maximize the probability of the observed data, and this is the maximum livelihood estimate.\n12:31\nWe also talk about the Bayesian estimation or inference. In this case we, must define a prior on the parameters p of theta. And then we're interested in computing the posterior distribution of the parameters, which is proportional to the prior and the likelihood.\n12:48\nAnd this distribution would allow us then to infer any derive that is from theta. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/naLsv/9-5-probabilistic-topic-models-expectation-maximization-algorithm-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 9\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] So this is indeed a general idea of the Expectation-Maximization, or EM, Algorithm.\n0:14\nSo in all the EM algorithms we introduce a hidden variable to help us solve the problem more easily. In our case the hidden variable is a binary variable for each occurrence of a word. And this binary variable would indicate whether the word has been generated from 0 sub d or 0 sub p. And here we show some possible values of these variables. For example, for the it's from background, the z value is one. And text on the other hand. Is from the topic then it's zero for z, etc.\n0:53\nNow, of course, we don't observe these z values, we just imagine they're all such. Values of z attaching to other words.\n1:02\nAnd that's why we call these hidden variables.\n1:06\nNow, the idea that we talked about before for predicting the word distribution that has been used when we generate the word is it a predictor, the value of this hidden variable? And, so, the EM algorithm then, would work as follows. First, we'll initialize all the parameters with random values. In our case, the parameters are mainly the probability. of a word, given by theta sub d. So this is an initial addition stage. These initialized values would allow us to use base roll to take a guess of these z values, so we'd guess these values. We can't say for sure whether textt is from background or not. But we can have our guess. This is given by this formula. It's called an E-step. And so the algorithm would then try to use the E-step to guess these z values. After that, it would then invoke another that's called M-step. In this step we simply take advantage of the inferred z values and then just group words that are in the same distribution like these from that ground including this as well.\n2:27\nWe can then normalize the count to estimate the probabilities or to revise our estimate of the parameters.\n2:36\nSo let me also illustrate that we can group the words that are believed to have come from zero sub d, and that's text, mining algorithm, for example, and clustering.\n2:51\nAnd we group them together to help us re-estimate the parameters that we're interested in. So these will help us estimate these parameters.\n3:06\nNote that before we just set these parameter values randomly. But with this guess, we will have somewhat improved estimate of this. Of course, we don't know exactly whether it's zero or one. So we're not going to really do the split in a hard way. But rather we're going to do a softer split. And this is what happened here.\n3:29\nSo we're going to adjust the count by the probability that would believe this word has been generated by using the theta sub d.\n3:39\nAnd you can see this, where does this come from? Well, this has come from here, right? From the E-step. So the EM Algorithm would iteratively improve uur initial estimate of parameters by using E-step first and then M-step. The E-step is to augment the data with additional information, like z. And the M-step is to take advantage of the additional information to separate the data. To split the data accounts and then collect the right data accounts to re-estimate our parameter. And then once we have a new generation of parameter, we're going to repeat this. We are going the E-step again. To improve our estimate of the hidden variables. And then that would lead to another generation of re-estimated parameters.\n4:34\nFor the word distribution that we are interested in.\n4:39\nOkay, so, as I said, the bridge between the two is really the variable z, hidden variable, which indicates how likely this water is from the top water distribution, theta sub p.\n4:56\nSo, this slide has a lot of content and you may need to. Pause the reader to digest it. But this basically captures the essence of EM Algorithm. Start with initial values that are often random themself. And then we invoke E-step followed by M-step to get an improved setting of parameters. And then we repeated this, so this a Hill-Climbing algorithm that would gradually improve the estimate of parameters. As I will explain later there is some guarantee for reaching a local maximum of the log-likelihood function. So lets take a look at the computation for a specific case, so these formulas are the EM. Formulas that you see before, and you can also see there are superscripts, here, like here, n, to indicate the generation of parameters. Like here for example we have n plus one. That means we have improved. From here to here we have an improvement. So in this setting we have assumed the two numerals have equal probabilities and the background model is null. So what are the relevance of the statistics? Well these are the word counts. So assume we have just four words, and their counts are like this. And this is our background model that assigns high probabilities to common words like the.\n6:25\nAnd in the first iteration, you can picture what will happen. Well first we initialize all the values. So here, this probability that we're interested in is normalized into a uniform distribution of all the words.\n6:40\nAnd then the E-step would give us a guess of the distribution that has been used. That will generate each word. We can see we have different probabilities for different words. Why? Well, that's because these words have different probabilities in the background. So even though the two distributions are equally likely. And then our initial audition say uniform distribution because of the difference in the background of the distribution, we have different guess the probability. So these words are believed to be more likely from the topic.\n7:15\nThese on the other hand are less likely. Probably from background.\n7:20\nSo once we have these z values, we know in the M-step these probabilities will be used to adjust the counts. So four must be multiplied by this 0.33 in order to get the allocated accounts toward the topic.\n7:39\nAnd this is done by this multiplication. Note that if our guess says this is  If this is one point zero,\n7:52\nthen we just get the full count of this word for this topic. In general it's not going to be one point zero. So we're just going to get some percentage of this counts toward this topic. Then we simply normalize these counts to have a new generation of parameters estimate. So you can see, compare this with the older one, which is here.\n8:18\nSo compare this with this one and we'll see the probability is different. Not only that, we also see some words that are believed to have come from the topic will have a higher probability. Like this one, text.\n8:32\nAnd of course, this new generation of parameters would allow us to further adjust the inferred latent variable or hidden variable values. So we have a new generation of values, because of the E-step based on the new generation of parameters. And these new inferred values of Zs will give us then another generation of the estimate of probabilities of the word. And so on and so forth so this is what would actually happen when we compute these probabilities using the EM Algorithm. As you can see in the last row where we show the log-likelihood, and the likelihood is increasing as we do the iteration. And note that these log-likelihood is negative because the probability is between 0 and 1 when you take a logarithm, it becomes a negative value. Now what's also interesting is, you'll note the last column. And these are the inverted word split. And these are the probabilities that a word is believed to have come from one distribution, in this case the topical distribution, all right. And you might wonder whether this would be also useful. Because our main goal is to estimate these word distributions. So this is our primary goal. We hope to have a more discriminative order of distribution. But the last column is also bi-product. This also can actually be very useful. You can think about that. We want to use, is to for example is to estimate to what extent this document has covered background words. And this, when we add this up or take the average we will kind of know to what extent it has covered background versus content was that are not explained well by the background. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/PgzsP/lesson-2-5-system-implementation-inverted-index-constructiondict_values(["List\nCS 410: Text Information Systems\nWeek 2\nLesson 2.5: System Implementation - Inverted Index Construction\nPrevious\nNext\nWeek 2 Information\nWeek 2 Lessons\nVideo:\nVideo\nLesson 2.1: Vector Space Model - Improved Instantiation\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\nLesson 2.2: TF Transformation\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 2.3: Doc Length Normalization\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.4: Implementation of TR Systems\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 2.5: System Implementation - Inverted Index Construction\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.6: System Implementation - Fast Search\n. Duration: 17 minutes\n17 min\nWeek 2 Activities\nProgramming Assignment 1\nLesson 2.5: System Implementation - Inverted Index Construction\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:07\nThis lecture is about the inverted index construction.\n0:13\nIn this lecture, we will continue the discussion of system implementation. In particular, we're going to discuss how to construct the inverted index.\n0:25\nThe construction of the inverted index is actually very easy if the dataset is very small. It's very easy to construct a dictionary and then store the postings in a file.\n0:36\nThe problem is that when our data is not able to fit to the memory then we have to use some special method to deal with it.\n0:46\nAnd unfortunately, in most retrieval applications the dataset will be large. And they generally cannot be loaded into memory at once.\n0:56\nAnd there are many approaches to solve that problem, and sorting-based method is quite common and works in four steps as shown here. First, you collect the local termID, documentID and frequency tuples. Basically you will locate the terms in a small set of documents. And then once you collect those accounts you can sort those count based on terms. So that you will be able to local a partial inverted index and these are called rounds. And then you write them into a temporary file on the disk and then you merge in step 3. Do pairwise merging of these runs, until you eventually merge all the runs and generate a single inverted index.\n1:47\nSo this is an illustration of this method. On the left you see some documents and on the right we have a term lexicon and a document ID lexicon. These lexicons are to map string-based representations of document IDs or terms into integer representations or map back from integers to the stream representation. The reason why we want our interest using integers to present these IDs is because integers are often easier to handle. For example, integers can be used as index for array, and they are also easy to compress.\n2:34\nSo this is one reason why we tend to map these strings into integers,\n2:42\nso that we don't have to carry these strings around. So how does this approach work? Well, it's very simple. We're going to scan these documents sequentially and then parse the documents and count the frequencies of terms. And in this stage we generally sort the frequencies by document IDs, because we process each document sequentially. So we'll first encounter all the terms in the first document. Therefore the document IDs are all ones in this case. And this will be followed by document IDs two and they are natural results in this only just because we process the data in a sequential order. At some point, we will run out of memory and that would have to write them into the disc. Before we do that we 're going to sort them, just use whatever memory we have. We can sort them and then this time we're going to sort based on term IDs. Note that here, we're using the term IDs as a key to sort. So all the entries that share the same term would be grouped together. In this case, we can see all the IDs of documents that match term 1 would be grouped together. And we're going to write this into that this is a temporary file. And would that allows you to use the memory to process and makes a batch of documents. And we're going to do that for all the documents. So we're going to write a lot of temporary files into the disc. And then the next stage is we do merge sort basically. We're going to merge them and then sort them. Eventually, we will get a single inverted index, where the entries are sorted based on term IDs.\n4:46\nAnd on the top, we're going to see these are the older entries for the documents that match term ID 1. So this is basically, how we can do the construction of inverted index. Even though the data cannot be all loaded into the manner. Now, we mention earlier that because of hostings are very large, it's desirable to compress them. So let's now take a little bit how we compressed inverted index. Well the idea of compression in general, is for leverage skewed distributions of values. And we generally have to use variable-length encoding, instead of the fixed-length encoding as we use by default in a program manager like C++. And so how can we leverage the skewed distributions of values to compress these values? Well in general, we will use few bits to encode those frequent words at the cost of using longer bit string code those rare values. So in our case, let's think about how we can compress the TF, tone frequency.\n6:05\nNow, if you can picture what the inverted index look like, and you will see in post things, there are a lot of tone frequencies. Those are the frequencies of terms in all those documents. Now, if you think about it, what kind of values are most frequent there? You probably will be able to guess that small numbers tend to occur far more frequently than large numbers. Why? Well, think about the distribution of words and this is to do the sip of slopes, and many words occur just rarely so we see a lot of small numbers. Therefore, we can use fewer bits for the small, but highly frequent integers and that's cost of using more bits for larger integers.\n6:58\nThis is a trade off of course. If the values are distributed to uniform, then this won't save us any space, but because we tend to see many small values, they are very frequent. We can save on average even though sometimes when we see a large number we have to use a lot of bits.\n7:19\nWhat about the document IDs that we also saw in postings? Well they are not distributed in the skewed way. So how can we deal with that? Well it turns out that we can use a trick called a d-gap and that is to store the difference of these term IDs. And we can imagine if a term has matched that many documents then there will be longest of document IDs. So when we take the gap, and we take the difference between adjacent document IDs, those gaps will be small. So again, see a lot of small numbers. Whereas if a term occurred in only a few documents, then the gap would be large, the large numbers would not be frequent. So this creates some skewed distribution, that would allow us to compress these values.\n8:11\nThis is also possible because in order to uncover or uncompress these document IDs, we have to sequentially process the data. Because we stored the difference and in order to recover the exact document ID we have to first recover the previous document ID. And then we can add the difference to the previous document ID to restore the current document ID. Now this was possible because we only needed to have sequential access to those document IDs. Once we look up the term, we look up all the document IDs that match the term, then we sequentially process them. So it's very natural, that's why this trick actually works.\n8:53\nAnd there are many different methods for encoding. So binary code is a commonly used code in just any program language. We use basically fixed glance in coding. Unary code, gamma code, and delta code are all possibilities and there are many other possibilities. So let's look at some of them in more detail. Binary coding is really equal length coding, and that's a property for randomly distributed values. The unary coding is a variable length in coding method. In this case, integer this 1 will be encoded as x -1, 1 bit followed by 0. So for example, 3 will be encoded as 2, 1s followed by 0, whereas 5 will be encoded as 4, 1s, followed by 0, etc. So now you can imagine how many bits do we have to use for a large number like 100? So how many bits do you have to use exactly for a number like 100? Well exactly, we have to use 100 bits. So it's the same number of bits as the value of this number. So this is very inefficient if you were likely to see some large numbers. Imagine if you occasionally see a number like 1,000, you have to use 1,000 bits. So this only works well if you are absolutely sure that there will be no large numbers, mostly very often you see very small numbers. Now, how do you decode this code? Now since these are variable length encoding methods, you can't just count how many bits and then just stop.\n10:38\nYou can't say 8-bits or 32-bits, then you will start another code. They are variable length, so you will have to rely on some mechanism. In this case for unary, you can see it's very easy to see the boundary. Now you can easily see 0 would signal the end of encoding. So you just count up how many 1s you have seen and at the end you hit 0. You have finished one number, you will start another number.\n11:07\nNow we just saw that unary coding is too aggressive. In rewarding small numbers, and if you occasionally can see a very big number, it would be a disaster. So what about some other less aggressive method? Well gamma coding's one of them and in this method we can use unary coding for a transform form of that. So it's 1 plus the floor of log of x. So the magnitude of this value is much lower than the original x. So that's why we can afford using unary code for that. And so first I have the unary code for coding this log of x. And this would be followed by a uniform code or binary code. And this basically the same uniform code, and binary code are the same. And we're going to use this coder to code the remaining part of the value of x. And this is basically precisely x-1 to the floor of log of x\n12:25\nSo the unary code are basically called the flow of log of x, well add one there and here. But the remaining part we'll be using uniform code through actually code the difference between the x and this 2 to the log of x.\n12:49\nAnd it's easy to show that for this\n12:55\ndifference we only need to use up to this many bits and the floor of log of x bits.\n13:06\nAnd this is easy to understand, if the difference is too large, then we would have a higher floor of log of x.\n13:14\nSo here are some examples for example, 3 is is encoded as 101. The first two digits are the unary code. So this isn't for the value 2, 10 encodes 2 in unary coding.\n13:32\nAnd so that means the floor of log of x is 1, because we won't actually use unary codes. In code 1 plus the flow of log of x, since this is two then we know that the flow of log of x is actually 1.\n13:52\nSo that 3 is still larger than 2 to the 1. So the difference is 1, and the 1 is encoded here at the end.\n14:01\nSo that's why we have 101 for 3. Now similarly 5 is encoded as 110, followed by 01.\n14:12\nAnd in this case the unary code in code 3. And so this is a unary code 110 and so the flow of log of x is 2. And that means we're going to compute a difference between 5 and the 2 to the 2 and that's 1. And so we now have again 1 at the end. But this time we're going to use 2 bits, because with this level of flow of log of x. We could have more numbers a 5, 6, 7 they would all share the same prefix here, 110. So in order to differentiate them, we have to use 2 bits in the end to differentiate them. So you can imagine 6 would be 10 here in the end instead of 01 after 10.\n15:04\nIt's also true that the form of a gamma code is always the first odd number of bits, and in the center there is a 0. That's the end of the unary code.\n15:18\nAnd before that or on the left side of this 0, there will be all 1s. And on the right side of this 0, it's binary coding or uniform coding.\n15:32\nSo how can you decode such code? Well you again first do unary coding. Once you hit 0, you have got the unary code and this also tell you how many bits you have to read further to decode the uniform code. So this is how you can decode a gamma code. There is also a delta code that's basically the same as a gamma code except that you replace the unary prefix with the gamma code. So that's even less conservative than gamma code in terms of wording the small integers. So that means, it's okay if you occasionally see a large number.\n16:14\nIt's okay with delta code.\n16:16\nIt's also fine with the gamma code, it's really a big loss for unary code. And they are all operating of course, at different degrees of favoring short or favoring small integers. And that also means they would be appropriate for a sorting distribution. But none of them is perfect for all distributions. And which method works the best would have to depend on the actual distribution in your dataset. For inverted index compression, people have found that gamma coding seems to work well.\n16:55\nSo how to uncompress inverted index? I will just talk about this. Firstly, you decode those encoded integers. And we just I think discussed the how we decode unary coding and gamma coding. What about the document IDs that might be compressed using d-gap? Well, we're going to do sequential decoding so supposed the encoded I list is x1, x2, x3 etc. We first decode x1 to obtain the first document ID, ID1. Then we can decode x2, which is actually the difference between the second ID and the first one. So we have to add the decoder value of x2 to ID1 to recover the value of the ID at this secondary position.\n17:46\nSo this is where you can see the advantages of converting document IDs to integers. And that allows us to do this kind of compression. And we just repeat until we decode all the documents. Every time we use the document ID in the previous position to help to recover the document ID in the next position.\n18:08\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/W0NZe/lesson-2-2-tf-transformationdict_values(["List\nCS 410: Text Information Systems\nWeek 2\nLesson 2.2: TF Transformation\nPrevious\nNext\nWeek 2 Information\nWeek 2 Lessons\nVideo:\nVideo\nLesson 2.1: Vector Space Model - Improved Instantiation\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\nLesson 2.2: TF Transformation\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 2.3: Doc Length Normalization\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.4: Implementation of TR Systems\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 2.5: System Implementation - Inverted Index Construction\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.6: System Implementation - Fast Search\n. Duration: 17 minutes\n17 min\nWeek 2 Activities\nProgramming Assignment 1\nLesson 2.2: TF Transformation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRomanian\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:10\nIn this lecture, we continue the discussion of vector space model. In particular, we're going to talk about the TF transformation. In the previous lecture, we have derived a TF idea of weighting formula using the vector space model.\n0:27\nAnd we have assumed that this model actually works pretty well for these examples as shown on this slide, except for d5, which has received a very high score. Indeed, it has received the highest score among all these documents. But this document is intuitive and non-relevant, so this is not desirable.\n0:53\nIn this lecture, we're going to talk about, how we're going to use TF transformation to solve this problem.\n1:00\nBefore we discuss the details, let's take a look at the formula for this simple TF-IDF weighting ranking function. And see why this document has received such a high score. So this is the formula, and if you look at the formula carefully, then you will see it involves a sum over all the matched query terms.\n1:23\nAnd inside the sum, each matched query term has a particular weight. And this weight is TF-IDF weighting.\n1:31\nSo it has an idea of component, where we see two variables. One is the total number of documents in the collection, and that is M. The other is the document of frequency. This is the number of documents that are contained. This word w. The other variables involved in the formula include the count of the query term.\n2:01\nW in the query, and the count of the word in the document.\n2:07\nIf you look at this document again, now it's not hard to realize that the reason why it hasn't received a high score is because it has a very high count of campaign. So the count of campaign in this document is a 4, which is much higher than the other documents, and has contributed to the high score of this document. So in treating the amount to lower the score for this document, we need to somehow restrict the contribution of the matching of this term in the document. And if you think about the matching of terms in the document carefully, you actually would realize, we probably shouldn't reward multiple occurrences so generously. And by that I mean, the first occurrence of a term says a lot about the matching of this term, because it goes from zero count to a count of one. And that increase means a lot.\n3:17\nOnce we see a word in the document, it's very likely that the document is talking about this word. If we see a extra occurrence on top of the first occurrence, that is to go from one to two, then we also can say that, well the second occurrence kind of confirmed that it's not a accidental managing of the word. Now we are more sure that this document is talking about this word. But imagine we have seen, let's say, 50 times of the word in the document. Now, adding one extra occurrence is not going to test more about the evidence, because we're already sure that this document is about this word.\n4:01\nSo if you're thinking this way, it seems that we should restrict the contribution of a high count of a term, and that is the idea of TF Transformation. So this transformation function is going to turn the real count of word into a term frequency weight for the word in the document. So here I show in x axis that we'll count, and y axis I show the term frequency weight.\n4:33\nSo in the previous breaking functions, we actually have imprison rate use some kind of transformation. So for example, in the 0/1 bit vector recantation,\n4:44\nwe actually use such a transformation function, as shown here. Basically if the count is 0, then it has 0 weight, otherwise it would have a weight of 1. It's flat.\n4:59\nNow, what about using term count as TF weight? Well, that's a linear function, so it has just exactly the same weight as the count.\n5:11\nNow we have just seen that this is not desirable.\n5:18\nSo what we want is something like this. So for example, with an algorithm function, we can't have a sublinear transformation that looks like this. And this will control the influence of really high weight, because it's going to lower its inference. Yet, it will retain the inference of small counts.\n5:36\nOr we might want to even bend the curve more by applying logarithm twice.\n5:42\nNow people have tried all these methods. And they are indeed working better than the linear form of the transformation.\n5:50\nBut so far, what works the best seems to be this special transformation, called a BM25 transformation.\n5:58\nBM stands for best matching.\n6:01\nNow in this transformation, you can see there's a parameter k here.\n6:06\nAnd this k controls the upper bound of this function. It's easy to see this function has a upper bound, because if you look at the x divided by x + k, where k is a non-active number, then the numerator will never be able to exceed the denominator, right? So it's upper bounded by k+1. Now, this is also difference between this transformation function and a logarithm transformation.\n6:37\nWhich it doesn't have upper bound.\n6:39\nFurthermore, one interesting property of this function is that, as we vary k,\n6:45\nwe can actually simulate different transformation functions. Including the two extremes that are shown here. That is, the 0/1 bit transformation and the linear transformation. So for example, if we set k to 0, now you can see\n7:03\nthe function value will be 1. So we precisely recover the 0/1 bit transformation.\n7:15\nIf you set k to very large number on the other hand, it's going to look more like the linear transformation function.\n7:24\nSo in this sense, this transformation is very flexible. It allows us to control the shape of the transformation. It also has a nice property of the upper bound.\n7:38\nAnd this upper bound is useful to control the inference of a particular term.\n7:43\nAnd so that we can prevent a spammer from just increasing the count of one term to spam all queries that might match this term.\n7:57\nIn other words, this upper bound might also ensure that all terms would be counted when we aggregate the weights to compute the score.\n8:06\nAs I said, this transformation function has worked well so far.\n8:12\nSo to summarize this lecture, the main point is that we need to do Sublinear TF Transformation, and this is needed to capture the intuition of diminishing return from higher term counts.\n8:26\nIt's also to avoid the dominance by one single term over all others. This BM25 transformation that we talked about is very interesting. It's so far one of the best-performing TF Transformation formulas. It has upper bound, and so it's also robust and effective.\n8:47\nNow if we're plugging this function into our TF-IDF weighting vector space model. Then we'd end up having the following ranking function, which has a BM25 TF component.\n9:01\nNow, this is already very close to a state of the odd ranking function called BM25. And we'll discuss how we can further improve this formula in the next lecture. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/gxNMo/lesson-4-7-smoothing-methods-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 4\nLesson 4.7: Smoothing Methods - Part 2\nPrevious\nNext\nWeek 4 Information\nWeek 4 Lessons\nVideo:\nVideo\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.2: Statistical Language Model\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 4.3: Query Likelihood Retrieval Function\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.4: Statistical Language Model - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.5: Statistical Language Model - Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.6: Smoothing Methods - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.7: Smoothing Methods - Part 2\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nProgramming Assignment 2.2\nLesson 4.7: Smoothing Methods - Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:13\nSo let's plug in these model masses into the ranking function to see what we will get, okay? This is a general smoothing. So a general ranking function for smoothing with subtraction and you have seen this before.\n0:28\nAnd now we have a very specific smoothing method, the JM smoothing method.\n0:33\nSo now let's see what what's a value for office of D here.\n0:40\nAnd what's the value for p sub c here? Right, so we may need to decide this in order to figure out the exact form of the ranking function. And we also need to figure out of course alpha. So let's see. Well this ratio is basically this, right, so, here, this is the probability of c board on the top, and this is the probability of unseen war or, in other words basically 11 times basically the alpha here, this, so it's easy to see that. This can be then rewritten as this. Very simple. So we can plug this into here.\n1:28\nAnd then here, what's the value for alpha? What do you think? So it would be just lambda, right?\n1:38\nAnd what would happen if we plug in this value here, if this is lambda. What can we say about this?\n1:47\nDoes it depend on the document?\n1:50\nNo, so it can be ignored.\n1:53\nRight? So we'll end up having this ranking function shown here.\n2:00\nAnd in this case you can easy to see, this a precisely a vector space model because this part is a sum over all the matched query terms, this is an element of the query map. What do you think is a element of the document up there?\n2:18\nWell it's this, right. So that's our document left element. And let's further examine what's inside of this logarithm.\n2:30\nWell one plus this. So it's going to be nonnegative, this log of this, it's going to be at least 1, right?\n2:39\nAnd these, this is a parameter, so lambda is parameter. And let's look at this. Now this is a TF. Now we see very clearly this TF weighting here.\n2:49\nAnd the larger the count is, the higher the weighting will be. We also see IDF weighting, which is given by this.\n2:58\nAnd we see docking the lan's relationship here. So all these heuristics are captured in this formula.\n3:04\nWhat's interesting that we kind of have got this weighting function automatically by making various assumptions. Whereas in the vector space model, we had to go through those heuristic design in order to get this. And in this case note that there's a specific form. And when you see whether this form actually makes sense.\n3:26\nAll right so what do you think is the denominator here, hm? This is a math of document. Total number of words, multiplied by the probability of the word\n3:38\ngiven by the collection, right? So this actually can be interpreted as expected account over word. If we're going to draw, a word, from the connection that we model. And, we're going to draw as many as the number of words in the document.\n3:59\nIf you do that, the expected account of a word, w, would be precisely given by this denominator.\n4:08\nSo, this ratio basically, is comparing the actual count, here.\n4:15\nThe actual count of the word in the document with expected count given by this product if the word is in fact following the distribution in the clutch this. And if this counter is larger than the expected counter in this part, this ratio would be larger than one.\n4:37\nSo that's actually a very interesting interpretation, right? It's very natural and intuitive, it makes a lot of sense.\n4:45\nAnd this is one advantage of using this kind of probabilistic reasoning where we have made explicit assumptions. And, we know precisely why we have a logarithm here. And, why we have these probabilities here.\n5:00\nAnd, we also have a formula that intuitively makes a lot of sense and does TF-IDF weighting and documenting and some others.\n5:09\nLet's look at the, the Dirichlet Prior Smoothing. It's very similar to the case of JM smoothing. In this case, the smoothing parameter is mu and that's different from lambda that we saw before. But the format looks very similar. The form of the function looks very similar.\n5:34\nSo we still have linear operation here.\n5:38\nAnd when we compute this ratio, one will find that is that the ratio is equal to this.\n5:46\nAnd what's interesting here is that we are doing another comparison here now. We're comparing the actual count. Which is the expected account of the world if we sampled meal worlds according to the collection world probability. So note that it's interesting we don't even see docking the lens here and lighter in the JMs model. All right so this of course should be plugged into this part.\n6:15\nSo you might wonder, so where is docking lens. Interestingly the docking lens is here in alpha sub d so this would be plugged into this part. As a result what we get is the following function here and this is again a sum over all the match query words.\n6:36\nAnd we're against the queer, the query, time frequency here.\n6:41\nAnd you can interpret this as the element of a document vector, but this is no longer a single dot product, right?\n6:50\nBecause we have this part, I know that n is the name of the query, right? So that just means if we score this function, we have to take a sum over all the query words, and then do some adjustment of the score based on the document.\n7:11\nBut it's still, it's still clear that it does documents lens modulation because this lens is in the denominator so a longer document will have a lower weight here. And we can also see it has tf here and now idf. Only that this time the form of the formula is different from the previous one in JMs one. But intuitively it still implements TFIDF waiting and document lens rendition again, the form of the function is dictated by the probabilistic reasoning and assumptions that we have made. Now there are also disadvantages of this approach. And that is, there's no guarantee that there's such a form of the formula will actually work well. So if we look about at this geo function, all those TF-IDF waiting and document lens rendition for example it's unclear whether we have sub-linear transformation. Unfortunately we can see here there is a logarithm function here. So we do have also the, so it's here right? So we do have the sublinear transformation, but we do not intentionally do that. That means there's no guarantee that we will end up in this, in this way. Suppose we don't have logarithm, then there's no sub-linear transformation. As we discussed before, perhaps the formula is not going to work so well. So that's an example of the gap between a formal model like this and the relevance that we have to model, which is really a subject motion that is tied to users.\n8:50\nSo it doesn't mean we cannot fix this. For example, imagine if we did not have this logarithm, right? So we can take a risk and we're going to add one, or we can even add double logarithm. But then, it would mean that the function is no longer a proper risk model. So the consequence of the modification is no longer as predictable as what we have been doing now.\n9:15\nSo, that's also why, for example, PM45 remains very competitive and still, open channel how to use public risk models as they arrive, better model than the PM25.\n9:30\nIn particular how do we use query like how to derive a model and that would work consistently better than DM 25. Currently we still cannot do that.\n9:40\nStill interesting open question.\n9:43\nSo to summarize this part, we've talked about the two smoothing methods. Jelinek-Mercer which is doing the fixed coefficient linear interpolation. Dirichlet Prior this is what add a pseudo counts to every word and is doing adaptive interpolation in that the coefficient would be larger for shorter documents.\n10:05\nIn most cases we can see, by using these smoothing methods, we will be able to reach a retrieval function where the assumptions are clearly articulate. So they are less heuristic.\n10:19\nExplaining the results also show that these, retrieval functions. Also are very effective and they are comparable to BM 25 or pm lens adultation. So this is a major advantage of probably smaller where we don't have to do a lot of heuristic design.\n10:40\nYet in the end that we naturally implemented TF-IDF weighting and doc length normalization.\n10:46\nEach of these functions also has precise ones smoothing parameter. In this case of course we still need to set this smoothing parameter. There are also methods that can be used to estimate these parameters.\n10:59\nSo overall, this shows by using a probabilistic model, we follow very different strategies then the vector space model. Yet, in the end, we end up uh,with some retrievable functions that look very similar to the vector space model. With some advantages in having assumptions clearly stated. And then, the form dictated by a probabilistic model. Now, this also concludes our discussion of the query likelihood probabilistic model. And let's recall what assumptions we have made in order to derive the functions that we have seen in this lecture. Well we basically have made four assumptions that I listed here. The first assumption is that the relevance can be modeled by the query likelihood.\n11:49\nAnd the second assumption with med is, are query words are generated independently that allows us to decompose the probability of the whole query into a product of probabilities of old words in the query.\n12:03\nAnd then, the third assumption that we have made is, if a word is not seen, the document or in the late, its probability proportional to its probability in the collection. That's a smoothing with a collection ama model. And finally, we made one of these two assumptions about the smoothing. So we either used JM smoothing or Dirichlet prior smoothing. If we make these four assumptions then we have no choice but to take the form of the retrieval function that we have seen earlier. Fortunately the function has a nice property in that it implements TF-IDF weighting and document machine and these functions also work very well. So in that sense, these functions are less heuristic compared with the vector space model.\n12:50\nAnd there are many extensions of this, this basic model and you can find the discussion of them in the reference at the end of this lecture.\n13:04\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/12dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 12\nWeek 12\n1h 59m of videos left\n10 min of readings left\n1 graded assignment left\nDuring this module, you will learn about techniques for joint mining of text and non-text data, including contextual text mining techniques for analyzing topics in text in association with various context information such as time, location, authors, and sources of data. You will also see a summary of the Text Mining content.\nWeek 12 Information\nWeek 12 Overview\nReading•\n. Duration: 10 minutes\n10 min\nResume\n. Click to resume\nWeek 12 Lessons\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\nVideo•\n. Duration: 15 minutes\n15 min\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\nVideo•\n. Duration: 14 minutes\n14 min\n12.3 Text-Based Prediction\nVideo•\n. Duration: 12 minutes\n12 min\n12.4 Contextual Text Mining: Motivation\nVideo•\n. Duration: 6 minutes\n6 min\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\nVideo•\n. Duration: 17 minutes\n17 min\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\nVideo•\n. Duration: 14 minutes\n14 min\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\nVideo•\n. Duration: 19 minutes\n19 min\n12.8 Summary for Exam 2\nVideo•\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n1 graded assignment left\nWeek 12 Practice Quiz\nPractice Quiz•8 questions\nWeek 12 Quiz\nGraded\nQuiz•7 questions\n•Grade: \nMP4\nDue, Nov 21, 12:59 AM EST\nProgramming Assignment•\n. Duration: 3 hours\n3h\n•Grade: --'])
https://www.coursera.org/learn/cs-410/lecture/ZAjmz/8-2-syntagmatic-relation-discovery-conditional-entropydict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the syntagmatic relation discovery and conditional entropy. In this lecture, we're going to continue the discussion of word association mining and analysis.\n0:18\nWe're going to talk about the conditional entropy, which is useful for discovering syntagmatic relations. Earlier, we talked about using entropy to capture how easy it is to predict the presence or absence of a word.\n0:34\nNow, we'll address a different scenario where we assume that we know something about the text segment. So now the question is, suppose we know that eats occurred in the segment. How would that help us predict the presence or absence of water, like in meat? And in particular, we want to know whether the presence of eats has helped us predict the presence of meat.\n1:02\nAnd if we frame this using entrophy, that would mean we are interested in knowing whether knowing the presence of eats could reduce uncertainty about the meats. Or, reduce the entrophy of the random variable corresponding to the presence or absence of meat. We can also ask as a question, what if we know of the absents of eats?\n1:28\nWould that also help us predict the presence or absence of meat?\n1:34\nThese questions can be addressed by using another concept called a conditioning entropy. So to explain this concept, let's first look at the scenario we had before, when we know nothing about the segment. So we have these probabilities indicating whether a word like meat occurs, or it doesn't occur in the segment. And we have an entropy function that looks like what you see on the slide.\n2:03\nNow suppose we know eats is present, so now we know the value of another random variable that denotes eats.\n2:12\nNow, that would change all these probabilities to conditional probabilities. Where we look at the presence or absence of meat,\n2:21\ngiven that we know eats occurred in the context. So as a result, if we replace these probabilities with their corresponding conditional probabilities in the entropy function, we'll get the conditional entropy.\n2:37\nSo this equation now here would be the conditional entropy. Conditional on the presence of eats.\n2:52\nSo, you can see this is essentially the same entropy function as you have seen before, except that all the probabilities now have a condition.\n3:04\nAnd this then tells us the entropy of meat, after we have known eats occurring in the segment.\n3:14\nAnd of course, we can also define this conditional entropy for the scenario where we don't see eats. So if we know it did not occur in the segment, then this entry condition of entropy would capture the instances of meat in that condition. So now, putting different scenarios together, we have the completed definition of conditional entropy as follows.\n3:39\nBasically, we're going to consider both scenarios of the value of eats zero, one, and this gives us a probability that eats is equal to zero or one. Basically, whether eats is present or absent. And this of course, is the conditional entropy of meat in that particular scenario.\n4:05\nSo if you expanded this entropy, then you have the following equation.\n4:15\nWhere you see the involvement of those conditional probabilities.\n4:21\nNow in general, for any discrete random variables x and y, we have\n4:27\nthe conditional entropy is no larger than the entropy of the variable x. So basically, this is upper bound for the conditional entropy. That means by knowing more information about the segment, we want to be able to increase uncertainty. We can only reduce uncertainty. And that intuitively makes sense because as we know more information, it should always help us make the prediction. And cannot hurt the prediction in any case.\n5:05\nNow, what's interesting here is also to think about what's the minimum possible value of this conditional entropy? Now, we know that the maximum value is the entropy of X.\n5:17\nBut what about the minimum, so what do you think?\n5:22\nI hope you can reach the conclusion that the minimum possible value, would be zero. And it will be interesting to think about under what situation will achieve this.\n5:34\nSo, let's see how we can use conditional entropy to capture syntagmatic relation.\n5:39\nNow of course, this conditional entropy gives us directly one way to measure the association of two words. Because it tells us to what extent, we can predict the one word given that we know the presence or absence of another word. Now before we look at the intuition of conditional entropy in capturing syntagmatic relations, it's useful to think of a very special case, listed here. That is, the conditional entropy of the word given itself.\n6:19\nSo here, we listed this conditional entropy in the middle. So, it's here.\n6:33\nSo, what is the value of this?\n6:36\nNow, this means we know where the meat occurs in the sentence. And we hope to predict whether the meat occurs in the sentence. And of course, this is 0 because there's no incident anymore. Once we know whether the word occurs in the segment, we'll already know the answer of the prediction. So this is zero. And that's also when this conditional entropy reaches the minimum.\n7:06\nSo now, let's look at some other cases.\n7:09\nSo this is a case of knowing the and trying to predict the meat. And this is a case of knowing eats and trying to predict the meat. Which one do you think is smaller? No doubt smaller entropy means easier for prediction.\n7:31\nWhich one do you think is higher? Which one is not smaller?\n7:36\nWell, if you at the uncertainty, then in the first case, the doesn't really tell us much about the meat. So knowing the occurrence of the doesn't really help us reduce entropy that much. So it stays fairly close to the original entropy of meat. Whereas in the case of eats, eats is related to meat. So knowing presence of eats or absence of eats, would help us predict whether meat occurs. So it can help us reduce entropy of meat. So we should expect the sigma term, namely this one, to have a smaller entropy.\n8:21\nAnd that means there is a stronger association between meat and eats.\n8:29\nSo we now also know when this w is the same as this meat, then the conditional entropy would reach its minimum, which is 0. And for what kind of words would either reach its maximum? Well, that's when this stuff is not really related to meat. And like the for example, it would be very close to the maximum, which is the entropy of meat itself.\n8:59\nSo this suggests that when you use conditional entropy for mining syntagmatic relations, the hours would look as follows.\n9:10\nFor each word W1, we're going to enumerate the overall other words W2. And then, we can compute the conditional entropy of W1 given W2.\n9:22\nWe thought all the candidate was in ascending order of the conditional entropy because we're out of favor, a world that has a small entropy. Meaning that it helps us predict the time of the word W1. And then, we're going to take the top ring of the candidate words as words that have potential syntagmatic relations with W1.\n9:41\nNote that we need to use a threshold to find these words. The stresser can be the number of top candidates take, or absolute value for the conditional entropy.\n9:55\nNow, this would allow us to mine the most strongly correlated words with a particular word, W1 here.\n10:06\nBut, this algorithm does not help us mine the strongest that K syntagmatical relations from an entire collection. Because in order to do that, we have to ensure that these conditional entropies are comparable across different words. In this case of discovering the mathematical relations for a targeted word like W1, we only need to compare the conditional entropies\n10:34\nfor W1, given different words. And in this case, they are comparable.\n10:41\nAll right. So, the conditional entropy of W1, given W2, and the conditional entropy of W1, given W3 are comparable.\n10:51\nThey all measure how hard it is to predict the W1. But, if we think about the two pairs, where we share W2 in the same condition, and we try to predict the W1 and W3. Then, the conditional entropies are actually not comparable. You can think of about this question. Why? So why are they not comfortable? Well, that was because they have a different outer bounds. Right? So those outer bounds are precisely the entropy of W1 and the entropy of W3. And they have different upper bounds. So we cannot really compare them in this way. So how do we address this problem?\n11:38\nWell later, we'll discuss, we can use mutual information to solve this problem. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/8d6Wn/8-4-syntagmatic-relation-discovery-mutual-information-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:06\nIn general, we can use the empirical count of events in the observed data to estimate the probabilities. And a commonly used technique is called a maximum likelihood estimate, where we simply normalize the observe accounts. So if we do that, we can see, we can compute these probabilities as follows. For estimating the probability that we see a water current in a segment, we simply normalize the count of segments that contain this word. So let's first take a look at the data here. On the right side, you see a list of some, hypothesizes the data. These are segments. And in some segments you see both words occur, they are indicated as ones for both columns. In some other cases only one will occur, so only that column has one and the other column has zero. And in all, of course, in some other cases none of the words occur, so they are both zeros. And for estimating these probabilities, we simply need to collect the three counts.\n1:20\nSo the three counts are first, the count of W1. And that's the total number of segments that contain word W1. It's just as the ones in the column of W1. We can count how many ones we have seen there. The segment count is for word 2, and we just count the ones in the second column. And these will give us the total number of segments that contain W2. The third count is when both words occur. So this time, we're going to count the sentence where both columns have ones.\n1:56\nAnd then, so this would give us the total number of segments where we have seen both W1 and W2. Once we have these counts, we can just normalize these counts by N, which is the total number of segments, and this will give us the probabilities that we need to compute original information. Now, there is a small problem, when we have zero counts sometimes. And in this case, we don't want a zero probability because our data may be a small sample and in general, we would believe that it's potentially possible for a [INAUDIBLE] to avoid any context. So, to address this problem, we can use a technique called smoothing. And that's basically to add some small constant to these counts, and so that we don't get the zero probability in any case. Now, the best way to understand smoothing is imagine that we actually observed more data than we actually have, because we'll pretend we observed some pseudo-segments. I illustrated on the top, on the right side on the slide. And these pseudo-segments would contribute additional counts of these words so that no event will have zero probability. Now, in particular we introduce the four pseudo-segments. Each is weighted at one quarter. And these represent the four different combinations of occurrences of this word. So now each event, each combination will have at least one count or at least a non-zero count from this pseudo-segment. So, in the actual segments that we'll observe, it's okay if we haven't observed all of the combinations. So more specifically, you can see the 0.5 here after it comes from the two ones in the two pseudo-segments, because each is weighted at one quarter. We add them up, we get 0.5. And similar to this, 0.05 comes from one single pseudo-segment that indicates the two words occur together.\n4:09\nAnd of course in the denominator we add the total number of pseudo-segments that we add, in this case, we added a four pseudo-segments. Each is weighed at one quarter so the total of the sum is, after the one. So, that's why in the denominator you'll see a one there.\n4:25\nSo, this basically concludes the discussion of how to compute a these four syntagmatic relation discoveries.\n4:36\nNow, so to summarize, syntagmatic relation can generally be discovered by measuring correlations between occurrences of two words. We've introduced the three concepts from information theory. Entropy, which measures the uncertainty of a random variable X. Conditional entropy, which measures the entropy of X given we know Y. And mutual information of X and Y, which matches the entropy reduction of X due to knowing Y, or entropy reduction of Y due to knowing X. They are the same. So these three concepts are actually very useful for other applications as well. That's why we spent some time to explain this in detail. But in particular, they are also very useful for discovering syntagmatic relations. In particular, mutual information is a principal way for discovering such a relation. It allows us to have values computed on different pairs of words that are comparable and so we can rank these pairs and discover the strongest syntagmatic from a collection of documents. Now, note that there is some relation between syntagmatic relation discovery and [INAUDIBLE] relation discovery. So we already discussed the possibility of using BM25 to achieve waiting for terms in the context to potentially also suggest the candidates that have syntagmatic relations with the candidate word. But here, once we use mutual information to discover syntagmatic relations, we can also represent the context with this mutual information as weights. So this would give us another way to represent the context of a word, like a cat. And if we do the same for all the words, then we can cluster these words or compare the similarity between these words based on their context similarity. So this provides yet another way to do term weighting for paradigmatic relation discovery. And so to summarize this whole part about word association mining. We introduce two basic associations, called a paradigmatic and a syntagmatic relations. These are fairly general, they apply to any items in any language, so the units don't have to be words, they can be phrases or entities.\n7:11\nWe introduced multiple statistical approaches for discovering them, mainly showing that pure statistical approaches are visible, are variable for discovering both kind of relations. And they can be combined to perform joint analysis, as well. These approaches can be applied to any text with no human effort, mostly because they are based on counting of words, yet they can actually discover interesting relations of words.\n7:44\nWe can also use different ways with defining context and segment, and this would lead us to some interesting variations of applications. For example, the context can be very narrow like a few words, around a word, or a sentence, or maybe paragraphs, as using differing contexts would allows to discover different flavors of paradigmatical relations. And similarly, counting co-occurrences using let's say, visual information to discover syntagmatical relations. We also have to define the segment, and the segment can be defined as a narrow text window or a longer text article. And this would give us different kinds of associations. These discovery associations can support many other applications, in both information retrieval and text and data mining. So here are some recommended readings, if you want to know more about the topic. The first is a book with a chapter on collocations, which is quite relevant to the topic of these lectures. The second is an article about using various statistical measures to discover lexical atoms. Those are phrases that are non-compositional. For example, hot dog is not really a dog that's hot,\n9:08\nblue chip is not a chip that's blue. And the paper has a discussion about some techniques for discovering such phrases.\n9:17\nThe third one is a new paper on a unified way to discover both paradigmatical relations and a syntagmatical relations, using random works on word graphs. [SOUND]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/kgKI9/11-1-text-categorization-discriminative-classifier-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 11\n11.1 Text Categorization: Discriminative Classifier Part 1\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nVideo:\nVideo\n11.1 Text Categorization: Discriminative Classifier Part 1\n. Duration: 20 minutes\n20 min\nVideo:\nVideo\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\n. Duration: 31 minutes\n31 min\nVideo:\nVideo\n11.3 Text Categorization: Evaluation Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n11.4 Text Categorization: Evaluation Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n11.5 Opinion Mining and Sentiment Analysis: Motivation\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\n11.1 Text Categorization: Discriminative Classifier Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the discriminative classifiers for text categorization.\n0:13\nIn this lecture we're going to continue talking about how to do text categorization and cover discriminative approaches. This is a slide that you have seen from the discussion of Naive Bayes Classifier, where we have shown that although Naive Bayes Classifier tries to model the generation of text data, from each categories, we can actually use Bayes' rule to eventually rewrite the scoring function as you see on this slide. And this scoring function is basically a weighted combination of a lot of word features, where the feature values are word counts, and the feature weights are the log of probability ratios of the word given by two distributions here.\n0:57\nNow this kind of scoring function can be actually a general scoring function where we can in general present text data as a feature vector. Of course the features don't have to be all the words. Their features can be other signals that we want to use. And we mentioned that this is precisely similar to logistic regression. So, in this lecture we're going to introduce some discriminative classifiers. They try to model the conditional distribution of labels given the data directly rather than using Bayes' rule to compute that interactively as we have seen in naive Bayes. So the general idea of logistic regression is to model the dependency of a binary response variable Y on some predictors that are denoted as X. So here we have also changed the notation to X for future values.\n2:07\nYou may recall in the previous slides we have used FI to represent the future values.\n2:13\nAnd here we use the notation of X factor, which is more common when we introduce such discriminative algorithms. So, X is our input. It's a vector with n features and each feature has a value x sub i here. And I will go with a model that dependency of this binary response variable of these features. So in our categorization problem when I have two categories theta 1 and theta 2, and we can use the Y value to denote the two categories when Y is 1, it means the category of the document, the first class, is theta 1. Now, the goal here is the model, the conditional property of Y given X directly as opposed to model of the generation of X and Y as in the case of Naive Bayes. And another advantage of this kind of approach is that it would allow many other features than words to be used in this vector since we're not modeling the generation of this vector. And we can plug in any signals that we want. So this is potentially advantageous for doing text categorization. So more specifically, in logistic regression, assume the functional form of Y depending on X is the following. And this is very closely related to the log odds that I introduced in the Naive Bayes or log of probability ratio of the two categories that you have seen on the previous slide.\n3:57\nSo this is what I meant. So in the case of Naive Bayes, we compute this by using those words and eventually we have reached a formula that looks like this.\n4:12\nBut here we actually would assume explicitly that we with the model our probability of Y given X\n4:29\ndirectly as a function of these features.\n4:37\nSo, most specifically we assume that the ratio of the probability of Y equals 1 and the probability of Y equals 0 is a function of X.\n4:54\nAll right, so it's a function of x and it's a linear combination of these feature values controlled by theta values.\n5:02\nAnd it seems we know that the probability of Y equals zero is one minus probability of Y equals one and this can be also written in this way. So this is a log out ratio here.\n5:22\nAnd so in logistic regression, we're basically assuming that the probability of Y equals 1. Okay my X is dependent on this linear combination of all these features. So it's just one of the many possible ways, assuming that the dependency. But this particular form has been quite useful and it also has some nice properties.\n5:47\nSo if we rewrite this equation to actually express the probability of Y given X. In terms of X by getting rid of the logarithm we get this functional form, and this is called a logistical function. It's a transformation of X into Y, as you see\n6:08\non the right side here, so that the X's will be map into a range of values from 0 to 1.0, you can see. And that's precisely what we want since we have a probability here.\n6:24\nAnd the function form looks like this.\n6:28\nSo this is the basic idea of logistic regression. And it's a very useful classifier that can be used to do a lot of classification tasks including text categorization.\n6:41\nSo as in all cases of model we would be interested in estimating the parameters. And in fact in all of the machine running programs, once you set up with the model, set up object and function to model the file, then the next step is to compute the parameter values. In general, we're going to adjust to these parameter values. Optimize the performance of classify on the training data. So in our case just assume we have the training data here, xi and yi, and each pair is basically a future vector of x and a known label for that x. Y is either 1 or 0. So in our case we are interested maximize this conditional likelihood.\n7:31\nThe conditional likelihood here is basically to model why given observe the x, so it's not like a moderate x, but rather we're going to model this. Note that this is a conditional probability of Y given X and this is also precisely what we wanted For classification. Now so the likelihood function would be just a product of all the training cases. And in each case, this is the model of the probability of observing this particular training case. So given a particular Xi, how likely we are to observe the corresponding Yi? Of course, Yi could be 1 or 0, and in fact, the function found here would vary depending on whether Yi is 1 or 0. If it's a 1, we'll be taking this form. And that's basically the logistic regression function. But what about this, if it's 0? Well, if it's 0, then we have to use a different form, and that's this one.\n8:48\nNow, how do we get this one? Well, that's just a 1 minus the probability of Y=1, right?\n8:55\nAnd you can easily see this. Now the key point in here is that the function form here depends on the observer Yi, if it's a 1, it has a different form than when it's 0. And if you think about when we want to maximize this probability, we're basically going to want this probability to be as high as possible. When the label is 1, that means the document is in probability 1. But if the document is not, we're going to maximize this value, and what's going to happen is actually to make this value as small as possible because this sum's 1. When I maximize this one, it's equivalent to minimize this one.\n9:48\nSo you can see basically, if we maximize the conditional likelihood, we're going to basically try to make the prediction on the training data as accurate as possible.\n10:00\nSo as another occasion, when you compute the maximum likelihood data, basically you'll find a beta value, a set of beta values that would maximize this conditional likelihood.\n10:12\nAnd this, again, then gives us a standard optimization problem. In this case, it can be also solved in many ways. Newton's method is a popular way to solve this problem, there are other methods as well. But in the end, we will look at a set of data values. Once we have the beta values, then we have a way to find the scoring function to help us classify a document.\n10:39\nSo what's the function? Well, it's this one. See, if we have all the beta values, are they known? All we need is to compute the Xi for that document and then plug in those values. That will give us an estimated probability that the document is in category one.\n10:59\nOkay so, so much for logistical regression. Let's also introduce another discriminative classifier called K-Nearest Neighbors. Now in general, I should say there are many such approaches, and a thorough introduction to all of them is clearly beyond the scope of this course. And you should take a machine learning course or read more about machine learning to know about them. Here, I just want to include the basic introduction to some of the most commonly used classifiers, since you might use them often for text calculation. So the second classifier is called K-Nearest Neighbors. In this approach, we're going to also estimate the conditional probability of label given data, but in a very different way. So the idea is to keep all the training examples and then once we see a text object that we want to classify, we're going to find the K examples in the training set and that are most similar to this text object. Basically, this is to find the neighbors of this text objector in the training data set. So once we found the neighborhood and we found the object that are close to the object we are interested in classifying, and let's say we have found the K-Nearest Neighbors. That's why this method is called K-Nearest Neighbors. Then we're going to assign the category that's most common in these neighbors. Basically we're going to allow these neighbors to vote for the category of the objective that we're interested in classifying.\n12:33\nNow that means if most of them have a particular category and it's a category one, they're going to say this current object will have category one.\n12:43\nThis approach can also be improved by considering the distance of a neighbor and of a current object. Basically, we can assume a closed neighbor would have more say about the category of the subject. So, we can give such a neighbor more influence on the vote. And we can take away some of the votes based on the distances.\n13:06\nBut the general idea is look at the neighborhood, and then try to assess the category based on the categories of the neighbors. Intuitively, this makes a lot of sense. But mathematically, this can also be regarded as a way to directly estimate there's a conditional probability of label given data, that is p of Y given X.\n13:28\nNow I'm going to explain this intuition in a moment, but before we proceed, let me emphasize that we do need a similarity function here in order for this to work. Note that in naive base class five, we did not need a similarity function. And in logistical regression, we did not talk about those similarity function either, but here we explicitly require a similarity function. Now this similarity function actually is a good opportunity for us to inject any of our insights about the features. Basically effective features are those that would make the objects that are on the same category look more similar, but distinguishing objects in different categories. So the design of this similarity function is closely tied it to the design of the features in logistical regression and other classifiers. So let's illustrate how K-NN works. Now suppose we have a lot of training instances here. And I've colored them differently and to show just different categories. Now suppose we have a new object in the center that we want to classify. So according to this approach, you work on finding the neighbors. Now, let's first think of a special case of finding just one neighbor, the closest neighbor.\n14:53\nNow in this case, let's assume the closest neighbor is the box filled with diamonds. And so then we're going to say, well, since this is in this object that is in category of diamonds, let's say. Then we're going to say, well, we're going to assign the same category to our text object. But let's also look at another possibility of finding a larger neighborhood, so let's think about the four neighbors.\n15:26\nIn this case, we're going to include a lot of other solid field boxes in red or pink, right? So in this case now, we're going to notice that among the four neighbors, there are three neighbors in a different category. So if we take a vote, then we'll conclude the object is actually of a different category. So this both illustrates how can nearest neighbor works and also it illustrates some potential problems of this classifier. Basically, the results might depend on the K and indeed, k's an important parameter to optimize. Now, you can intuitively imagine if we have a lot of neighbors around this object, and then we'd be okay because we have a lot of neighbors who will help us decide the categories. But if we have only a few, then the decision may not be reliable. So on the one hand, we want to find more neighbor, right? And then we have more votes. But on the other hand, as we try to find more neighbors we actually could risk on getting neighbors that are not really similar to this instance. They might actually be far away as you try to get more neighbors. So although you get more neighbors but those neighbors aren't necessarily so helpful because they are not very similar to the object. So the parameter still has to be set empirically. And typically, you can optimize such a parameter by using cross validation. Basically, you're going to separate your training data into two parts and then you're going to use one part to actually help you choose the parameter k here or some other parameters in other class files. And then you're going to assume this number that works well on your training that will be actually be the best for your future data.\n17:23\nSo as I mentioned, K-NN can be actually regarded as estimate of conditional problem within y given x an that's why we put this in the category of discriminative approaches. So the key assumption that we made in this approach is that the distribution of the label given the document probability a category given for example probability of theta i given document d is locally smooth. And that just means we're going to assume that this probability is the same for all the documents in these region R here. And suppose we draw a neighborhood and we're going to assume in this neighborhood since the data instances are very similar we're going to assume that the conditional distribution of the label given the data will be roughly the same. If these are very different then we're going to assume that the probability of c doc given d would be also similar. So that's a very key assumption. And that's actually important assumption that would allow us to do a lot of machinery. But in reality, whether this is true of course, would depend on how we define similarity. Because neighborhood is largely determined by our similarity function. If our similarity function captures objects that do follow similar distributions then these assumptions are okay but if our similarity function could not capture that, obviously these assumption would be a problem and then the classifier would not be accurate.\n18:59\nOkay, let's proceed with these assumption. Then what we are saying is that, in order to estimate the probability of category given a document. We can try to estimate the probability of the category given that entire region. Now, this has a benefit, of course, of bringing additional data points to help us estimate this probability.\n19:22\nAnd so this is precisely the idea of K-NN. Basically now we can use the known categories of all the documents in this region to estimate this probability. And I have even given a formula here where you can see we just count the topics in this region and then normalize that by the total number of documents in the region. So the numerator that you see here, c of theta i and r, is a counter of the documents in region R was category theta i. Since these are training document and we know they are categories. We can simply count how many times it was since here. How many times we have the same signs, etc. And then the denominator is just the total number of training documents in this region. So this gives us a rough estimate of which categories most popular in this neighborhood. And we are going to assign the popular category to our data object since it falls into this region. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/CV8fN/7-9-paradigmatic-relation-discovery-part-2dict_values(['List\nCS 410: Text Information Systems\nWeek 7\n7.9 Paradigmatic Relation Discovery Part 2\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.9 Paradigmatic Relation Discovery Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:05\nIn this lecture, we continue discussing Paradigmatical Relation Discovery. Earlier we introduced a method called Expected Overlap of Words in Context. In this method, we represent each context by a word vector that represents the probability of a word in the context. We measure the similarity by using the.product, which can be interpreted as the probability that two randomly picked words from the two contexts are identical. We also discussed the two problems of this method. The first is that it favors matching one frequent term very well over matching more distinct terms. It put too much emphasis on matching one term very well. The second is that it treats every word equally. Even a common word like the will contribute equally as content word like eats. So now we are going to talk about how to solve these problems. More specifically, we\'re going to introduce some retrieval heuristics used in text retrieval. These heuristics can effectively solve these problems, as these problems also occur in text retrieval when we match a query that though with a document vector. So to address the first problem, we can use a sublinear transformation of tone frequency. That is, we don\'t have to use the raw frequency count of a term to represent the context. We can transform it into some form that wouldn\'t emphasize so much on the raw frequency. To address the synchronous problem, we can put more weight on rare terms. That is we can reward matching a real-world. This heuristic is called the IDF term weighting in text retrieval. IDF stands for Inverse Document Frequency. So now, we\'re going to talk about the two heuristics in more detail. First let\'s talk about the TF Transformation. That is to convert the raw count of a word in the document into some weight that reflects our belief about how important this word in the document. So that will be denoted by TF of w,d. That\'s shown in the y-axis. Now, in general, there are many ways to map that. Let\'s first look at the simple way of mapping. In this case, we\'re going to say, well, any non-zero counts will be mapped to one and the zero count will be mapped to zero. So with this mapping all the frequencies will be mapped to only two values; zero or one. The mapping function is shown here as a flat line here. Now, this is naive because it\'s not the frequency of words. However, this actually has the advantage of emphasizing matching all the words in the context. So it does not allow a frequency of word to dominate the matching. Now, the approach that we have taken earlier in the expected overlap count approach, is a linear transformation. We basically, take y as the same as x. So we use the raw count as a representation. That created the problem that we just talked about namely; it emphasize too much on just matching one frequent term. Matching one frequent term can contribute a lot. So we can have a lot of other interesting transformations in between the two extremes, and they generally form a sublinear transformation. So for example, one possibility is to take logarithm of the raw count, and this will give us curve that looks like this, that you are seeing here. In this case, you can see the high frequency counts. The high counts are penalize a little bit, so the curve is a sublinear curve and it brings down the weight of those really high counts. This is what we want, because it prevents that terms from dominating the scoring function.\n4:48\nNow, there is also another interesting transformation called a BM25 transformation which has been shown to be very effective for retrieval. In this transformation, we have a form that looks like this. So it\'s k plus one multiplied by x divided by x plus k, where k is a parameter, x is the count, the raw count of a word. Now, the transformation is very interesting in that it can actually go from one extreme to the other extreme by varying k. It also interesting that it has upper bound, k plus one in this case. So this puts a very strict constraint on high frequency terms, because their weight would never exceed k plus one. As we vary k, if we can simulate the two extremes. So when k is set to zero, we roughly have the 0,1 vector. Whereas when we set k to a very large value, it will behave more like the linear transformation. So this transformation function is by far the most effective transformation function for text retrieval and it also makes sense for our problem setup. So we just talked about how to solve the problem of overemphasizing a frequency term Now let\'s look at the second problem, and that is how we can penalize popular terms. Matching "the" is not surprising, because "the" occurs everywhere. But matching "eats" would count a lot. So how can we address that problem? Now in this case, we can use the IDF weighting. That\'s commonly used in retrieval. IDF stands for Inverse Document Frequency. Document frequency means the count of the total number of documents that contain a particular word. So here we show that the IDF measure is defined as a logarithm function of the number of documents that match a term or document frequency. So K is the number of documents containing word or document frequency and M here is the total number of documents in the collection. The IDF function is giving a higher value for a lower K, meaning that it rewards rare term. The maximum value is log of M plus one. That\'s when the word occurred just once in a context. So that\'s a very rare term, the rare is term in the whole collection. The lowest value you can see here is when K reaches its maximum which would be M. So that would be a very low value, close to zero in fact. So this of course measure is used in search where we naturally have a collection. In our case, what would be our collection? Well, we can also use the context that we can collect all the words as our collection. That is to say, a word that\'s popular in the collection in general, would also have a low IDF. Because depending on the dataset, we can construct the context vectors in different ways. But in the end if a term is very frequent in the original dataset, then it will still be frequent in the collective context documents. So how can we add these heuristics to improve our similarity function? Well, here\'s one way and there are many other ways that are possible. But this is a reasonable way, where we can adapt the BM25 retrieval model for paradigmatical relation mining.\n9:14\nIn this case, we define the document vector as containing elements representing normalized BM25 values. So in this normalization function, we take sum over all the words and we normalize the weight of each word by the sum of the weights of all the words. This is to again ensure all the xi\'s will sum to one in this vector. So this would be very similar to what we had before, in that this vector is actually something similar to a word distribution, all the xi\'s will sum to one. Now, the weight of BM25 for each word is defined here. If you compare this with our old definition where we just have a normalized count of this one, right? So we only have this one and the document lens or the total counts of words in that context to document, and that\'s what we had before. But now with the BM25 transformation, we introduced something else. First, of course, this extra occurrence of this count is just to achieve the sub-linear normalization. But we also see we introduced the parameter, k, here, and this parameter is generally a non-active number, although zero is also possible. But this controls the upper bound, and also controls to what extent it simulates the linear transformation. So this is one parameter, but we also see there is another parameter here, b, and this would be within zero and one. This is a parameter to control lens normalization. In this case, the normalization formula has a average document lens here. This is computed up by taking the average of the lenses of all the documents in the collection. In this case, all the lenses of all the context of documents that we\'re considering. So this average documents will be a constant for any given collection. So it actually is only affecting the effect of the parameter, b, here because this is a constant. But I kept it here because it\'s a constant that\'s used for in retrieval where it would give us a stabilized interpretation of parameter, b. But for our purpose, this will be a constant so it would only be affecting the lens normalization together with parameter, b. Now, with this definition then, we have a new way to define our document of vectors, and we can compute the vector d2 in the same way. The difference is that the high-frequency terms will now have a somewhat lower weights. This would help us control the inference of these high-frequency terms. Now, the idea can be added here in the scoring function. That means we\'ll introduce a weight for matching each term. So you may recall this sum indicates all the possible words that can be overlap between the two contexts. The x_i and the y_i are probabilities of picking the word from both contexts. Therefore, it indicates how likely we\'ll see a match on this word. Now, IDF would give us the importance of matching this word. A common word will be worth less than a rare word. So we emphasize more on matching rare words now. So with this modification, then the new function will likely address those two problems. Now, interestingly we can also use this approach to discover syntagmatic relations. In general, when we re-brand a context with a term vector, we would likely see some terms have high weights and other terms have low weights. Depending on how we assign weights to these terms, we might be able to use these weights to discover the words that are strongly associated with the candidate word in the context. So let\'s take a look at the term vector in more detail here. We have each x_i defined as the normalized weight of BM25. Now, this weight alone only reflects how frequent the word occurs in the context. But we can\'t just say any frequent term in the context that would be correlated with the candidate word because many common words like \'the\' will occur frequently in all the context. But if we apply IDF weighting as you see here, we can then re-weight these terms based on IDF. That means the words that are common like \'the\' will get penalized. So now the highest weighted terms will not be those common terms because they have lower IDFs. Instead, those terms would be the terms that are frequent in the context, but not frequent in the collection. So those are clearly the words that tend to occur in the context of the candidate word, for example, cat. So for this reason, the highly weighted terms in this idea of weighted vector can also be assumed to be candidates for syntagmatic relations. Now, of course, this is only a by-product of our approach for discovering paradigmatic relations. In the next lecture, we\'re going to talk more about how to discover syntagmatic relations. But it clearly shows the relation between discovering the two relations. Indeed they can be discovered in a joint manner by leveraging such associations. So to summarize, the main idea for discovering paradigmatic relations is to collect the context of a candidate word to form a pseudo document. This is typically represented as a bag of words. Then compute the similarity of the corresponding context documents of two candidate words. Then we can take the highly similar word pairs, and treat them as having paradigmatic relations. These are the words that share similar contexts. There are many different ways to implement this general idea. We just talked about some of the approaches. More specifically, we talked about using text retrieval models to help us design effective similarity function to compute the paradigmatic relations. More specifically, we have used the BM25 and IDF weighting to discover paradigmatic relation. These approaches also represent the state of the art in text retrieval techniques. Finally, syntagmatic relations can also be discovered as a by-product when we discover paradigmatic relations.\nLike\nDislike\nReport an issue\nShare'])
https://www.coursera.org/learn/cs-410/supplement/0wyle/week-3-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 3\nWeek 3 Overview\nPrevious\nNext\nWeek 3 Information\nReading:\nReading\nWeek 3 Overview\n. Duration: 10 minutes\n10 min\nWeek 3 Lessons\nWeek 3 Activities\nProgramming Assignment 2.1\nWeek 3 Overview\nDuring this week's lessons, you will learn how to evaluate an information retrieval system (a search engine), including the basic measures for evaluating a set of retrieved results and the major measures for evaluating a ranked list, including the average precision (AP) and the normalized discounted cumulative gain (nDCG), and practical issues in evaluation, including statistical significance testing and pooling.     \nTime\nThis module should take approximately 7 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with assignments in bold):\nActivity\nEstimated Time Required\nWeek 3 Video Lectures\n2 hours\nWeek 3 Graded Quiz\n1 hour\nProgramming Assignment 2.1\n4 hours\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain the Cranfield evaluation methodology and how it works for evaluating a text retrieval system.\nExplain how to evaluate a set of retrieved documents and how to compute precision, recall, and F1.\nExplain how to evaluate a ranked list of documents.\nExplain how to compute and plot a precision-recall curve.\nExplain how to compute average precision and mean average precision (MAP).\nExplain how to evaluate a ranked list with multi-level relevance judgments.\nExplain how to compute normalized discounted cumulative gain.\nExplain why it is important to perform statistical significance tests.\nGuiding Questions\nDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\nWhy is evaluation so critical for research and application development in text retrieval? \nHow does the Cranfield evaluation methodology work? \nHow do we evaluate a set of retrieved documents? \nHow do you compute precision, recall, and F1? \nHow do we evaluate a ranked list of search results? \nHow do you compute average precision? How do you compute mean average precision (MAP) and geometric mean average precision (gMAP)? \nWhat is mean reciprocal rank? \nWhy is MAP more appropriate than precision at k documents when comparing two retrieval methods?\nWhy is precision at k documents more meaningful than average precision from a user’s perspective? \nHow can we evaluate a ranked list of search results using multi-level relevance judgments? \nHow do you compute normalized discounted cumulative gain (nDCG)? \nWhy is normalization necessary in nDCG? Does MAP need a similar normalization?  Why is it important to perform statistical significance tests when we compare the retrieval accuracies of two search engine systems?\nAdditional Readings and Resources\nMark Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval 4, 4 (2010), 247-375.\nC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapter 9\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nCranfield evaluation methodology\nPrecision and recall\nAverage precision, mean average precision (MAP), and geometric mean average precision (gMAP) \nReciprocal rank and mean reciprocal rank \nF-measure \nNormalized discounted cumulative Gain (nDCG) \nStatistical significance test  \nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/OxeOx/12-2-opinion-mining-and-sentiment-analysis-latent-aspect-rating-analysis-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 12\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\nPrevious\nNext\nWeek 12 Information\nWeek 12 Lessons\nVideo:\nVideo\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.3 Text-Based Prediction\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n12.4 Contextual Text Mining: Motivation\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\n12.8 Summary for Exam 2\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is a continued discussion of Latent Aspect Rating Analysis. Earlier, we talked about how to solve the problem of LARA in two stages. But we first do segmentation of different aspects. And then we use a latent regression model to learn the aspect ratings and then later the weight. Now it's also possible to develop a unified generative model for solving this problem, and that is we not only model the generational over-rating based on text. We also model the generation of text, and so a natural solution would be to use topic model. So given the entity, we can assume there are aspects that are described by word distributions. Topics. And then we an use a topic model to model the generation of the reviewed text.\n1:01\nI will assume words in the review text are drawn from these distributions.\n1:08\nIn the same way as we assumed for generating model like PRSA.\n1:13\nAnd then we can then plug in the latent regression model to use the text to further predict the overrating. And that means when we first predict the aspect rating and then combine them with aspect weights to predict the overall rating. So this would give us a unified generated model, where we model both the generation of text and the overall ready condition on text.\n1:40\nSo we don't have time to discuss this model in detail as in many other cases in this part of the cause where we discuss the cutting edge topics, but there's a reference site here where you can find more details.\n1:57\nSo now I'm going to show you some simple results that you can get by using these kind of generated models. First, it's about rating decomposition. So here, what you see are the decomposed ratings for three hotels that have the same overall rating. So if you just look at the overall rating, you can't really tell much difference between these hotels. But by decomposing these ratings into aspect ratings we can see some hotels have higher ratings for some dimensions, like value, but others might score better in other dimensions, like location. And so this can give you detailed opinions at the aspect level.\n2:38\nNow here, the ground-truth is shown in the parenthesis, so it also allows you to see whether the prediction is accurate. It's not always accurate but It's mostly still reflecting some of the trends.\n2:53\nThe second result you compare different reviewers on the same hotel. So the table shows the decomposed ratings for two reviewers about same hotel. Again their high level overall ratings are the same. So if you just look at the overall ratings, you don't really get that much information about the difference between the two reviewers. But after you decompose the ratings, you can see clearly that they have high scores on different dimensions. So this shows that model can review differences in opinions of different reviewers and such a detailed understanding can help us understand better about reviewers and also better about their feedback on the hotel. This is something very interesting, because this is in some sense some byproduct. In our problem formulation, we did not really have to do this. But the design of the generating model has this component. And these are sentimental weights for words in different aspects. And you can see the highly weighted words versus the negatively loaded weighted words here for each of the four dimensions. Value, rooms, location, and cleanliness. The top words clearly make sense, and the bottom words also make sense.\n4:10\nSo this shows that with this approach, we can also learn sentiment information directly from the data. Now, this kind of lexicon is very useful because in general, a word like long, let's say, may have different sentiment polarities for different context. So if I say the battery life of this laptop is long, then that's positive. But if I say the rebooting time for the laptop is long, that's bad, right? So even for reviews about the same product, laptop, the word long is ambiguous, it could mean positive or it could mean negative. But this kind of lexicon, that we can learn by using this kind of generated models, can show whether a word is positive for a particular aspect. So this is clearly very useful, and in fact such a lexicon can be directly used to tag other reviews about hotels or tag comments about hotels in social media like Tweets.\n5:08\nAnd what's also interesting is that since this is almost completely unsupervised, well assuming the reviews whose overall rating are available And then this can allow us to learn form potentially larger amount of data on the internet to reach sentiment lexicon.\n5:28\nAnd here are some results to validate the preference words. Remember the model can infer wether a reviewer cares more about service or the price. Now how do we know whether the inferred weights are correct? And this poses a very difficult challenge for evaluation. Now here we show some interesting way of evaluating.\n5:50\nWhat you see here are the prices of hotels in different cities, and these are the prices of hotels that are favored by different groups of reviewers. The top ten are the reviewers was the highest inferred value to other aspect ratio.\n6:09\nSo for example value versus location, value versus room, etcetera. Now the top ten of the reviewers that have the highest ratios by this measure. And that means these reviewers tend to put a lot of weight on value as compared with other dimensions. So that means they really emphasize on value.\n6:30\nThe bottom ten on the other hand of the reviewers. The lowest ratio, what does that mean? Well it means these reviewers have put higher weights on other aspects than value. So those are people that cared about another dimension and they didn't care so much the value in some sense, at least as compared with the top ten group.\n6:52\nNow these ratios are computer based on the inferred weights from the model.\n6:57\nSo now you can see the average prices of hotels favored by top ten reviewers are indeed much cheaper than those that are favored by the bottom ten. And this provides some indirect way of validating the inferred weights. It just means the weights are not random. They are actually meaningful here. In comparison, the average price in these three cities, you can actually see the top ten tend to have below average in price, whereas the bottom half, where they care a lot about other things like a service or room condition tend to have hotels that have higher prices than average. So with these results we can build a lot of interesting applications. For example, a direct application would be to generate the rated aspect, the summary, and because of the decomposition we have now generated the summaries for each aspect. The positive sentences the negative sentences about each aspect. It's more informative than original review that just has an overall rating and review text. Here are some other results about the aspects that's covered from reviews with no ratings. These are mp3 reviews, and these results show that the model can discover some interesting aspects. Commented on low overall ratings versus those higher overall per ratings. And they care more about the different aspects.\n8:22\nOr they comment more on the different aspects. So that can help us discover for example, consumers' trend in appreciating different features of products. For example, one might have discovered the trend that people tend to like larger screens of cell phones or light weight of laptop, etcetera. Such knowledge can be useful for manufacturers to design their next generation of products. Here are some interesting results on analyzing users rating behavior. So what you see is average weights along different dimensions by different groups of reviewers. And on the left side you see the weights of viewers that like the expensive hotels. They gave the expensive hotels 5 Stars, and you can see their average rates tend to be more for some service. And that suggests that people like expensive hotels because of good service, and that's not surprising. That's also another way to validate it by inferred weights.\n9:34\nIf you look at the right side where, look at the column of 5 Stars. These are the reviewers that like the cheaper hotels, and they gave cheaper hotels five stars. As we expected and they put more weight on value, and that's why they like the cheaper hotels.\n9:52\nBut if you look at the, when they didn't like expensive hotels, or cheaper hotels, then you'll see that they tended to have more weights on the condition of the room cleanness.\n10:04\nSo this shows that by using this model, we can infer some information that's very hard to obtain even if you read all the reviews. Even if you read all the reviews it's very hard to infer such preferences or such emphasis. So this is a case where text mining algorithms can go beyond what humans can do, to review interesting patterns in the data. And this of course can be very useful. You can compare different hotels, compare the opinions from different consumer groups, in different locations. And of course, the model is general. It can be applied to any reviews with overall ratings. So this is a very useful technique that can support a lot of text mining applications.\n10:50\nFinally the results of applying this model for personalized ranking or recommendation of entities.\n10:57\nSo because we can infer the reviewers weights on different dimensions, we can allow a user to actually say what do you care about. So for example, I have a query here that shows  of the weight should be on value and  on others. So that just means I don't care about other aspect. I just care about getting a cheaper hotel. My emphasis is on the value dimension. Now what we can do with such query is we can use reviewers that we believe have a similar preference to recommend a hotels for you. How can we know that? Well, we can infer the weights of those reviewers on different aspects. We can find the reviewers whose weights are more precise, of course inferred rates are similar to yours. And then use those reviewers to recommend hotels for you and this is what we call personalized or rather query specific recommendations. Now the non-personalized recommendations now shown on the top, and you can see the top results generally have much higher price, than the lower group and that's because when the reviewer's cared more about the value as dictated by this query they tended to really favor low price hotels. So this is yet another application of this technique.\n12:18\nIt shows that by doing text mining we can understand the users better. And once we can handle users better we can solve these users better. So to summarize our discussion of opinion mining in general, this is a very important topic and with a lot of applications.\n12:33\nAnd as a text sentiment analysis can be readily done by using just text categorization. But standard technique tends to not be enough. And so we need to have enriched feature implementation.\n12:45\nAnd we also need to consider the order of those categories. And we'll talk about ordinal regression for some of these problem. We have also assume that the generating models are powerful for mining latent user preferences. This in particular in the generative model for mining latent regression. And we embed some interesting preference information and send the weights of words in the model as a result we can learn most useful information when fitting the model to the data. Now most approaches have been proposed and evaluated. For product reviews, and that was because in such a context, the opinion holder and the opinion target are clear. And they are easy to analyze. And there, of course, also have a lot of practical applications. But opinion mining from news and social media is also important, but that's more difficult than analyzing review data, mainly because the opinion holders and opinion targets are all interested. So that calls for natural management processing techniques to uncover them accurately.\n13:50\nHere are some suggested readings. The first two are small books that are of some use of this topic, where you can find a lot of discussion about other variations of the problem and techniques proposed for solving the problem.\n14:08\nThe next two papers about generating models for rating the aspect rating analysis. The first one is about solving the problem using two stages, and the second one is about a unified model where the topic model is integrated with the regression model to solve the problem using a unified model.\n14:30\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/1gMIe/syllabusdict_values(["List\nCS 410: Text Information Systems\nWeek 1\nSyllabus\nPrevious\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nSyllabus\nCS 410: Text Information Systems\nCourse Description\nThe growth of “big data” created unprecedented opportunities to leverage computational and statistical approaches, which turn raw data into actionable knowledge that can support various application tasks. This is especially true for the optimization of decision making in virtually all application domains, such as health and medicine, security and safety, learning and education, scientific discovery, and business intelligence. This course covers general computational techniques for building intelligent text information systems to help users manage and make use of large amounts of text data in all kinds of applications.\nText data include all data in the form of natural language text (e.g., English text or Chinese text), including all web pages, social media data such as tweets, news, scientific literature, emails, government documents, and many other kinds of enterprise data. Text data play an essential role in our lives. Since we communicate using natural languages, we produce and consume a large amount of text data every day covering all kinds of topics. The explosive growth of text data makes it impossible for people to consume all the relevant text data in a timely manner.\nThe two main techniques to assist people in consuming, digesting, and making use of the text data are:\nText retrieval, which helps identify the most relevant text data to a particular problem from a large collection of text documents, thus avoiding processing a large number of non-relevant documents\nText mining, which helps users further analyze and digest the found relevant text data and extract actionable knowledge for finishing a task\nThis course covers both text retrieval and text mining, so as to provide you with the opportunity to see the complete spectrum of techniques used in building an intelligent text information system. Building on two MOOCs covering the same topic and including a course project, this course enables you to learn the basic concepts, principles, and general techniques in text retrieval and mining, as well as gain hands-on experience with using software tools to develop interesting text data applications.\nCourse Goals and Objectives\nUpon successful completion of this course, you will be able to:\nExplain all the basic concepts in text retrieval and text mining.\nExplain the main ideas behind the major models and algorithms for text retrieval and text mining.\nExplain how the major models and algorithms for text retrieval and text mining work.\nExplain how to implement some of the commonly used algorithms for text retrieval and text mining.\nExplain how to evaluate applications of text retrieval and text mining.\nTextbook\nThere is not a required textbook for this course, but there are several optional readings suggested in each week's overview page. All readings listed in the weekly overview pages are optional and are primarily from the following textbook:\nZhai, C. & Massung, S. (2016). Text data management and analysis: A practical introduction to information retrieval and text mining. ACM Book Series. Morgan & Claypool Publishers.\nCourse Outline\nWeek\nDates\nTopics\nWeek 1\nAugust 22 - August 28\nPart of Speech tagging, syntactic analysis, semantic analysis, ambiguity, “bag of words” representation, push, pull, querying, browsing, probability ranking principle, relevance, vector space model, dot product, bit vector representation  \nWeek 2\nAugust 29 -September 4\nTerm frequency (TF), document frequency (DF), and inverse document frequency (IDF), TF transformation, pivoted length normalization, BM25, inverted index and postings, binary coding, unary coding, gamma-coding, d-gap, Zipf’s law\nWeek 3\nSeptember 5 -11\nCranfield evaluation methodology, precision and recall, average precision, mean average precision (MAP), geometric mean average precision (gMAP), reciprocal rank, mean reciprocal rank, F-measure , Normalized Discounted Cumulative Gain (nDCG), statistical significance test\nWeek 4\nSeptember 12 - 18\np(R=1|q,d), query likelihood, p(q|d), statistical and unigram language models, maximum likelihood estimate, background, collection, and document language models, smoothing of unigram language models, relation between query likelihood and TF-IDF weighting, linear interpolation (i.e., Jelinek-Mercer) smoothing, Dirichlet Prior smoothing\nWeek 5\nSeptember 19 - 25\nRelevance feedback, pseudo-relevance feedback, implicit feedback, Rocchio feedback, Kullback-Leiber divergence (KL-divergence) retrieval function, mixture language model, scalability and efficiency, spams, crawler, focused crawling, and incremental crawling, Google File System (GFS), MapReduce, link analysis and anchor text, PageRank\nWeek 6\nSeptember 26 - October 2\nContent-based filtering, collaborative filtering, Beta-Gamma threshold learning, linear utility , user profile, exploration-exploitation tradeoff, memory-based collaborative filtering, cold start\nWeek 7\nOctober 3 - 9\nText representation (especially bag-of-words representation), context of a word, context similarity, paradigmatic relation, syntagmatic relation. \nWeek 8\nOctober 10 - 16\nEntropy, conditional entropy, mutual information, topics, coverage of topic , language model, generative model, unigram language model, word distribution, background language model, parameters of a probabilistic model, likelihood, Bayes rule, maximum likelihood estimation, prior and posterior distributions, Bayesian estimation & inference, maximum a posteriori (MAP) estimate, prior model, posterior mode.\nWeek of Exam 1\nWeek 9\nOctober 17 - 23\nMixture model, component model, constraints on probabilities, Probabilistic Latent Semantic Analysis (PLSA), Expectation-Maximization (EM) algorithm, E-step and M-step, hidden variables, hill climbing, local maximum, Latent Dirichlet Allocation (LDA)\nWeek 10\nOctober 24 - October 30\nClustering, document clustering, term clustering, clustering bias, perspective of similarity, Hierarchical Agglomerative Clustering, k-Means, direction evaluation (of clustering), indirect evaluation (of clustering), text categorization, topic categorization, sentiment categorization, email routing , spam filtering, naïve Bayes classifier, smoothing\nWeek 11\nOctober 31 - November 6\nGenerative classifier vs. discriminative classifier, training data, logistic regression, K-Nearest Neighbor classifier, classification accuracy, precision, recall, F measure, macro-averaging, micro-averaging, opinion holder, opinion target, sentiment, opinion representation, sentiment classification, features, n-grams, frequent patterns, overfitting\nWeek 12\nNovember 7 - 13\nText-based prediction, the “data mining loop”, context (of text data), contextual text mining, contextual probabilistic latent semantic analysis (CPLSA), views of a topic, coverage of topics, spatiotemporal trends of topics, event impact analysis, network-regularized topic modeling, NetPLSA, causal topics, iterative topic modeling with time series supervision\nWeek 13\nNovember 14 - 20\nProject work week.\nWeek 14\nNovember 21 - 27\nThanksgiving Break\nWeek 15\nNovember 28 -December 4\nExam 2; No new content - work on your final project! \nWeek 16\nDecember 5 - 11\nFinal Project Presentation and Report Due Start of Finals Week\nElements of This Course\nLecture videos. Each week your instructor will teach you the concepts you need to know through a collection of short video lectures. You may either stream these videos for playback within the browser by clicking on their titles, or you can download each video for later offline playback by clicking the download icon. The videos usually total 1.5 to 2 hours each week, but you generally need to spend at least the same amount of time digesting the content in the videos. The actual amount of time needed to digest the content will naturally vary according to your background.\nQuizzes. Most weeks will include one for-credit quiz. You will have two attempts for each quiz, with your highest score used toward your final grade. Your top 10 quiz scores will be used to calculate your final grade (i.e., we will drop the two lowest quiz scores).\nExams. This course will have two 1-hour exams. The exams are intended to test your understanding of the material you learn in the course and will contain questions similar to those seen in the weekly quizzes.\nProgramming Assignments. The programming assignments for this course provide an opportunity for you to practice your programming skills and apply what you've learned in the course. Set aside about 2-4 hours each week to work on the programming assignment if you plan to finish it.\nTech Review (for 4-credit students). It will require you to generate a short 1-2 page review on an interesting course-related cutting-edge technology topic not covered in any lecture.\nFinal Course Project. There will also be one culminating project due at the end of course. It will require you to work in groups of at most three. You will build a tool using methods from the course to perform data analysis and also generate documentations and presentation.\nGrading\nYour final grade will be calculated based on the activities listed in the table below. As a note, the grade on Coursera will NOT accurately reflect your grade in the course. \nActivity\nPercent of Final Grade\nQuizzes\n\nProgramming Assignments\n\nCourse Project\n\nExam 1\n\nExam 2\n\nLetter Grade\nPercent Needed\nLetter Grade\nPercent Needed\nLetter Grade\nPercent Needed\nA+\n95\nB+\n80\nC\n60\nA\n90\nB\n75\nD\n55\nA-\n85\nB-\n70\nF\n<55\nYour final grade will be calculated based on the activities listed in the table below. Your official final course grade will be listed in Enterprise. The course grade you see displayed in Coursera may not match your official final course grade.\nAdditional Course Policies\nStudent Code and Policies\nA student at the University of Illinois at the Urbana‑Champaign campus is a member of a University community of which all members have at least the rights and responsibilities common to all citizens, free from institutional censorship; affiliation with the University as a student does not diminish the rights or responsibilities held by a student or any other community member as a citizen of larger communities of the state, the nation, and the world. See the University of Illinois Student Code for more information.\nThe CS department also maintains a policies handbook for graduate student. For more information, see the Graduate Student Handbook.\nAdditionally, all Coursera learners are required to follow an Honor Code and a Code of Conduct. Please review both of these items before commencing your studies.\nAcademic Integrity\nAll students are expected to abide by the campus regulations on academic integrity found in the Student Code of Conduct. These standards will be enforced and infractions of these rules will not be tolerated in this course. Sharing, copying, or providing any part of a homework solution or code is an infraction of the University’s rules on academic integrity. We will be actively looking for violations of this policy in homework and project submissions. Any violation will be punished as severely as possible with sanctions and penalties typically ranging from a failing grade on this assignment up to a failing grade in the course, including a letter of the offending infraction kept in the student's permanent university record.\nAgain, a good rule of thumb: Keep every typed word and piece of code your own. If you think you are operating in a gray area, you probably are. If you would like clarification on specifics, please contact the course staff.\nDisability Accommodations\nStudents with learning, physical, or other disabilities requiring assistance should contact the instructor as soon as possible. If you’re unsure if this applies to you or think it may, please contact the instructor and Disability Resources and Educational Services (DRES) as soon as possible. You can contact DRES at 1207 S. Oak Street, Champaign, via phone at (217) 333-1970, or via email at disability@illinois.edu.\nLate Policy\nLate homework and homework by email will not be accepted by the TA or the instructors without prior instructor approval.\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/awVwS/lesson-6-8-recommender-systems-collaborative-filtering-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] And here we're going to talk about basic strategy. And that would be based on similarity of users and then predicting the rating of and object by an active user using the ratings of similar users to this active user. This is called a memory based approach because it's a little bit similar to\n0:40\nstoring all the user information and when we are considering a particular user we going to try to retrieve the rating users or the similar users to this user case. And then try to use this information about those users to predict the preference of this user. So here is the general idea and we use some notations here, so x sub i j denotes the rating of object o j by user u i\n1:17\nand n sub i is average rating of object by this user.\n1:26\nSo this n i is needed because we would like to normalize the ratings of objects by this user. So how do you do normalization? Well, we're going to just subtract the average rating from all the ratings. Now, this is to normalize these ratings so that the ratings from different users would be comparable.\n1:55\nBecause some users might be more generous, and they generally give more high ratings but some others might be more critical so their ratings cannot be directly compared with each other or aggregate them together. So we need to do this normalization. Another prediction of the rating on the item by another user or active user, u sub a here\n2:24\ncan be based on the average ratings of similar users.\n2:30\nSo the user u sub a is the user that we are interested in recommending items to. And we now are interested in recommending this o sub j. So we're interested in knowing how likely this user will like this object. How do we know that?\n2:50\nWhere the idea here is to look at whether similar users to this user have liked this object.\n2:59\nSo mathematically this is to say well the predicted the rating of this user on this app object, user a on object o j is basically combination of the normalized ratings of different users, and in fact here, we're taking a sum over all the users. But not all users contribute equally to the average, and this is conjured by the weights. So this weight controls the inference of the user on the prediction. And of course, naturally this weight should be related to the similarity between ua and this particular user, ui. The more similar they are, then the more contribution user ui can make in predicting the preference of ua.\n4:03\nSo, the formula is extremely simple. You can see, it's a sum of all the possible users. And inside the sum we have their ratings, well, their normalized ratings as I just explained. The ratings need to be normalized in order to be comparable with each other.\n4:22\nAnd then these ratings are weighted by their similarity.\n4:26\nSo you can imagine w of a and i is just a similarity of user a and user i.\n4:34\nNow what's k here? Well k is simply a normalizer. It's just one over the sum of all the weights, over all the users.\n4:47\nSo this means, basically, if you consider the weight here together with k, and we have coefficients of weight that will sum to one for all the users.\n5:00\nAnd it's just a normalization strategy so that you get this predictor rating in the same range as these ratings that we used to make the prediction.\n5:13\nRight? So this is basically the main idea of memory-based approaches for collaborative filtering.\n5:22\nOnce we make this prediction, we also would like to map back through the rating that the user would actually make, and this is to further add the mean rating or average rating of this user u sub a to the predicted value. This would recover a meaningful rating for this user. So if this user is generous, then the average it would be is somewhat high, and when we add that the rating will be adjusted to our relatively high rate. Now when you recommend an item to a user this actually doesn't really matter, because you are interested in basically the normalized reading, that's more meaningful. But when they evaluate these rather than filter approaches, they typically assume that actual ratings of the user on these objects to be unknown and then you do the prediction and then you compare the predicted ratings with their actual ratings. So, you do have access to the actual ratings. But, then you pretend that you don't know, and then you compare your systems predictions with the actual ratings. In that case, obviously, the systems prediction would be adjusted to match the actual ratings of the user and this is what's happening here basically.\n7:01\nOkay so this is the memory based approach. Now, of course, if you look at the formula, if you want to write the program to implement it, you still face the problem of determining what is this w function? Once you know the w function, then the formula is very easy to implement.\n7:22\nSo, indeed, there are many different ways to compute this function or this weight, w, and specific approaches generally differ in how this is computed.\n7:35\nSo here are some possibilities and you can imagine there are many other possibilities. One popular approach is we use the Pearson correlation coefficient.\n7:48\nThis would be a sum over commonly rated items. And the formula is a standard appears in correlation coefficient formula as shown here.\n8:00\nSo this basically measures whether the two users tended to all give higher ratings to similar items or lower ratings to similar items.\n8:11\nAnother measure is the cosine measure, and this is going to treat the rating vectors as vectors in the vector space. And then, we're going to measure the angle and compute the cosine of the angle of the two vectors. And this measure has been using the vector space model for retrieval, as well. So as you can imagine there are just as many different ways of doing that. In all these cases, note that the user's similarity is based on their preferences on items and we did not actually use any content information of these items. It didn't matter these items are, they can be movies, they can be books, they can be products, they can be text documents which has been cabled the content and so this allows such approach to be applied to a wide range of problems. Now in some newer approaches of course, we would like to use more information about the user. Clearly, we know more about the user, not just these preferences on these items. So in the actual filtering system, is in collaborative filtering, we could also combine that with content based filtering. We could use more context information, and those are all interesting approaches that people are just starting, and there are new approaches proposed. But, this memory based approach has been shown to work reasonably well, and it's easy to implement in practical applications this could be a starting point to see if the strategy works well for your application.\n9:56\nSo, there are some obvious ways to also improve this approach and mainly we would like to improve the user similarity measure. And there are some practical issues we deal with here as well. So for example, there will be a lot of missing values. What do you do with them? Well, you can set them to default values or the average ratings of the user. And that would be a simple solution. But there are advanced approaches that can actually try to predict those missing values, and then use predictive values to improve the similarity. So in fact that the memory based apology can predict those missing values, right? So you get you have iterative approach where you first use some preliminary prediction and then you can use the predictive values to further improve the similarity function.\n10:49\nSo this is a heuristic way to solve the problem. And the strategy obviously would affect the performance of claritative filtering just like any other heuristics would improve these similarity functions.\n11:06\nAnother idea which is actually very similar to the idea of IDF that we have seen in text search is called a Inverse User Frequency or IUF. Now here the idea is to look at where the two users share similar ratings. If the item is a popular item that has been viewed by many people and seen [INAUDIBLE] to people interested in this item may not be so interesting but if it's a rare item, it has not been viewed by many users. But these two users deal with this item and they give similar ratings. And, that says more about their similarity. It's kind of to emphasize more on similarity on items that are not viewed by many users. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/fTuOi/course-project-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 6\nCourse Project Overview\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nWeek 6 Activities\nProject Information\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nProgramming Assignment 2.4\nCourse Project Overview\nIntroduction\nThe course project is to give the students hands-on experience on developing some novel information retrieval and/or text mining tools. It would allow the students to potentially apply all the knowledge and skills learned in the course to solve a real-world problem. Group work is encouraged, but not required (i.e., you can have a one-person team). The maximum size of a team is 5 members to avoid challenges in efficient coordination of the work by too many team members. However, a team of a larger size is also possible subject to the approval of the instructor. A typical reason for a larger team is because the project has a very natural task division among the team members so that the need for frequent interactions and coordination of team members may be minimum despite the large size of the team. Whenever possible, collaboration of multiple project teams is strongly encouraged to minimize the amount of work of each team via expertise or resource sharing, as well as to generate “combined impact” (e.g., one team may develop a crawler that can be used by another team that develops a search engine).\nMore details (what to submit at each step) can be found here: \nhttps://docs.google.com/document/d/1b-EagO17Og7_ESj5hkP5x4EFrVPQAtBlH9YvsgEzjnY/edit?usp=sharing\nGrading criteria\nYour project will be graded based on the following weighting scheme, corresponding to three stages of work including 1) team  search and formation; 2) proposal development; 3) project result submission. All team members will receive the same grade provided that every member has made sufficient effort (at least 20 hours of quality time to work on the project).\nTeam formation (): Every student is required to form a team by the beginning of Week 9 (Oct 17, 2022). The team shall need to designate one of the team members as the team leader who will be responsible for submitting and/or uploading the course project deliverables.\nProject proposal (): Each project team is required to submit a roughly one-page project proposal by the end of Week 9 (Oct 23, 2022), and graded based on completion. \nProgress report (): Each project team is required to submit a short progress report by the beginning of Week 13 (Nov 14, 2022) listing the progress made, remaining tasks, any challenges/issues being faced, etc. It will be graded based on completion. \nSoftware code submission with documentation (): Due during  Week 16 (Dec 08, 2022), each project team will be asked to submit the produced source code with reasonable documentation. The documentation should cover both how to use the software and how the software is implemented. The  of the grade would be distributed as follows:  for source code submission;  for documentation submission. Both would be graded based on completion. \nSoftware usage tutorial presentation (): Due during Week 15 (Dec 08, 2022).  of the grading is based on completion and the remaining  is based on result of testing the software by graders\nThus, your course project work would be graded primarily based on your effort. If you have followed our guidelines and completed all the required tasks, you should receive at least  of the total points for the project. This is to encourage the students to pay attention to time management and set realistic goals that can actually be completed by the end of the semester. The remaining  is based on how well your software works; a fully functioning software would be given the whole , whereas a buggy software or a software with missing functions would result in losing some of the  of the grade. Completion of a functioning software is emphasized also due to the potential dependency between multiple projects when they are all contributing to a larger project (e.g., one team may produce a crawler to crawl data for use by another team to build a search engine).\nThe proposal, progress report and final submission will be peer-graded by you. The TAs will review all peer reviews and make sure that (1) the reviews are fair, and (2) the submissions satisfy all requirements. \nPeer-Grading Instructions\nWe'll be using peer-grading for grading project proposals, progress reports and final submissions. Later in the semester, you will receive an email from CMT inviting you as a reviewer, and we will make an announcement when the emails are sent. \nPeer grading will begin soon after the submission deadlines have passed. \nThe proposal peer-grading should be done between Oct 24 (12:00 am CST)-Oct 30 (12:00am CST). \nThe progress report peer-grading should be done between Nov 15 (12:00 am CST)-Nov 20 (11:59 pm CST). \nThe final submissions peer-grading should be done between Dec 08 (12:00 am CST)-Dec 16 (11:59 pm CST). \nPlease finish the gradings on time.\nEach project will be reviewed by at least 2 students and each student will review ~2-3 projects. We will be using your comments and scores as a guide to assign the final scores to the projects. Please grade them carefully and honestly. We appreciate your help with managing the grading of such a large class. We hope the process would also be a good learning experience for you. More specific instructions will be announced and sent later. \nInstructions\n1. Form a team (,  due beginning of Week 9, Oct 17, 2022)\nBy the beginning of Week 9, you should form teams. Groups of more than 5 members are highly discouraged. The team also needs to designate a member of the team as team leader. Your team leader will be responsible for submitting and/or uploading the course project deliverables such as: 1) this form, 2) the proposal, 3) the presentation, 4) the code, etc. \n2. Pick a topic & Write a proposal (, due end of Week 9, Oct 23 2022)\nPicking a project topic\nStudents will be able to choose their own topic or select a topic from a list of suggested topics which we will provide by Week 4. \nWriting a project proposal\nEach project team is required to write a one-page proposal before you actually go in depth on a topic. The proposal is due end of Week 9.\nIn the proposal, you should include the names and email addresses of all the team members. One member must be designated as the project coordinator/leader for the team, so please make sure to indicate that. The project coordinator would be responsible for the coordination of the project work by the team and also communication with the instructor or TA when the team needs help. \nDetailed instructions about the required content of the proposal will be provided by Week 4, including a short set of questions that need to be answered in the proposal. As long as those questions are addressed (wherever applicable), the proposal does not have to be very long. A couple of sentences for each question would be sufficient.\n3. Peer-review project proposals (due Middle of Week 10, Oct 30 2022)\nEach student will review the progress reports of ~1-2 groups and provide feedback/suggestions.  TAs will go through the peer assessments and make sure that the proposals satisfies all requirements listed in the Google Doc.\n4. Work on the project\nYou should try to reuse any existing tools as much as possible so as to minimize the amount of work without sacrificing your goal. Discuss any problems or issues with your teammates or classmates on Campuswire and leverage Campuswire to collaborate with each other. Consistent with our course policy, we strongly encourage you to help each other in all the course work so as to maximize your gain of new knowledge and skills while minimizing your work as much as possible. We will do our best to help you as well. Consider documenting your work regularly. This way, you will already have a lot of things written down by the end of the semester.\n5. Submit progress report (, due beginning of Week 13, Nov 14 2022)\nEach team must submit a short report detailing 1) Progress made thus far, 2) Remaining tasks, 3) Any challenges/issues being faced. The graders, TAs and instructor may provide help and suggestions based on the report. Continue working on the project until the final submission.\n6. Peer-review progress reports (due end of Week 13, Nov 20 2022)\nEach student will review the progress reports of ~1-2 groups and provide feedback/suggestions. \n7. Software code submission with documentation (, due middle of Week 16, Dec 08 2022)\nEach team must submit the software code produced for the project along with a written documentation. The documentation should consist of the following elements: 1) An overview of the function of the code (i.e., what it does and what it can be used for). 2) Documentation of how the software is implemented with sufficient detail so that others can have a basic understanding of your code for future extension or any further improvement. 3) Documentation of the usage of the software including either documentation of usages of APIs or detailed instructions on how to install and run a software, whichever is applicable. 4) Brief description of contribution of each team member in case of a multi-person team. Note that if you are in a team, it is your responsibility to figure out how to contribute to your group project, so you will need to act proactively and in a timely manner if your group coordinator has not assigned a task to you. There will be no opportunity to make up for any task that you failed to accomplish. In general, all the members of a team will get the same grade for the project unless the documentation submission indicates that some member(s) only superficially participated in the project without doing much actual work; in that case, we will discount the grade. Everyone is expected to spend at least 20 hours to seriously work on your course project as a minimum, not including the time spent for preparing the documentation.\nThe  of the grade would be distributed as follows:  for source code submission;  for documentation submission. The  for the documentation submission includes  for overview of functions,  for implementation documentation,  for usage documentation. There is no strict length requirement for the documentation.\n8. Software usage tutorial presentation (, due middle of Week 16, Dec 08 2022)\nAt the end of the semester, every project team will be asked to submit a short tutorial presentation (e.g., a voiced ppt presentation) to explain how the developed software is to be used. The presentation must include (1) sufficient instructions on how to install the software if applicable, (2) sufficient instructions on how to use the software, and (3) at least one example of use case so as to allow a grader to use the provided use case to test the software. There is no strict length requirement for this video submission, but you should target at 5~10 minutes. A presentation shorter than 5 minutes is unlikely detailed enough to help users understand how to use the software, whereas a longer video than 10 minutes might be too long for impatient users. However, feel free to produce a longer presentation if needed.\nThe tutorial presentation would be graded based on\n1) completion of the presentation (); and\n2) result of testing the software by graders ().\nIf the software passes the test (i.e., is working as expected), full points will be given; otherwise, points will be deducted from the  allocated to the “result of testing the software by graders.”\n9. Peer-review project code, documentation, and presentations (due Dec 16 2022)\nEach student will review the final project submissions of ~2-3 groups and provide feedback/suggestions. \nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/supplement/ZcMrZ/week-8-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 8\nWeek 8 Overview\nPrevious\nNext\nWeek 8 Information\nReading:\nReading\nWeek 8 Overview\n. Duration: 10 minutes\n10 min\nWeek 8 Lessons\nWeek 8 Activities\nTechnology Review (4-credit students only)\nWeek 8 Overview\nDuring this week's lessons, you will learn more about word association mining with a particular focus on mining the other basic form of word association (i.e., syntagmatic relations), and start learning topic analysis with a focus on techniques for mining one topic from text.\nTime\nThis module should take approximately 3.5 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with required assignments in bold):\nActivity\nEstimated Time Required\nWeek 8 Video Lectures\n2 hours\nWeek 8 Graded Quiz\n1 hour\nMP2.4 Submission for Grading\n30 mins\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain some basic concepts of probability including entropy, conditional entropy and mutual information.\nExplain some ways of discovering syntagmatic and paradigmatic relations.\nExplain the basic idea of Bayesian estimation theory.\nGuiding Questions\nDevelop your answers to the following guiding questions while watching the video lectures throughout the week.\nWhat is entropy? For what kind of random variables does the entropy function reach its minimum and maximum, respectively? \nWhat is conditional entropy? \nWhat is the relation between conditional entropy H(X|Y) and entropy H(X)? Which is larger? \nHow can conditional entropy be used for discovering syntagmatic relations? \nWhat is mutual information I(X;Y)? How is it related to entropy H(X) and conditional entropy H(X|Y)? \nWhat’s the minimum value of I(X;Y)? Is it symmetric? \nFor what kind of X and Y, does mutual information I(X;Y) reach its minimum? For a given X, for what Y does I(X;Y) reach its maximum? \nWhy is mutual information sometimes more useful for discovering syntagmatic relations than conditional entropy? \nWhat is a topic? \nHow can we define the task of topic mining and analysis computationally? What’s the input? What’s the output? \nHow can we heuristically solve the problem of topic mining and analysis by treating a term as a topic? What are the main problems of such an approach? \nWhat are the benefits of representing a topic by a word distribution? \nWhat is a statistical language model? What is a unigram language model? How can we compute the probability of a sequence of words given a unigram language model? \nWhat is Maximum Likelihood estimate of a unigram language model given a text article? \nWhat is the basic idea of Bayesian estimation? What is a prior distribution? What is a posterior distribution? How are they related with each other? What is Bayes rule?  \nAdditional Readings and Resources\nThe following readings are optional:\nC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM and Morgan & Claypool Publishers, 2016. Chapters 13, 17.\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nEntropy\nConditional entropy\nMutual information\nTopic and coverage of topic\nLanguage model\nGenerative model\nUnigram language model\nWord distribution\nBackground language model\nParameters of a probabilistic model\nLikelihood\nBayes rule\nMaximum likelihood estimation\nPrior and posterior distributions\nBayesian estimation & inference\nMaximum a posteriori (MAP) estimate\nPrior model\nPosterior mode\nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/A1bUb/8-6-topic-mining-and-analysis-term-as-topicdict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.6 Topic Mining and Analysis: Term as Topic\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.6 Topic Mining and Analysis: Term as Topic\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:07\nThis lecture is about topic mining and analysis.\n0:12\nWe're going to talk about using a term as topic. This is a slide that you have seen in a earlier lecture where we define the task of topic mining and analysis. We also raised the question, how do we exactly define the topic of theta?\n0:31\nSo in this lecture, we're going to offer one way to define it, and that's our initial idea. Our idea here is defining a topic simply as a term.\n0:42\nA term can be a word or a phrase.\n0:45\nAnd in general, we can use these terms to describe topics. So our first thought is just to define a topic as one term. For example, we might have terms like sports, travel, or science, as you see here. Now if we define a topic in this way, we can then analyze the coverage of such topics in each document. Here for example, we might want to discover to what extent document one covers sports. And we found that  of the content of document one is about sports. And  is about the travel, etc. We might also discover document two does not cover sports at all. So the coverage is zero, etc.\n1:32\nSo now, of course, as we discussed in the task definition for topic mining and analysis, we have two tasks. One is to discover the topics. And the second is to analyze coverage. So let's first think about how we can discover topics if we represent each topic by a term. So that means we need to mine k topical terms from a collection.\n2:01\nNow there are, of course, many different ways of doing that.\n2:05\nAnd we're going to talk about a natural way of doing that, which is also likely effective. So first of all, we're going to parse the text data in the collection to obtain candidate terms. Here candidate terms can be words or phrases. Let's say the simplest solution is to just take each word as a term. These words then become candidate topics. Then we're going to design a scoring function to match how good each term is as a topic.\n2:35\nSo how can we design such a function? Well there are many things that we can consider. For example, we can use pure statistics to design such a scoring function.\n2:45\nIntuitively, we would like to favor representative terms, meaning terms that can represent a lot of content in the collection. So that would mean we want to favor a frequent term. However, if we simply use the frequency to design the scoring function, then the highest scored terms would be general terms or functional terms like the, etc. Those terms occur very frequently English.\n3:14\nSo we also want to avoid having such words on the top so we want to penalize such words. But in general, we would like to favor terms that are fairly frequent but not so frequent. So a particular approach could be based on TF-IDF weighting from retrieval.\n3:35\nAnd TF stands for term frequency. IDF stands for inverse document frequency. We talked about some of these ideas in the lectures about the discovery of word associations. So these are statistical methods, meaning that the function is defined mostly based on statistics. So the scoring function would be very general. It can be applied to any language, any text. But when we apply such a approach to a particular problem, we might also be able to leverage some domain-specific heuristics. For example, in news we might favor title words actually general. We might want to favor title words because the authors tend to use the title to describe the topic of an article.\n4:27\nIf we're dealing with tweets, we could also favor hashtags, which are invented to denote topics. So naturally, hashtags can be good candidates for representing topics.\n4:44\nAnyway, after we have this design scoring function, then we can discover the k topical terms by simply picking k terms with the highest scores. Now, of course, we might encounter situation where the highest scored terms are all very similar. They're semantically similar, or closely related, or even synonyms. So that's not desirable. So we also want to have coverage over all the content in the collection. So we would like to remove redundancy. And one way to do that is to do a greedy algorithm, which is sometimes called a maximal marginal relevance ranking. Basically, the idea is to go down the list based on our scoring function and gradually take terms to collect the k topical terms. The first term, of course, will be picked. When we pick the next term, we're going to look at what terms have already been picked and try to avoid picking a term that's too similar. So while we are considering the ranking of a term in the list, we are also considering the redundancy of the candidate term with respect to the terms that we already picked.\n5:58\nAnd with some thresholding, then we can get a balance of the redundancy removal and also high score of a term. Okay, so after this that will get k topical terms. And those can be regarded as the topics that we discovered from the connection. Next, let's think about how we're going to compute the topic coverage pi sub ij.\n6:23\nSo looking at this picture, we have sports, travel and science and these topics. And now suppose you are give a document. How should we pick out coverage of each topic in the document?\n6:36\nWell, one approach can be to simply count occurrences of these terms. So for example, sports might have occurred four times in this this document and travel occurred twice, etc. And then we can just normalize these counts as our estimate of the coverage probability for each topic. So in general, the formula would be to collect the counts of all the terms that represent the topics. And then simply normalize them so that the coverage of each topic in the document would add to one.\n7:15\nThis forms a distribution of the topics for the document to characterize coverage of different topics in the document. Now, as always, when we think about idea for solving problem, we have to ask the question, how good is this one? Or is this the best way of solving problem?\n7:38\nSo now let's examine this approach. In general, we have to do some empirical evaluation\n7:46\nby using actual data sets and to see how well it works.\n7:52\nWell, in this case let's take a look at a simple example here. And we have a text document that's about a NBA basketball game.\n8:04\nSo in terms of the content, it's about sports.\n8:08\nBut if we simply count these words that represent our topics, we will find that the word sports actually did not occur in the article, even though the content is about the sports.\n8:22\nSo the count of sports is zero. That means the coverage of sports would be estimated as zero. Now of course, the term science also did not occur in the document and it's estimate is also zero. And that's okay. But sports certainly is not okay because we know the content is about sports. So this estimate has problem.\n8:50\nWhat's worse, the term travel actually occurred in the document. So when we estimate the coverage of the topic travel, we have got a non-zero count. So its estimated coverage will be non-zero. So this obviously is also not desirable.\n9:08\nSo this simple example illustrates some problems of this approach. First, when we count what words belong to to the topic, we also need to consider related words. We can't simply just count the topic word sports. In this case, it did not occur at all. But there are many related words like basketball, game, etc. So we need to count the related words also. The second problem is that a word like star can be actually ambiguous. So here it probably means a basketball star, but we can imagine it might also mean a star on the sky. So in that case, the star might actually suggest, perhaps, a topic of science.\n9:54\nSo we need to deal with that as well. Finally, a main restriction of this approach is that we have only one term to describe the topic, so it cannot really describe complicated topics. For example, a very specialized topic in sports would be harder to describe by using just a word or one phrase. We need to use more words. So this example illustrates some general problems with this approach of treating a term as topic. First, it lacks expressive power. Meaning that it can only represent the simple general topics, but it cannot represent the complicated topics that might require more words to describe.\n10:37\nSecond, it's incomplete in vocabulary coverage, meaning that the topic itself is only represented as one term. It does not suggest what other terms are related to the topic. Even if we're talking about sports, there are many terms that are related. So it does not allow us to easily count related terms to order, conversion to coverage of this topic. Finally, there is this problem of word sense disintegration. A topical term or related term can be ambiguous. For example, basketball star versus star in the sky.\n11:10\nSo in the next lecture, we're going to talk about how to solve the problem with of a topic. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/deiXc/9-9-latent-dirichlet-allocation-lda-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 9\n9.9 Latent Dirichlet Allocation (LDA): Part 1\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.9 Latent Dirichlet Allocation (LDA): Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nAlbanian\nArabic\nBengali\nBulgarian\nCatalan\nChinese (Traditional)\nCroatian\nCzech\nDutch\nEnglish\nEstonian\nFrench\nGerman\nGreek\nHebrew\nHindi\nHungarian\nIndonesian\nItalian\nJapanese\nKannada\nKazakh\nKorean\nLithuanian\nMacedonian (FYROM)\nMalay\nMongolian\nNepali\nPersian\nPolish\nPortuguese (Brazilian)\nPortuguese (European)\nRomanian\nRussian\nSerbian\nSlovak\nSpanish\nSwedish\nTagalog\nTamil\nTelugu\nThai\nTurkish\nUkrainian\nUrdu\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:07\nThis lecture is about that Latent Dirichlet Allocation or LDA. In this lecture, we are going to continue talking about topic models. In particular, we are going to talk about some extension of PLSA, and one of them is LDA or Latent Dirichlet Allocation. So the plan for this lecture is to cover two things. One is to extend the PLSA with prior knowledge and that would allow us to have in some sense a user-controlled PLSA, so it doesn't apply to they just listen to data, but also would listen to our needs. The second is to extend the PLSA as a generative model, a fully generative model. This has led to the development of Latent Dirichlet Allocation or LDA. So first, let's talk about the PLSA with prior knowledge. Now in practice, when we apply PLSA to analyze text data, we might have additional knowledge that we want to inject to guide the analysis. The standard PLSA is going to blindly listen to the data by using maximum [inaudible]. We are going to just fit data as much as we can and get some insight about data. This is also very useful, but sometimes a user might have some expectations about which topics to analyze. For example, we might expect to see retrieval models as a topic in information retrieval or we also may be interesting in certain aspects, such as battery and memory, when looking at opinions about a laptop because the user is particularly interested in these aspects. A user may also have knowledge about topic coverage and we may know which topic is definitely not covering which document or is covering the document. For example, we might have seen those tags, topic tags assigned to documents. And those tags could be treated as topics. If we do that then a document account will be generated using topics corresponding to the tags already assigned to the document. If the document is not assigned a tag, we're going to say there is no way for using that topic to generate document. The document must be generated by using the topics corresponding to that assigned tags. So question is how can we incorporate such knowledge into PLSA. It turns out that there is a very elegant way of doing that and that would incorporate such knowledge as priors on the models. And you may recall in Bayesian inference, we use prior together with data to estimate parameters and this is precisely what would happen. So in this case, we can use maximum a posteriori estimate also called MAP estimate and the formula is given here. Basically, this is to maximize the posteriori distribution probability. And this is a combination of the likelihood of data and the prior. So what would happen is that we are going to have an estimate that listens to the data and also listens to our prior preferences. We can use this prior which is denoted as p of lambda to encode all kinds of preferences and the constraints. So for example, we can use this to encode the need of having precise background of the topic. Now this could be encoded as a prior because we can say the prior for the parameters is only a non-zero if the parameters contain one topic that is equivalent to the background language model. In other words, in other cases if it is not like that, we are going to say the prior says it is impossible. So the probability of that kind of models I think would be zero according to our prior. So now we can also for example use the prior to force particular choice of topic to have a probability of a certain number. For example, we can force document D to choose topic one with probability of one half or we can prevent topic from being used in generating document. So we can say the third topic should not be used in generating document D, we will set to the Pi zero for that topic. We can also use the prior to favor a set of parameters with topics that assign high probability to some particular words. In this case, we are not going to say it is impossible but we can just strongly favor certain kind of distributions and you will see example later. The MAP can be computed using a similar EM algorithm as we have used for the maximum likelihood estimate. With just some modifications, most of the parameters would reflect the prior preferences and in such an estimate if we use a special form of the prior code or conjugate the prior, then the functional form of the prior will be similar to the data. As a result, we can combine the two and the consequence is that you can basically convert the inference of the prior into the inference of having additional pseudo data because the two functional forms are the same and they can be combined. So the effect is as if we had more data and this is convenient for computation. It does not mean conjugate prior is the best way to define prior. So now let us look at the specific example. Suppose the user is particularly interested in battery life of a laptop and we are analyzing reviews. So the prior says that the distribution should contain one distribution that would assign high probability to battery and life. So we could say well there is distribution that is kind of concentrated on battery life and prior says that one of distributions should be very similar to this. Now if we use MAP estimate with conjugate prior, which is the original prior, the original distribution based on this preference, then the only difference in the EM is that when we re-estimate words distributions, we are going to add additional counts to reflect our prior. So here you can see the pseudo counts are defined based on the probability of words in a prior. So battery obviously would have high pseudo counts and similarly life would have also high pseudo counts. All the other words would have zero pseudo counts because their probability is zero in the prior and we see this is also controlled by a parameter mu and we are going to add a mu much by the probability of W given prior distribution to the connected accounts when we re-estimate this word distribution. So this is the only step that is changed and the change is happening here. And before we just connect the counts of words that we believe have been generated from this topic but now we force this distribution to give more probabilities to these words by adding them to the pseudo counts. So in fact we artificially inflated their probabilities. To make this distribution, we also need to add this many pseudo counts to the denominator. This is total sum of all the pseudo counts we have added for all the words This would make this a gamma distribution. Now this is intuitively very reasonable way of modifying EM and theoretically speaking, this works and it computes the MAP estimate. It is useful to think about the two specific extreme cases of mu. Now, [inaudible] the picture. Think about what would happen if we set mu to zero. Well that essentially to remove this prior. So mu in some sense indicates our strengths on prior. Now what would happen if we set mu to positive infinity. Well that is to say that prior is so strong that we are not going to listen to the data at all. So in the end, you see in this case we are going to make one of the distributions fixed to the prior. You see why? When mu is infinitive, we basically let this one dominate. In fact we are going to set this one to precise this distribution. So in this case, it is this distribution. And that is why we said the background language model is in fact a way to impose the prior because it would force one distribution to be exactly the same as what we give, that is background distribution. So in this case, we can even force the distribution to entirely focus on battery life. But of course this would not work well because it cannot attract other words. It would affect the accuracy of counting topics about battery life. So in practice, mu is set somewhere in between of course. So this is one way to impose a prior. We can also impose some other constraints. For example, we can set any parameters that will constantly include zero as needed. For example, we may want to set one of the Pi's to zero and this would mean we do not allow that topic to participate in generating that document. And this is only reasonable of course when we have prior analogy that strongly suggests this.\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/o8WNd/lesson-1-5-vector-space-model-basic-ideadict_values(["List\nCS 410: Text Information Systems\nWeek 1\nLesson 1.5: Vector Space Model - Basic Idea\nPrevious\nNext\nOrientation Information\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nVideo:\nVideo\nLesson 1.1: Natural Language Content Analysis\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 1.2: Text Access\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.3: Text Retrieval Problem\n. Duration: 26 minutes\n26 min\nVideo:\nVideo\nLesson 1.4: Overview of Text Retrieval Methods\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 1.5: Vector Space Model - Basic Idea\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\n. Duration: 17 minutes\n17 min\nWeek 1 Activities\nLesson 1.5: Vector Space Model - Basic Idea\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nAlbanian\nArabic\nEnglish\nEstonian\nFrench\nGerman\nItalian\nJapanese\nKazakh\nKorean\nLithuanian\nMongolian\nNepali\nPortuguese (European)\nRomanian\nRussian\nSerbian\nSlovak\nSpanish\nSwedish\nTamil\nTelugu\nThai\nTurkish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the vector space retrieval model. We're going to give an introduction to its basic idea.\n0:18\nIn the last lecture, we talked about the different ways of designing a retrieval model, which would give us a different arranging function.\n0:30\nIn this lecture, we're going to talk about a specific way of designing a ramping function called a vector space retrieval model.\n0:37\nAnd we're going to give a brief introduction to the basic idea.\n0:44\nVector space model is a special case of similarity based models as we discussed before. Which means we assume relevance is roughly similarity, between the document and the query.\n1:02\nNow whether is this assumption is true is actually a question. But in order to solve the search problem, we have to convert the vague notion of relevance into a more precise definition that can be implemented with the program analogy. So in this process, we have to make a number of assumptions. This is the first assumption that we make here. Basically, we assume that if a document is more similar to a query than another document. Then the first document will be assumed it will be more relevant than the second one. And this is the basis for ranking documents in this approach.\n1:46\nAgain, it's questionable whether this is really the best definition for randoms. As we will see later there are other ways to model randoms.\n1:58\nThe basic idea of vectors for base retrieval model is actually very easy to understand. Imagine a high dimensional space where each dimension corresponds to a term.\n2:11\nSo here I issue a three dimensional space with three words, programming, library and presidential. So each term here defines one dimension.\n2:24\nNow we can consider vectors in this, three dimensional space. And we're going to assume that all our documents and the query will be placed in this vector space. So for example, on document might be represented by this vector, d1. Now this means this document probably covers library and presidential, but it doesn't really talk about programming. What does this mean in terms of representation of document? That just means we're going to look at our document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is only the vector root condition of the document.\n3:14\nOf course, the document has all information. For example, the orders of words are [INAUDIBLE] model and that's because we assume that the [INAUDIBLE] of words will [INAUDIBLE]. So with this presentation you can really see d1 simply suggests a [INAUDIBLE] library. Now this is different from another document which might be recommended as a different vector, d2 here. Now in this case, the document that covers programming and library, but it doesn't talk about presidential. So what does this remind you? Well you can probably guess the topic is likely about program language and the library is software lab library.\n3:58\nSo this shows that by using this vector space reproduction, we can actually capture the differences between topics of documents.\n4:09\nNow you can also imagine there are other vectors. For example, d3 is pointing into that direction, that might be a presidential program. And in fact we can place all the documents in this vector space. And they will be pointing to all kinds of directions. And similarly, we're going to place our query also in this space, as another vector.\n4:32\nAnd then we're going to measure the similarity between the query vector and every document vector. So in this case for example, we can easily see d2 seems to be the closest to this query vector. And therefore, d2 will be rendered above others.\n4:51\nSo this is basically the main idea of the vector space model.\n4:58\nSo to be more precise, vector space model is a framework. In this framework, we make the following assumptions. First, we represent a document and query by a term vector.\n5:18\nSo here a term can be any basic concept. For example, a word or a phrase or even n gram of characters. Those are just sequence of characters inside a word.\n5:34\nEach term is assumed that will be defined by one dimension. Therefore n terms in our vocabulary, we define N-dimensional space.\n5:44\nA query vector would consist of a number of elements\n5:49\ncorresponding to the weights on different terms.\n5:56\nEach document vector is also similar. It has a number of elements and each value of each element is indicating the weight of the corresponding term. Here, you can see, we assume there are N dimensions. Therefore, they are N elements\n6:15\neach corresponding to the weight on the particular term.\n6:21\nSo the relevance in this case will be assumed to be the similarity between the two vectors.\n6:29\nTherefore, our ranking function is also defined as the similarity between the query vector and document vector.\n6:37\nNow if I ask you to write a program to implement this approach in a search engine.\n6:44\nYou would realize that this was far from clear. We haven't said a lot of things in detail, therefore it's impossible to actually write the program to implement this. That's why I said, this is a framework.\n6:59\nAnd this has to be refined in order to actually\n7:04\nsuggest a particular ranking function that you can implement on a computer.\n7:10\nSo what does this framework not say? Well, it actually hasn't said many things that would be required in order to implement this function.\n7:24\nFirst, it did not say how we should define or select the basic concepts exactly.\n7:32\nWe clearly assume the concepts are orthogonal. Otherwise, there will be redundancy. For example, if two synonyms or somehow distinguish it as two different concepts. Then they would be defining two different dimensions and that would clearly cause redundancy here. Or all the emphasizing of matching this concept, because it would be as if you match the two dimensions when you actually matched one semantic concept.\n8:11\nSecondly, it did not say how we exactly should place documents and the query in this space. Basically that show you some examples of query and document vectors. But where exactly should the vector for a particular document point to?\n8:29\nSo this is equivalent to how to define the term weights? How do you compute the lose element values in those vectors? This is a very important question, because term weight in the query vector indicates the importance of term.\n8:48\nSo depending on how you assign the weight, you might prefer some terms to be matched over others.\n8:56\nSimilarly, the total word in the document is also very meaningful. It indicates how well the term characterizes the document. If you got it wrong then you clearly don't represent this document accurately.\n9:10\nFinally, how to define the similarity measure is also not given. So these questions must be addressed before we can have a operational function that we can actually implement using a program language.\n9:25\nSo how do we solve these problems is the main topic of the next lecture. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/FKWgc/technology-review-informationdict_values(['List\nCS 410: Text Information Systems\nWeek 1\nTechnology Review Information\nPrevious\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nTechnology Review Information\nCS410 Technology Review  (4-credit students only)\n  CS410 Technology Review\nThe Technology Review assignment is designed to provide students with an opportunity to go beyond the materials covered in the course lectures to learn about an interesting course-related cutting-edge technology topic not covered in any lecture. In this assignment, a student is required to write a short review article on a chosen topic by the student from a list of suggested topics by the instructor and TAs. A student can also propose a topic not on the list subject to the approval of the instructor.\nThe topic of a technology review can be one of the three broad topic categories related to the general topic of “text data retrieval and analysis”:   \n1) Useful software toolkits for processing text data or building text data applications. \n2) Emerging new applications of text retrieval or analysis. \n3) New techniques for text retrieval or analysis. \nA list of specific topics will be provided for students to choose, but students may also propose additional topics interesting to them subject to the approval of the instructor. A review may cover one toolkit/application/technique in-depth or compare multiple toolkits/applications/techniques. The former is only allowed if the toolkit/application/technique is sufficiently complex to justify devoting an entire review to it. In any case, your review must have novel content that does not exist in any existing literature or webpages so that it would offer unique information/knowledge that others can learn from your review. So please make sure to check whether there is already a review on the topic before you devote time to complete a review. If you find an existing review on the topic, you may still write about the topic, but just need to make sure that you take a somewhat different perspective than the existing review or add new content on top of the existing review (i.e., extending it in some way).\nThe Technology Review should be completed individually. It will be graded based on completion of the following two tasks\n1. Topic proposal: Every student is required to select a topic from a provided topic list or propose a topic by the end of Week 9 (Oct 23, 2021) in the signup sheet (access with illinois.edu email address): \nhttps://docs.google.com/spreadsheets/d/1hWAyxd82FcitN9eG3yMW6ckq6l1VASBtYWpaPbywMPc/edit?usp=sharing\nSome sample topics are provided here: https://docs.google.com/spreadsheets/d/1yeKm8hJbyRGhiUDvZv9-S3Zzu5hDtET-O6Yeci-VPOs/edit?usp=sharing   \n2. Review submission: Each student is required to submit a complete Technology Review by the end of Week 11 (Nov 6, 2021).  The review must have a coherent storyline (Intro, Body, Conclusion) and cite relevant references. It must be at least ~2 pages   \nThe deadline is set to an earlier time than that of course project code submission so as to give the students an opportunity to read some relevant reviews (especially those on toolkits) before finishing their projects if they want to. \n      Mark as completed\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/lecture/uGa00/lesson-3-5-evaluation-of-tr-systems-multi-level-judgementsdict_values(["List\nCS 410: Text Information Systems\nWeek 3\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\nPrevious\nNext\nWeek 3 Information\nWeek 3 Lessons\nVideo:\nVideo\nLesson 3.1: Evaluation of TR Systems\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.2: Evaluation of TR Systems - Basic Measures\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.6: Evaluation of TR Systems - Practical Issues\n. Duration: 15 minutes\n15 min\nWeek 3 Activities\nProgramming Assignment 2.1\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:07\nThis lecture is about how to evaluate the text retrieval system when we have multiple levels of judgements. In this lecture, we will continue the discussion of evaluation. We're going to look at how to evaluate a text retrieval system, when we have multiple levels of judgements.\n0:27\nSo far we have talked about the binary judgements, that means a document is judged as being relevant or not relevant.\n0:35\nBut earlier, we also talk about the relevance as a medal of degrees. So we often can distinguish very high relevant documents, those are very useful documents, from moderately relevant documents. They are okay, they are useful perhaps. And further from now, we're adding the documents, those are not useful.\n0:57\nSo imagine you can have ratings for these pages. Then, you would have multiple levels of ratings. For example, here I show example of three levels, 3 for relevant, sorry 3 for very relevant, 2 for marginally relevant, and 1 for non-relevant. Now, how do we evaluate the search engine system using these judgements? Obvious that the map doesn't work, average of precision doesn't work, precision, and recall doesn't work, because they rely on binary judgements. So let's look at some top ranked results when using these judgements. Imagine the user would be mostly care about the top ten results here.\n1:43\nAnd we marked the rating levels, or relevance levels, for these documents as shown here, 3, 2, 1, 1, 3, etcetera. And we call these gain. And the reason why we call it the gain is because the measure that we are infusing is called the NDCG normalized or accumulated gain.\n2:10\nSo this gain, basically, can measure how much a gain of random information a user can obtain by looking at each document, right? So looking at the first document, the user can gain 3 points. Looking at the non-relevant document user would only gain 1 point.\n2:29\nLooking at the moderator or marginally relevant, document the user would get 2 points, etcetera. So, this gain to each of the measures is a utility of the document from a user's perspective. Of course, if we assume the user stops at the 10 documents and we're looking at the cutoff at 10, we can look at the total gain of the user. And what's that? Well, that's simply the sum of these, and we call it the Cumulative Gain. So if the user stops after the position 1, that's just a 3. If the user looks at another document, that's a 3+2. If the user looks at the more documents, then the cumulative gain is more. Of course this is at the cost of spending more time to examine the list. So cumulative gain gives us some idea about how much total gain the user would have if the user examines all these documents. Now, in NDCG, we also have another letter here, D, discounted cumulative gain.\n3:29\nSo, why do we want to do discounting? Well, if you look at this cumulative gain, there is one deficiency, which is it did not consider the rank position of these documents. So for example, looking at this sum here, and we only know there is 1 highly relevant document, 1 marginally relevant document, 2 non-relevant documents. We don't really care where they are ranked. Ideally, we want these two to be ranked on the top which is the case here.\n4:03\nBut how can we capture that intuition? Well we have to say, well this is 3 here is not as good as this 3 on the top. And that means the contribution of the gain from different positions has to be weighted by their position. And this is the idea of discounting, basically. So we're going to to say, well, the first one does not need to be discounted because the user can be assumed that will always see this document. But the second one, this one will be discounted a little bit because there's a small possibility that the user wouldn't notice it. So we divide this gain by a weight based on the position. So log of 2, 2 is the rank position of this document. And when we go to the third position, we discounted even more, because the normalizer is log of 3, and so on and so forth. So when we take such a sum that a lower ranked document would not contribute that much as a highly ranked document. So that means if you, for example, switch the position of this, let's say this position, and this one, and then you would get more discount if you put, for example very relevant document here as opposed to here. Imagine if you put the 3 here, then it would have to be discounted. So it's not as good as if you we would put the 3 here. So this is the idea of discounting.\n5:37\nOkay, so now at this point that we have got a discounted cumulative gain for measuring the utility of this ranked list with multiple levels of judgements.\n5:51\nSo are we happy with this? Well, we can use this to rank systems. Now, we still need to do a little bit more in order to make this measure comparable across different topics. And this is the last step, and by the way, here we just show the DCG at 10, so this is the total sum of DCG, all these 10 documents. So the last step is called N, normalization. And if we do that, then we'll get the normalized DCG. So how do we do that? Well, the idea here is we're going to normalize DCG by the ideal DCG at the same cutoff. What is the ideal DCG? Well, this is the DCG of an ideal ranking. So imagine if we have 9 documents in the whole collection rated 3 here. And that means in total we have 9 documents rated 3.\n6:53\nThen our ideal rank lister would have put all these 9 documents on the very top. So all these would have to be 3 and then this would be followed by a 2 here. Because that's the best we could do after we have run out of the 3. But all these positions would be 3. Right? So this would our ideal ranked list.\n7:18\nAnd then we had computed the DCG for this ideal rank list.\n7:23\nSo this would be given by this formula that you see here. And so this ideal DCG would then be used as the normalizer DCG. So here. And this idea of DCG would be used as a normalizer. So you can imagine now, normalization essentially is to compare the actual DCG with the best DCG you can possibly get for this topic. Now why do we want to do this? Well, by doing this we'll map the DCG values into a range of 0 through 1.\n7:57\nSo the best value, or the highest value, for every query would be 1. That's when your rank list is, in fact, the ideal list but otherwise, in general, you will be lower than one.\n8:13\nNow, what if we don't do that? Well, you can see, this transformation, or this normalization, doesn't really affect the relative comparison of systems for just one topic, because this ideal DCG is the same for all the systems, so the ranking of systems based on only DCG would be exactly the same as if you rank them based on the normalized DCG. The difference however is when we have multiple topics. Because if we don't do normalization, different topics will have different scales of DCG.\n8:46\nFor a topic like this one, we have 9 highly relevant documents, the DCG can get really high, but imagine in another case, there are only two very relevant documents in total in the whole collection. Then the highest DCG that any system could achieve for such a topic would not be very high. So again, we face the problem of different scales of DCG values. When we take an average, we don't want the average to be dominated by those high values. Those are, again, easy queries. So, by doing the normalization, we can have avoided the problem, making all the queries contribute to equal to the average. So, this is a idea of NDCG, it's used for measuring a rank list based on multiple level of relevance judgements.\n9:42\nIn a more general way this is basically a measure that can be applied to any ranked task with multiple level of judgements. And the scale of the judgements can be multiple, can be more than binary not only more than binary they can be much multiple levels like 1, 0, 5 or even more depending on your application. And the main idea of this measure, just to summarize, is to measure the total utility of the top k documents. So you always choose a cutoff and then you measure the total utility. And it would discount the contribution from a lowly ranked document. And then finally, it would do normalization to ensure comparability across queries. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/ai3kj/8-7-topic-mining-and-analysis-probabilistic-topic-modelsdict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:06\nThis lecture is about Probabilistic Topic Models for topic mining and analysis.\n0:13\nIn this lecture, we're going to continue talking about the topic mining and analysis.\n0:18\nWe're going to introduce probabilistic topic models.\n0:22\nSo this is a slide that you have seen earlier, where we discussed the problems with using a term as a topic. So, to solve these problems intuitively we need to use more words to describe the topic. And this will address the problem of lack of expressive power. When we have more words that we can use to describe the topic, that we can describe complicated topics. To address the second problem we need to introduce weights on words. This is what allows you to distinguish subtle differences in topics, and to introduce semantically related words in a fuzzy manner. Finally, to solve the problem of word ambiguity, we need to split ambiguous word, so that we can disambiguate its topic.\n1:15\nIt turns out that all these can be done by using a probabilistic topic model. And that's why we're going to spend a lot of lectures to talk about this topic. So the basic idea here is that, improve the replantation of topic as one distribution. So what you see now is the older replantation. Where we replanted each topic, it was just one word, or one term, or one phrase. But now we're going to use a word distribution to describe the topic. So here you see that for sports. We're going to use the word distribution over theoretical speaking all the words in our vocabulary.\n1:54\nSo for example, the high probability words here are sports, game, basketball, football, play, star, etc. These are sports related terms. And of course it would also give a non-zero probability to some other word like Trouble which might be related to sports in general, not so much related to topic.\n2:18\nIn general we can imagine a non zero probability for all the words. And some words that are not read and would have very, very small probabilities. And these probabilities will sum to one.\n2:31\nSo that it forms a distribution of all the words.\n2:36\nNow intuitively, this distribution represents a topic in that if we assemble words from the distribution, we tended to see words that are ready to dispose.\n2:48\nYou can also see, as a very special case, if the probability of the mass is concentrated in entirely on just one word, it's sports. And this basically degenerates to the symbol foundation of a topic was just one word.\n3:04\nBut as a distribution, this topic of representation can, in general, involve many words to describe a topic and can model several differences in semantics of a topic. Similarly we can model Travel and Science with their respective distributions. In the distribution for Travel we see top words like attraction, trip, flight etc.\n3:31\nWhereas in Science we see scientist, spaceship, telescope, or genomics, and, you know, science related terms. Now that doesn't mean sports related terms will necessarily have zero probabilities for science. In general we can imagine all of these words we have now zero probabilities. It's just that for a particular topic in some words we have very, very small probabilities.\n3:58\nNow you can also see there are some words that are shared by these topics. When I say shared it just means even with some probability threshold, you can still see one word occurring much more topics. In this case I mark them in black. So you can see travel, for example, occurred in all the three topics here, but with different probabilities. It has the highest probability for the Travel topic, 0.05. But with much smaller probabilities for Sports and Science, which makes sense. And similarly, you can see a Star also occurred in Sports and Science with reasonably high probabilities. Because they might be actually related to the two topics. So with this replantation it addresses the three problems that I mentioned earlier. First, it now uses multiple words to describe a topic. So it allows us to describe a fairly complicated topics. Second, it assigns weights to terms. So now we can model several differences of semantics. And you can bring in related words together to model a topic. Third, because we have probabilities for the same word in different topics, we can disintegrate the sense of word. In the text to decode it's underlying topic, to address all these three problems with this new way of representing a topic. So now of course our problem definition has been refined just slightly. The slight is very similar to what you've seen before except we have added refinement for what our topic is. Now each topic is word distribution, and for each word distribution we know that all the probabilities should sum to one with all the words in the vocabulary. So you see a constraint here. And we still have another constraint on the topic coverage, namely pis. So all the Pi sub ij's must sum to one for the same document.\n5:59\nSo how do we solve this problem? Well, let's look at this problem as a computation problem. So we clearly specify it's input and output and illustrate it here on this side. Input of course is our text data. C is our collection but we also generally assume we know the number of topics, k. Or we hypothesize a number and then try to bind k topics, even though we don't know the exact topics that exist in the collection. And V is the vocabulary that has a set of words that determines what units would be treated as the basic units for analysis. In most cases we'll use words as the basis for analysis. And that means each word is a unique.\n6:47\nNow the output would consist of as first a set of topics represented by theta I's. Each theta I is a word distribution.\n6:56\nAnd we also want to know the coverage of topics in each document. So that's. That the same pi ijs that we have seen before.\n7:07\nSo given a set of text data we would like compute all these distributions and all these coverages as you have seen on this slide.\n7:18\nNow of course there may be many different ways of solving this problem. In theory, you can write the [INAUDIBLE] program to solve this problem, but here we're going to introduce a general way of solving this problem called a generative model. And this is, in fact, a very general idea and it's a principle way of using statistical modeling to solve text mining problems. And here I dimmed the picture that you have seen before in order to show the generation process. So the idea of this approach is actually to first design a model for our data. So we design a probabilistic model to model how the data are generated. Of course, this is based on our assumption. The actual data aren't necessarily generating this way. So that gave us a probability distribution of the data that you are seeing on this slide. Given a particular model and parameters that are denoted by lambda. So this template of actually consists of all the parameters that we're interested in. And these parameters in general will control the behavior of the probability risk model. Meaning that if you set these parameters with different values and it will give some data points higher probabilities than others. Now in this case of course, for our text mining problem or more precisely topic mining problem we have the following plans. First of all we have theta i's which is a word distribution snd then we have a set of pis for each document. And since we have n documents, so we have n sets of pis, and each set the pi up. The pi values will sum to one. So this is to say that we first would pretend we already have these word distributions and the coverage numbers. And then we can see how we can generate data by using such distributions. So how do we model the data in this way? And we assume that the data are actual symbols drawn from such a model that depends on these parameters. Now one interesting question here is to\n9:32\nthink about how many parameters are there in total? Now obviously we can already see n multiplied by K parameters. For pi's. We also see k theta i's. But each theta i is actually a set of probability values, right? It's a distribution of words. So I leave this as an exercise for you to figure out exactly how many parameters there are here. Now once we set up the model then we can fit the model to our data. Meaning that we can estimate the parameters or infer the parameters based on the data. In other words we would like to adjust these parameter values. Until we give our data set the maximum probability. I just said, depending on the parameter values, some data points will have higher probabilities than others. What we're interested in, here, is what parameter values will give our data set the highest probability? So I also illustrate the problem with a picture that you see here. On the X axis I just illustrate lambda, the parameters, as a one dimensional variable. It's oversimplification, obviously, but it suffices to show the idea. And the Y axis shows the probability of the data, observe. This probability obviously depends on this setting of lambda. So that's why it varies as you change the value of lambda. What we're interested here is to find the lambda star.\n11:05\nThat would maximize the probability of the observed data.\n11:10\nSo this would be, then, our estimate of the parameters. And these parameters, note that are precisely what we hoped to discover from text data. So we'd treat these parameters as actually the outcome or the output of the data mining algorithm. So this is the general idea of using a generative model for text mining. First, we design a model with some parameter values to fit the data as well as we can. After we have fit the data, we will recover some parameter value. We will use the specific parameter value And those would be the output of the algorithm. And we'll treat those as actually the discovered knowledge from text data. By varying the model of course we can discover different knowledge. So to summarize, we introduced a new way of representing topic, namely representing as word distribution and this has the advantage of using multiple words to describe a complicated topic.It also allow us to assign weights on words so we have more than several variations of semantics. We talked about the task of topic mining, and answers. When we define a topic as distribution. So the importer is a clashing of text articles and a number of topics and a vocabulary set and the output is a set of topics. Each is a word distribution and also the coverage of all the topics in each document. And these are formally represented by theta i's and pi i's. And we have two constraints here for these parameters. The first is the constraints on the worded distributions. In each worded distribution the probability of all the words must sum to 1, all the words in the vocabulary. The second constraint is on the topic coverage in each document. A document is not allowed to recover a topic outside of the set of topics that we are discovering. So, the coverage of each of these k topics would sum to one for a document. We also introduce a general idea of using a generative model for text mining. And the idea here is, first we're design a model to model the generation of data. We simply assume that they are generative in this way. And inside the model we embed some parameters that we're interested in denoted by lambda.\n13:36\nAnd then we can infer the most likely parameter values lambda star, given a particular data set.\n13:43\nAnd we can then take the lambda star as knowledge discovered from the text for our problem.\n13:50\nAnd we can adjust the design of the model and the parameters to discover various kinds of knowledge from text. As you will see later in the other lectures. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/9zE5i/11-6-opinion-mining-and-sentiment-analysis-sentiment-classificationdict_values(["List\nCS 410: Text Information Systems\nWeek 11\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nVideo:\nVideo\n11.1 Text Categorization: Discriminative Classifier Part 1\n. Duration: 20 minutes\n20 min\nVideo:\nVideo\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\n. Duration: 31 minutes\n31 min\nVideo:\nVideo\n11.3 Text Categorization: Evaluation Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n11.4 Text Categorization: Evaluation Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n11.5 Opinion Mining and Sentiment Analysis: Motivation\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[NOISE] This lecture is about the sentiment classification.\n0:11\nIf we assume that\n0:13\nmost of the elements in the opinion representation are all ready known, then our only task may be just a sentiment classification, as shown in this case. So suppose we know who's the opinion holder and what's the opinion target, and also know the content and the context of the opinion, then we mainly need to decide the opinion sentiment of the review. So this is a case of just using sentiment classification for understanding opinion.\n0:46\nSentiment classification can be defined more specifically as follows. The input is opinionated text object, the output is typically a sentiment label, or a sentiment tag, and that can be designed in two ways. One is polarity analysis, where we have categories such as positive, negative, or neutral.\n1:08\nThe other is emotion analysis that can go beyond a polarity to characterize the feeling of the opinion holder.\n1:21\nIn the case of polarity analysis, we sometimes also have numerical ratings as you often see in some reviews on the web.\n1:30\nFive might denote the most positive, and one maybe the most negative, for example. In general, you have just disk holder categories to characterize the sentiment.\n1:43\nIn emotion analysis, of course, there are also different ways for design the categories.\n1:49\nThe six most frequently used categories are happy, sad, fearful, angry, surprised, and disgusted.\n1:59\nSo as you can see, the task is essentially a classification task, or categorization task, as we've seen before, so it's a special case of text categorization. This also means any textual categorization method can be used to do sentiment classification.\n2:15\nNow of course if you just do that, the accuracy may not be good because sentiment classification does requires some improvement over regular text categorization technique, or simple text categorization technique. In particular, it needs two kind of improvements. One is to use more sophisticated features that may be more appropriate for sentiment tagging as I will discuss in a moment.\n2:41\nThe other is to consider the order of these categories, and especially in polarity analysis, it's very clear there's an order here, and so these categories are not all that independent. There's order among them, and so it's useful to consider the order. For example, we could use ordinal regression to do that, and that's something that we'll talk more about later. So now, let's talk about some features that are often very useful for text categorization and text mining in general, but some of them are especially also needed for sentiment analysis.\n3:18\nSo let's start from the simplest one, which is character n-grams. You can just have a sequence of characters as a unit, and they can be mixed with different n's, different lengths. All right, and this is a very general way and very robust way to represent the text data. And you could do that for any language, pretty much.\n3:42\nAnd this is also robust to spelling errors or recognition errors, right? So if you misspell a word by one character and this representation actually would allow you to match this word when it occurs in the text correctly. Right, so misspell the word and the correct form can be matched because they contain some common n-grams of characters. But of course such a recommendation would not be as discriminating as words.\n4:10\nSo next, we have word n-grams, a sequence of words and again, we can mix them with different n's. Unigram's are actually often very effective for a lot of text processing tasks, and it's mostly because words are word designed features by humans for communication, and so they are often good enough for many tasks. But it's not good, or not sufficient for sentiment analysis clearly. For example, we might see a sentence like, it's not good or it's not as good as something else, right? So in such a case if you just take a good and that would suggest positive that's not good, all right so it's not accurate. But if you take a bigram, not good together, and then it's more accurate. So longer n-grams are generally more discriminative, and they're more specific. If you match it, and it says a lot, and it's accurate it's unlikely, very ambiguous. But it may cause overfitting because with such very unique features that machine oriented program can easily pick up such features from the training set and to rely on such unique features to distinguish the categories. And obviously, that kind of classify, one would generalize word to future there when such discriminative features will not necessarily occur. So that's a problem of overfitting that's not desirable. We can also consider part of speech tag, n-grams if we can do part of speech tagging an, for example, adjective noun could form a pair. We can also mix n-grams of words and n-grams of part of speech tags. For example, the word great might be followed by a noun, and this could become a feature, a hybrid feature, that could be useful for sentiment analysis.\n6:06\nSo next we can also have word classes. So these classes can be syntactic like a part of speech tags, or could be semantic, and they might represent concepts in the thesaurus or ontology, like WordNet. Or they can be recognized the name entities, like people or place, and these categories can be used to enrich the presentation as additional features. We can also learn word clusters and parodically, for example, we've talked about the mining associations of words. And so we can have cluster of paradigmatically related words or syntaxmatically related words, and these clusters can be features to supplement the word base representation. Furthermore, we can also have frequent pattern syntax, and these could be frequent word set, the words that form the pattern do not necessarily occur together or next to each other. But we'll also have locations where the words my occur more closely together, and such patterns provide a more discriminative features than words obviously.\n7:14\nAnd they may also generalize better than just regular n-grams because they are frequent. So you expected them to occur also in tested data. So they have a lot of advantages, but they might still face the problem of overfeeding as the features become more complex. This is a problem in general, and the same is true for parse tree-based features, when you can use a parse tree to derive features such as frequent subtrees, or paths, and those are even more discriminating, but they're also are more likely to cause over fitting. And in general, pattern discovery algorithm's are very useful for feature construction because they allow us to search in a large space of possible features that are more complex than words that are sometimes useful. So in general, natural language processing is very important that they derive complex features, and they can enrich text representation. So for example, this is a simple sentence that I showed you a long time ago in another lecture. So from these words we can only derive simple word n-grams, representations or character n-grams. But with NLP, we can enrich the representation with a lot of other information such as part of speech tags, parse trees or entities, or even speech act. Now with such enriching information of course, then we can generate a lot of other features, more complex features like a mixed grams of a word and the part of speech tags, or even a part of a parse tree.\n8:55\nSo in general, feature design actually affects categorization accuracy significantly, and it's a very important part of any machine learning application. In general, I think it would be most effective if you can combine machine learning, error analysis, and domain knowledge in design features. So first you want to use the main knowledge, your understanding of the problem, the design seed features, and you can also define a basic feature space with a lot of possible features for the machine learning program to work on, and machine can be applied to select the most effective features or construct the new features. That's feature learning, and these features can then be further analyzed by humans through error analysis. And you can look at the categorization errors, and then further analyze what features can help you recover from those errors, or what features cause overfitting and cause those errors. And so this can lead into feature validation that will revised the feature set, and then you can iterate. And we might consider using a different features space.\n10:07\nSo NLP enriches text recognition as I just said, and because it enriches the feature space, it allows much larger such a space of features and there are also many, many more features that can be very useful for a lot of tasks. But be careful not to use a lot of category features because it can cause overfitting, or otherwise you would have to training careful not to let overflow happen. So a main challenge in design features, a common challenge is to optimize a trade off between exhaustivity and the specificity, and this trade off turns out to be very difficult. Now exhaustivity means we want the features to actually have high coverage of a lot of documents. And so in that sense, you want the features to be frequent. Specifity requires the feature to be discriminative, so naturally infrequent the features tend to be more discriminative. So this really cause a trade off between frequent versus infrequent features. And that's why a featured design is usually odd. And that's probably the most important part in machine learning any problem in particularly in our case, for text categoration or more specifically the senitment classification. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/3CisM/week-9-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 9\nWeek 9 Overview\nPrevious\nNext\nWeek 9 Information\nReading:\nReading\nWeek 9 Overview\n. Duration: 10 minutes\n10 min\nWeek 9 Lessons\nWeek 9 Activities\nWeek 9 Overview\nDuring this week's lessons, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture model, the basic topic model, Probabilistic Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA.\nTime\nThis module should take approximately 3.5 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with required assignments in bold):\nActivity\nEstimated Time Required\nWeek 9 Video Lectures\n2 hours\nWeek 9 Graded Quiz\n1 hour\nProject Proposal and Team Formation Submission for Grading\n30 mins\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain what a mixture of unigram language model is and why using a background language in a mixture can help “absorb” common words in English. \nExplain what PLSA is and how it can be used to mine and analyze topics in text. \nExplain the general idea of using a generative model for text mining. \nExplain how to compute the probability of observing a word from a mixture model like PLSA. \nExplain the basic idea of the EM algorithm and how it works. \nExplain the main difference between LDA and PLSA.  \nGuiding Questions\nDevelop your answers to the following guiding questions while watching the video lectures throughout the week.\nWhat is a mixture model? In general, how do you compute the probability of observing a particular word from a mixture model? What is the general form of the expression for this probability? \nWhat does the maximum likelihood estimate of the component word distributions of a mixture model behave like? In what sense do they “collaborate” and/or “compete”? Why can we use a fixed background word distribution to force a discovered topic word distribution to reduce its probability on the common (often non-content) words? \nWhat is the basic idea of the EM algorithm? What does the E-step typically do? What does the M-step typically do? In which of the two steps do we typically apply the Bayes rule? Does EM converge to a global maximum?\nWhat is PLSA? How many parameters does a PLSA model have? How is this number affected by the size of our data set to be mined? How can we adjust the standard PLSA to incorporate a prior on a topic word distribution? \nHow is LDA different from PLSA? What is shared by the two models?  \nAdditional Readings and Resources\nThe following readings are optional:\nC. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM and Morgan & Claypool Publishers, 2016. Chapter 17.\nBlei, D. 2012. Probabilistic Topic Models. Communications of the ACM 55 (4): 77–84. doi: 10.1145/2133806.2133826.\nQiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. Automatic Labeling of Multinomial Topic Models. Proceedings of ACM KDD 2007, pp. 490-499, DOI=10.1145/1281192.1281246.\nYue Lu, Qiaozhu Mei, and Chengxiang Zhai. 2011. Investigating task performance of probabilistic topic models: an empirical study of PLSA and LDA. Information Retrieval, 14, 2 (April 2011), 178-203. doi: 10.1007/s10791-010-9141-9.\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nMixture model\nComponent model\nConstraints on probabilities\nProbabilistic Latent Semantic Analysis (PLSA)\nExpectation-Maximization (EM) algorithm\nE-step and M-step\nHidden variables\nHill climbing\nLocal maximum\nLatent Dirichlet Allocation (LDA)\nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues.\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/FcCdt/10-1-text-clustering-motivationdict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.1 Text Clustering: Motivation\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.1 Text Clustering: Motivation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is the first one about the text clustering.\n0:14\nIn this lecture, we are going to talk about the text clustering.\n0:18\nThis is a very important technique for doing topic mining and analysis. In particular, in this lecture we're going to start with some basic questions about the clustering.\n0:31\nAnd that is, what is text clustering and why we are interested in text clustering.\n0:38\nIn the following lectures, we are going to talk about how to do text clustering. How to evaluate the clustering results?\n0:47\nSo what is text clustering?\n0:49\nWell, clustering actually is a very general technique for data mining as you might have learned in some other courses.\n0:56\nThe idea is to discover natural structures in the data.\n1:01\nIn another words, we want to group similar objects together. In our case, these objects are of course, text objects. For example, they can be documents, terms, passages, sentences, or websites, and then I'll go group similar text objects together. So let's see an example, well, here you don't really see text objects, but I just used some shapes to denote objects that can be grouped together.\n1:33\nNow if I ask you, what are some natural structures or natural groups where you, if you look at it and you might agree that we can group these objects based on chips, or their locations on this two dimensional space.\n1:53\nSo we got the three clusters in this case.\n1:56\nAnd they may not be so much this agreement about these three clusters but it really depends on the perspective to look at the objects.\n2:07\nMaybe some of you have also seen thing in a different way, so we might get different clusters. And you'll see another example about this ambiguity more clearly. But the main point of here is, the problem is actually not so well defined.\n2:29\nAnd the problem lies in how to define similarity. And what do you mean by similar objects?\n2:38\nNow this problem has to be clearly defined in order to have a well defined clustering problem.\n2:46\nAnd the problem is in general that any two objects can be similar depending on how you look at them. So for example, this will kept the two words like car and horse.\n3:00\nSo are the two words similar? Well, it depends on how if we look at the physical\n3:11\nproperties of car and horse they are very different but if you look at them functionally, a car and a horse, can both be transportation tools. So in that sense, they may be similar. So as we can see, it really depends on our perspective, to look at the objects.\n3:32\nAnd so it ought to make the clustering problem well defined. A user must define the perspective for assessing similarity.\n3:44\nAnd we call this perspective the clustering bias.\n3:49\nAnd when you define a clustering problem, it's important to specify\n3:55\nyour perspective for similarity or for defining the similarity that will be used to group similar objects. because otherwise, the similarity is not well defined and one can have different ways to group objects. So let's look at the example here. You are seeing some objects, or some shapes, that are very similar to what you have seen on the first slide, but if I ask you to group these objects, again, you might\n4:38\nfeel there is more than here than on the previous slide. For example, you might think, well, we can steer a group by ships, so that would give us cluster that looks like this. However, you might also feel that, well, maybe the objects can be grouped based on their sizes. So that would give us a different way to cluster the data if we look at the size and look at the similarity in size.\n5:12\nSo as you can see clearly here, depending on the perspective, we'll get different clustering result. So that also clearly tells us that in order to evaluate the clustering without, we must use perspective. Without perspective, it's very hard to define what is the best clustering result.\n5:36\nSo there are many examples of text clustering setup.\n5:42\nAnd so for example, we can cluster documents in the whole text collection. So in this case, documents are the units to be clustered.\n5:52\nWe may be able to cluster terms. In this case, terms are objects. And a cluster of terms can be used to define concept, or theme, or a topic. In fact, there's a topic models that you have seen some previous lectures can give you cluster of terms in some sense if you take terms with high probabilities from word distribution. Another example is just to cluster any text segments, for example, passages, sentences, or any segments that you can extract the former larger text objects.\n6:32\nFor example, we might extract the order text segments about the topic, let's say, by using a topic model. Now once we've got those text objects then we can\n6:45\ncluster the segments that we've got to discover interesting clusters that might also ripple in the subtopics. So this is a case of combining text clustering with some other techniques. And in general you will see a lot of text mining\n7:05\ncan be accurate combined in a flexible way to achieve the goal of doing more sophisticated mining and analysis of text data.\n7:16\nWe can also cluster fairly large text objects and by that, I just mean text objects may contain a lot of documents. So for example, we might cluster websites. Each website is actually compose of multiple documents. Similarly, we can also cluster articles written by the same author, for example. So we can trigger all the articles published by also as one unit for clustering. In this way, we might group authors together based on whether they're published papers or similar.\n7:55\nFor the more text clusters will be for the cluster to generate a hierarchy. That's because we can in general cluster any text object at different levels.\n8:08\nSo more generally why is text clustering interesting? Well, it's because it's a very useful technique for text mining, particularly exploratory text analysis.\n8:20\nAnd so a typical scenario is that you were getting a lot of text data, let's say all the email messages from customers in some time period, all the literature articles, etc. And then you hope to get a sense about what are the overall content of the connection, so for example, you might be interested in getting a sense about major topics, or what are some typical or representative documents in the connection. And clustering to help us achieve this goal. We sometimes also want to link a similar text objects together. And these objects might be duplicated content, for example. And in that case, such a technique can help us remove redundancy and remove duplicate documents.\n9:10\nSometimes they are about the same topic and by linking them together we can have more complete coverage of a topic.\n9:19\nWe may also used text clustering to create a structure on the text data and sometimes we can create a hierarchy of structures and this is very useful for problems.\n9:31\nWe may also use text clustering to induce additional features to represent text data when we cluster documents together, we can treat each cluster as a feature. And then we can say when a document is in this cluster and then the feature value would be one. And if a document is not in this cluster, then the feature value is zero. And this helps provide additional discrimination that might be used for text classification as we will discuss later.\n9:59\nSo there are, in general, many applications of text clustering. And I just thought of two very specific ones. One is to cluster search results, for example, [INAUDIBLE] search engine can cluster such results so that the user can see overall structure of the results of return the fall query. And when the query's ambiguous this is particularly useful because the clusters likely represent different senses of ambiguous word.\n10:28\nAnother application is to understand the major complaints from customers based on their emails, right. So in this case, we can cluster email messages and then find in the major clusters from there, we can understand what are the major complaints about them. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/RnXhr/lesson-2-3-doc-length-normalizationdict_values(["List\nCS 410: Text Information Systems\nWeek 2\nLesson 2.3: Doc Length Normalization\nPrevious\nNext\nWeek 2 Information\nWeek 2 Lessons\nVideo:\nVideo\nLesson 2.1: Vector Space Model - Improved Instantiation\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\nLesson 2.2: TF Transformation\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 2.3: Doc Length Normalization\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.4: Implementation of TR Systems\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 2.5: System Implementation - Inverted Index Construction\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.6: System Implementation - Fast Search\n. Duration: 17 minutes\n17 min\nWeek 2 Activities\nProgramming Assignment 1\nLesson 2.3: Doc Length Normalization\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:08\nThis lecture is about Document Length Normalization in the Vector Space Model. In this lecture, we will continue the discussion of the vector space model. In particular, we're going to discuss the issue of document length normalization.\n0:25\nSo far in the lectures about the vector space model, we have used the various signals from the document to assess the matching of the document with a query. In particular, we have considered the tone frequency. The count of a tone in a document. We have also considered it's global statistics such as, IDF, Inverse Document Frequency. But we have not considered document lengths.\n0:54\nSo here I show two example documents, d4 is much shorter with only 100 words.\n1:01\nD6 on the other hand, has a 5000 words. If you look at the matching of these query words, we see that in d6, there are more matchings of the query words. But one might reason that, d6 may have matched these query words in a scattered manner.\n1:24\nSo maybe the topic of d6, is not really about the topic of the query.\n1:31\nSo, the discussion of the campaign at the beginning of the document, may have nothing to do with the managing of presidential at the end.\n1:40\nIn general, if you think about the long documents, they would have a higher chance for matching any query. In fact, if you generate a long document randomly by assembling words from a distribution of words, then eventually you probably will match an inquiry.\n2:00\nSo in this sense, we should penalize on documents because they just naturally have better chance matching to any query, and this is idea of document normalization.\n2:12\nWe also need to be careful in avoiding to over penalize long documents.\n2:19\nOn the one hand, we want to penalize the long document. But on the other hand, we also don't want to over-penalize them. Now, the reasoning is because a document that may be long because of different reasons.\n2:32\nIn one case, the document may be long because it uses more words.\n2:38\nSo for example, think about the vortex article on the research paper. It would use more words than the corresponding abstract.\n2:49\nSo, this is a case where we probably should penalize the matching of\n2:54\nlong documents such as a full paper. When we compare the matching of words in such a long document with matching of the words in the shop abstract.\n3:07\nThen long papers in general, have a higher chance of matching clearer words, therefore, we should penalize them. However, there is another case when the document is long, and that is when the document simply has more content. Now consider another case of long document, where we simply concatenate a lot of abstracts of different papers. In such a case, obviously, we don't want to over-penalize such a long document. Indeed, we probably don't want to penalize such a document because it's long.\n3:39\nSo that's why, we need to be careful about using the right degree of penalization.\n3:48\nA method of that has been working well, based on recent results, is called a pivoted length normalization. And in this case, the idea is to use the average document length as a pivot, as a reference point. That means we'll assume that for the average length documents, the score is about right so the normalizer would be 1. But if the document is longer than the average document length,\n4:14\nthen there will be some penalization. Whereas if it's a shorter, then there is even some reward. So this is illustrated at using this slide, on the axis, x-axis you can see the length of document. On the y-axis, we show the normalizer. In this case, the Pivoted Length Normalization formula for the normalizer, is seeing to be interpolation of 1 and the normalize the document in length controlled by a parameter B here.\n4:53\nSo you can see here, when we first divide the length of the document by the average documents, this not only gives us some sense about how this document is compared with average documents, but also gives us a benefit of not worrying about the unit of length. We can measure the length by words or by characters.\n5:20\nAnyway, this normalizer has interesting property. First we see that, if we set the parameter b to 0 then the value would be 1. So, there's no lens normalization at all. So, b, in this sense, controls the lens normalization.\n5:39\nWhereas, if we set b to a nonzero value, then the normalizer would look like this. All right, so the value would be higher for documents that are longer than the average document lens.\n5:53\nWhereas, the value of the normalizer would be shorter, would be smaller for shorter documents. So in this sense, we see there is a penalization for long documents, and there's a reward for short documents.\n6:09\nThe degree of penalization is controlled by b, because if we set b to a larger value, then the normalizer would look like this. There's even more penalization for long documents and more reward for the short documents. By adjusting b, which varies from 0 to 1, we can control the degree of length normalization. So, if we plug in this length normalization fact that into the vector space model, ranking functions is that we have already examined them.\n6:41\nThen we will end up having the following formulas.\n6:46\nAnd these are in fact the state of the vector space model formulas. Let's take a look at each of them. The first one is called a pivoted length normalization vector space model, and a reference in [INAUDIBLE] duration of this model. And here we see that, it's basically a TFI model that we have discussed, the idea of component should be very familiar to you.\n7:18\nThere is also a query term frequency component here.\n7:24\nAnd then, in the middle, there is the normalizer tf and in this case, we see we use the double logarithm as we discussed before and this is to achieve a sublinear transformation. But we also put a document the length normalizer in the bottom. Right, so this would cause penalization for long document, because the larger the denominator is, then the smaller the is. And this is of course controlled by the parameter b here.\n8:01\nAnd you can see again, if b is set to 0 then there is no length normalization.\n8:08\nOkay, so this is one of the two most effective at these base model formulas. The next one called a BM25 or Okapi, is also similar in that it also has a IDF component here, and query IDF component here.\n8:32\nBut in the middle, the normal issue's a little bit different. As we explained, there is our copy tf transformation here, and that does sublinear transformation with the upper bound.\n8:48\nIn this case we have put the length normalization factor here. We're adjusting k but it achieves a similar factor, because we put a normalizer in the denominator. Therefore, again, if a document is longer then the term weight will be smaller.\n9:10\nSo you can see after we have gone through all the n answers that we talked about, and we have in the end reached the basically the state of god functions. So, So far, we have talked about mainly how to place the document vector in the vector space.\n9:35\nAnd, this has played an important role in determining the effectiveness of the simple function. But there are also other dimensions, where we did not really examine details. For example, can we further improve the instantiation of the dimension of the Vector Space Model? Now, we've just assumed that the bag of words representation should issue dimension as a word but obviously, we can see there are many other choices. For example, a stemmed word, those are the words that haven't transformed into the same root form, so that computation and computing were all become the same and they can be match. We get those stop word removal. This is to remove some very common words that don't carry any content like the off.\n10:26\nWe get use of phrases to define dimensions. We can even use later in the semantical analysis, it will find some clusters of words that represent the a late in the concept as one by an engine.\n10:39\nWe can also use smaller unit, like a character end grams those are sequences of and the characters for dimensions.\n10:50\nHowever, in practice, people have found that the bag-of-words representation with phrases is still the most effective one and it's also efficient. So, this is still so far the most popular dimension instantiation method.\n11:10\nAnd it's used in all major search engines.\n11:13\nI should also mention, that sometimes we need to do language specific and domain specific tokenization. And this is actually very important, as we might have variations of terms that might prevent us from matching them with each other, even when they mean the same thing. In some languages like Chinese, there is also the challenge in segmenting\n11:40\ntext to obtain word band rates because it's just a sequence of characters. A word might correspond to one character or two characters or even three characters. So, it's easier in English when we have a space to separate the words. In some other languages, we may need to do some Americanize processing to figure a way out of what are the boundaries for words. There is also the possibility to improve the similarity of the function. And so far we have used as a top product, but one can imagine there are other measures. For example, we can measure the cosine of the angle between two vectors. Or we can use Euclidean distance measure.\n12:24\nAnd these are all possible, but dot product seems still the best and one reason is because it's very general.\n12:33\nIn fact that it's sufficiently general, if you consider the possibilities of doing waiting in different ways.\n12:44\nSo, for example, cosine measure can be thought of as the thought product of two normalized factors. That means, we first normalize each factor and then we take the thought product. That would be critical to the cosine measure. I just mentioned that the BM25, seems to be one of the most effective formulas.\n13:04\nBut there has been also further developments in improving BM25. Although, none of these words have changed the BM25 fundamental. So in one line work, people have divide the BM25 F. Here, F stands for field, and this is use BM25 for documents with structures. So for example, you might consider a title field, the abstract, or body of the research article. Or even anchor text on the web page, those are the text fields that describe links to other pages and these can all be combined with a proper way of different fields to help improve scoring for different documents. When we use BM25 for such a document and the obvious choice is to apply BM25 for each field and then combine the scores. Basically, the idea of BM25F is to first combine the frequency counts of terms in all the fields, and then apply BM25. Now, this has advantage of avoiding over counting the first occurrence of the term. Remember in the sublinear transformation of TF, the first occurrence is very important and it contributes a large weight. And if we do that for all the fields, then the same term might have gained a lot of advantage in every field. But when we combine these word frequencies together, we just do the transformation one time. At that time, then the extra occurrences will not be counted as fresh first recurrences.\n14:48\nAnd this method has been working very well for scoring structure with documents.\n14:55\nThe other line of extension is called a BM25+. In this line, risk is to have to address the problem of over penalization of long documents by BM25.\n15:08\nSo to address this problem, the fix is actually quite simple. We can simply add a small constant to the TF normalization formula. But what's interesting is that, we can analytically prove that by doing such a small modification, we will fix the problem of over penalization of law documents by the original BM25. So the new formula called BM25+, is empirically and analytically shown to be better than BM25.\n15:42\nSo to summarize all what we have said about vector space model, here are the major take away points. First, in such a model, we use the similarity of relevance. Assuming that relevance of a document with respect to a query, is\n16:02\nbasically proportional to the similarity between the query and the document. So naturally, that implies that the query and document must have been represented in the same way. And in this case, we will present them as vectors in high-dimensional vector space. Where the dimensions are defined by words, or concepts, or terms, in general.\n16:25\nAnd we generally, need to use a lot of heuristics to design the ranking function. We use some examples, which show the needs for several heuristics, including Tf weighting and transformation.\n16:38\nAnd IDF weighting, and document length normalization. These major heuristics are the most important of heuristics, to ensure such a general ranking function to work well for all kinds of test. And finally, BM25 and pivoted normalization seem to be the most effective formulas out of the vector space model. Now I have to say that, I put BM25 in the category of vector space model, but in fact, the BM25 has been derived using probabilistic model.\n17:11\nSo the reason why I've put it in the vector space model is first, the ranking function actually has a nice interpretation in the vector space model. We can easily see, it looks very much like a vector space model, with a special waiting function.\n17:28\nThe second reason is because the original BM25, has somewhat different form of IDF.\n17:36\nAnd that form of IDF after the [INAUDIBLE] doesn't work so well as the standard IDF that you have seen here. So as effective retrieval function, BM25 should probably use a heuristic modification of the IDF. To make them even more look like a vector space model\n17:59\nThere are some additional readings. The first is, a paper about the pivoted length normalization. It's an excellent example of using empirical data analysis to suggest the need for length normalization and then further derive the length normalization formula. The second, is the original paper where the BM25 was proposed.\n18:24\nThe third paper, has a thorough discussion of BM25 and its extensions, particularly BM25 F.\n18:32\nAnd finally, in the last paper has a discussion of improving BM25 to correct the over penalization of long documents. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/f4CYl/lesson-4-4-statistical-language-model-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 4\nLesson 4.4: Statistical Language Model - Part 1\nPrevious\nNext\nWeek 4 Information\nWeek 4 Lessons\nVideo:\nVideo\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.2: Statistical Language Model\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 4.3: Query Likelihood Retrieval Function\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.4: Statistical Language Model - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.5: Statistical Language Model - Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.6: Smoothing Methods - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.7: Smoothing Methods - Part 2\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nProgramming Assignment 2.2\nLesson 4.4: Statistical Language Model - Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about smoothing of language models.\n0:11\nIn this lecture, we're going to continue talking about the probabilistic retrieval model. In particular, we're going to talk about the smoothing of language model in the query likelihood retrieval method.\n0:23\nSo you have seen this slide from a previous lecture. This is the ranking function based on the query likelihood.\n0:32\nHere, we assume that the independence of generating each query word And the formula would look like the following where we take a sum of all the query words. And inside the sum there is a log of probability of a word given by the document or document image model. So the main task now is to estimate this document language model as we said before different methods for estimating this model would lead to different retrieval functions. So in this lecture, we're going to be looking to this in more detail. So how do we estimate this language model? Well the obvious choice would be the maximum likelihood estimate that we have seen before. And that is we're going to normalize the word frequencies in the document.\n1:24\nAnd estimate the probability it would look like this.\n1:30\nThis is a step function here.\n1:35\nWhich means all of the words that have the same frequency count will have identical problem with it. This is another freedom to count, that has different probability. Note that for words that have not occurred in the document here\n1:52\nthey will have 0 probability. So we know this is just like the model that we assume earlier in the lecture. Where we assume that the use of the simple word from the document to a formula to clear it.\n2:09\nAnd there's no chance of assembling any word that's not in the document and we know that's not good.\n2:15\nSo how do we improve this? Well in order to assign a none 0 probability to words that have not been observed in the document, we would have to take away some probability mass from the words that are observed in the document. So for example here, we have to take away some probability of the mass because we need some extra probability mass for the words otherwise they won't sum to 1. So all these probabilities must sum to 1. So to make this transformation and to improve the maximum likelihood estimated by assigning non zero probabilities to words that are not observed in the data.\n3:01\nWe have to do smoothing and smoothing has to do with improving the estimate by considering the possibility that if the author\n3:13\nhad been asking to write more words for the document, the author might have written other words. If you think about this factor then the a smoothed language model would be a more accurate than the representation of the actual topic. Imagine you have seen an abstract of a research article. Let's say this document is abstract.\n3:39\nIf we assume and see words in this abstract that we have a probability of 0. That would mean there's no chance of sampling a word outside the abstract of the formulated query. But imagine a user who is interested in the topic of this subject. The user might actually choose a word that's not in that chapter to use as query. So obviously, if we has asked this author to write more author would have written a full text of the article. So smoothing of the language model is an attempt to try to recover the model for the whole article. And then of course, we don't have knowledge about any words that are not observed in the abstract. So that's why smoothing is actually a tricky problem. So let's talk a little more about how to smooth a language model. The key question here is, what probability should be assigned to those unseen words?\n4:50\nAnd there are many different ways of doing that.\n4:53\nOne idea here, that's very useful for retrieval is let the probability of unseen word be proportional to its probability given by a reference language model. That means if you don't observe the word in the dataset. We're going to assume that its probability is kind of governed by another reference language model that we will construct. It will tell us which unseen words would have a higher probability.\n5:22\nIn the case of retrieval, a natural choice would be to take the collection language model as the reference language model. That is to say, if you don't observe a word in the document, we're going to assume that the probability of this word would be proportional to the probability of word in the whole collection. So more formally, we'll be estimating the probability of a word key document as follows.\n5:48\nIf the word is seen in the document then the probability would be this counted the maximum likelihood estimate P sub c here. Otherwise, if the word is not seen in the document we're going to let probability be proportional to the probability of the word in the collection. And here the coefficient that offer is to control the amount of probability mass that we assign to unseen words.\n6:22\nObviously, all these probabilities must sum to 1, so alpha sub d is constrained in some way.\n6:29\nSo what if we plug in this smoothing formula into our query likelihood ranking function? This is what we will get.\n6:37\nIn this formula, we have this as a sum over all the query words and those that we have written here as the sum of all the vocabulary, you see here. This is the sum of all the words in the vocabulary, but not that we have a count of the word in the query. So in fact, we are just taking a sample of query words. This is now a common way that we would use, because of its convenience in some transformations.\n7:18\nSo this is as I said, this is sum of all the query words.\n7:23\nIn our smoothing method, we assume that the words that are not observed in the method would have a somewhat different form of probability. Name it's four, this foru. So we're going to do then, decompose the sum into two parts.\n7:38\nOne sum is over all the query words that are matching the document. That means that in this sum, all the words have a non zero probability in the document. Sorry, it's the non zero count of the word in the document. They all occur in the document.\n8:02\nAnd they also have to of course have a non zero count in the query. So these are the query words that are matching the document. On the other hand, in this sum we are taking a sum of all the words that are not all query was not matching the document.\n8:25\nSo they occur in the query due to this term, but they don't occur in the document. In this case, these words have this probability because of our assumption about the smoothing. That here, these seen words have a different probability.\n8:47\nNow, we can go further by rewriting the second sum\n8:52\nas a difference of two other sums. Basically, the first sum is the sum of all the query words.\n9:00\nNow, we know that the original sum is not over all the query words. This is over all the query words that are not matched in the document.\n9:12\nSo here we pretend that they are actually over all the query words. So we take a sum over all the query words. Obviously, this sum has extra terms that are not in this sum.\n9:30\nBecause, here we're taking sum over all the query words. There, it's not matched in the document. So in order to make them equal, we will have to then subtract another sum here. And this is the sum over all the query words that are matching in the document.\n9:51\nAnd this makes sense, because here we are considering all query words. And then we subtract the query that was matched in the document. That would give us the query that was not matched in the document.\n10:05\nAnd this is almost a reverse process of the first step here.\n10:12\nAnd you might wonder why do we want to do that. Well, that's because if we do this, then we have different forms of terms inside of these sums. So now, you can see in this sum we have all the words matched, the query was matching the document with this kind of term.\n10:36\nHere we have another sum over the same set of terms, matched query terms in document. But inside the sum, it's different.\n10:49\nBut these two sums can clearly be merged.\n10:54\nSo if we do that, we'll get another form of the formula that looks like before me at the bottom here.\n11:04\nAnd note that this is a very interesting formula. Because here we combine these two that all or some of the query words matching in the document in the one sum here.\n11:19\nAnd the other sum now is decomposing into two parts. And these two parts look much simpler just, because these are the probabilities of unseen words.\n11:31\nThis formula is very interesting because you can see the sum is now over\n11:37\nthe match the query terms.\n11:41\nAnd just like in the vector space model, we take a sum\n11:46\nof terms that are in the intersection of query vector and the document vector.\n11:51\nSo it already looks a little bit like the vector space model. In fact, there's even more similarity here as we explain on this slide. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/VCyVP/week-12-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 12\nWeek 12 Overview\nPrevious\nNext\nWeek 12 Information\nReading:\nReading\nWeek 12 Overview\n. Duration: 10 minutes\n10 min\nWeek 12 Lessons\nWeek 12 Activities\nWeek 12 Overview\nDuring this week's lessons, you will learn about techniques for joint mining of text and non-text data, including contextual text mining techniques for analyzing topics in text in association with various context information such as time, location, authors, and sources of data. You will also see a summary of the Text Mining content.\nTime\nThis module should take approximately 3 of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with required assignments in bold):\nActivity\nEstimated Time Required\nWeek 12 Video Lectures\n2 hours\nWeek 12 Graded Quiz\n1 hour\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain why it is necessary and useful to perform joint analysis and mining for text and non-text data.\nExplain the general idea of Contextual Probabilistic Latent Semantic Analysis (CPLSA) and the main difference between CPLSA and PLSA.\nGive multiple application examples of CPLSA for contextual text mining.\nExplain the general idea of using the social network of authors as context to analyze topics in text data and its potential benefit from an application perspective.\nExplain how a time series (such as stock prices) can be used as context to analyze topics in text data that have time stamps using topic models \nGuiding Questions\nDevelop your answers to the following guiding questions while watching the video lectures throughout the week.\nWhy is text-based prediction interesting from an application perspective? Why are humans playing an important role in text-based prediction? What is the “data mining loop”? \nWhy is it necessary and useful to jointly mine and analyze text and non-text data? How can non-text data potentially help in analyzing text data? How can text data potentially help in mining non-text data? \nCan you give some examples of context of a text article? How can we partition text data using context information? Can you give some examples where we can leverage context information to perform interesting comparative analysis of topics in text data? \nWhat’s the general idea of Contextual Probabilistic Latent Semantic Analysis (CPLSA)? How is it different from PLSA? \nCan you give some examples of interesting topic patterns that can be found by CPLSA? What’s the general idea of using CPLSA for analyzing the impact of an event? Can you think of an interesting application of this kind? \nWhat’s the general idea of using the social network of authors of text data as a complex context to improve topic analysis for text data? Can you give an example of an interesting application of this kind? \nWhat’s the general idea of using a time series like stock prices over time to supervise the discovery of topics from text data? Can you give an example of an interesting application of this kind? \nAdditional Readings and Resources\nThe following readings are optional:\nC. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM and Morgan & Claypool Publishers, 2016. Chapters 18 & 19.\nHongning Wang, Yue Lu, and ChengXiang Zhai, Latent aspect rating analysis on review text data: a rating regression approach. In Proceedings of ACM KDD 2010, pp. 783-792, 2010. doi: 10.1145/1835804.1835903\nHongning Wang, Yue Lu, and ChengXiang Zhai. 2011. Latent aspect rating analysis without aspect keyword supervision. In Proceedings of ACM KDD 2011, pp. 618-626. doi: 10.1145/2020408.2020505\nChengXiang Zhai, Atulya Velivelli, and Bei Yu. A cross-collection mixture model for comparative text mining. In Proceedings of the 10th ACM SIGKDD international conference on knowledge discovery and data mining (KDD 2004). ACM, New York, NY, USA, 743-748. doi: 10.1145/1014052.1014150\nQiaozhu Mei, Contextual Text Mining, Ph.D. Thesis, University of Illinois at Urbana-Champaign, 2009.\nHyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Thomas Rietz, and Daniel Diermeier. Mining causal topics in text data: Iterative topic modeling with time series feedback. In Proceedings of the 22nd ACM international conference on information & knowledge management (CIKM 2013). ACM, New York, NY, USA, 885-890. doi: 10.1145/2505515.2505612\nNoah Smith, Text-Driven Forecasting. Retrieved on May 31, 2015 from http://www.cs.cmu.edu/~nasmith/papers/smith.whitepaper10.pdf\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nText-based prediction\nThe “data mining loop”\nContext (of text data) and contextual text mining\nContextual probabilistic latent semantic analysis (CPLSA): views of a topic and coverage of topics\nSpatiotemporal trends of topics\nEvent impact analysis\nNetwork-regularized topic modeling\nNetPLSA\nCausal topics\nIterative topic modeling with time series supervision\nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issue2\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/supplement/okACu/week-2-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 2\nWeek 2 Overview\nPrevious\nNext\nWeek 2 Information\nReading:\nReading\nWeek 2 Overview\n. Duration: 10 minutes\n10 min\nWeek 2 Lessons\nWeek 2 Activities\nProgramming Assignment 1\nWeek 2 Overview\nDuring this week's lessons, you will learn how the vector space model works in detail, the major heuristics used in designing a retrieval function for ranking documents with respect to a query, and how to implement an information retrieval system (i.e., a search engine), including how to build an inverted index and how to score documents quickly for a query.\nTime\nThis module should take approximately 6 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with assignments in bold):\nActivity\nEstimated Time Required\nWeek 2 Video Lectures\n2 hours\nWeek 2 Graded Quiz \n1 hour\nProgramming Assignment 1\n3 hours\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain what TF-IDF weighting is and why TF transformation and document length normalization are necessary for the design of an effective ranking function.\nExplain what an inverted index is and how to construct it for a large set of text documents that do not fit into the memory.\nExplain how variable-length encoding can be used to compress integers and how unary coding and gamma-coding work.\nExplain how scoring of documents in response to a query can be done quickly by using an inverted index.\nExplain Zipf’s law.\nGuiding Questions\nDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\nWhat are some different ways to place a document as a vector in the vector space?\nWhat is term frequency (TF)?\nWhat is TF transformation?\nWhat is document frequency (DF)?\nWhat is inverse document frequency (IDF)?\nWhat is TF-IDF weighting?\nWhy do we need to penalize long documents in text retrieval?\nWhat is pivoted document length normalization?\nWhat are the main ideas behind the retrieval function BM25?\nWhat is the typical architecture of a text retrieval system?\nWhat is an inverted index?\nWhy is it desirable to compress an inverted index?\nHow can we create an inverted index when the collection of documents does not fit into the memory?\nHow can we leverage an inverted index to score documents quickly?   \nAdditional Readings and Resources\nThe following readings are optional:\nC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapter 6 - Section 6.3, and Chapter 8. \nIan H. Witten, Alistair Moffat, and Timothy C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition. Morgan Kaufmann, 1999.  \nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nTerm frequency (TF)\nDocument frequency (DF) and inverse document frequency (IDF)\nTF transformation\nPivoted length normalization\nBM25\nInverted index and postings\nBinary coding, unary coding, gamma-coding, and d-gap\nZipf’s law \nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues2\nCompleted\nGo to next item\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/irt/pHUCQ/project-code-documentation-and-presentationdict_values(['List\nCS 410: Text Information Systems\nWeek 16\nProject Code, Documentation and Presentation\nPrevious\nNext\nWeek 16 Activities\nGraded Assignment: Project Code, Documentation and Presentation\n. Duration: 3 hours\n3h\nEnd of Course Survey\nProject Code, Documentation and Presentation\nGraded Assignment\n3 hours\n • 3h\nSubmit your assignment\nDue \nDecember 8, 12:59 PM EST\nDec 8, 12:59 PM EST\nStart assignment\nReceive feedback\nYour grade\n-\nNot available\nLike\nDislike\nReport an issue\nClose\nCoursera Honor Code\nWe’re dedicated to protecting the integrity of your work on Coursera.\nAs part of this effort, we’ve created an honor code that we ask everyone to follow. Learn more\nAll learners should:\nSubmit their own original work\nAvoid sharing answers with others\nReport suspected violations\nContinue'])
https://www.coursera.org/learn/cs-410/supplement/wBlrv/course-deadlines-late-policies-and-academic-calendardict_values(["List\nCS 410: Text Information Systems\nWeek 1\nCourse Deadlines, Late Policies, and Academic Calendar\nPrevious\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nCourse Deadlines, Late Policies, and Academic Calendar\nCourse Deadlines\nQuizzes\nAssignment\nRelease Date\nHard Deadline\nQuiz 1\nFirst day of class\nEnd of Week 7\nQuiz 2\nFirst day of class\nEnd of Week 7\nQuiz 3\nFirst day of class\nEnd of Week 7\nQuiz 4\nFirst day of class\nEnd of Week 7\nQuiz 5\nFirst day of class\nEnd of Week 7\nQuiz 6\nFirst day of class\nEnd of Week 7\nQuiz 7\nFirst day of class\nEnd of Week 13\nQuiz 8\nFirst day of class\nEnd of Week 13\nQuiz 9\nFirst day of class\nEnd of Week 13\nQuiz 10\nFirst day of class\nEnd of Week 13\nQuiz 11\nFirst day of class\nEnd of Week 13\nQuiz 12\nFirst day of class\nEnd of Week 13\nProgramming Assignments\nAssignment\nRelease Date\nHard Deadline\nProgramming Assignment 1\nWeek 1\nSept 4, 11:59 pm CDT\nProgramming Assignment 2.1\nWeek 2 \nSept 11, 11:59 pm CDT\nProgramming Assignment 2.2\nSept 12, 11:59pm CDT\nSept 18, 11:59 pm CDT\nProgramming Assignment 2.3\nSept 12, 11:59pm CDT\nSept 25, 11:59 pm CDT\nProgramming Assignment 2.4\nSept 21, 11:59pm CDT\nOct 2, 11:59 pm CDT\nProgramming Assignment 3\nSept 25, 11:59pm CDT\nOct 23, 11:59 pm CDT\nProgramming Assignment 4\nNov 8, 11:59pm CDT\nNov. 20, 11:59pm CDT\nCourse Project\nAssignment\nRelease Date\nHard Deadline\nTeam Formation \nSept 25, 11:59pm CDT\nOct 17, 11:59 pm CDT\nProject Proposal Submission\nOct. 2, 11:59pm CDT\nOct 23, 11:59 pm CDT\nProject Progress Report Submission\nNov. 6, 11:59pm CDT\nNov 14, 11:59 pm CDT\nProject Progress Report Peer Review\nNov 14, 11:59pm CDT\nNov 18, 11:59 pm CDT\nProject Code and Documentation Submission\nNov. 18, 11:59pm CDT\nDec 8, 11:59 pm CDT\nProject Presentation Submission \nNov. 18, 11:59pm CDT\nDec 8, 11:59 pm CDT\nProject Code, Documentation, Presentation Peer Review\nDec. 9, 11:59pm CDT\nDec 16, 11:59 pm CDT\nTechnology Review (4-credit students only)\nAssignment\nRelease Date\nHard Deadline\nTechnology review submission\nOct. 2, 11:59pm CDT\nNov 6, 11:59 pm CDT\nExams\nExam Name\nExam Start Date\nExam End Date\nExam 1\nMon., Oct 10, 9:00am CDT\nSun., Oct 16, 10:00pm CDT\nExam 2\nMon., Nov 28, 9:00am CDT\nSun. Dec 4, 10:00pm CDT\n* dates are tentative and are subject to change\nPlease see the informational item about using ProctorU in Orientation module.\nLate Policy\nUnless otherwise specified, all assignments are due at 11:59 p.m. US Central Time on the due date. (Time Zone Converter)\nThe hard deadline is the deadline after which you will receive 0 points on assignments regardless how well you did on the assignment. No late submission will be accepted except under extremely rare non-academic circumstances (which usually require approval from the Dean's office).\nEach week's Quiz is due on Sunday. Late quiz submissions will be accepted up until the Hard Deadline. However, a late penalty of  will be deducted daily until a submission is made or the grade reaches .\nRequests for extensions for Programming Assignments will be handled on a case by case basis. Please send an email to TAs or post privately on the class Campuswire to request an extension. Make sure to state your reason for requesting an extension. Extensions are not guaranteed and penalties may apply.\nNo assignment will be accepted after December 9th.\nAcademic Calendar\nThe Graduate College at the University of Illinois maintains a Graduate College Calendar. The calendar includes important dates such as final exam dates, course registration and cancellation, and holidays.\nThere is also a campus wide calendar available.\nThe CS Department also sends reminders about upcoming deadlines. You will also receive the Graduate College newsletter in your Exchange email account.\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/BWexZ/lesson-4-3-query-likelihood-retrieval-functiondict_values(["List\nCS 410: Text Information Systems\nWeek 4\nLesson 4.3: Query Likelihood Retrieval Function\nPrevious\nNext\nWeek 4 Information\nWeek 4 Lessons\nVideo:\nVideo\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.2: Statistical Language Model\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 4.3: Query Likelihood Retrieval Function\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.4: Statistical Language Model - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.5: Statistical Language Model - Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.6: Smoothing Methods - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.7: Smoothing Methods - Part 2\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nProgramming Assignment 2.2\nLesson 4.3: Query Likelihood Retrieval Function\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:07\nThis lecture is about query likelihood, probabilistic retrieval model.\n0:14\nIn this lecture, we continue the discussion of probabilistic retrieval model. In particular, we're going to talk about the query light holder retrieval function.\n0:25\nIn the query light holder retrieval model, our idea is model. How like their user who likes a document with pose a particular query?\n0:36\nSo in this case, you can imagine if a user likes this particular document about a presidential campaign news. Now we assume, the user would use this a document as a basis to impose a query to try and retrieve this document.\n0:57\nSo again, imagine use a process that works as follows. Where we assume that the query is generated by assembling words from the document.\n1:10\nSo for example, a user might pick a word like presidential, from this document and then use this as a query word.\n1:20\nAnd then the user would pick another word like campaign, and that would be the second query word.\n1:27\nNow this of course is an assumption that we have made about how a user would pose a query. Whether a user actually followed this process may be a different question, but this assumption has allowed us to formerly characterize this conditional probability.\n1:46\nAnd this allows us to also not rely on the big table that I showed you earlier\n1:52\nto use empirical data to estimate this probability.\n1:56\nAnd this is why we can use this idea then to further derive retrieval function that we can implement with the program language.\n2:04\nSo as you see the assumption that we made here is each query word is independent of the sample. And also each word is basically obtained from the document.\n2:20\nSo now let's see how this works exactly. Well, since we are completing a query likelihood\n2:29\nthen the probability here is just the probability of this particular query, which is a sequence of words. And we make the assumption that each word is generated independently. So as a result, the probability of the query is just a product of the probability of each query word.\n2:50\nNow how do we compute the probability of each query word? Well, based on the assumption that a word is picked from the document that the user has in mind. Now we know the probability of each word is just the relative frequency of each word in the document. So for example, the probability of presidential given the document. Would be just the count of presidential document divided by the total number of words in the document or document s. So with these assumptions we now have actually a simple formula for retrieval. We can use this to rank our documents.\n3:32\nSo does this model work? Let's take a look. Here are some example documents that you have seen before. Suppose now the query is presidential campaign and we see the formula here on the top.\n3:45\nSo how do we score this document? Well, it's very simple. We just count how many times do we have seen presidential or how many times do we have seen campaigns, etc. And we see here 44, and we've seen presidential twice. So that's 2 over the length of document 4 multiplied by 1 over the length of document 4 for the probability of campaign. And similarly, we can get probabilities for the other two documents.\n4:13\nNow if you look at these numbers or these formulas for scoring all these documents, it seems to make sense. Because if we assume d3 and d4 have about the same length, then looks like a nominal rank d4 above d3 and which is above d2. And as we would expect, looks like it did captures a TF query state, and so this seems to work well. However, if we try a different query like this one, presidential campaign update then we might see a problem. Well what problem? Well think about the update. Now none of these documents has mentioned update. So according to our assumption that a user would pick a word from a document to generate a query, then the probability of obtaining the word update would be what? Would be 0.\n5:17\nSo that causes a problem, because it would cause all these documents to have zero probability of generating this query.\n5:25\nNow why it's fine to have zero probability for d2, which is non-relevant? It's not okay to have 0 for d3 and d4 because now we no longer can distinguish them. What's worse? We can't even distinguish them from d2. So that's obviously not desirable. Now when a [INAUDIBLE] has such result, we should think about what has caused this problem?\n5:52\nSo we have to examine what assumptions have been made, as we derive this ranking function. Now is you examine those assumptions carefully you will realize, what has caused this problem? So take a moment to think about it. What do you think is the reason why update has zero probability and how do we fix it? So if you think about this from the moment you realize that that's because we have made an assumption that every query word must be drawn from the document in the user's mind. So in order to fix this, we have to assume that the user could have drawn a word not necessarily from the document. So that's the improved model. An improvement here is to say that, well instead of drawing a word from the document, let's imagine that the user would actually draw a word from a document model. And so I show a model here. And we assume that this document is generated using this unigram language model. Now, this model doesn't necessarily assign zero probability for update in fact, we can assume this model does not assign zero probability for any word. Now if we're thinking this way then the generation process is a little bit different. Now the user has this model in mind instead of this particular document. Although the model has to be estimated based on the document. So the user can again generate the query using a singular process. Namely, pick a word for example, presidential and another word campaign.\n7:29\nNow the difference is that this time we can also pick a word like update, even though update doesn't occur in the document to potentially generate a query word like update. So that a query was updated 1 times 0 probabilities. So this would fix our problem. And it's also reasonable because when our thinking of what the user is looking for in a more general way, that is unique language model instead of fixed document. So how do we compute this query likelihood? If we make this sum wide involved two steps. The first one is compute this model, and we call it document language model here. For example, I've shown two pulse models here, it's major based on two documents. And then given a query like a data mining algorithms the thinking is that we'll just compute the likelihood of this query. And by making independence assumptions we could then have this probability as a product of the probability of each query word. We do this for both documents, and then we can score these two documents and then rank them.\n8:37\nSo that's the basic idea of this query likelihood retrieval function. So more generally this ranking function would look like in the following. Here we assume that the query has n words, w1 through wn, and then the scoring function. The ranking function is the probability that we observe this query, given that the user is thinking of this document. And this is assume it will be product of probabilities of all individual words. This is based on independent assumption. Now we actually often score the document before this query by using log of the query likelihood as shown on the second line.\n9:26\nNow we do this to avoid having a lot of small probabilities, mean multiply together. And this could cause under flow and we might loose the precision by transforming the value in our algorithm function. We maintain the order of these documents yet we can avoid the under flow problem. And so if we take longer than transformation of course, the product would become a sum as you on the second line here. So the sum of all the query words inside of the sum that is one of the probability of this word given by the document.\n10:09\nAnd then we can further rewrite the sum to a different form.\n10:14\nSo in the first sum here, in this sum,\n10:21\nwe have it over all the query words and query word. And in this sum we have a sum of all the possible words. But we put a counter here of each word in the query. Essentially we are only considering the words in the query, because if a word is not in the query, the count will be 0. So we're still considering only these n words. But we're using a different form as if we were going to take a sample of all the words in the vocabulary.\n10:52\nAnd of course, a word might occur multiple times in the query. That's why we have a count here.\n11:00\nAnd then this part is log of the probability of the word, given by the document language model.\n11:08\nSo you can see in this retrieval function, we actually know the count of the word in the query. So the only thing that we don't know is this document language model.\n11:17\nTherefore, we have converted the retrieval problem include the problem of estimating this document language model.\n11:25\nSo that we can compute the probability of each query word given by this document.\n11:32\nAnd different estimation methods would lead to different ranking functions. This is just like a different way to place document in the vector space which leads to a different ranking function in the vector space model. Here different ways to estimate will lead to a different ranking function for query likelihood. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/mFYTD/lesson-6-1-learning-to-rank-part-1-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:07\nThis lecture is about the Learning to Rank. In this lecture, we are going to continue talking about web search. In particular we're going to talk about the using machine learning to combine different features to improve the ranking function.\n0:22\nSo the question that we address in this lecture is how we can combine many features to generate a single ranking function to optimize search results? In the previous lectures we have talked about a number of ways to rank documents. We have talked about some retrieval models like a BM25 or Query Light Code. They can generate a based this course for matching documents with a query. And we also talked about the link based approaches like page rank\n0:59\nthat can give additional scores to help us improve ranking.\n1:03\nNow the question now is, how can we combine all these features and potentially many other features to do ranking? And this will be very useful for ranking webpages, not only just to improve accuracy, but also to improve the robustness of the ranking function. So that it's not easy for a spammer to just perturb a one or a few features to promote a page.\n1:27\nSo the general idea of learning to rank is to use machine learning to combine this features to optimize the weights on different features to generate the optimal ranking function.\n1:40\nSo we will assume that the given a query document pair Q and D, we can define a number of features. And these features can vary from content based features such as a score of the document with respect to the query according to a retrieval function such as BM25 or Query Light Hold of punitive commands from a machine or PL2 etcetera. It can also be a link based score like or page rank score like. It can be also application of retrieval models to the ink text of the page. Those are the types of descriptions of links that point to this page.\n2:29\nSo, these can all the clues whether this document is relevant, or not.\n2:35\nWe can even include a feature such as whether the URL has a tilde because this might be indicator of home page or entry page.\n2:48\nSo all these features can then be combined together to generate a ranking function. The question is, of course. How can we combine them? In this approach, we simply hypothesize that the probability that this document isn't relevant to this query is a function of all these features. So we can hypothesize this\n3:11\nthat the probability of relevance is related to these features through a particular form of the function that has some parameters. These parameters can control the influence of different features of the final relevance. Now this is of course just an assumption. Whether this assumption really makes sense is a big question and that's they have to empirically evaluate the function.\n3:45\nBut by hypothesizing that the relevance is related to these features in the particular way, we can then combine these features to generate the potential more powerful ranking function, a more robust ranking function. Naturally the next question is how do we estimate those parameters? How do we know which features should have a higher weight, and which features will have lower weight? So this is the task of training or learning, so in this approach what we will do is use some training data. Those are the data that have been charted by users so that we already know the relevant judgments. We already know which documents should be ranked high for which queries. And this information can be based on real judgments by users or this can also be approximated by just using click through information, where we can assume the clicked documents are better than the skipped documents clicked documents are relevant and the skipped documents are non-relevant. So in general with the fit such hypothesize ranking function to the training data meaning that we will try to optimize it's retrieval accuracy on the training data. And we can adjust these parameters to see\n5:09\nhow we can optimize the performance of the functioning on the training data\n5:16\nin terms of some measures such as MAP or NDCG.\n5:20\nSo the training date would look like a table of tuples. Each tuple has three elements, the query, the document, and the judgement. So it looks very much like our relevance judgement that we talked about in the evaluation of retrieval systems. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/3dict_values(["Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 3\nWeek 3\nAll videos completed\n10 min of readings left\nAll graded assignments completed\nIn this week's lessons, you will learn how to evaluate an information retrieval system (a search engine), including the basic measures for evaluating a set of retrieved results and the major measures for evaluating a ranked list, including the...\nShow more\nWeek 3 Information\nWeek 3 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 3 Lessons\nComplete\nLesson 3.1: Evaluation of TR Systems\nVideo•\n. Duration: 10 minutes\n10 min\nLesson 3.2: Evaluation of TR Systems - Basic Measures\nVideo•\n. Duration: 12 minutes\n12 min\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\nVideo•\n. Duration: 15 minutes\n15 min\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\nVideo•\n. Duration: 10 minutes\n10 min\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\nVideo•\n. Duration: 10 minutes\n10 min\nLesson 3.6: Evaluation of TR Systems - Practical Issues\nVideo•\n. Duration: 15 minutes\n15 min\nWeek 3 Activities\nComplete\nWeek 3 Practice Quiz\nPractice Quiz•10 questions\nWeek 3 Quiz\nGraded\nQuiz•10 questions\n•Grade: \nProgramming Assignment 2.1\nComplete\nMP2.1\nDue, Sep 12, 12:59 AM EDT\nGraded\nGraded External Tool•Your grade has been overridden\n•Grade: "])
https://www.coursera.org/learn/cs-410/lecture/8Q2Tw/lesson-3-4-evaluation-of-tr-systems-evaluating-ranked-lists-part-2dict_values(['List\nCS 410: Text Information Systems\nWeek 3\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\nPrevious\nNext\nWeek 3 Information\nWeek 3 Lessons\nVideo:\nVideo\nLesson 3.1: Evaluation of TR Systems\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.2: Evaluation of TR Systems - Basic Measures\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.6: Evaluation of TR Systems - Practical Issues\n. Duration: 15 minutes\n15 min\nWeek 3 Activities\nProgramming Assignment 2.1\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:11\nSo average precision is computer for just one. one query. But we generally experiment with many different queries and this is to avoid the variance across queries. Depending on the queries you use you might make different conclusions. Right, so it\'s better then using more queries.\n0:33\nIf you use more queries then, you will also have to take the average of the average precision over all these queries.\n0:41\nSo how can we do that?\n0:43\nWell, you can naturally. Think of just doing arithmetic mean as we\n0:50\nalways tend to, to think in, in this way. So, this would give us what\'s called a "Mean Average Position", or MAP. In this case, we take arithmetic mean of all the average precisions over several queries or topics.\n1:09\nBut as I just mentioned in another lecture, is this good?\n1:15\nWe call that. We talked about the different ways of combining precision and recall. And we conclude that the arithmetic mean is not as good as the MAP measure. But here it\'s the same. We can also think about the alternative ways of aggregating the numbers. Don\'t just automatically assume that, though. Let\'s just also take the arithmetic mean of the average position over these queries. Let\'s think about what\'s the best way of aggregating them. If you think about the different ways, naturally you will, probably be able to think about another way, which is geometric mean.\n1:51\nAnd we call this kind of average a gMAP.\n1:55\nThis is another way. So now, once you think about the two different ways. Of doing the same thing. The natural question to ask is, which one is better? So.\n2:05\nSo, do you use MAP or gMAP?\n2:09\nAgain, that\'s important question. Imagine you are again testing a new algorithm in, by comparing the ways your old algorithms made the search engine.\n2:18\nNow you tested multiple topics. Now you\'ve got the average precision for these topics. Now you are thinking of looking at the overall performance. You have to take the average.\n2:30\nBut which, which strategy would you use?\n2:34\nNow first, you should also think about the question, well did it make a difference? Can you think of scenarios where using one of them would make a difference? That is they would give different rankings of those methods. And that also means depending on the way you average or detect the. Average of these average positions.\n2:55\nYou will get different conclusions. This makes the question becoming even more important.\n3:01\nRight? So, which one would you use?\n3:05\nWell again, if you look at the difference between these. Different ways of aggregating the average position. You\'ll realize in arithmetic mean, the sum is dominating by large values. So what does large value here mean? It means the query is relatively easy. You can have a high pres, average position.\n3:25\nWhereas gMAP tends to be affected more by low values.\n3:30\nAnd those are the queries that don\'t have good performance. The average precision is low.\n3:37\nSo if you think about the, improving the search engine for those difficult queries, then gMAP would be preferred, right?\n3:47\nOn the other hand, if you just want to. Have improved a lot.\n3:52\nOver all the kinds of queries or particular popular queries that might be easy and you want to make the perfect and maybe MAP would be then preferred. So again, the answer depends on your users, your users tasks and their pref, their preferences.\n4:08\nSo the point that here is to think about the multiple ways to solve the same problem, and then compare them, and think carefully about the differences. And which one makes more sense. Often, when one of them might make sense in one situation and another might make more sense in a different situation. So it\'s important to pick out under what situations one is preferred.\n4:35\nAs a special case of the mean average position, we can also think about the case where there was precisely one rank in the document. And this happens often, for example, in what\'s called a known item search. Where you know a target page, let\'s say you have to find Amazon, homepage. You have one relevant document there, and you hope to find it. That\'s call a "known item search". In that case, there\'s precisely one relevant document. Or in another application, like a question and answering, maybe there\'s only one answer. Are there. So if you rank the answers, then your goal is to rank that one particular answer on top, right? So in this case, you can easily verify the average position, will basically boil down to reciprocal rank. That is, 1 over r where r is the rank position of that single relevant document. So if that document is ranked on the very top or is 1, and then it\'s 1 for reciprocal rank. If it\'s ranked at the, the second, then it\'s 1 over 2. Et cetera.\n5:41\nAnd then we can also take a, a average of all these average precision or reciprocal rank over a set of topics, and that would give us something called a mean reciprocal rank. It\'s a very popular measure. For no item search or, you know, an problem where you have just one relevant item.\n6:03\nNow again here, you can see this r actually is meaningful here. And this r is basically indicating how much effort a user would have to make in order to find that relevant document. If it\'s ranked on the top it\'s low effort that you have to make, or little effort. But if it\'s ranked at 100 then you actually have to,\n6:27\nread presumably 100 documents in order to find it. So, in this sense r is also a meaningful measure and the reciprocal rank will take the reciprocal of r, instead of using r directly.\n6:42\nSo my natural question here is why not simply using r? I imagine if you were to design a ratio to, measure the performance of a random system, when there is only one relevant item.\n6:55\nYou might have thought about using r directly as the measure. After all, that measures the user\'s effort, right? But, think about if you take a average of this over a large number of topics.\n7:12\nAgain it would make a difference. Right, for one single topic, using r or using 1 over r wouldn\'t make any difference. It\'s the same. Larger r with corresponds to a small 1 over r, right?\n7:26\nBut the difference would only show when, show up when you have many topics. So again, think about the average of Mean Reciprocal Rank versus average of just r. What\'s the difference? Do you see any difference? And would, would this difference change the oath of systems. In our conclusion.\n7:49\nAnd this, it turns out that, there is actually a big difference, and if you think about it, if you want to think about it and then, yourself, then pause the video.\n7:59\nBasically, the difference is, if you take some of our directory, then. Again it will be dominated by large values of r. So what are those values? Those are basically large values that indicate that lower ranked results. That means the relevant items rank very low down on the list. And the sum that\'s also the average that would then be dominated by. Where those relevant documents are ranked in, in ,in, in the lower portion of the ranked. But from a users perspective we care more about the highly ranked documents. So by taking this transformation by using reciprocal rank.\n8:40\nHere we emphasize more on the difference on the top. You know, think about the difference between 1 and the 2, it would make a big difference, in 1 over r, but think about the 100, and 1, and where and when won\'t make much difference if you use this. But if you use this there will be a big difference in 100 and let\'s say 1,000, right. So this is not the desirable.\n9:06\nOn the other hand, a 1 and 2 won\'t make much difference. So this is yet another case where there may be multiple choices of doing the same thing and then you need to figure out which one makes more sense.\n9:17\nSo to summarize, we showed that the precision-recall curve. Can characterize the overall accuracy of a ranked list. And we emphasized that the actual utility of a ranked list depends on how many top ranked results a user would actually examine. Some users will examine more. Than others. An average person uses a standard measure for comparing two ranking methods. It combines precision and recall and it\'s sensitive to the rank of every random document. [MUSIC]\nLike\nDislike\nReport an issue\nShare'])
https://www.coursera.org/learn/cs-410/home/week/7dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 7\nWeek 7\nAll videos completed\n30 min of readings left\nAll graded assignments completed\nDuring this module, you will receive an overview of natural language processing techniques and text representation, which are the foundation for all kinds of text-mining applications, and word association mining with a particular focus on mining one of the two basic forms of word associations (i.e., paradigmatic relations).\nWeek 7 Information\nWeek 7 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 7 Lessons\nComplete\n7.1 Overview Text Mining and Analytics: Part 1\nVideo•\n. Duration: 11 minutes\n11 min\n7.2 Overview Text Mining and Analytics: Part 2\nVideo•\n. Duration: 11 minutes\n11 min\n7.3 Natural Language Content Analysis: Part 1\nVideo•\n. Duration: 12 minutes\n12 min\n7.4 Natural Language Content Analysis: Part 2\nVideo•\n. Duration: 4 minutes\n4 min\n7.5 Text Representation: Part 1\nVideo•\n. Duration: 10 minutes\n10 min\n7.6 Text Representation: Part 2\nVideo•\n. Duration: 9 minutes\n9 min\n7.7 Word Association Mining and Analysis\nVideo•\n. Duration: 15 minutes\n15 min\n7.8 Paradigmatic Relation Discovery Part 1\nVideo•\n. Duration: 14 minutes\n14 min\n7.9 Paradigmatic Relation Discovery Part 2\nVideo•\n. Duration: 17 minutes\n17 min\nExam 1\nHow to Schedule and Take the Exam\nReading•\n. Duration: 10 minutes\n10 min\nExam Policies and Technical Support\nReading•\n. Duration: 10 minutes\n10 min\nProctorU Password - Exam 1\nGraded\nQuiz•1 question\n•Grade: \nAvailable October 10, 10:00 AM EDT - November 9, 11:00 PM EST\nExam 1\nGraded\nQuiz•20 questions\n•Grade: \nAvailable October 10, 10:00 AM EDT - November 8, 2:00 PM EST\nWeek 7 Activities\nComplete\nWeek 7 Practice Quiz\nPractice Quiz•10 questions\nWeek 7 Quiz\nGraded\nQuiz•10 questions\n•Grade: \nProgramming Assignment 3\nComplete\nMP3\nDue, Oct 27, 12:59 AM EDT\nGraded\nGraded External Tool•Your grade has been overridden\n•Grade: '])
https://www.coursera.org/learn/cs-410/lecture/QORNe/lesson-6-5-recommender-systems-content-based-filtering-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC] This lecture is about the Recommender Systems.\n0:12\nSo far we have talked about a lot of aspects of search engines.\n0:19\nWe have talked about the problem of search and ranking problem, different methods for ranking, implementation of search engine and how to evaluate a search engine, etc.\n0:36\nThis is important because we know that web search engines are by far the most important applications of text retrieval. And they are the most useful tools to help people convert big raw text data into a small set of relevant documents.\n0:56\nAnother reason why we spend so many lectures on search engines, is because many techniques used in search engines are actually also very useful for Recommender Systems, which is the topic of this lecture. And so, overall, the two systems are actually well connected. And there are many techniques that are shared by them.\n1:22\nSo this is a slide that you have seen before, when we talked about the two different modes of text access. Pull and the Push.\n1:31\nAnd we mentioned that recommender systems are the main systems to serve users in the Push Mode, where the systems will take the initiative to recommend the information to the user or pushes information to the user. And this often works well when the user has stable information need in the system has a good. So a Recommender System is sometimes called a filtering system and it's because recommending useful items to people is like discarding or filtering out the the useless articles, and so in this sense they are kind of similar.\n2:16\nAnd in all the cases the system must make a binary decision and usually there's a dynamic source of information items, and that you have some knowledge about the users' interest. And then the system would make a decision about whether this item is interesting to the user, and then if it's interesting then the system would recommend the article to the user.\n2:43\nSo the basic filtering question here is really will this user like this item? Will U like item X? And there are two ways to answer this question, if you think about it.\n2:56\nAnd one is look at what items U likes and then we can see if X is actually like those items.\n3:05\nThe other is to look at who likes X, and we can see if this user looks like a one of those users, or like most of those users. And these strategies can be combined. If we follow the first strategy and look at item similarity in the case of recommending text objects, then we're talking about a content-based filtering or content-based recommendation. If we look at the second strategy, then, it's to compare users and in this case we're user similarity and the technique is often called collaborative filtering.\n3:46\nSo, let's first look at the content-based filtering system. This is what the system would look like.\n3:52\nInside the system, there will be a Binary Classifier that would have some knowledge about the user's interests, and this is called a User Interest Profile.\n4:02\nIt maintains this profile to keep track of all users interests, and then there is a utility function to guide the user to make decision a nice plan utility function in the moment. It helps the system decide where to set the threshold. And then the accepted documents will be those that have passed the threshold according to the classified. There should be also an initialization module that would take a user's input, maybe from a user's specified keywords or chosen category, etc., and this would be to feed into the system with the initiator's profile.\n4:39\nThere is also typically a learning module that would learn from users' feedback over time. Now note that in this case typical users information is stable so the system would have a lot more opportunities to observe the users. If the user has taken a recommended item, has viewed that, and this a signal to indicate that the recommended item may be relevant. If the user discarded it, no, it's not relevant. And so such feedback can be a long term feedback, and can last for a long time. And the system can collect a lot of information about the user's interest and this then can then be used to improve the classify. Now what's the criteria for evaluating such a system?\n5:24\nHow do we know this filtering system actually performs well? Now in this case we cannot use the ranking evaluation measures like a map because we can't afford waiting for a lot of documents and then rank the documents to make a decision for the users. And so the system must make a decision in real time in general to decide whether the item is above the threshold or not. So in other words, we're trying to decide on absolute relevance.\n5:56\nSo in this case, one common user strategy is to use a utility function to evaluate the system. So here, I show linear utility function. That's defined as for example three multiplied the number of good items that you delivered, minus two multiplied by the number of bad items that you delivered. So in other words, we could kind of just\n6:22\ntreat this as almost in a gambling game. If you delete one good item, let's say you win three dollars, you gain three dollars but if you deliver a bad one you will lose two dollars. And this utility function basically kind of measures how much money you are get by doing this kind of game, right? And so it's clear that if you want to maximize this utility function, this strategy should be delivered as many good articles as possible, and minimize the delivery of bad articles. That's obvious, right?\n7:03\nNow one interesting question here is how should we set these coefficients? I just showed a three and negative two as possible coefficients. But one can ask the question, are they reasonable?\n7:17\nSo what do you think?\n7:21\nDo you think that's a reasonable choice? What about the other choices?\n7:26\nSo for example, we can have 10 and minus 1, or 1, minus 10. What's the difference? What do you think?\n7:36\nHow would this utility function affect the systems' threshold of this issue.\n7:43\nRight, you can think of these two extreme cases. (10, -1) + (1, -10), which one do you think would encourage this system to over do it and which one would encourage this system to be conservative? If you think about it you will see that when we get a bigger award for delivering our good document you incur only a small penalty for delivering a bad one. Intuitively, you would be encouraged to deliver more. And you can try to deliver more in hope of getting a good one delivered. And then we'll get a big reward.\n8:19\nSo on the other hand, if you choose (1,-10), you really don't get such a big prize if you deliver a good document. On the other hand, you will have a big loss if you deliver a bad one. You can imagine that, the system would be very reluctant to deliver a lot of documents. It has to be absolutely sure that it's not. So this utility function has to be designed based on a specific application. The three basic problems in content-based filtering are the following, first, it has to make a filtering decision. So it has to be a binary decision maker, a binary classifier. Given a text document and a profile description of the user, it has to say yes or no, whether this document should be deleted or not.\n9:08\nSo that's a decision module, and it should be an initialization module as you have seen earlier and this will get the system started. And we have to initialize the system based on only very limited text exclusion or very few examples from the user.\n9:26\nAnd the third model is a learning model which you have, has to be able to learn from limited relevance judgements, because we counted them from the user about their preferences on the deliver documents. If we don't deliver document to the user we'll never be able to know whether the user likes it or not.\n9:50\nAnd we had accumulate a lot of documents even then from entire history.\n9:56\nAll these modules will have to be optimized to maximize the utility. So how can we deal with such a system? And there are many different approaches. Here we're going to talk about how to extend a retrieval system, a search engine for information filtering. Again, here's why we've spent a lot of time talking about the search engines. Because it's actually not very hard to extend the search engine for information filtering. So here's the basic idea for extending a retrieval system for information filtering. First, we can reuse a lot of retrieval techniques to do scoring. Right, so we know how to score documents against queries, etc. We're going to match the similarity between profile text description and a document. And then we can use a score threshold for the filtering decision. We do retrieval and then we kind of find the scores of documents and then we'll apply a threshold to see whether the document is passing the threshold or not. And if it's passing the threshold, we're going to say it's relevant and we're going to deliver it to the user. Another component that we have to add is, of course, to learn from the history, and we had used is the traditional feedback techniques to learn to improve scoring. And we know rock hill can be using for scoring improvement. And, but we have to develop a new approaches to learn how to accept this. And we need to set it initially and then we have to learn how to update the threshold over time. So here's what the system might look like if we just generalize the vector-space model for filtering problems, right? So you can see the document vector could be fed into a scoring module which already exists in a search engine that implements a vector-space model. And the profile will be treated as a query essentially, and then the profile vector can be matched with the document vector to generate the score.\n12:03\nAnd then this score would be fed into a thresholding module that would say yes or no, and then the evaluation would be based on the utility for the filtering results. If it says yes and then the document would be sent to the user. And then user could give some feedback. The feedback information would be used to both adjust the threshold and to adjust the vector representation. So the vector learning is essentially the same as query modification or feedback in the case of search. The threshold of learning is a new component and that we need to talk a little bit more about. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/KKDXC/10-8-text-categorization-methodsdict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.8 Text Categorization: Methods\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.8 Text Categorization: Methods\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:06\nThis lecture is about the methods for text categorization.\n0:12\nSo in this lecture we're going to discuss how to do text for categorization.\n0:19\nFirst, there're many methods for text categorization.\n0:25\nIn such a method the idea is to determine the category based on some rules that we design carefully to reflect the domain knowledge about the category prediction problem. So for example, if you want to do topic categorization for news articles you can say well, if the news article mentions word like a game and sports three times. That we're going to say it's about sports things like that and this would allow us to deterministically decide which category a document that should be put into.\n1:02\nNow such a strategy would work well if the following conditions hold. First the categories must be very well defined and this allows the person to clearly decide the category based on some clear rules.\n1:21\nA certainly the categories as\n1:25\nhalf to be easy to distinguished at the based on a surface features in text. So that means some official features like keywords or punctuations or whatever, you can easily identify in text to data.\n1:41\nFor example, if there is some special vocabulary that is known to only occur in a particular category. And that would be most effective because we can easily use such a vocabulary or padding of such a vocabulary to recognize this category.\n1:57\nNow we also should have sufficient knowledge for designing these words, and so if that's the case then such a can be effective. And so it does have a in some domains and sometimes. However, in general, there are several problems with this approach. First off, because it's label intensive it requires a lot of manual work. Obviously, we can't do this for all kinds of categorization problems. We have to do it from scratch for a different problem. problem because given the rules, what they need. So it doesn't scale up well.\n2:41\nSecondly, it cannot handle uncertainty in rules, often the rules Aren't  reliable. Take for example looking at occurrences of words in texts and trying to decide the topic.\n2:57\nIt's actually very hard to have  correct rule. So for example you can say well, if it has game, sports, basketball Then for sure it's about sports. But one can also imagine some types of articles that mention these cures, but may not be exactly about sports or only marginally touching sports. The main topic could be another topic, a different topic than sports.\n3:27\nSo that's one disadvantage of this approach. And then finally, the rules maybe inconsistent and this would lead to robustness. More specifically, and sometimes, the results of categorization may be different that depending on which rule to be applied. So as in that case that you are facing uncertainty. And you will also have to decide an order of applying the rules, or combination of results that are contradictory. So all these are problems with this approach. And it turns out that both problems can be solved or alleviated by using machine learning.\n4:07\nSo these machine learning methods are more automatic. But, I still put automatic in quotation marks because they are not really completely automatic cause it still require many work. More specifically we have to use a human experts to help in two ways. First the human experts must annotate data cells was category labels. And would tell the computer which documents should receive which categories. And this is called training data.\n4:38\nAnd then secondly, the human experts also need to provide a set of features to represent each text object. That can potentially provide a clue about the category. So, we need to provide some basic features for the computers to look into.\n4:55\nIn the case of tax a natural choice would be the words. So, using each has a feature is a very common choice to start with, but of course there are other sophisticated features like phrases or even parts of ancients tags or even syntax to the structures. So once human experts can provide this then we can use machine running to learn soft rules for categorization from the training data. So, soft rules just means, we're going to get decided which category we should be assigned for a document, but it's not going to be use using a rule that is deterministic. So we might use something similar to saying that if it matches games, sports many times, it's likely to be sports. But, we're not going to say exactly for sure but instead, we're going to use probabilities or weights. So that we can combine much more evidences. So, the learning process, basically is going to figure out which features are most useful for separating different categories. And it's going to also figure out how to optimally combine features to minimize errors of the categorization of the training data. So the training data, as you can see here, is very important. It's the basis for learning. And then, the trained classifier can be applied to a new text object to predict the most likely category. And that's to simulate the prediction of what human Would assign to this text object. If the human were to make a judgement. So when we use machine learning for text categorization we can also talk about the problem in the general setting of supervisement. So the set up is to learn a classifier to map a value of X. Into a map of Y so here X is all the text objects and Y is all the categories, a set of categories. So the class phi will take any value in x as input and would generate a value in y as output. We hope that output y with this right category for x. And here correct, of course, is judged based on the training data. So that's a general goal in machine learning problems or supervised learning problems where you are given some examples of input and output for a function. And then the computer's going to figure out the, how the function behaves like based on this examples. And then try to be able to compute the values for future x's that when we have not seen.\n7:38\nSo in general all methods would rely on discriminative features of text objects to distinguish different categories. So that's why these features are very important and they have to be provided by humans. And they will also combine multiple features in a weight map with weights to be optimized to minimize errors on the training data. So after the learning processes optimization problem. An objective function is often tied into the errors on the training data.\n8:12\nDifferent methods tend to vary in their ways of measuring the errors on the training data. They might optimize a different objective function, which is often also called a loss function or cost function.\n8:26\nThey also tend to vary in their ways of combining the features. So a linear combination for example is simple, is often used. But they are not as powerful as nonlinear combinations. But nonlinear models might be more complex for training, so there are tradeoffs as well. But that would lead to different variations of\n8:50\nmany variations of these learning methods. So in general we can distinguish two kinds of classifiers at a high level. One is called generative classifiers. The other is called discriminative classifiers. The generative classifiers try to learn what the data looks like in each category. So it attempts to model the joint distribution of the data and the label x and y and this can then be factored out to a product of why the distribution of labels. And the joint probability of sorry the conditional probability of X given Y, so it's Y. So we first model the distribution of labels and then we model how the data is generate a particular label here.\n9:48\nAnd once we can estimate these models, then we can compute this conditional probability of label given data based on the probability of data given label.\n10:02\nAnd the label distribution here by using the Bayes Rule.\n10:07\nNow this is the most important thing, because this conditional probability of the label can then be used directly to decide which label is most likely.\n10:18\nSo in such approaches objective function is actually likelihood. And so, we model how the data are generated. So it only indirectly captures the training errors. But if we can model the data in each category accurately, then we can also classify accurately.\n10:38\nOne example is Naïve Bayes classifier, in this case. The other kind of approaches are called discriminative classifies, and these classifies try to learn what features separate categories. So they direct or attack the problem of categorization for separation of classes. So sorry for the problem.\n11:04\nSo, these discriminative classifiers attempt to model the conditional probability of the label given the data point directly.\n11:17\nSo, the objective function tends to directly measure the errors of categorization on the training data.\n11:24\nSome examples include a logistical regression, support vector machines, and k-nearest neighbors. We will cover some of these classifiers in detail in the next few lectures. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/ZxnI1/10-9-text-categorization-generative-probabilistic-modelsdict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.9 Text Categorization: Generative Probabilistic Models\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.9 Text Categorization: Generative Probabilistic Models\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about how to use generative probabilistic models for text categorization.\n0:14\nThere are in general about two kinds of approaches to text categorization by using machine learning. One is by generating probabilistic models. The other is discriminative approaches. In this lecture, we're going to talk about the generative models. In the next lecture, we're going to talk about discriminative approaches. So the problem of text categorization is actually a very similar to document clustering. In that, we'll assume that each document it belongs to one category or one cluster. The main difference is that in clustering we don't really know what are the predefined categories are, what are the clusters. In fact, that's the goal of text clustering.\n0:55\nWe want to find such clusters in the data.\n0:59\nBut in the case of categorization, we are given the categories. So we kind of have pre-defined categories and then based on these categories and training data, we would like to allocate a document to one of these categories or sometimes multiple categories. But because of the similarity of the two problems, we can actually get the document clustering models for text categorization. And we understand how we can use generated models to do text categorization from the perspective of clustering. And so, this is a slide that we've talked about before, about text clustering, where we assume there are multiple topics represented by word distributions. Each topic is one cluster. So once we estimated such a model, we faced a problem of deciding which cluster document d should belong to. And this question boils down to decide which theta i has been used to generate d.\n2:06\nNow, suppose d has L words represented as xi here. Now, how can you compute the probability that a particular topic word distribution zeta i has been used to generate this document?\n2:27\nWell, in general, we use base wall to make this influence and you can see this prior information here that we need to consider if a topic or cluster has a higher prior then it's more likely that the document has been from this cluster. And so, we should favor such a cluster. The other is a likelihood part, it's this part.\n2:56\nAnd this has to do with whether the topic word of distribution can explain the content of this document well. And we want to pick a topic that's high by both values. So more specifically, we just multiply them together and then choose which topic has the highest product. So more rigorously, this is what we'd be doing. So we're going to choose the topic that would maximize. This posterior probability at the top of a given document gets posterior because this one, p of the i, is the prior. That's our belief about which topic is more likely, before we observe any document. But this conditional probability here is the posterior probability of the topic after we have observed the document of d.\n3:49\nAnd base wall allows us to update this probability based on the prior and I have shown the details, below here you can see how the prior here is related to the posterior, on the left-hand side.\n4:05\nAnd this is related to how well this word distribution explains the document here, and the two are related in this way. So to find the topic that has the higher posterior probability here it's equivalent to maximize this product as we have seen also, multiple times in this course.\n4:32\nAnd we can then change the probability of document in your product of the probability of each word, and that's just because we've made an assumption about independence in generating each word. So this is just something that you have seen in document clustering.\n4:50\nAnd we now can see clearly how we can assign a document to a category based on the information about word distributions for these categories and the prior on these categories. So this idea can be directly adapted to do categorization. And this is precisely what a Naive Bayes Classifier is doing. So here it's most really the same information except that we're looking at the categorization problem now. So we assume that if theta i represents category i accurately, that means the word distribution characterizes the content of documents in category i accurately. Then, what we can do is precisely like what we did for text clustering. Namely we're going to assign document d to the category that has the highest probability of generating this document. In other words, we're going to maximize this posterior probability as well.\n5:56\nAnd this is related to the prior and the [INAUDIBLE] as you have seen on the previous slide. And so, naturally we can decompose this [INAUDIBLE] into a product as you see here. Now, here, I change the notation so that we will write down the product as product of all the words in the vocabulary, and even though the document doesn't contain all the words. And the product is still accurately representing the product of all the words in the document because of this count here.\n6:37\nWhen a word, it doesn't occur in the document. The count would be 0, so this time will just disappear. So if actively we'll just have the product over other words in the document. So basically, with Naive Bayes Classifier, we're going to score each category for the document by this function.\n6:56\nNow, you may notice that here it involves a product of a lot of small probabilities. And this can cause and the four problem. So one way to solve the problem is thru take logarithm of this function, which it doesn't changes all the often these categories. But will helps us preserve precision. And so, this is often the function that we actually use to score each category and then we're going to choose the category that has the highest score by this function. So this is called an Naive Bayes Classifier, now the keyword base is understandable because we are applying a base rule here when we go from the posterior probability of the topic to a product of the likelihood and the prior.\n7:47\nNow, it's also called a naive because we've made an assumption that every word in the document is generated independently, and this is indeed a naive assumption because in reality they're not generating independently. Once you see some word, then other words will more likely occur. For example, if you have seen a word like a text. Than that mixed category, they see more clustering more likely to appear than if you have not the same text.\n8:15\nBut this assumption allows us to simplify the problem. And it's actually quite effective for many text categorization tasks. But you should know that this kind of model doesn't have to make this assumption. We could for example, assume that words may be dependent on each other. So that would make it a bigram analogy model or a trigram analogy model. And of course you can even use a mixture model to model what the document looks like in each category. So in nature, they will be all using base rule to do classification. But the actual generating model for documents in each category can vary. And here, we just talk about very simple case perhaps, the simplest case.\n9:00\nSo now the question is, how can we make sure theta i actually represents category i accurately? Now in clustering, we learned that this category i or what are the distributions for category i from the data. But in our case, what can we do to make sure this theta i represents indeed category i.\n9:25\nWell if you think about the question, and you likely come up with the idea of using the training data.\n9:34\nIndeed in the textbook, we typically assume that there is training data available and those are the documents that unknown to have generator from which category. In other words, these are the documents with known categories assigned and of course human experts must do that. In here, you see that T1 represents the set of documents that are known to have the generator from category 1. And T2 represents the documents that are known to have been generated from category 2, etc. Now if you look at this picture, you'll see that the model here is really a simplified unigram language model. It's no longer mixed modal, why? Because we already know which distribution has been used to generate which documents. There's no uncertainty here, there's no mixing of different categories here.\n10:30\nSo the estimation problem of course would be simplified. But in general, you can imagine what we want to do is estimate these probabilities that I marked here. And what other probability is that we have to estimate it in order to do relation. Well there are two kinds. So one is the prior, the probability of theta i and this indicates how popular each category is or how likely will it have observed the document in that category. The other kind is the water distributions and we want to know what words have high probabilities for each category.\n11:11\nSo the idea then is to just use observe the training data to estimate these two probabilities.\n11:18\nAnd in general, we can do this separately for the different categories. That's just because these documents are known to be generated from a specific category. So once we know that, it's in some sense irrelevant of what other categories we are also dealing with.\n11:37\nSo now this is a statistical estimation problem. We have observed some data from some model and we want to guess the parameters of this model. We want to take our best guess of the parameters.\n11:51\nAnd this is a problem that we have seen also several times in this course. Now, if you haven't thought about this problem, haven't seen life based classifier. It would be very useful for you to pause the video for a moment and to think about how to solve this problem. So let me state the problem again. So let's just think about with category 1, we know there is one word of distribution that has been used to generate documents. And we generate each word in the document independently, and we know that we have observed a set of n sub 1 documents in the set of Q1. These documents have been all generated from category 1. Namely have been all generated using this same word distribution. Now the question is, what would be your guess or estimate of the probability of each word in this distribution? And what would be your guess of the entire probability of this category? Of course, this singular probability depends on how likely are you to see documents in other categories?\n12:55\nSo think for a moment, how do you use all this training data including all these documents that are known to be in these k categories, to estimate all these parameters? Now, if you spend some time to think about this and it would help you understand the following few slides. So do spend some time to make sure that you can try to solve this problem, or do you best to solve the problem yourself. Now if you have thought about and then you will realize the following to it.\n13:29\nFirst, what's the bases for estimating the prior or the probability of each category. Well this has to do with whether you have observed a lot of documents form that category. Intuitively, you have seen a lot of documents in sports and very few in medical science. Then you guess is that the probability of the sports category is larger or your prior on the category would be larger.\n13:57\nAnd what about the basis for estimating the probability of where each category? Well the same, and you'll be just assuming that words that are observed frequently in the documents that are known to be generated from a category will likely have a higher probability. And that's just a maximum Naive Bayes made of. Indeed, that's what we can do, so this made the probability of which category and to answer the question, which category is most popular? Then we can simply normalize, the count of documents in each category. So here you see N sub i denotes the number of documents in each category.\n14:37\nAnd we simply just normalize these counts to make this a probability. In other words, we make this probability proportional to the size of training intercept in each category that's a size of the set t sub i.\n14:55\nNow what about the word distribution? Well, we do the same. Again this time we can do this for each category. So let's say, we're considering category i or theta i. So which word has a higher probability? Well, we simply count the word occurrences in the documents that are known to be generated from theta i.\n15:20\nAnd then we put together all the counts of the same word in the set. And then we just normalize these counts to make this distribution of all the words make all the probabilities off these words to 1. So in this case, you're going to see this is a proportional through the count of the word in the collection of training documents T sub i and that's denoted by c of w and T sub i.\n15:49\nNow, you may notice that we often write down probable estimate in the form of being proportional for certain numbers. And this is often sufficient, because we have some constraints on these distributions. So the normalizer is dictated by the constraint. So in this case, it will be useful for you to think about what are the constraints on these two kinds of probabilities? So once you figure out the answer to this question, and you will know how to normalize these accounts. And so this is a good exercise to work on if it's not obvious to you. There is another issue in Naive Bayes which is a smoothing. In fact the smoothing is a general problem in older estimate of language morals. And this has to do with, what would happen if you have observed a small amount of data? So smoothing is an important technique to address that outsmarts this. In our case, the training data can be small and when the data set is small when we use maximum likely estimator we often face the problem of zero probability. That means if an event is not observed then the estimated probability would be zero. In this case, if we have not seen a word in the training documents for let's say, category i. Then our estimator would be zero for the probability of this one in this category, and this is generally not accurate. So we have to do smoothing to make sure it's not zero probability. The other reason for smoothing is that this is a way to bring prior knowledge, and this is also generally true for a lot of situations of smoothing. When the data set is small, we tend to rely on some prior knowledge to solve the problem. So in this case our [INAUDIBLE] says that no word should have zero probability. So smoothing allows us to inject these to prior initial that no order has a real zero probability.\n17:54\nThere is also a third reason which us sometimes not very obvious, but we explain that in a moment. And that is to help achieve discriminative weighting of terms. And this is also called IDF weighting, inverse document frequency weighting that you have seen in mining word relations.\n18:14\nSo how do we do smoothing? Well in general we add pseudo counts to these events, we'll make sure that no event has 0 count.\n18:22\nSo one possible way of smoothing the probability of the category is to simply add a small non active constant delta to the count. Let's pretend that every category has actually some extra number of documents represented by delta.\n18:40\nAnd in the denominator we also add a k multiplied by delta because we want the probability to some to 1. So in total we've added delta k times because we have a k categories. Therefore in this sum, we have to also add k multiply by delta as a total pseudocount that we add up to the estimate.\n19:06\nNow, it's interesting to think about the influence of that data, obvious data is a smoothing parameter here. Meaning that the larger data is and the more we will do smoothing and that means we'll more rely on pseudocounts. And we might indeed ignore the actual counts if they are delta is set to infinity. Imagine what would happen if there are approaches positively to infinity? Well, we are going to say every category has an infinite amount of documents. And then there's no distinction to them so it become just a uniform.\n19:44\nWhat if delta is 0? Well, we just go back to the original estimate based on the observed training data to estimate to estimate the probability of each category. Now we can do the same for the word distribution. But in this case, sometimes we find it useful to use a nonuniform seudocount for the word. So here you'll see we'll add a pseudocounts to each word and that's mule multiplied by the probability of the word given by a background language model, theta sub b. Now that background model in general can be estimated by using a logic collection of tests. Or in this case we will use the whole set of all the training data to estimate this background language model. But we don't have to use this one, we can use larger test data that are available from somewhere else.\n20:36\nNow if we use such a background language model that has pseudocounts, we'll find that some words will receive more pseudocounts. So what are those words? Well those are the common words because they get a high probability by the background average model. So the pseudocounts added for such words will be higher. Real words on the other hand will have smaller pseudocounts. Now this addition of background model would cause a nonuniform smoothing of these word distributions. We're going to bring the probability of those common words to a higher level, because of the background model. Now this helps make the difference of the probability of such words smaller across categories. Because every category has some help from the background four words, and I get the, a, which have high probabilities. Therefore, it's not always so important that each category has documents that contain a lot of occurrences of such words or the estimate is more influenced by the background model. And the consequence is that when we do categorization, such words tend not to influence the decision that much as words that have small probabilities from the background language model. Those words don't get some help from the background language model. So the difference would be primary because of the differences of the occurrences in the training documents in different categories.\n22:05\nWe also see another smoothing parameter mu here, which controls the amount of smoothing and just like a delta does for the other probability.\n22:14\nAnd you can easily understand why we add mu to the denominator, because that represents the sum of all the pseudocounts that we add for all the words.\n22:25\nSo view is also a non negative constant and it's [INAUDIBLE] set to control smoothing. Now there are some interesting special cases to think about as well. First, let's think about when mu approaches infinity what would happen? Well in this case the estimate would approach\n22:43\nto the background language model we'll attempt to the background language model. So we will bring every word distribution to the same background language model and that essentially remove the difference between these categories. Obviously, we don't want to do that. The other special case is the thing about the background model and suppose, we actually set the two uniform distribution. And let's say, 1 over the size of the vocabulary. So each one has the same probability, then this smoothing formula is going to be very similar to the one on the top when we add delta. It's because we're going to add a constant pseudocounts to every word.\n23:29\nSo in general, in Naive Bayes categorization we have to do such a small thing. And then once we have these probabilities, then we can compute the score for each category. For a document and then choose the category where it was the highest score as we discussed earlier.\n23:49\nNow, it's useful to further understand whether the Naive Bayes scoring function actually makes sense. So to understand that, and also to understand why adding a background model will actually achieve the effect of IDF weighting and to penalize common words. So suppose we have just two categories and we're going to score based on their ratio of probability, right? So this is the.\n24:24\nLets say this is our scoring function for two categories, right? So, this is a score of a document for these two categories. And we're going to score based on this probability ratio. So if the ratio is larger, then it means it's more likely to be in category one. So the larger the score is the more likely the document is in category one. So by using Bayes' rule, we can write down this ratio as follows, and you have seen this before.\n25:09\nNow, we generally take logarithm of this ratio, and to avoid small probabilities. And this would then give us this formula in the second line. And here we see something really interesting, because this is our scoring function for deciding between the two categories.\n25:30\nAnd if you look at this function, we'll see it has several parts. The first part here is actually log of probability ratio. And so this is a category bias.\n25:41\nIt doesn't really depend on the document. It just says which category is more likely and then we would then favor this category slightly, right? So, the second part has a sum of all the words, right? So, these are the words that are observed in the document but in general we can consider all the words in the vocabulary. So here we're going to collect the evidence about which category is more likely, right? So inside of the sum you can see there is product of two things. The first, is a count of the word. And this count of the word serves as a feature to represent the document.\n26:27\nAnd this is what we can collect from document. The second part is the weight of this feature, here it's the weight on which word, right? This weight tells us to what extent observing this word helps contribute in our decision to put this document in category one. Now remember, the higher the scoring function is, the more likely it's in category one. Now if you look at this ratio, basically, sorry this weight it's basically based on the ratio of the probability of the word from each of the two distributions. Essentially we're comparing the probability of the word from the two distributions. And if it's a higher according to theta 1, then according to theta 2, then this weight would be positive. And therefore it means when we observe such a word, we will say that it's more likely to be from category one. And the more we observe such a word, the more likely the document will be classified as theta 1.\n27:35\nIf, on the other hand, the probability of the word from theta 1 is smaller than the probability of the word from theta 2, then you can see that this word is negative. Therefore, this is negative evidence for supporting category one. That means the more we observe such a word, the more likely the document is actually from theta 2.\n27:58\nSo this formula now makes a little sense, right? So we're going to aggregate all the evidence from the document, we take a sum of all the words. We can call this the features that we collected from the document that would help us make the decision. And then each feature has a weight that tells us how\n28:19\ndoes this feature support category one or just support category two. And this is estimated as the log of probability ratio here in naïve Bayes.\n28:32\nAnd then finally we have this constant of bias here. So that formula actually is a formula that can be generalized to accommodate more features and that's why I have introduce some other symbols here. To introduce beta 0 to denote the Bayes and fi to denote the each feature and beta sub i to denote the weight on each feature. Now we do this generalisation, what we see is that in general we can represent the document by feature vector fi, here of course in this case fi is the count of a word. But in general, we can put any features that we think are relevant for categorization. For example, document length or font size or count of other patterns in the document.\n29:27\nAnd then our scoring function can be defined as a sum of a constant beta 0 and the sum of the feature weights of all the features.\n29:42\nSo if each f sub i is a feature value then we multiply the value by the corresponding weight, beta sub i, and we just take the sum. And this is the aggregate of all evidence that we can collect from all these features. And of course there are parameters here. So what are the parameters? Well, these are the betas. These betas are weights. And with a proper setting of the weights, then we can expect such a scoring function to work well to classify documents, just like in the case of naive Bayes. We can clearly see naive Bayes classifier as a special case of this general classifier. Actually, this general form is very close to a classifier called a logistical regression, and this is actually one of those conditional approaches or discriminative approaches to classification.\n30:32\nAnd we're going to talk more about such approaches later, but here I want you to note that there is a strong connection, a close connection between the two kinds of approaches. And this slide shows how naive Bayes classifier can be connected to a logistic regression. And you can also see that in discriminative classifiers that tend to use more general form on the bottom, we can accommodate more features to solve the problem. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/MsDCs/course-introduction-videodict_values(["List\nCS 410: Text Information Systems\nWeek 1\nCourse Introduction Video\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nCourse Introduction Video\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nEnglish\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\nHello welcome to CS410 DSO Text Information Systems. This is an online course offered by University of Illinois at Urbana-Champaign. My name is ChengXiang Zhai. I also have a nickname, Cheng. I'm a professor of Computer Science at the University of Illinois at Urbana-Champaign. I'm the instructor of this course. This first lecture is just an introduction to the course. Let's first to start with some motivation. The problem we are trying to address in this course is how to harness big text data. Text data is all kinds of data in the form of natural languages such as English and Chinese. Now, this kind of data is everywhere and also growing very quickly. For example, you can find all kinds of web pages on the Internet and the number of pages is growing quickly. You can also find the blogs articles, news articles, or Emails and other kind of documents and enterprise environment. Of course, we will also have a lot of scientific literature in text form. And nowadays, social media has been growing quickly. So we now see, tweets and other social media data also in the form of text. All such text data encode a lot of useful knowledge about the world because it's in some sense the data reported by human census about the observe the world. So we can analyzed this kind of data to discover a lot of useful knowledge. Especially the knowledge about the human opinions or preferences. So this kind of data is very useful and we can use computational methods to turn such data into useful knowledge, which can then be further used in many applications. The main techniques for making this happen include the Text Retrieval and Text Mining. And these are the main techniques that we will cover in this course. Logically, in order to make use a lot of text data. We would first do text retrieval, and that's due to a large set of text data into a smaller but much more relevant set of data, that we actually need for a particular problem. And this step, is usually implemented by using text retrieval techniques that involve humans in the loop to find and locate the most relevant documents to a particular problem. Once we find the relevant documents, the next step is to do text mining, which is to further analyze the found of relevant documents to discover useful knowledge to extract the knowledge that can be directly used in application, especially in a application such as decision making. These two steps corresponding to Text Retrieval and Text Mining Techniques, that we will cover in this course. Based on this picture that I show you, this course is designed to leverage to corresponding books that I've offered on Coursera, which cover Text Retrieval and Text Mining respectively. The first book is called, Text Retrieval and Search Engines. The second book is called, the Text Mining and Analytics. These two books have comprehensive lecture videos that cover basic concepts and principles and methods for text retrieval and text mining respectively. So, this online course, will leverage all those lecture videos and also the online quizzes that are provided through the Coursera platform. But in addition to that, we also need to add additional components in order to explore applications because those books have covered a general techniques without a necessary discussing in depths how those techniques are using applications. So, to make the coverage more complete in CS410, we were adding two additional components as shown here. That is of course Products and Technology Review. Both are meant to give the students freedom to choose a particular way to apply some of the techniques that you have learned in those two books to solve a real world problem or to further learn about a particular topic. In the project, you will have opportunities to apply the knowledge that you have learned in those two books to solve a real world problem in an integrated manner. In technical review, you can leverage what you have learned to learn even more about either tool kit or a particular technology that can be used to extend your knowledge about how to solve a problem. The format of the course thus is a mixture of online videos plus some high engagement module which mostly consists of our interactions through forums as I will explain later. And also, we will have a lot of interactions to help you finish the course Project and Technology Review. There are a number of goals that we have kept in mind when designing this course and I want to share with you these goals because they would help you understand why the course has this particular format and why you are asked to do certain components of the tasks. The first goal is we want to emphasize both theory and practice. Theory is obviously very important because the basic concepts and general principles covering the theoretical part would be applicable to all applications. That means, they are not just used for today's applications but they can be used for solving future problems as well so, they are more general. Therefore, they have long lasting impact and utility value. And this is achieved by having you to watch all the lecture videos, and then to use quizzes and exams to make sure that you have mastered all the basic ideas, basic concepts and general principles and those general methods. Practical skills are also very important, and because of specific practical skills can be immediately useful for solving the problems today and maybe you will be able to resolve the problems that you encountered in your job. And this is achieved by having you to do Programming assignments, where you would be able to use existing tool kits, to look into some algorithms in depths and to use some of the algorithms to have some sense about how they work or how to improve them. And finally, of course it's very important to integrate the theory and practice. And this will be done by, having you to work on Course projects. The second goal is to Personalized Learning, because every one of you has a different in need and different in preference, and you will have a different in schedules as well, because many of you are full time workers, perhaps. And so, Personalized Learning is very important to ensure everyone to receive the best education. So to achieve this goal, we have intentionally designed the deadlines and the format of the course to be flexible so that, you can have a lot of freedom to do self-paced learning. For example, you can watch over the lecture videos at anytime that you want to. Watch it, watch that. And you can also finish all the quizzes pretty much anytime and you can do the assignment, program assigns particularly again in a flexible way. In addition to self paced learning, you would also have a choices of topics for project and the technology review. And you will be able to work on a topic that's most interesting to you, for both course project and technology review. Finally, we also have the goal of collaborative learning because this would maximize the efficiency of learning. Our common goal is to help everyone learn maximum amount of knowledge, while spending hopefully minimum amount of effort. So, this is a common goal that we should all work toward, and we should help each other achieve this goal. To promote collaborative learning and to facilitate collaborative learning, we will use forum-based interactions to enable all of us, including many of you from different time zones, to interact with each other effectively. Another way to support a collaborative learning is to have you to work in groups, to finish cost projects and technology reviews. So, both technology reviews and products can be done by working together with others in a group. So, overall the format is as follows: first, you will have to watch the lecture videos of those two MOOCs, and this is also shown here on this slide, and then you also need to take quizzes. And these quizzes are given in a weekly manner. But as I said, the deadlines are actually flexible. So, you should be able to finish quizzes as soon as you finish the corresponding lecture videos. These quizzes are two. There are also two kinds of quizzes. There are practice quizzes that you can use to help you understand some concepts. And there are also test quizzes. Those are of course to make sure that you have indeed mastered the materials. Then there are two exams. Those two exams will be given at the end of each MOOC corresponding. So, the first exam will be given at the end of the first MOOC on Test Retrieval, and the second one will be given at the end of the second MOOC on Test Mining. And then, during this period of watching these videos, you will be also working on programming assignments. Then at the end of the semester, we'll leave about two weeks time for you to work intensively on the course project, although you will be working on the course project throughout the semester. But most of the time will be in the end of the semester. Now, the course project will be down sequentially by following multiple steps. The first is for you to select a topic. You can select the topic from a list of topics that we provide or you can propose your topic, and then we'll ask you to submit a short proposal to be more specific about what you want to work on. And then, in the middle of the semester, you will be asked to provide a short progress report so that we can check your progress and we can provide help in a timely manner. And then, at the end of the semester, you will deliver two things: one is a software and its documentation. You upload that to a public website so that it's available to people. The second one is a tutorial presentation, a short presentation to explain how your software can be used by people. There is also a technology review component which is mandatory and this component will be correlated based on completion. Everyone is required to finish this component. Now, this component is designed to give you an opportunity to explore further any topic that you're interested in. This can be in that examination of a tool kit that you are interested in, perhaps a tool that you have used in the course project, or a comparison of multiple tools that can do similar things, and you might have looked into multiple tools, and you have some opportunity to compare them and to figure out which one is the best. Such a writing would be helpful for others to understand how to use a tool kit or which one to choose. Of course, those of you who are interested in the methods, you can also go in-depth to examine a certain type of methods and write a brief review of them, or looking to a cutting edge topic in your research and write a review of the recent papers about the topic. Now, we hope that you might be able to align the technology review with your course project so that the two will be kind of synergistic in that technology review will help you learn more about the topic related to your course project, while the course project will give you also a motivation for doing an in-depth study of the topic via technology review. Of course, we will try to help you finish all these tasks, and we'll do our best to help you. That means, the TAs and I will interact with you in various ways to help you, and one way is a synchronous question answering and discussing via forums. And the other way is to have synchronous weekly office hours and that will be down by using video teleconferencing. The grading will be done as follows:  of your grade will be based on the quizzes, all the quizzes for the two MOOCs;  will be based on the two exams, those two exams will be proctored exams;  will be based on the programming assignments, there will be multiple programming assignments throughout the semester. The remaining  will be based on your course project and that's for the distribution as follows:  is for topic selection,  is for proposal, and another  for progress report,  in most of the grade of the course project will be based on software deposit, your deliverable for the project. And finally,  is based on the tutorial presentation. Now, most of these components in the course project will be graded based on completion. Indeed, a lot of tasks you see in programming assignments are graded in the same way and that is because we believe we have designed the program assignments and course project in such a way that you will be able to learn a lot by simply going through these tasks. So, this is the effective way of learning by doing and that's why we choose to grade based on completion. That also means you have a lot of control over these grades, because you can ensure that you finish all these tasks and then you will get most of these grades. The only part that will be based on the quality of solution is the tutorial presentation. This is actually not just based on the quality of your tutorial presenting but rather based on whether you are software actually work as you propose. So, does it provide all the functions that you propose? Does it really work? And this will likely affect some of the grade for tutorial presentation, meaning that if a software for some reason doesn't really pass our test, then we would deduct the points from here. As I said, the technology review is not actually contributing to you grade, but it's a metric component and it will be graded based on completion. We also provide up to  of extra credit based on your participation in the forum discussions. And this is to encourage you to help each other and particularly by answering questions posed by others. We will be able to use log data from the forum to give you credit, extra credit, and these extra credit points will be actually added to the regular points. So that will allow you to actually increase your grade as will be shown here in more detail. And this is how we are going to determine your final letter grades. They are based on the grade points you have collected over the semester using this map shown here on the slide. Mostly it's five points for each bracket but not always. And as you can see, if you have earned  extra credit and when adding this extra credit, will likely help you move your grades up by one bracket, although it won't be more than one bracket. So, we feel that this fixed mapping would give you complete control over your grade, so you can monitor your grade over the semester and kind of assess your progress and also to adjust your schedule accordingly. So, this is a visualization of your workload. Horizontally, we show the timeline, from the first day of instruction to the last day of instruction. And then vertically, you can see there are mainly six tasks for you over the semester. First, you will spend most of your time on watching the lecture videos, and this is why we have a thick black line there that shows that most of your effort probably will be spent on watching lecture videos. And then, you will be taking 12 quizzes, and these will be spreading over most of the semester, corresponding to about 12 weeks when you are expected to watch those lecture videos. Now, we want to emphasize that you should spend most of your time to watch videos and make sure you understand the materials before you take quizzes, and this will ensure that you don't leave any holes. If you do it the other way and by using quizzes to guide you through the lecture videos, then you might leave some holes, and that might hurt your performance on exams. There will be two proctored exams that will be given in the middle of the semester and also later in the semester, at the end of the two box. So, this is your third task. The fourth task is programming assignments, and this will be, again, through the entire semester, actually not really all the weeks in the semester, but most of the weeks, because we don't want you to use up the last two weeks, which we reserve for you to work on course project. And the course project is the fifth component. And finally, you need to finish technology review. Now, although most of the work of the project is expected to be done at the end of the semester, you will actually start working on it from the very beginning. And, soon after the semester starts, we will ask you to think about the topics and you will discuss the topics and then form teams to submit the proposal. But of course, during the first 12 weeks, you will be most occupied by the lecture videos, quizzes, and your assignments for programming. So, that's why you will likely have more time to work on the project in the end. The technology review is kind of designed to extend your knowledge based on your project. But of course, you have the complete freedom to choose whatever topic that you want to review. So, you don't have to tie it to the project. And so, we imagine that you would start working on it a little bit after you have decided the project, the topic, so that you can decide whether you want to tie the technology review with your project. So, it's good to keep this picture in mind throughout the semester because you might have irregular schedule sometimes. For example, you might find that you are very busy in the middle of the semester, then this picture can help you adjust your schedule and the degree you probably want to then work more on some of the tasks at the beginning of the semester, so that you don't overwhelm yourself in the middle of the semester. And similarly, if you expect to be very busy the end of the semester, you might want to start working on the project much earlier. So, I hope this picture will be always in your mind, and you will be able to adjust your schedule accordingly and to particularly work on tasks proactively, in case you anticipate any busy time period. Some of you have already taken maybe both MOOCs. Now, if you are one of them, then I think naturally, you will have more time to work on other problems in this course. So, you should take advantage of this to proactively finish some of the tasks much more quickly. In particular, since you have already watched those videos and then you can simply review them and then you try to work on the quizzes so that you can finish all the quizzes much more quickly. You may be able to finish most of the programming assignments quickly too. This is not only going to be helpful for you in the sense that you will have a lot of time to work on course project, but also, it would allow you to help others by answering their questions on forums or helping them in other way, like teaming up with them to work on the project. However, I should also say that there are a few tasks that are, unfortunately, have very fixed time that you cannot really work on in advance. For example, the two exams will be scheduled on some particular dates that would have no flexibility for you to work on earlier. There may be some tasks in the programming assignments that have to be synchronized. For example, we might run competition of some tasks and there may be some synchronization that's needed, but we would like to minimize the dependency so that you could hopefully work on many of those tasks as early as you can. And of course, we encourage you to use more time to finish a more challenging course project or to finish a higher quality technology review. Now, we rely a lot on forum discussion, and this is because we are in different time zones and it's very hard to find a time that works well for everyone. But forum has important advantages of being able to accommodate everyone in the discussion. So, this will be the primary way of our interactions and engagement, and in particular, we will be using Piazza, which is a forum that we have used for many other courses, and it has proven to be a useful forum with a lot of useful functions. The second advantage of forum is it would also enable you to ask your questions as soon as you have a question. And therefore, we can hopefully accelerate question answering and so that you can have your question answered quickly on the forum without waiting until office hour. Finally, we hope that the forum discussions would help us identify difficult concepts in those lectures, so that we can focus on discussing these concepts or explaining these concepts that are hard to understand in our office hours. This would make better use of our office hours to help all of you. Because of these reasons, so the protocol of question answering will be as follows, and we want to emphasize that it's important to follow this protocol so that we can make effective use of our office hours, so that you can get your questions answered more quickly, so that you can all help each other in learning. So, first, as soon as you have a question or issue to discuss, post it immediately on the forum. And this has a number of advantages, and first is, it will give you the opportunity to have the question answered quickly because often, your question may be answered by your peers or may be answered by a teacher or by me. So, by posting the question immediately, you have a better chance of getting the question answered quickly, so that you won't have to wait. And secondly, your question might also help others because sometimes other students may have a similar question or may not realize that they have encountered this question, but you just articulated this question. So, this is helpful for your peers as well. And discussing of this question is often also a good way to learn. However, if your question is not answered in a timely manner on the forum, or addressed adequately from your perspective, then you should email the question to all of us including me and TAs. Please use a subject line that contains the keyword CS410DSO all together. Of course, your subject line, you can contain other keywords based on your question. And then, if you don't receive a reply from us by email in a timely manner, join an office-hour. Now, we would do our best to reply to your emails but then, depending on the number of emails, depending on our own schedule, we may not be able to always reply to your emails in a timely manner. Of course, we would do our best to respond quickly but if we can't, then you should come to one of our office hours. We would try to schedule our office hours in different time slots during the day. For example, we would have some time slot in the morning, or late morning, or early afternoon, and then another time slot in the evening. And this is so that, we can hopefully accommodate different time zones, because the same slot of time may not be equally convenient with all of you. So, we'll do our best again, to diversify the time slots, both in terms of the time in a day, and also in terms of the days in a week. The format of office hours is as follows, and this again, is based on our need to make effective use and the efficient use of our limited office hours to help you in the best way. So, we will hold weekly office hours, as I said, and we'll publish those time slots. And the office hours will be given by using video-teleconference, and particularly using Zoom, and its used for system that has worked well. And you can join or leave an office hour at any time and that means you can join late, or you can leave early, and it's very flexible. So, don't feel that you always have to come at the beginning of the office hour. Feel free to stop by in the last ten minutes, if that's the best time for you. And again, by taking advantage of the forum discussion, hopefully, by the time of going to office hour, we will only need to deal with some of the relatively difficult questions. And the priority we will use is to give the highest priority to any issues that have already been posted on forums, but have not been resolved even after some email communications. Those are clearly the toughest issues because it has been posted on forum without a good answer, and then was emailed to us and still not satisfactory solved. So, we would have to give those issues the highest priority, that means if there are such issues being raised during office hour, those issues would be given the first priority. After that, we'll look at the any other unresolved issues on forums. That's why it's very important for you to post the issue on the forum first, and only after resolving those issues that have already been posted on forums would we take other questions or issues that have not yet been posted on forums. And of course, you should not hesitate to bring any questions that you have or any issues you want to discuss. It's just that, we want to have a policy to prioritize the issues that we want to handle, so that we can provide the maximum benefit to all of you by using our office hours efficiently. Finally, I want to just say something in general about how to get the most out of this course. Perhaps, the most important advice is to plan ahead based on your own schedule, because many of you are very busy. So, you want to kind of take a look at the picture that I showed you earlier about the tasks, and then consider your own schedule. Try to imagine which periods will be a relatively busy period for you and identify what tasks you're supposed to work on in that period, and try to finish those tasks earlier, so that you don't overlap with yourself during that busy period. And of course, at any time, please let us know how we can help, again, by using the forums as the first step. And another thing to mention is also to allocate a sufficient time for the preparation of two proctored exams because they will be given only once. That means, you only have one chance to take each exam, so you want to really prepare well for them. However, those exams are mostly to confirm that you have indeed mastered the materials. So, they will have similar questions to the quiz questions that you have already seen. And in fact, some questions may be exactly the same as the questions in the quizzes. And we do that because this would allow you to have some sense about what the questions might look like in the exams. So, they should not be much surprise if you have actually worked on all the questions in those quizzes and have made sure that you have understood the answers to those questions. Of course, if you can, try to complete the quizzes and program assignments ahead of time. This would be to your advantage because you can now raise your questions earlier that would allow you to have more time to get your questions answered. Therefore, by the time when you take the quiz, then you would have all the questions resolved. And it also would allow you to actively help others and to discuss any of the problems that you have encountered that would help you earn the extra credit also. The second advice is to post questions on forum immediately and whenever you have difficulty understanding any part of the course materials. And again, I want to emphasize the immediate action of posting any issue, any question on the forums. We would do our best to resolve those issues through the forums. And it's the effective way to also engage the peers to help each other. So, do not hesitate to post questions and we won't to penalize you for posting many questions. In fact, we'll reward that perhaps, because that's one way to contribute to the forum discussion. Finally, you should leverage collaborative learning, this is kind of related to you posting your questions on forums, and again, to actively participate in the forum discussion. You will actually learn a lot from reading other people's posts, even if you know the answers because they are often opinions expressed about those materials. So, we do hope that you have active discussion on forums to help each other and to help each other, particularly, to save time, to understand the difficult concepts, to do well in the quizzes and exams, and to finish assignments smoothly. And finally, we do encourage you to help each other understand materials too. So, we encourage you to answer others' questions, and the system will record those answers. We'll have a statistics about that, and then we'll use that to give extra credit. So, this was just the overall introduction to the course. For more information, you can visit the course website on Coursera. We hope you enjoy this course, and I look forward to working with you. Thank you.\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/kv4Aj/lesson-4-2-statistical-language-modeldict_values(["List\nCS 410: Text Information Systems\nWeek 4\nLesson 4.2: Statistical Language Model\nPrevious\nNext\nWeek 4 Information\nWeek 4 Lessons\nVideo:\nVideo\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.2: Statistical Language Model\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 4.3: Query Likelihood Retrieval Function\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.4: Statistical Language Model - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.5: Statistical Language Model - Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.6: Smoothing Methods - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.7: Smoothing Methods - Part 2\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nProgramming Assignment 2.2\nLesson 4.2: Statistical Language Model\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:07\n[SOUND] This lecture is about the statistical language model. In this lecture, we're going to give an introduction to statistical language model. This has to do with how do you model text data with probabilistic models. So it's related to how we model query based on a document.\n0:31\nWe're going to talk about what is a language model. And then we're going to talk about the simplest language model called the unigram language model, which also happens to be the most useful model for text retrieval. And finally, what this class will use is a language model.\n0:47\nWhat is a language model? Well, it's just a probability distribution over word sequences. So here, I'll show one.\n0:55\nThis model gives the sequence Today is Wednesday a probability of 0.001. It give Today Wednesday is a very, very small probability because it's non-grammatical.\n1:11\nYou can see the probabilities given to these sentences or sequences of words can vary a lot depending on the model. Therefore, it's clearly context dependent. In ordinary conversation, probably Today is Wednesday is most popular among these sentences. Imagine in the context of discussing apply the math, maybe the eigenvalue is positive, would have a higher probability. This means it can be used to represent the topic of a text.\n1:42\nThe model can also be regarded as a probabilistic mechanism for generating text. And this is why it's also often called a generating model. So what does that mean? We can imagine this is a mechanism that's visualised here as a stochastic system that can generate sequences of words. So, we can ask for a sequence, and it's to send for a sequence from the device if you want, and it might generate, for example, Today is Wednesday, but it could have generated any other sequences. So for example, there are many possibilities, right?\n2:24\nSo in this sense, we can view our data as basically a sample observed from such a generating model. So, why is such a model useful? Well, it's mainly because it can quantify the uncertainties in natural language. Where do uncertainties come from? Well, one source is simply the ambiguity in natural language that we discussed earlier in the lecture. Another source is because we don't have complete understanding, we lack all the knowledge to understand the language. In that case, there will be uncertainties as well. So let me show some examples of questions that we can answer with a language model that would have interesting applications in different ways. Given that we see John and feels, how likely will we see happy as opposed to habit as the next word in a sequence of words? Now, obviously, this would be very useful for speech recognition because happy and habit would have similar acoustic sound, acoustic signals. But, if we look at the language model, we know that John feels happy would be far more likely than John feels habit.\n3:35\nAnother example, given that we observe baseball three times and game once in a news article, how likely is it about sports? This obviously is related to text categorization and information retrieval.\n3:48\nAlso, given that a user is interested in sports news, how likely would the user use baseball in a query? Now, this is clearly related to the query likelihood that we discussed in the previous lecture.\n4:02\nSo now, let's look at the simplest language model, called a unigram language model. In such a case, we assume that we generate a text by generating each word independently.\n4:14\nSo this means the probability of a sequence of words would be then the product of the probability of each word. Now normally, they're not independent, right? So if you have single word in like a language, that would make it far more likely to observe model than if you haven't seen the language. So this assumption is not necessarily true, but we make this assumption to simplify the model.\n4:41\nSo now the model has precisely N parameters, where N is vocabulary size. We have one probability for each word, and all these probabilities must sum to 1. So strictly speaking, we actually have N-1 parameters.\n5:00\nAs I said, text can then be assumed to be assembled, drawn from this word distribution.\n5:08\nSo for example, now we can ask the device or the model to stochastically generate the words for us, instead of sequences. So instead of giving a whole sequence, like Today is Wednesday, it now gives us just one word. And we can get all kinds of words. And we can assemble these words in a sequence. So that will still allow you to compute the probability of Today is Wednesday as the product of the three probabilities.\n5:37\nAs you can see, even though we have not asked the model to generate the sequences, it actually allows us to compute the probability for all the sequences, but this model now only needs N parameters to characterize. That means if we specify all the probabilities for all the words, then the model's behavior is completely specified. Whereas if we don't make this assumption, we would have to specify probabilities for all kinds of combinations of words in sequences.\n6:11\nSo by making this assumption, it makes it much easier to estimate these parameters. So let's see a specific example here.\n6:19\nHere I show two unigram language models with some probabilities. And these are high probability words that are shown on top.\n6:29\nThe first one clearly suggests a topic of text mining, because the high probability was all related to this topic. The second one is more related to health.\n6:39\nNow we can ask the question, how likely were observe a particular text from each of these two models? Now suppose we sample words to form a document. Let's say we take the first distribution, would you like to sample words? What words do you think would be generated while making a text or maybe mining maybe another word? Even food, which has a very small probability, might still be able to show up.\n7:03\nBut in general, high probability words will likely show up more often.\n7:08\nSo we can imagine what general text of that looks like in text mining.\n7:12\nIn fact, with small probability, you might be able to actually generate the actual text mining paper. Now, it will actually be meaningful, although the probability will be very, very small.\n7:26\nIn an extreme case, you might imagine we might be able to generate a text mining paper that would be accepted by a major conference. And in that case, the probability would be even smaller. But it's a non-zero probability, if we assume none of the words have non-zero probability.\n7:47\nSimilarly from the second topic, we can imagine we can generate a food nutrition paper. That doesn't mean we cannot generate this paper from text mining distribution.\n7:59\nWe can, but the probability would be very, very small, maybe smaller than even generating a paper that can be accepted by a major conference on text mining.\n8:10\nSo the point is that the keeping distribution,\n8:13\nwe can talk about the probability of observing a certain kind of text. Some texts will have higher probabilities than others.\n8:21\nNow let's look at the problem in a different way. Suppose we now have available a particular document. In this case, many of the abstract or the text mining table, and we see these word counts here. The total number of words is 100. Now the question you ask here is an estimation question. We can ask the question which model, which one of these distribution has been used to generate this text, assuming that the text has been generated by assembling words from the distribution.\n8:51\nSo what would be your guess?\n8:54\nWhat we have to decide are what probabilities text mining, etc., would have.\n9:01\nSuppose the view for a second, and try to think about your best guess.\n9:09\nIf you're like a lot of people, you would have guessed that well, my best guess is text has a probability of 10 out of 100 because I've seen text 10 times, and there are in total 100 words. So we simply normalize these counts.\n9:27\nAnd that's in fact the word justified, and your intuition is consistent with mathematical derivation. And this is called the maximum likelihood estimator. In this estimator, we assume that the parameter settings of those that would give our observe the data the maximum probability. That means if we change these probabilities, then the probability of observing the particular text data would be somewhat smaller.\n9:55\nSo you can see, this has a very simple formula. Basically, we just need to look at the count of a word in a document, and then divide it by the total number of words in the document or document lens. Normalize the frequency. A consequence of this is, of course, we're going to assign zero probabilities to unseen words. If we have an observed word, there will be no incentive to assign a non-zero probability using this approach. Why? Because that would take away probability mass for these observed words. And that obviously wouldn't maximize the probability of this particular observed text data. But one has still question whether this is our best estimate. Well, the answer depends on what kind of model you want to find, right? This estimator gives a best model based on this particular data. But if you are interested in a model that can explain the content of the full paper for this abstract, then you might have a second thought, right? So for thing, there should be other words in the body of that article, so they should not have zero probabilities, even though they're not observed in the abstract. So we're going to cover this a little bit more later in this class in the query likelihood model.\n11:24\nSo let's take a look at some possible uses of these language models. One use is simply to use it to represent the topics. So here I show some general English background texts. We can use this text to estimate a language model, and the model might look like this.\n11:42\nRight, so on the top, we have those all common words, the, a, is, we, etc., and then we'll see some common words like these, and then some very, very rare words in the bottom. This is a background language model. It represents the frequency of words in English in general. This is the background model. Now let's look at another text, maybe this time, we'll look at the computer science research papers.\n12:11\nSo we have a collection of computer science research papers, we do as mentioned again, we can just use the maximum likelihood estimator, where we simply normalize the frequencies.\n12:20\nNow in this case, we'll get the distribution that looks like this. On the top, it looks similar because these words occur everywhere, they are very common. But as we go down, we'll see words that are more related to computer science, computer software, text, etc. And so although here, we might also see these words, for example, computer, but we can imagine the probability here is much smaller than the probability here. And we will see many other words here that would be more common in general English. So you can see this distribution characterizes a topic of the corresponding text. We can look at even the smaller text.\n13:03\nSo in this case, let's look at the text mining paper. Now if we do the same, we have another distribution, again the can be expected to occur in the top. The sooner we see text, mining, association, clustering, these words have relatively high probabilities. In contrast, in this distribution, the text has a relatively small probability. So this means, again, based on different text data, we can have a different model, and the model captures the topic. So we call this document the language model, and we call this collection language model. And later, you will see how they're used in the retrieval function.\n13:47\nBut now, let's look at another use of this model. Can we statistically find what words are semantically related to computer?\n13:56\nNow how do we find such words? Well, our first thought is that let's take a look at the text that match computer. So we can take a look at all the documents that contain the word computer. Let's build a language model. We can see what words we see there. Well, not surprisingly, we see these common words on top as we always do. So in this case, this language model gives us the conditional probability of seeing the word in the context of computer. And these common words will naturally have high probabilities. But we also see the computer itself and software will have relatively high probabilities. But if we just use this model, we cannot just say all these words are semantically related to computer.\n14:43\nSo ultimately, what we'd like to get rid of is these common words. How can we do that?\n14:52\nIt turns out that it's possible to use language model to do that.\n14:57\nBut I suggest you think about that. So how can we know what words are very common, so that we want to kind of get rid of them?\n15:07\nWhat model will tell us that? Well, maybe you can think about that. So the background language model precisely tells us this information. It tells us what was our common in general. So if we use this background model, we would know that these words are common words in general. So it's not surprising to observe them in the context of computer. Whereas computer has a very small probability in general, so it's very surprising that we have seen computer with this probability, and the same is true for software.\n15:44\nSo then we can use these two models to somehow figure out the words that are related to computer. For example, we can simply take the ratio of these group probabilities and normalize the topic of language model by the probability of the word in the background language model. So if we do that, we take the ratio, we'll see that then on the top, computer is ranked, and then followed by software, program, all these words related to computer. Because they occur very frequently in the context of computer, but not frequently in the whole collection, whereas these common words will not have a high probability. In fact, they have a ratio about 1 down there because they are not really related to computer. By taking the sample of text that contains the computer, we don't really see more occurrences of that than in general.\n16:40\nSo this shows that even with these simple language models, we can do some limited analysis of semantics.\n16:48\nSo in this lecture, we talked about language model, which is basically a probability distribution over text. We talked about the simplest language model called unigram language model, which is also just a word distribution. We talked about the two uses of a language model. One is we represent the topic in a document, in a collection, or in general. The other is we discover word associations.\n17:16\nIn the next lecture, we're going to talk about how language model can be used to design a retrieval function.\n17:23\nHere are two additional readings. The first is a textbook on statistical natural language processing.\n17:30\nThe second is an article that has a survey of statistical language models with a lot of pointers to research work. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/qGZrA/8-1-syntagmatic-relation-discovery-entropydict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.1 Syntagmatic Relation Discovery: Entropy\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.1 Syntagmatic Relation Discovery: Entropy\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]. This lecture is about the syntagmatic relation discovery, and entropy. In this lecture, we're going to continue talking about word association mining. In particular, we're going to talk about how to discover syntagmatic relations. And we're going to start with the introduction of entropy, which is the basis for designing some measures for discovering such relations.\n0:32\nBy definition, syntagmatic relations hold between words that have correlated co-occurrences. That means, when we see one word occurs in context, we tend to see the occurrence of the other word.\n0:48\nSo, take a more specific example, here. We can ask the question, whenever eats occurs, what other words also tend to occur?\n1:01\nLooking at the sentences on the left, we see some words that might occur together with eats, like cat, dog, or fish is right. But if I take them out and if you look at the right side where we only show eats and some other words, the question then is. Can you predict what other words occur to the left or to the right?\n1:28\nRight so this would force us to think about what other words are associated with eats. If they are associated with eats, they tend to occur in the context of eats.\n1:38\nMore specifically our prediction problem is to take any text segment which can be a sentence, a paragraph, or a document. And then ask I the question, is a particular word present or absent in this segment?\n1:54\nRight here we ask about the word W. Is W present or absent in this segment?\n2:02\nNow what's interesting is that some words are actually easier to predict than other words.\n2:10\nIf you take a look at the three words shown here, meat, the, and unicorn, which one do you think is easier to predict?\n2:20\nNow if you think about it for a moment you might conclude that\n2:24\nthe is easier to predict because it tends to occur everywhere. So I can just say, well that would be in the sentence.\n2:31\nUnicorn is also relatively easy because unicorn is rare, is very rare. And I can bet that it doesn't occur in this sentence.\n2:42\nBut meat is somewhere in between in terms of frequency. And it makes it harder to predict because it's possible that it occurs in a sentence or the segment, more accurately.\n2:53\nBut it may also not occur in the sentence, so now let's study this problem more formally.\n3:02\nSo the problem can be formally defined as predicting the value of a binary random variable. Here we denote it by X sub w, w denotes a word, so this random variable is associated with precisely one word.\n3:18\nWhen the value of the variable is 1, it means this word is present. When it's 0, it means the word is absent. And naturally, the probabilities for 1 and 0 should sum to 1, because a word is either present or absent in a segment.\n3:35\nThere's no other choice.\n3:38\nSo the intuition with this concept earlier can be formally stated as follows. The more random this random variable is, the more difficult the prediction will be.\n3:49\nNow the question is how does one quantitatively measure the randomness of a random variable like X sub w?\n3:56\nHow in general, can we quantify the randomness of a variable and that's why we need a measure called entropy and this measure introduced in information theory to measure the randomness of X. There is also some connection with information here but that is beyond the scope of this course.\n4:17\nSo for our purpose we just treat entropy function as a function defined on a random variable. In this case, it is a binary random variable, although the definition can be easily generalized for a random variable with multiple values.\n4:32\nNow the function form looks like this, there's the sum of all the possible values for this random variable. Inside the sum for each value we have a product of the probability\n4:45\nthat the random variable equals this value and log of this probability.\n4:53\nAnd note that there is also a negative sign there.\n4:56\nNow entropy in general is non-negative. And that can be mathematically proved.\n5:02\nSo if we expand this sum, we'll see that the equation looks like the second one. Where I explicitly plugged in the two values, 0 and 1. And sometimes when we have 0 log of 0, we would generally define that as 0, because log of 0 is undefined.\n5:28\nSo this is the entropy function. And this function will give a different value for different distributions of this random variable.\n5:37\nAnd it clearly depends on the probability that the random variable taking value of 1 or 0. If we plot this function against the probability that the random variable is equal to 1.\n5:56\nAnd then the function looks like this.\n6:01\nAt the two ends, that means when the probability of X\n6:07\nequals 1 is very small or very large, then the entropy function has a low value. When it's 0.5 in the middle then it reaches the maximum.\n6:20\nNow if we plot the function against the probability that X\n6:25\nis taking a value of 0 and the function would show exactly the same curve here, and you can imagine why. And so that's because\n6:42\nthe two probabilities are symmetric, and completely symmetric.\n6:48\nSo an interesting question you can think about in general is for what kind of X does entropy reach maximum or minimum. And we can in particular think about some special cases. For example, in one case, we might have a random variable that\n7:08\nalways takes a value of 1. The probability is 1.\n7:16\nOr there's a random variable that\n7:19\nis equally likely taking a value of one or zero. So in this case the probability that X equals 1 is 0.5.\n7:30\nNow which one has a higher entropy?\n7:34\nIt's easier to look at the problem by thinking of a simple example\n7:40\nusing coin tossing.\n7:43\nSo when we think about random experiments like tossing a coin,\n7:48\nit gives us a random variable, that can represent the result. It can be head or tail. So we can define a random variable X sub coin, so that it's 1 when the coin shows up as head, it's 0 when the coin shows up as tail.\n8:09\nSo now we can compute the entropy of this random variable. And this entropy indicates how difficult it is to predict the outcome\n8:22\nof a coin toss.\n8:25\nSo we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head or tail equally likely. So the two probabilities would be a half. Right? So both are equal to one half.\n8:44\nAnother extreme case is completely biased coin, where the coin always shows up as heads. So it's a completely biased coin.\n8:54\nNow let's think about the entropies in the two cases. And if you plug in these values you can see the entropies would be as follows. For a fair coin we see the entropy reaches its maximum, that's 1.\n9:11\nFor the completely biased coin, we see it's 0. And that intuitively makes a lot of sense. Because a fair coin is most difficult to predict.\n9:22\nWhereas a completely biased coin is very easy to predict. We can always say, well, it's a head. Because it is a head all the time. So they can be shown on the curve as follows. So the fair coin corresponds to the middle point where it's very uncertain. The completely biased coin corresponds to the end point where we have a probability of 1.0 and the entropy is 0. So, now let's see how we can use entropy for word prediction. Let's think about our problem is to predict whether W is present or absent in this segment. Again, think about the three words, particularly think about their entropies.\n10:06\nNow we can assume high entropy words are harder to predict.\n10:11\nAnd so we now have a quantitative way to tell us which word is harder to predict.\n10:20\nNow if you look at the three words meat, the, unicorn, again, and we clearly would expect meat to have a higher entropy than the unicorn. In fact if you look at the entropy of the, it's close to zero. Because it occurs everywhere. So it's like a completely biased coin.\n10:44\nTherefore the entropy is zero.\n10:48\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/SVcVq/course-communicationdict_values(['List\nCS 410: Text Information Systems\nWeek 1\nCourse Communication\nPrevious\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nCourse Communication\nCourse Communication\nImportant announcements will be posted on the Campuswire forum, so please check it out periodically. This is also your go-to place to ask clarifying questions on course content, quizzes, programming assignments, and your final project. Before you post, check if someone has already asked your question. It is highly recommended to check the Campuswire forum at least twice a day.\nmcs-support@illinois.edu - this is your go-to place for non-content related, administrative questions you may have for course staff, the program and technical issues with the platform. We are committed to answering your questions with 24 - 48 hours during the working week.\nCourse Staff Response Turnaround Time\nThe course staff will attempt to ensure your question is answered within 24-48 hours of it being posted. This does not mean you should post questions at the last minute (e.g., right before a deadline).\nOffice Hours\nThe course staff will host weekly office hours via Zoom. To check the weekly schedule and to attend office hours, please refer to the Live Events page.\nZoom\nIf you plan to attend the office hours, please sign up for a free Zoom account  and download the Zoom client application. For Zoom training and support, please refer to the end of this page.\nSlack\nSlack offers instant messaging and collaboration when you want to connect with the class. As a student, you need to sign up for a free Slack account at https://mcs-dsstudents.slack.com/signup. Make sure that when you register, use your @illinois.edu email address. Other emails will NOT work.\nIf you have any issues with setting up your Slack account, please send an inquiry email to mcs-support@illinois.edu.\nOnce you have a Slack account, you can join the slack channel for the course by searching #cs-410-text-info-syst.\nCourse Help Resources\nFor Course Content Issues and Discussions\nIf you have questions about any lecture videos or assignments, the course Campuswire forum is your first go-to place. You can search to see if someone has already posted a similar question. If not, you can post your own thread in the forum.\nFor MCS-DS Program Support\nThe MCS-DS Support Team of teaching and operational specialists from both the University of Illinois and Coursera are dedicated to providing you with program support help with any question you have from course enrollment to letters of course completion. You can reach this team at mcsds-support@illinois.edu. \nFor Coursera Platform Technical Support\nIf you have a technical issue, the Coursera Help Center is a great place to start. The Help Center has detailed information on:\nAccount setup\nVideo troubleshooting and downloads\nAssignments\nPeer reviews\nDegree student features\nCoursera policies\nFor Coursera Platform Urgent Technical Support\nIf you need to speak to someone for urgent technical support, you are very welcome to use our chat support. This support is offered 24 hours a day 7 days per week. As a degree student, you can access chat support on any page of the Course Help Center.\nRemember that Coursera chat support does not always know that you are a degree student. Please let them know that you are a degree student for the MCS-DS program as soon as you start a chat. That way we can ensure that your problem is solved as quickly as possible.\nFor Zoom Training and Support\nZoom Help Center: Zoom help center offers the most comprehensive resources on Zoom. You can type in issues and search for a solution.\nZoom Video Tutorials: Video tutorials that can be taken at your own pace\nZoom Technical Webinars: Hosted once every month\nZoom Weekly Training Webinars: Free to register and join. This is a 90-minute user onboarding session with live Q&A. If you cannot attend, you can also view the latest recording.\nZoom 24/7 phone support: Phone dial-in +1.888.799.9666 ext 2 or +1.650.397.6096 ext 2\nZoom 24/7 Live Chat: The live chat feature is available to the lower bottom on any Zoom pages. You will see a pop-up button Help. Click on the Help button and type in the question you have. After typing in your question, you will see the live chat option to chat with a Zoom agent.\nZoom Ticket: Submit a ticket to Zoom for non-urgent account or technical issues.\nMark as completed\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/lecture/OvxTu/lesson-1-2-text-accessdict_values(["List\nCS 410: Text Information Systems\nWeek 1\nLesson 1.2: Text Access\nPrevious\nNext\nOrientation Information\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nVideo:\nVideo\nLesson 1.1: Natural Language Content Analysis\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 1.2: Text Access\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.3: Text Retrieval Problem\n. Duration: 26 minutes\n26 min\nVideo:\nVideo\nLesson 1.4: Overview of Text Retrieval Methods\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 1.5: Vector Space Model - Basic Idea\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 1.6: Vector Space Retrieval Model - Simplest Instantiation\n. Duration: 17 minutes\n17 min\nWeek 1 Activities\nLesson 1.2: Text Access\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] In this lecture, we're going to talk about the text access.\n0:14\nIn the previous lecture, we talked about the natural language content, analysis.\n0:19\nWe explained that the state of the are natural language processing techniques are still not good enough to process a lot of unrestricted text data in a robust manner. As a result, bag of words remains very popular in applications like a search engine.\n0:39\nIn this lecture, we're going to talk about some high-level strategies to help users get access to the text data. This is also important step to convert raw big text data into small random data. That are actually needed in a specific application. So the main question we'll address here, is how can a text information system, help users get access to the relevant text data? We're going to cover two complimentary strategies, push versus pull.\n1:12\nAnd then we're going to talk about two ways to implement the pull mode, querying versus browsing.\n1:20\nSo first push versus pull.\n1:24\nThese are two different ways connect the users with the right information at the right time.\n1:31\nThe difference is which takes the initiative,\n1:37\nwhich party takes the initiative.\n1:40\nIn the pull mode, the users take the initiative to start the information access process.\n1:47\nAnd in this case, a user typically would use a search engine to fulfill the goal. For example, the user may type in the query and then browse the results to find the relevant information.\n2:02\nSo this is usually appropriate for satisfying a user's ad hoc information need.\n2:10\nAn ad hoc information need is a temporary information need. For example, you want to buy a product so you suddenly have a need to read reviews about related product. But after you have cracked information, you have purchased in your product. You generally no longer need such information, so it's a temporary information need.\n2:31\nIn such a case, it's very hard for a system to predict your need, and it's more proper for the users to take the initiative, and that's why search engines are very useful. Today because many people have many information needs all the time. So as we're speaking Google is probably processing many queries from this. And those are all, or mostly adequate. Information needs.\n2:57\nSo this is a pull mode. In contrast in the push mode in the system would take the initiative to push the information to the user or to recommend information to the user. So in this case this is usually supported by a recommender system.\n3:13\nNow this would be appropriate if. The user has a stable information.\n3:17\nFor example you may have a research interest in some topic and that interest tends to stay for a while. So, it's rather stable. Your hobby is another example of. A stable information need is such a case the system can interact with you and can learn your interest, and then to monitor the information stream. If the system hasn't seen any relevant items to your interest, the system could then take the initiative to recommend the information to you. So, for example, a news filter or news recommended system could monitor the news stream and identify interesting news to you and simply push the news articles to you.\n3:59\nThis mode of information access may be also a property that when this system has good knowledge about the users need and this happens in the search context. So for example, when you search for information on the web a search engine might infer you might be also interested in something related. Formation. And they would recommend the information to you, so that just reminds you, for example, of an advertisement placed on the search page.\n4:27\nSo this is about the two high level strategies or two modes of text access.\n4:35\nNow let's look at the pull mode in more detail.\n4:39\nIn the pull mode, we can further distinguish it two ways to help users. Querying versus browsing. In querying, a user would just enter a query. Typical the keyword query, and the search engine system would return relevant documents to use.\n4:54\nAnd this works well when the user knows what exactly are the keywords to be used. So if you know exactly what you are looking for, you tend to know the right keywords. And then query works very well, and we do that all of the time.\n5:09\nBut we also know that sometimes it doesn't work so well. When you don't know the right keywords to use in the query, or you want to browse information in some topic area. You use because browsing would be more useful. So in this case, in the case of browsing, the users would simply navigate it, into the relevant information by following the paths\n5:34\nsupported by the structures of documents. So the system would maintain some kind of structures and then the user could follow these structures to navigate.\n5:47\nSo this really works well when the user wants to explore the information space or the user doesn't know what are the keywords to using the query. Or simply because the user finds it inconvenient to type in a query. So even if a user knows what query to type in if the user is using a cellphone to search for information. It's still harder to enter the query. In such a case, again, browsing tends to be more convenient. The relationship between browsing and querying is best understood by making and imagine you're site seeing.\n6:25\nImagine if you're touring a city. Now if you know the exact address of attraction.\n6:31\nTaking a taxi there is perhaps the fastest way. You can go directly to the site. But if you don't know the exact address, you may need to walk around. Or you can take a taxi to a nearby place and then walk around.\n6:44\nIt turns out that we do exactly the same in the information studies. If you know exactly what you are looking for, then you can use the right keywords in your query to find the information you're after. That's usually the fastest way to do, find information.\n6:59\nBut what if you don't know the exact keywords to use? Well, you clearly probably won't so well. You will not related pages. And then, you need to also walk around in the information space, meaning by following the links or by browsing. You can then finally get into the relevant page.\n7:17\nIf you want to learn about again. You will likely do a lot of browsing so just like you are looking around in some area and you want to see some interesting attractions related in the same. [INAUDIBLE]. So this analogy also tells us that today we have very good support for query, but we don't really have good support for browsing. And this is because in order to browse effectively, we need a map to guide us, just like you need a map to. Of Chicago, through the city of Chicago, you need a topical map to tour the information space. So how to construct such a topical map is in fact a very interesting research question that might bring us more interesting browsing experience on the web or in applications.\n8:19\nSo, to summarize this lecture, we've talked about the two high level strategies for text access; push and pull. Push tends to be supported by the Recommender System, and Pull tends to be supported by the Search Engine. Of course, in the sophisticated [INAUDIBLE] information system, we should combine the two.\n8:38\nIn the pull mode, we can further this [INAUDIBLE] Querying and Browsing. Again we generally want to combine the two ways to help you assist, so that you can support the both querying nad browsing.\n8:51\nIf you want to know more about the relationship between pull and push, you can read this article. This give excellent discussion of the relationship between machine filtering and information retrieval. Here informational filtering is similar to information recommendation or the push mode of information access. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/qNSPo/7-3-natural-language-content-analysis-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.3 Natural Language Content Analysis: Part 1\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.3 Natural Language Content Analysis: Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:09\nThis lecture is about natural language content analysis. Natural language content analysis is the foundation of text mining. So we're going to first talk about this.\n0:24\nAnd in particular, natural language processing with a factor how we can present text data.\n0:33\nAnd this determines what algorithms can be used to analyze and mine text data.\n0:40\nWe're going to take a look at the basic concepts in natural language first.\n0:46\nAnd I'm going to explain these concepts using a similar example that you've all seen here. A dog is chasing a boy on the playground. Now this is a very simple sentence. When we read such a sentence we don't have to think about it to get the meaning of it. But when a computer has to understand the sentence, the computer has to go through several steps.\n1:13\nFirst, the computer needs to know what are the words, how to segment the words in English. And this is very easy, we can just look at the space. And then the computer will need the know the categories of these words, syntactical categories. So for example, dog is a noun, chasing's a verb, boy is another noun etc. And this is called a Lexical analysis. In particular, tagging these words with these syntactic categories is called a part-of-speech tagging.\n1:45\nAfter that the computer also needs to figure out the relationship between these words. So a and dog would form a noun phrase. On the playground would be a prepositional phrase, etc. And there is certain way for them to be connected together in order for them to create meaning. Some other combinations may not make sense.\n2:07\nAnd this is called syntactical parsing, or syntactical analysis, parsing of a natural language sentence. The outcome is a parse tree that you are seeing here. That tells us the structure of the sentence, so that we know how we can interpret this sentence. But this is not semantics yet. So in order to get the meaning we would have to map these phrases and these structures into some real world antithesis that we have in our mind. So dog is a concept that we know, and boy is a concept that we know. So connecting these phrases that we know is understanding.\n2:52\nNow for a computer, would have to formally represent these entities by using symbols. So dog, d1 means d1 is a dog.\n3:04\nBoy, b1 means b1 refers to a boy etc. And also represents the chasing action as a predicate. So, chasing is a predicate here with three arguments, d1, b1, and p1. Which is playground. So this formal rendition of the semantics of this sentence. Once we reach that level of understanding, we might also make inferences. For example, if we assume there's a rule that says if someone's being chased then the person can get scared, then we can infer this boy might be scared. This is the inferred meaning, based on additional knowledge. And finally, we might even further infer what this sentence is requesting, or why the person who say it in a sentence, is saying the sentence. And so, this has to do with purpose of saying the sentence. This is called speech act analysis or pragmatic analysis. Which first to the use of language. So, in this case a person saying this may be reminding another person to bring back the dog.\n4:35\nSo this means when saying a sentence, the person actually takes an action. So the action here is to make a request.\n4:46\nNow, this slide clearly shows that in order to really understand a sentence there are a lot of things that a computer has to do. Now, in general it's very hard for a computer will do everything, especially if you would want it to do everything correctly. This is very difficult.\n5:08\nNow, the main reason why natural language processing is very difficult, it's because it's designed it will make human communications efficient.\n5:15\nAs a result, for example, with only a lot of common sense knowledge.\n5:21\nBecause we assume all of us have this knowledge, there's no need to encode this knowledge.\n5:29\nThat makes communication efficient.\n5:32\nWe also keep a lot of ambiguities, like, ambiguities of words.\n5:39\nAnd this is again, because we assume we have the ability to disambiguate the word. So, there's no problem with having the same word to mean possibly different things in different context.\n5:52\nYet for a computer this would be very difficult because a computer does not have the common sense knowledge that we do. So the computer will be confused indeed. And this makes it hard for natural language processing. Indeed, it makes it very hard for every step in the slide that I showed you earlier.\n6:16\nAmbiguity is a main killer. Meaning that in every step there are multiple choices, and the computer would have to decide whats the right choice and that decision can be very difficult as you will see also in a moment.\n6:31\nAnd in general, we need common sense reasoning in order to fully understand the natural language. And computers today don't yet have that. That's why it's very hard for computers to precisely understand the natural language at this point.\n6:48\nSo here are some specific examples of challenges. Think about the world-level ambiguity. A word like design can be a noun or a verb, so we've got ambiguous part of speech tag.\n7:00\nRoot also has multiple meanings, it can be of mathematical sense, like in the square of, or can be root of a plant.\n7:12\nSyntactic ambiguity refers to different interpretations\n7:19\nof a sentence in terms structures. So for example, natural language processing can actually be interpreted in two ways.\n7:28\nSo one is the ordinary meaning that we will be getting as we're talking about this topic. So, it's processing of natural language. But there's is also another possible interpretation which is to say language processing is natural.\n7:48\nNow we don't generally have this problem, but imagine for the computer to determine the structure, the computer would have to make a choice between the two.\n7:59\nAnother classic example is a man saw a boy with a telescope. And this ambiguity lies in the question who had the telescope? This is called a prepositional phrase attachment ambiguity.\n8:14\nMeaning where to attach this prepositional phrase with the telescope. Should it modify the boy? Or should it be modifying, saw, the verb. Another problem is anaphora resolution. In John persuaded Bill to buy a TV for himself. Does himself refer to John or Bill?\n8:39\nPresupposition is another difficulty. He has quit smoking implies that he smoked before, and we need to have such a knowledge in order to understand the languages.\n8:52\nBecause of these problems, the state of the art natural language processing techniques can not do anything perfectly. Even for the simplest part of speech tagging, we still can not solve the whole problem. The accuracy that are listed here, which is about , was just taken from some studies earlier.\n9:17\nAnd these studies obviously have to be using particular data sets so the numbers here are not really meaningful if you take it out of the context of the data set that are used for evaluation. But I show these numbers mainly to give you some sense about the accuracy, or how well we can do things like this. It doesn't mean any data set accuracy would be precisely . But, in general, we can do parsing speech tagging fairly well although not perfect.\n9:53\nParsing would be more difficult, but for partial parsing, meaning to get some phrases correct, we can probably achieve  or better accuracy.\n10:06\nBut to get the complete parse tree correctly is still very, very difficult.\n10:13\nFor semantic analysis, we can also do some aspects of semantic analysis, particularly, extraction of entities and relations. For example, recognizing this is the person, that's a location, and this person and that person met in some place etc. We can also do word sense to some extent.\n10:38\nThe occurrence of root in this sentence refers to the mathematical sense etc. Sentiment analysis is another aspect of semantic analysis that we can do.\n10:50\nThat means we can tag the senses as generally positive when it's talking about the product or talking about the person.\n11:02\nInference, however, is very hard, and we generally cannot do that for any big domain and if it's only feasible for a very limited domain. And that's a generally difficult problem in artificial intelligence. Speech act analysis is also very difficult and we can only do this probably for very specialized cases. And with a lot of help from humans to annotate enough data for the computers to learn from.\n11:36\nSo the slide also shows that computers are far from being able to understand natural language precisely. And that also explains why the text mining problem is difficult. Because we cannot rely on mechanical approaches or computational methods to understand the language precisely. Therefore, we have to use whatever we have today. A particular statistical machine learning method of statistical analysis methods to try to get as much meaning out from the text as possible. And, later you will see that there are actually\n12:20\nmany such algorithms that can indeed extract interesting model from text even though we cannot really fully understand it. Meaning of all the natural language sentences precisely. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/qkTHD/lesson-5-4-web-search-introduction-web-crawlerdict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.4: Web Search: Introduction & Web Crawler\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.4: Web Search: Introduction & Web Crawler\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:07\nThis lecture is about Web Search.\n0:11\nIn this lecture, we're going to talk about one of the most important applications of text retrieval, web search engines. So let's first look at some general challenges and opportunities in web search. Now, many informational retrieval algorithms had been developed before the web was born. So when the web was born, it created the best opportunity to apply those algorithms to major application problem that everyone would care about. So naturally, there have to be some further extensions of the classical search algorithms to address new challenges encountered in web search. So here are some general challenges. First, this is a scalability challenge. How to handle the size of the web and ensure completeness of coverage of all information.\n1:03\nHow to serve many users quickly and by answering all their queries. And so that's one major challenge and before the web was born the scale search was relatively small. The second problem is that there's no quality information and there are often spams. The third challenge is Dynamics of the Web. The new pages are constantly create and some pages may be updated very quickly, so it makes it harder to keep it indexed fresh. So these are some of the challenges that we have to solve in order to deal with high quality web searching.\n1:44\nOn the other hand there are also some interesting opportunities that we can leverage to include the search results. There are many additional heuristics, for example,\n1:55\nusing links that we can leverage to improve scoring. Now everything that we talked about such as the vector space model are general algorithms.\n2:05\nThey can be applied to any search applications, so that's the advantage. On the other hand, they also don't take advantage of special characteristics of pages or documents in the specific applications, such as web search. Web pages are linked with each other, so obviously, the linking is something that we can also leverage. So, because of these challenges and opportunities and there are new techniques that have been developed for web search or due to need for web search. One is parallel indexing and searching and this is to address the issue of scalability. In particular, Google's imaging of map reduce is very influential and has been very helpful in that aspect. Second, there are techniques that are developing for addressing the problem of spams, so spam detection. We'll have to prevent those spam pages from being ranked high.\n3:04\nAnd there are also techniques to achieve robust ranking. And we're going to use a lot of signals to rank pages, so that it's not easy to spam the search engine with a particular trick. And the third line of techniques is link analysis and these are techniques that can allow us to improve such results by leveraging extra information. And in general in web searching, we're going to use multiple features for ranking not just for link analysis. But also exploring all kinds of crawls like the layout or anchor text that describes a link to another page. So, here's a picture showing the basic search engine technologies. Basically, this is the web on the left and then user on the right side and we're going to help this user to get the access for the web information. And the first component is a Crawler that would crawl pages and then the second component is Indexer that would take these pages create the inverted index.\n4:10\nThe third component there is a Retriever and that would use inverted index to answer user's query by talking to the user's browser. And then the search results will be given to the user and when the browser would show those results, it allows the user to interact with the web. So, we're going to talk about each of these components. First of all, we're going to talk about the crawler, also called a spider or software robot that would do something like crawling pages on the web. To build a toy crawler is relatively easy, because you just need to start with a set of seed pages. And then fetch pages from the web and parse these pages and figure out new links. And then add them to the priority que and then just explore those additional links. But to be able to real crawler actually is tricky and there are some complicated issues that we have to deal with. For example robustness, what if the server doesn't respond, what if there's a trap that generates dynamically generated webpages that might attract your crawler to keep crawling on the same side and to fetch dynamic generated pages? The results of this issue of crawling courtesy and you don't want to overload one particular server with many crawling requests and you have to respect the robot exclusion protocol. You also need to handle different types of files, there are images, PDF files, all kinds of formats on the web. And you have to also consider URL extension, so sometimes those are CGI scripts and there are internal references, etc, and sometimes you have JavaScripts on the page and they also create challenges. And you ideally should also recognize redundant pages because you don't have to duplicate those pages. And finally, you may be interested in the discover hidden URLs. Those are URLs that may not be linked to any page, but if you truncate the URL to a shorter path, you might be able to get some additional pages.\n6:27\nSo what are the Major Crawling Strategies? In general, Breadth-First is most common because it naturally balances the sever load. You would not keep probing a particular server with many requests.\n6:42\nAlso parallel crawling is very natural because this task is very easy to parallelize. And there is some variations of the crawling task, and one interesting variation is called a focused crawling. In this case, we're going to crawl just some pages about a particular topic. For example, all pages about automobiles, all right. And this is typically going to start with a query, and then you can use the query to get some results from a major search engine. And then you can start it with those results and then gradually crawl more. The one channel in crawling, is you will find the new channels that people created and people probably are creating new pages all the time. And this is very challenging if the new pages have not been actually linked to any old pages. If they are, then you can probably find them by re-crawling the old pages, so these are also some interesting challenges that have to be solved. And finally, we might face the scenario of incremental crawling or repeated crawling, right. Let's say, if you want to build a web search engine, and you first crawl a lot of data from the web. But then, once you have cracked all the data, in the future you just need to crawl the updated pages. In general, you don't have to re-crawl everything, right? It's not necessary.\n8:16\nSo in this case, your goal is to minimize the resource overhead by using minimum resources to just the update pages.\n8:27\nSo, this is actually a very interesting research question here, and this is a open research question, in that there aren't many standard algorithms established yet for doing this task.\n8:47\nBut in general, you can imagine, you can learn, from the past experience.\n8:53\nSo the two major factors that you have to consider are, first will this page be updated frequently? And do I have to quote this page again? If the page is a static page and that hasn't being changed for months, you probably don't have to re-crawl it everyday because it's unlikely that it will changed frequently. On the other hand, if it's a sports score page that gets updated very frequently and you may need to re-crawl it and maybe even multiple times on the same day. The other factor to consider is, is this page frequently accessed by users? If it is, then it means that it is a high utility page and then thus it's more important to ensure such a page to refresh. Compared with another page that has never been fetched by any users for a year, then even though that page has been changed a lot then. It's probably not that necessary to crawl that page or at least it's not as urgent as to maintain the freshness of frequently accessed page by users. So to summarize, web search is one of the most important applications of text retrieval and there are some new challenges particularly scalability, efficiency, quality information. There are also new opportunities particularly rich link information and layout, etc.\n10:17\nA crawler is an essential component of web search applications and in general, you can find two scenarios. One is initial crawling and here we want to have complete crawling\n10:30\nof the web if you are doing a general search engine or focused crawling if you want to just target as a certain type of pages.\n10:38\nAnd then, there is another scenario that's incremental updating of the crawl data or incremental crawling. In this case, you need to optimize the resource, try to use minimum resource to get the [INAUDIBLE]\n10:54\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/eJt9Y/12-4-contextual-text-mining-motivationdict_values(["List\nCS 410: Text Information Systems\nWeek 12\n12.4 Contextual Text Mining: Motivation\nPrevious\nNext\nWeek 12 Information\nWeek 12 Lessons\nVideo:\nVideo\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.3 Text-Based Prediction\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n12.4 Contextual Text Mining: Motivation\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\n12.8 Summary for Exam 2\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n12.4 Contextual Text Mining: Motivation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nJapanese\nKorean\nPortuguese (European)\nRussian\nSerbian\nSlovak\nSpanish\nTelugu\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the contextual text mining.\n0:11\nContextual text mining is related to multiple kinds of knowledge that we mine from text data, as I'm showing here. It's related to topic mining because you can make topics associated with context, like time or location. And similarly, we can make opinion mining more contextualized, making opinions connected to context.\n0:34\nIt's related to text based prediction because it allows us to combine non-text data with text data to derive sophisticated predictors for the prediction problem. So more specifically, why are we interested in contextual text mining? Well, that's first because text often has rich context information. And this can include direct context such as meta-data, and also indirect context. So, the direct context can grow the meta-data such as time, location, authors, and source of the text data. And they're almost always available to us.\n1:14\nIndirect context refers to additional data related to the meta-data. So for example, from office, we can further obtain additional context such as social network of the author, or the author's age.\n1:30\nSuch information is not in general directly related to the text, yet through the process, we can connect them. There could be other text data from the same source, as this one through the other text can be connected with this text as well. So in general, any related data can be regarded as context. So there could be removed or rated for context.\n1:55\nAnd so what's the use? What is text context used for? Well, context can be used to partition text data in many interesting ways. It can almost allow us to partition text data in other ways as we need. And this is very important because this allows us to do interesting comparative analyses. It also in general, provides meaning to the discovered topics, if we associate the text with context.\n2:25\nSo here's illustration of how context can be regarded as interesting ways of partitioning of text data. So here I just showed some research papers published in different years.\n2:41\nOn different venues, different conference names here listed on the bottom like the SIGIR or ACL, etc.\n2:49\nNow such text data can be partitioned in many interesting ways because we have context.\n2:56\nSo the context here just includes time and the conference venues. But perhaps we can include some other variables as well.\n3:06\nBut let's see how we can partition this interesting of ways. First, we can treat each paper as a separate unit. So in this case, a paper ID and the, each paper has its own context. It's independent. But we can also treat all the papers within 1998 as one group and this is only possible because of the availability of time. And we can partition data in this way. This would allow us to compare topics for example, in different years.\n3:39\nSimilarly, we can partition the data based on the menus. We can get all the SIGIR papers and compare those papers with the rest. Or compare SIGIR papers with KDD papers, with ACL papers.\n3:52\nWe can also partition the data to obtain the papers written by authors in the U.S., and that of course, uses additional context of the authors. And this would allow us to then compare such a subset with another set of papers written by also seen in other countries.\n4:13\nOr we can obtain a set of papers about text mining, and this can be compared with papers about another topic. And note that these partitionings can be also intersected with each other to generate even more complicated partitions.\n4:29\nAnd so in general, this enables discovery of knowledge associated with different context as needed.\n4:37\nAnd in particular, we can compare different contexts. And this often gives us a lot of useful knowledge. For example, comparing topics over time, we can see trends of topics. Comparing topics in different contexts can also reveal differences about the two contexts. So there are many interesting questions that require contextual text mining. Here I list some very specific ones. For example, what topics have been getting increasing attention recently in data mining research? Now to answer this question, obviously we need to analyze text in the context of time.\n5:13\nSo time is context in this case. Is there any difference in the responses of people in different regions to the event, to any event? So this is a very broad an answer to this question. In this case of course, location is the context. What are the common research interests of two researchers? In this case, authors can be the context. Is there any difference in the research topics published by authors in the USA and those outside? Now in this case, the context would include the authors and their affiliation and location.\n5:47\nSo this goes beyond just the author himself or herself. We need to look at the additional information connected to the author. Is there any difference in the opinions of all the topics expressed on one social network and another? In this case, the social network of authors and the topic can be a context.\n6:06\nOther topics in news data that are correlated with sudden changes in stock prices. In this case, we can use a time series such as stock prices as context.\n6:17\nWhat issues mattered in the 2012 presidential campaign, or presidential election? Now in this case, time serves again as context. So, as you can see, the list can go on and on. Basically, contextual text mining can have many applications. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/caiOk/week-1-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 1\nWeek 1 Overview\nPrevious\nNext\nOrientation Information\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nReading:\nReading\nWeek 1 Overview\n. Duration: 10 minutes\n10 min\nModule 1 Lessons\nWeek 1 Activities\nWeek 1 Overview\nThe first six weeks of the course are based on the content of the Text Retrieval and Search Engines MOOC. During this week's lessons, you will learn the overall design of this MOOC, an overview of natural language processing techniques, which are the foundation for all kinds of text-processing applications, the concept of a retrieval model, and the basic idea of the vector space model.     \nTime\nThis module should take approximately 3 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with required assignments in bold):\nActivity\nEstimated Time Required\nWeek 1 Video Lectures\n2 hours\nWeek 1 Graded Quiz\n1 hour\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain some basic concepts in natural language processing, text information access.\nExplain why text retrieval is often defined as a ranking problem.\nExplain the basic idea of the vector space retrieval model and how to instantiate it with the simplest bit-vector representation.\nGuiding Questions\nDevelop your answers to the following guiding questions while watching the video lectures throughout the week.\nWhat does a computer have to do in order to understand a natural language sentence?\nWhat is ambiguity?\nWhy is natural language processing (NLP) difficult for computers?\nWhat is bag-of-words representation? Why do modern search engines use this simple representation of text?\nWhat are the two modes of text information access? Which mode does a web search engine such as Google support?\nWhen is browsing more useful than querying to help a user find relevant information?\nWhy is a text retrieval task defined as a ranking task?\nWhat is a retrieval model?\nWhat are the two assumptions made by the Probability Ranking Principle?\nWhat is the Vector Space Retrieval Model? How does it work?\nHow do we define the dimensions of the Vector Space Model? What does “bag of words” representation mean?\nWhat does the retrieval function intuitively capture when we instantiate a vector space model with bag of words representation and bit representation for documents and queries?  \nAdditional Readings and Resources\nThe following readings are optional:\nN. J. Belkin and W. B. Croft. 1992. Information filtering and information retrieval: Two sides of the same coin? Commun. ACM 35, 12 (Dec. 1992), 29-38.\nC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapters 1-6. \nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nPart of speech tagging, syntactic analysis, semantic analysis, and ambiguity\n“Bag of words” representation\nPush, pull, querying, browsing\nProbability ranking principle\nRelevance\nVector space model\nDot product\nBag of words representation\nBit vector representation  \nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/1exQ5/8-10-probabilistic-topic-models-mining-one-topicdict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.10 Probabilistic Topic Models: Mining One Topic\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.10 Probabilistic Topic Models: Mining One Topic\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is a continued discussion of probabilistic topic models. In this lecture, we're going to continue discussing probabilistic models. We're going to talk about a very simple case where we are interested in just mining one topic from one document.\n0:30\nSo in this simple setup, we are interested in analyzing one document and trying to discover just one topic. So this is the simplest case of topic model. The input now no longer has k, which is the number of topics because we know there is only one topic and the collection has only one document, also. In the output, we also no longer have coverage because we assumed that the document covers this topic . So the main goal is just to discover the world of probabilities for this single topic, as shown here.\n1:14\nAs always, when we think about using a generating model to solve such a problem, we start with thinking about what kind of data we are going to model or from what perspective we're going to model the data or data representation. And then we're going to design a specific model for the generating of the data, from our perspective. Where our perspective just means we want to take a particular angle of looking at the data, so that the model will have the right parameters for discovering the knowledge that we want. And then we'll be thinking about the microfunction or write down the microfunction to capture more formally how likely a data point will be obtained from this model.\n2:05\nAnd the likelihood function will have some parameters in the function. And then we argue our interest in estimating those parameters for example, by maximizing the likelihood which will lead to maximum likelihood estimated. These estimator parameters will then become the output of the mining hours, which means we'll take the estimating parameters as the knowledge that we discover from the text. So let's look at these steps for this very simple case. Later we'll look at this procedure for some more complicated cases. So our data, in this case is, just a document which is a sequence of words. Each word here is denoted by x sub i. Our model is a Unigram language model. A word distribution that we hope to denote a topic and that's our goal. So we will have as many parameters as many words in our vocabulary, in this case M.\n3:09\nAnd for convenience we're going to use theta sub i to denote the probability of word w sub i.\n3:20\nAnd obviously these theta sub i's will sum to 1.\n3:24\nNow what does a likelihood function look like? Well, this is just the probability of generating this whole document, that given such a model. Because we assume the independence in generating each word so the probability of the document will be just a product of the probability of each word.\n3:42\nAnd since some word might have repeated occurrences. So we can also rewrite this product in a different form.\n3:52\nSo in this line, we have rewritten the formula into a product over all the unique words in the vocabulary, w sub 1 through w sub M. Now this is different from the previous line. Well, the product is over different positions of words in the document.\n4:15\nNow when we do this transformation, we then would need to introduce a counter function here. This denotes the count of word one in document and similarly this is the count of words of n in the document because these words might have repeated occurrences. You can also see if a word did not occur in the document.\n4:41\nIt will have a zero count, therefore that corresponding term will disappear. So this is a very useful form of writing down the likelihood function that we will often use later. So I want you to pay attention to this, just get familiar with this notation. It's just to change the product over all the different words in the vocabulary. So in the end, of course, we'll use theta sub i to express this likelihood function and it would look like this. Next, we're going to find the theta values or probabilities of these words that would maximize this likelihood function. So now lets take a look at the maximum likelihood estimate problem more closely.\n5:32\nThis line is copied from the previous slide. It's just our likelihood function.\n5:38\nSo our goal is to maximize this likelihood function. We will find it often easy to\n5:47\nmaximize the local likelihood instead of the original likelihood. And this is purely for mathematical convenience because after the logarithm transformation our function will becomes a sum instead of product. And we also have constraints over these these probabilities. The sum makes it easier to take derivative, which is often needed for finding the optimal solution of this function. So please take a look at this sum again, here. And this is a form of a function that you will often see later also, the more general topic models. So it's a sum over all the words in the vocabulary. And inside the sum there is a count of a word in the document. And this is macroed by the logarithm of a probability.\n6:55\nSo let's see how we can solve this problem.\n6:58\nNow at this point the problem is purely a mathematical problem because we are going to just the find the optimal solution of a constrained maximization problem. The objective function is the likelihood function and the constraint is that all these probabilities must sum to one. So, one way to solve the problem is to use Lagrange multiplier approace.\n7:24\nNow this command is beyond the scope of this course but since Lagrange multiplier is a very useful approach, I also would like to just give a brief introduction to this, for those of you who are interested.\n7:39\nSo in this approach we will construct a Lagrange function, here. And this function will combine our objective function with another term that encodes our constraint and we introduce Lagrange multiplier here, lambda, so it's an additional parameter. Now, the idea of this approach is just to turn the constraint optimization into, in some sense, an unconstrained optimizing problem. Now we are just interested in optimizing this Lagrange function.\n8:19\nAs you may recall from calculus, an optimal point would be achieved when the derivative is set to zero. This is a necessary condition. It's not sufficient, though. So if we do that you will see the partial derivative, with respect to theta i here ,is equal to this. And this part comes from the derivative of the logarithm function and this lambda is simply taken from here. And when we set it to zero we can easily see theta sub i is related to lambda in this way.\n9:06\nSince we know all the theta i's must a sum to one we can plug this into this constraint, here. And this will allow us to solve for lambda.\n9:16\nAnd this is just a net sum of all the counts. And this further allows us to then solve the optimization problem, eventually, to find the optimal setting for theta sub i. And if you look at this formula it turns out that it's actually very intuitive because this is just the normalized count of these words by the document ns, which is also a sum of all the counts of words in the document. So, after all this mess, after all, we have just obtained something that's very intuitive and this will be just our intuition where we want to maximize the data by assigning as much probability mass as possible to all the observed the words here. And you might also notice that this is the general result of maximum likelihood raised estimator. In general, the estimator would be to normalize counts and it's just sometimes the counts have to be done in a particular way, as you will also see later. So this is basically an analytical solution to our optimization problem. In general though, when the likelihood function is very complicated, we're not going to be able to solve the optimization problem by having a closed form formula. Instead we have to use some numerical algorithms and we're going to see such cases later, also. So if you imagine what would we get if we use such a maximum likelihood estimator to estimate one topic for a single document d here? Let's imagine this document is a text mining paper. Now, what you might see is something that looks like this. On the top, you will see the high probability words tend to be those very common words, often functional words in English. And this will be followed by some content words that really characterize the topic well like text, mining, etc. And then in the end, you also see there is more probability of words that are not really related to the topic but they might be extraneously mentioned in the document. As a topic representation, you will see this is not ideal, right? That because the high probability words are functional words, they are not really characterizing the topic. So my question is how can we get rid of such common words?\n11:59\nNow this is the topic of the next module. We're going to talk about how to use probabilistic models to somehow get rid of these common words. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/h3Jru/lesson-6-3-learning-to-rank-part-3-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] There are many more of the Munster learning algorithms than the regression based approaches and they generally attempt to direct the optimizer retrieval method.\n0:16\nLike a MAP or nDCG.\n0:19\nNote that the optimization object or function that we have seen on the previous slide is not directly related to the retrieval measure.\n0:31\nBy maximizing the prediction of one or zero, we don't necessarily optimize the ranking of those documents. One can imagine that our prediction may not be too bad. And let's say both are around 0.5. So it's kind of in the middle of zero and one for the two documents. But the ranking can be wrong, so we might have a larger value for E2 and then E1.\n1:00\nSo that won't be good from retrieval perspective, even though function, it's not bad. In contrast, we might have another case where we predicted the values, or around the 0.9, it said. And by the objective function, the error would be larger. But if we didn't get the order of the two documents correct, that's actually a better result. So these new, more advanced approaches will try to correct that problem. Of course, then the challenge is that the optimization problem will be harder to solve. And then, researchers have posed many solutions to the problem, and you can read more of the references at the end, know more about these approaches. Now, these learning ranked approaches after the general. So there accounts would be be applied with many other ranking problems, not just the retrieval problem. So some people will go with recommender systems, computational advertising, or summarization and there are many others that you can probably encounter in your applications..\n2:11\nTo summarize this lecture we have talked about using machine learning to combine much more features including ranking results.\n2:22\nActually the use of machine learning\n2:25\nin information retrieval has started since many decades ago. So for example, the Rocchio feedback approach that we talked about earlier was a machine learning approach prior to relevance feedback. But the most recent use of machine learning has been driven by some changes in the environment of applications of retrieval systems.\n2:52\nFirst, it's mostly freedom of availability of a lot of training data in the form of critical, such as they are more available than before. So the data can provide a lot of useful knowledge about relevance and machine learning methods can be applied into a leverage list. Secondly, it's also freedom by the need for combining many features, and this is not only just because there are more features available on the web that can be naturally used for improved scoring. It's also because by combining them, we can improve the robustness of ranking, so this is desired for combating spams. Modern search engines all use some kind of machine learning techniques to combine many features to optimize ranking and this is a major feature of these commercial engines such a Google or Bing.\n3:56\nThe topic of learning to rank is still active research topic in the community, and so we can expect to see new results in development in the next few years, perhaps.\n4:12\nHere are some additional readings that can give you more information about how learning to rank at works and also some advanced methods.\n4:25\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/10dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 10\nWeek 10\n2h 21m of videos left\n10 min of readings left\nAll graded assignments completed\nDuring this module, you will learn text clustering, including the basic concepts, main clustering techniques, and how to evaluate text clustering. You will also start learning text categorization, which is related to text clustering, but with pre-defined categories that can be viewed as pre-defining clusters.\nWeek 10 Information\nWeek 10 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 10 Lessons\n10.1 Text Clustering: Motivation\nVideo•\n. Duration: 15 minutes\n15 min\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\nVideo•\n. Duration: 16 minutes\n16 min\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\nVideo•\n. Duration: 8 minutes\n8 min\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\nVideo•\n. Duration: 14 minutes\n14 min\n10.5 Text Clustering: Similarity-based Approaches\nVideo•\n. Duration: 17 minutes\n17 min\n10.6 Text Clustering: Evaluation\nVideo•\n. Duration: 10 minutes\n10 min\n10.7 Text Categorization: Motivation\nVideo•\n. Duration: 14 minutes\n14 min\n10.8 Text Categorization: Methods\nVideo•\n. Duration: 11 minutes\n11 min\n10.9 Text Categorization: Generative Probabilistic Models\nVideo•\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\nComplete\nWeek 10 Practice Quiz\nPractice Quiz•5 questions\nWeek 10 Quiz\nGraded\nQuiz•7 questions\n•Grade: '])
https://www.coursera.org/learn/cs-410/lecture/8tCQe/10-7-text-categorization-motivationdict_values(["List\nCS 410: Text Information Systems\nWeek 10\n10.7 Text Categorization: Motivation\nPrevious\nNext\nWeek 10 Information\nWeek 10 Lessons\nVideo:\nVideo\n10.1 Text Clustering: Motivation\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n10.2 Text Clustering: Generative Probabilistic Models Part 1 (OPTIONAL)\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\n10.3 Text Clustering: Generative Probabilistic Models Part 2 (OPTIONAL)\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n10.4 Text Clustering: Generative Probabilistic Models Part 3 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.5 Text Clustering: Similarity-based Approaches\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n10.6 Text Clustering: Evaluation\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n10.7 Text Categorization: Motivation\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n10.8 Text Categorization: Methods\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n10.9 Text Categorization: Generative Probabilistic Models\n. Duration: 31 minutes\n31 min\nWeek 10 Activities\n10.7 Text Categorization: Motivation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:06\nThis lecture is about text categorization.\n0:11\nIn this lecture, we're going to talk about text categorization.\n0:16\nThis is a very important technique for text data mining and analytics.\n0:22\nIt is relevant to discovery of various different kinds of knowledge as shown here. First, it's related to topic mining and analysis. And, that's because it has to do with analyzing text to data based on some predefined topics. Secondly, it's also related to opinion mining and sentiment analysis, which has to do with discovery knowledge about the observer, the human sensor. Because we can categorize the authors, for example, based on the content of the articles that they have written, right? We can, in general, categorize the observer based on the content that they produce.\n1:12\nFinally, it's also related to text-based prediction. Because, we can often use text categorization techniques to predict some variables in the real world that are only remotely related to text data.\n1:27\nAnd so, this is a very important technique for text to data mining.\n1:34\nThis is the overall plan for covering the topic. First, we're going to talk about what is text categorization and why we're interested in doing that in this lecture? And now, we're going to talk about how to do text categorization for how to evaluate the categorization results. So, the problem of text categorization is defined as follows. We're given a set of predefined categories possibly forming a hierarchy or so. And often, also a set of training examples or training set of labeled text objects which means the text objects have already been enabled with known categories. And then, the task is to classify any text object into one or more of these predefined categories. So, the picture on this slide shows what happens.\n2:30\nWhen we do text categorization, we have a lot of text objects to be processed by a categorization system and the system will, in general, assign categories through these documents. As shown on the right and the categorization results, and we often assume the availability of training examples and these are the documents that are tag with known categories. And these examples are very important for helping the system to learn patterns in different categories. And, this would further help the system then know how to recognize\n3:11\nthe categories of new text objects that it has not seen. So, here are some specific examples of text categorization. And in fact, there are many examples, here are just a few.\n3:27\nSo first, text objects can vary, so we can categorize a document, or a passage, or a sentence, or collections of text. As in the case of clustering, the units to be analyzed can vary a lot, so this creates a lot of possibilities. Secondly, categories can also vary. Allocate in general, there's two major kinds of categories. One is internal categories. These are categories that categorize content of text object. For example, topic categories or sentiment categories and they generally have to do with the content of the text objects throughout the categorization of the content.\n4:08\nThe other kind is external categories that can characterize an entity associated with the text object. For example, authors are entities associated with the content that they produce. And so, we can use their content in determining which author has written, which part, for example, and that's called author attribution.\n4:33\nOr, we can have any other mininal categories associate with text data as long as there is minimal connection between the entity and text data. For example, we might collect a lot of reviews about a restaurant or a lot of reviews about a product, and then, this text data can help us infer properties of a product or a restaurant. In that case, we can treat this as a categorization problem. We can categorize restaurants or categorize products based on their corresponding reviews. So, this is an example for external category. Here are some specific examples of the applications. News categorization is very common as being started a lot. News agencies would like to assign predefined categories to categorize news generated everyday. And, these virtual article categorizations are not important aspect. For example, in the biomedical domain, there's MeSH annotations. MeSH stands for Medical Subject Heading, and this is ontology of terms,\n5:49\ncharacterize content of literature articles in detail.\n5:54\nAnother example of application is spam email detection or filtering, right? So, we often have a spam filter to help us distinguish spams from legitimate emails and this is clearly a binary classification problem.\n6:14\nSentiment categorization of product reviews or tweets is yet another kind of applications where we can categorize, comparing to positive or negative or positive and negative or neutral.\n6:27\nSo, you can have send them to categories, assign the two text content.\n6:35\nAnother application is automatic email routing or sorting, so, you might want to automatically sort your emails into different folders and that's one application of text categorization where each folder is a category.\n6:48\nThe results are another important kind of applications of routing emails to the right person to handle, so, in helpdesk, email messaging is generally routed to a particular person to handle. Different people tend to handle different kinds of requests. And in many cases, a person would manually assign the messages to the right people. But, if you can imagine, you can't be able to automatically text categorization system to help routing request. And, this is a class file, the incoming request in the one of the categories where each category actually corresponds to a person to handle the request. And finally, author attribution, as I just mentioned, is yet another application, and it's another example of using text to actually infer properties of\n7:41\nsome other entities. And, there are also many variants of the problem formulation. And so, first, we have the simplest case, which is a binary categorization, where there are only two categories. And, there are many examples like that, information retrieval or search engine.\n7:59\nApplications with one distinguishing relevant documents from non-relevant documents for a particular query.\n8:06\nSpam filtering just distinguishing spams from non-spams, so, also two categories. Sometimes, classifications of opinions can be in two categories, positive and a negative.\n8:19\nA more general case would be K-category categorization and there are also many applications like that, there could be more than two categories. So, topic categorization is often such an example where you can have multiple topics. Email routing would be another example when you may have multiple folders or if you route the email to the right person to handle it, then there are multiple people to classify. So, in all these cases, there are more than two kinds of categories.\n8:49\nAnother variation is to have hierarchical categorization where categories form a hierarchy. Again, topical hierarchy is very common.\n8:58\nYet another variation is joint categorization. That's when you have multiple categorization tasks that are related and then you hope to kind of join the categorization. Further leverage the dependency of these tasks to improve accuracy for each individual task. Among all these binary categorizations is most fundamental and part of it also is because it's simple and probably it's because it can actually be used to perform all the other categorization tasks. For example, a K-category categorization task can be actually performed by using binary categorization.\n9:40\nBasically, we can look at each category separately and then the binary categorization problem is whether object is in this category or not, meaning in other categories.\n9:53\nAnd, the hierarchical categorization can also be done by progressively doing flat categorization at each level. So, we have, first, we categorize all the objects into, let's say, a small number of high-level categories, and inside each category, we have further categorized to sub-categories, etc.\n10:15\nSo, why is text categorization important? Well, I already showed that you, several applications but, in general, there are several reasons. One is text categorization helps enrich text representation and that's to achieve more understanding of text data that's all it was useful for text analysis. So, now with categorization text can be represented in multiple levels. The keyword conditions that's often used for a lot text processing tasks. But we can now also add categories and they provide two levels of transition.\n10:55\nSemantic categories assigned can also be directly or indirectly useful for application. So, for example, semantic categories could be already very useful or other attribution might be directly useful. Another example is when semantic categories can facilitate aggregation of text content and this is another case of applications of text categorization.\n11:25\nFor example, if we want to know the overall opinions about a product, we\n11:32\ncould first categorize the opinions in each individual view as positive or negative and then, that would allow us to easy to aggregate all the sentiment, and it would tell us about the  of the views are positive and  are negative, etc.\n11:53\nSo, without doing categorization, it will be much harder to aggregate such opinions to provide a concise way of coding text in some sense based on all of the vocabulary. And, sometimes you may see in some applications, text with categorizations called a text coded, encoded with some control of vocabulary. The second kind of reasons is to use text categorization to infer properties of entities, and text categories allows us to infer the properties of such entities that are associate with text data. So, this means we can use text categorization to discover knowledge about the world. In general, as long as we can associate the entity with text of data, we can always the text of data to help categorize the corresponding entities. So, it's used for single information network that will connect the other entities with text data. The obvious entities that can be directly connected are authors. But, you can also imagine the author's affiliations or the author's age and other things can be actually connected to text data indirectly. Once we have made the connection, then we can make a prediction about those values. So, this is a general way to allow us to use text mining through, so the text categorization to discover knowledge about the world. Very useful, especially in big text data analytics where we are often just using text data as extra sets of data extracted from humans to infer certain decision factors often together with non-textual data. Specifically with text, for example, we can also think of examples of inferring properties of entities. For example, discovery of non-native speakers of a language. And, this can be done by categorizing the content of speakers.\n14:00\nAnother example is to predict the party affiliation of a politician based on the political speech. And, this is again an example of using text data to infer some knowledge about the real world. In nature, the problems are all the same, and that's as we defined and it's a text categorization problem. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/7M0GD/lesson-6-6-recommender-systems-content-based-filtering-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 6\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\nPrevious\nNext\nWeek 6 Information\nWeek 6 Lessons\nVideo:\nVideo\nLesson 6.1: Learning to Rank - Part 1 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nVideo:\nVideo\nLesson 6.2: Learning to Rank - Part 2 (OPTIONAL)\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.3: Learning to Rank - Part 3 (OPTIONAL)\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.4: Future of Web Search\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\nLesson 6.5: Recommender Systems: Content-Based Filtering - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 6.7: Recommender Systems: Collaborative Filtering - Part 1\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 6.8: Recommender Systems: Collaborative Filtering - Part 2\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 6.9: Recommender Systems: Collaborative Filtering - Part 3\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\nLesson 6.10: Summary for Exam 1\n. Duration: 9 minutes\n9 min\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nLesson 6.6: Recommender Systems: Content-Based Filtering - Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:09\nThere are some interesting challenges in threshold for the learning the filtering problem. So here I show the historical data that you can collect in the filtering system, so you can see the scores and the status of relevance. So the first one has a score of 36.5 and it's relevant. The second one is not relevant and it's separate. Of course, we have a lot of documents for which we don't know the status, because we have never delivered them to the user. So as you can see here, we only see the judgements of documents delivered to the user. So this is not a random sample, so it's a sensitive data. It's kind of biased, so that creates some difficultly for learning.\n0:58\nSecondly, there are in general very little labeled data and very few relevant data, so it's also challenging for machine learning approaches, typically they require more training data.\n1:13\nAnd in the extreme case at the beginning we don't even have any labeled data as well. The system there has to make a decision, so that's a very difficult problem at the beginning. Finally, there is also this issue of exploration versus exploitation tradeoff. Now, this means we also want to explore the document space a little bit and to see if the user might be interested in documents that we have in data labeled. So in other words, we're going to explore the space of user interests by testing whether the user might be interested in some other documents that\n1:56\ncurrently are not matching the user's interests so well.\n2:01\nSo how do we do that? Well, we could lower the threshold a little bit until we just deliver some near misses to the user to see what the user would respond,\n2:13\nto see how the user would respond to this extra document.\n2:20\nAnd this is a tradeoff, because on the one hand, you want to explore, but on the other hand, you don't want to really explore too much, because then you will over deliver non-relevant Information. So exploitation means you would exploit what you learn about the user. Let's say you know the user is interested in this particular topic, so you don't want to deviate that much, but if you don't deviate at all then you don't exploit so that's also are not good. You might miss opportunity to learn another interest of the user.\n2:51\nSo this is a dilemma.\n2:54\nAnd that's also a difficulty problem to solve.\n2:58\nNow, how do we solve these problems? In general, I think one can use the empirical utility optimization strategy. And this strategy is basically to optimize the threshold based on historical data, just as you have seen on the previous slide. Right, so you can just compute the utility on the training data for each candidate score threshold. Pretend that, what if I cut at this point. What if I cut at the different scoring threshold point, what would happen? What's utility? Since these are training data, we can kind of compute the utility, and we know that relevant status, or we assume that we know relevant status based on approximation of click-throughs. So then we can just choose the threshold that gives the maximum utility on the training data. But this of course, doesn't account for exploration that we just talked about.\n3:56\nAnd there is also the difficulty of biased training sample, as we mentioned.\n4:01\nSo, in general, we can only get the upper bound for the true optimal threshold, because the threshold might be actually lower than this. So, it's possible that this could discarded item might be actually interesting to the user.\n4:19\nSo how do we solve this problem? Well, we generally, and as I said we can low with this threshold to explore a little bit. So here's on particular approach called beta-gamma threshold learning. So the idea is falling. So here I show a ranked list of all the training documents that we have seen so far, and they are ranked by their positions. And on the y axis we show the utility, of course, this function depends on how you specify the coefficients in the utility function, but we can then imagine, that depending on the cutoff position, we will have a utility.\n4:54\nSuppose I cut at this position and that would be a utility. For example, identify some cutting cutoff point. The optimal point, theta optimal, is the point when it will achieve the maximum utility if we had chosen this as threshold.\n5:17\nAnd there is also zero utility threshold. You can see at this cutoff the utility is zero. What does that mean? That means if I lower the threshold a little bit, now I reach this threshold. The utility would be lower but it's still non-active at least, right? So it's not as high as the optimal utility. But it gives us as a safe point to explore the threshold, as I have explained, it's desirable to explore the interest of space. So it's desirable to lower the threshold based on your training there. So that means, in general, we want to set the threshold somewhere in this range. Let's say we can use the alpha to control\n6:08\nthe deviation from the optimal utility point. So you can see the formula of the threshold would be just the interpolation of the zero utility threshold and the optimal utility threshold.\n6:22\nNow, the question is, how should we set alpha?\n6:27\nAnd when should we deviate more from the optimal utility point?\n6:33\nWell, this can depend on multiple factors, and the one way to solve the problem is to encourage this threshold mechanism to explore up to the zero point, and that's a safe point, but we're not going to necessarily reach all the way to the zero point. Rather, we're going to use other parameters to further define alpha and this specifically is as follows. So there will be a beta parameter to control the deviation from the optimal threshold and this can be based on can be accounting for the over-fitting to the training data let's say, and so this can be just an adjustment factor. But what's more interesting is this gamma parameter. Here, and you can see in this formula, gamma is controlling the inference of the number of examples in training that are set. So you can see in this formula as N which denotes the number of training examples becomes bigger, then it would actually encourage less exploration. In other words, when these very small it would try to explore more. And that just means if we have seen few examples we're not sure whether we have exhausted the space of interest. So we need to explore but as we have seen many examples from the user many that have we feel that we probably don't have to explore more. So this gives us a beta gamma for exploration, right. The more examples we have seen the less exploration we need to do. So the threshold would be closer to the optimal threshold so that's the basic idea of this approach. This approach actually has been working well in some evaluation studies, particularly effective. And also can work on arbitrary utility with the appropriate lower bound.\n8:43\nAnd explicitly addresses the exploration-exploitation tradeoff and it kind of uses the zero utility threshold point as a safeguard for exploration-exploitation tradeoff. We're not never going to explore further than the zero utility point. So if you take the analogy of gambling, and you don't want to risk on losing money. So it's a safe spend, really conservative strategy for exploration.\n9:13\nAnd the problem is of course, this approach is purely heuristic and the zero utility lower boundary is also often too conservative, and there are, of course, more advance in machine learning approaches that have been proposed for solving this problems and this is their active research area.\n9:35\nSo to summarize, there are two strategies for recommended systems or filtering systems, one is content based, which is looking at the item similarity, and the other is collaborative filtering that was looking at the user similarity. We've covered content-based filtering approach. In the next lecture, we will talk about the collaborative filtering. In content-based filtering system, we generally have to solve several problems relative to filtering decision and learning, etc. And such a system can actually be built based on a search engine system by adding a threshold mechanism and adding adaptive learning algorithm to allow the system to learn from long term feedback from the user.\n10:30\n[MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/QnGYn/9-2-probabilistic-topic-models-mixture-model-estimation-part-1dict_values(['List\nCS 410: Text Information Systems\nWeek 9\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:06\nThis lecture is about the mixture model estimation. In this lecture, we\'re going to continue discussing probabilistic topic models. In particular, we\'re going to talk about the how to estimate the parameters of a mixture model. So let\'s first look at our motivation for using a mixture model, and we hope to effect out the background words from the topic word distribution. So the idea is to assume that the text data actually contain two kinds of words. One kind is from the background here, so the "is", "we" etc. The other kind is from our topic word distribution that we\'re interested in. So in order to solve this problem of factoring out background words, we can set up our mixture model as follows. We are going to assume that we already know the parameters of all the values for all the parameters in the mixture model except for the word distribution of Theta sub d which is our target. So this is a case of customizing probably some model so that we embedded the unknown variables that we are interested in, but we\'re going to simplify other things. We\'re going to assume we have knowledge about others and this is a powerful way of customizing a model for a particular need. Now you can imagine, we could have assumed that we also don\'t know the background word distribution, but in this case, our goal is to affect out precisely those high probability in the background words. So we assume the background model is already fixed. The problem here is, how can we adjust the Theta sub d in order to maximize the probability of the observed document here and we assume all the other parameters are known? Now, although we designed the modal heuristically to try to factor out these background words, it\'s unclear whether if we use maximum likelihood estimator, we will actually end up having a word distribution where the common words like "the" will be indeed having smaller probabilities than before. So now, in this case, it turns out that the answer is yes. When we set up the probabilistic modeling this way, when we use maximum likelihood estimator, we will end up having a word distribution where the common words would be factored out by the use of the background distribution. So to understand why this is so, it\'s useful to examine the behavior of a mixture model. So we\'re going to look at a very simple case. In order to understand some interesting behaviors of a mixture model, the observed patterns here actually are generalizable to mixture model in general, but it\'s much easier to understand this behavior when we use a very simple case like what we\'re seeing here. So specifically in this case, let\'s assume that the probability of choosing each of the two models is exactly the same. So we\'re going to flip a fair coin to decide which model to use. Furthermore, we are going to assume there are precisely to words, "the" and "text." Obviously, this is a very naive oversimplification of the actual text, but again, it is useful to examine the behavior in such a special case. So we further assume that, the background model gives probability of 0.9 to the word "the" and "text" 0.1. Now, let\'s also assume that our data is extremely simple. The document has just two words "text" and then "the." So now, let\'s write down the likelihood function in such a case. First, what\'s the probability of "text" and what\'s the probability of "the"? I hope by this point, you will be able to write it down. So the probability of "text" is basically a sum of two cases where each case corresponds to each of the water distribution and it accounts for the two ways of generating text. Inside each case, we have the probability of choosing the model which is 0.5 multiplied by the probability of observing "text" from that model. Similarly, "the" would have a probability of the same form just as it was different exactly probabilities. So naturally, our likelihood function is just the product of the two. So it\'s very easy to see that, once you understand what\'s the probability of each word and which is also why it\'s so important to understand what\'s exactly the probability of observing each word from such a mixture model. Now, the interesting question now is, how can we then optimize this likelihood? Well, you will notice that, there are only two variables. They are precisely the two probabilities of the two words "text" and "the" given by Theta sub d. This is because we have assumed that, all the other parameters are known. So now, the question is a very simple algebra question. So we have a simple expression with two variables and we hope to choose the values of these two variables to maximize this function. It\'s exercises that we have seen some simple algebra problems, and note that the two probabilities must sum to one. So there\'s some constraint. If there were no constraint of course, we will set both probabilities to their maximum value which would be one to maximize this, but we can\'t do that because "text" and "the" must sum to one. We can\'t give those a probability of one. So now the question is, how should we allocate the probability in the mass between the two words? What do you think? Now, it will be useful to look at this formula for moment and to see intuitively what we do in order to set these probabilities to maximize the value of this function. If we look into this further, then we\'ll see some interesting behavior of the two component models in that, they will be collaborating to maximize the probability of the observed data which is dictated by the maximum likelihood estimator, but they\'re also competing in some way. In particular, they would be competing on the words and they will tend to bet high probabilities on different words to avoid this competition in some sense or to gain advantage in this competition. So again, looking at this objective function and we have a constraint on the two probabilities, now if you look at the formula intuitively, you might feel that you want to set the probability of "text" to be somewhat larger than "the". This intuition can be well-supported by mathematical fact which is, when the sum of two variables is a constant then the product of them which is maximum then they are equal, and this is a fact that we know from algebra. Now, if we plug that in, we will would mean that we have to make the two probabilities equal. When we make them equal and then if we consider the constraint that we can easily solve this problem, and the solution is the probability of "text" would be 0.9 and probability of "the" is 0.1. As you can see indeed, the probability of text is not much larger than probability of "the" and this is not the case when we have just one distribution. This is clearly because of the use of the background model which assign a very high probability to "the" low probability to "text". If you look at the equation, you will see obviously some interaction of the two distributions here. In particular, you will see in order to make them equal and then the probability assigned by Theta sub d must be higher for a word that has a smaller probability given by the background. This is obvious from examining this equation because "the" background part is weak for "text" it\'s a small. So in order to compensate for that, we must make the probability of "text" that\'s given by Theta sub d somewhat larger so that the two sides can be balanced. So this is in fact a very general behavior of this mixture model. That is, if one distribution assigns a high probability to one word than another, then the other distribution would tend to do the opposite. Basically, it would discourage other distributions to do the same and this is to balance them out so that, we can account for all words. This also means that, by using a background model that is fixed to assign high probabilities to background words, we can indeed encourage the unknown topic word distribution to assign smaller probabilities for such common words. Instead, put more probability mass on the content words that cannot be explained well by the background model meaning that, they have a very small probability from the background model like "text" here.\nLike\nDislike\nReport an issue\nShare'])
https://www.coursera.org/learn/cs-410/lecture/WDo7w/12-7-contextual-text-mining-mining-causal-topics-with-time-series-supervisiondict_values(["List\nCS 410: Text Information Systems\nWeek 12\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\nPrevious\nNext\nWeek 12 Information\nWeek 12 Lessons\nVideo:\nVideo\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.3 Text-Based Prediction\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n12.4 Contextual Text Mining: Motivation\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\n12.8 Summary for Exam 2\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nChinese (Simplified)\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:07\nThis lecture is about using a time series as context to potentially discover causal topics in text. In this lecture, we're going to continue discussing Contextual Text Mining. In particular, we're going to look at the time series as a context for analyzing text, to potentially discover causal topics. As usual, it started with the motivation. In this case, we hope to use text mining to understand a time series. Here, what you are seeing is Dow Jones Industrial Average stock price curves. And you'll see a sudden drop here. Right. So one would be interested knowing what might have caused the stock market to crash.\n0:48\nWell, if you know the background, and you might be able to figure it out if you look at the time stamp, or there are other data that can help us think about. But the question here is can we get some clues about this from the companion news stream? And we have a lot of news data that generated during that period.\n1:08\nSo if you do that we might actually discover the crash. After it happened, at the time of the September 11 attack. And that's the time when there is a sudden rise of the topic about September 11 happened in news articles.\n1:26\nHere's another scenario where we want to analyze the Presidential Election. And this is the time series that are from the Presidential Prediction Market. For example, I write a trunk of market would have stocks for each candidate. And if you believe one candidate that will win then you tend to buy the stock for that candidate, causing the price of that candidate to increase. So, that's a nice way to actual do survey of people's opinions about these candidates.\n2:00\nNow, suppose you see something drop of price for one candidate. And you might also want to know what might have caused the sudden drop.\n2:10\nOr in a social science study, you might be interested in knowing what method in this election, what issues really matter to people. Now again in this case, we can look at the companion news stream and ask for the question. Are there any clues in the news stream that might provide insight about this? So for example, we might discover the mention of tax cut\n2:35\nhas been increasing since that point. So maybe, that's related to the drop of the price. So all these cases are special cases of a general problem of joint analysis of text and a time series data to discover causal topics. The input in this case is time series plus text data that are produced in the same time period, the companion text stream.\n3:02\nAnd this is different from the standard topic models, where we have just to text collection. That's why we see time series here, it serves as context.\n3:13\nNow, the output that we want to generate is the topics whose coverage in the text stream has strong correlations with the time series.\n3:22\nFor example, whenever the topic is managing the price tends to go down, etc.\n3:28\nNow we call these topics Causal Topics. Of course, they're not, strictly speaking, causal topics. We are never going to be able to verify whether they are causal, or there's a true causal relationship here. That's why we put causal in quotation marks. But at least they are correlating topics that might potentially explain the cause and humans can certainly further analyze such topics to understand the issue better.\n3:59\nAnd the output would contain topics just like in topic modeling. But we hope that these topics are not just the regular topics with. These topics certainly don't have to explain the data of the best in text, but rather they have to explain the data in the text. Meaning that they have to reprehend the meaningful topics in text. Cement but also more importantly, they should be correlated with external hand series that's given as a context. So to understand how we solve this problem, let's first adjust to solve the problem with reactive topic model, for example PRSA. And we can apply this to text stream and with some extension like a CPRSA or Contextual PRSA. Then we can discover these topics in the correlation and also discover their coverage over time.\n4:53\nSo, one simple solution is, to choose the topics from this set that have the strongest correlation with the external time series.\n5:05\nBut this approach is not going to be very good. Why? Because awareness pictured to the topics is that they will discover by PRSA or LDA. And that means the choice of topics will be very limited. And we know these models try to maximize the likelihood of the text data. So those topics tend to be the major topics that explain the text data well. aAnd they are not necessarily correlated with time series. Even if we get the best one, the most correlated topics might still not be so\n5:34\ninteresting from causal perspective.\n5:37\nSo here in this work site here, a better approach was proposed. And this approach is called Iterative Causal Topic Modeling. The idea is to do an iterative adjustment of topic, discovered by topic models using time series to induce a product.\n5:57\nSo here's an illustration on how this work, how this works. Take the text stream as input and then apply regular topic modeling to generate a number of topics. Let's say four topics. Shown here.\n6:09\nAnd then we're going to use external time series to assess which topic is more causally related or correlated with the external time series. So we have something that rank them. And we might think that topic one and topic four are more correlated and topic two and topic three are not. Now we could have stopped here and that would be just like what the simple approached that I talked about earlier then we can get to these topics and call them causal topics. But as I also explained that these topics are unlikely very good because they are general topics that explain the whole text connection. They are not necessary. The best topics are correlated with our time series.\n6:51\nSo what we can do in this approach is to first zoom into word level and we can look into each word and the top ranked word listed for each topic. Let's say we take Topic 1 as the target examined. We know Topic 1 is correlated with the time series. Or is at least the best that we could get from this set of topics so far.\n7:18\nAnd we're going to look at the words in this topic, the top words.\n7:23\nAnd if the topic is correlated with the Time Series, there must be some words that are highly correlated with the Time Series. So here, for example, we might discover W1 and W3 are positively correlated with Time Series, but W2 and W4 are negatively correlated.\n7:41\nSo, as a topic, and it's not good to mix these words with different correlations. So we can then for the separate of these words. We are going to get all the red words that indicate positive correlations. W1 and W3. And we're going to also get another sub topic.\n8:00\nIf you want. That represents a negatively correlated words, W2 and W4.\n8:07\nNow, these subtopics, or these variations of topics, based on the correlation analysis, are topics that are still quite related to the original topic, Topic 1. But they are already deviating, because of the use of time series information for bias selection of words. So then in some sense, well we should expect so, some sense more correlated with the time series than the original Topic 1. Because the Topic 1 has mixed words, here we separate them.\n8:42\nSo each of these two subtopics\n8:46\ncan be expected to be better coherent in this time series. However, they may not be so coherent as it mention. So the idea here is to go back to topic model by using these each as a prior to further guide the topic modeling. And that's to say we ask our topic models now discover topics that are very similar to each of these two subtopics. And this will cause a bias toward more correlate to the topics was a time series. Of course then we can apply topic models to get another generation of topics. And that can be further ran to the base of the time series to set after the highly correlated topics. And then we can further analyze the components at work in the topic and then try to analyze.word level correlation. And then get the even more correlated subtopics that can be further fed into the process as prior to drive the topic of model discovery.\n9:46\nSo this whole process is just a heuristic way of optimizing causality and coherence, and that's our ultimate goal. Right? So here you see the pure topic models will be very good at maximizing topic coherence, the topics will be all meaningful.\n10:02\nIf we only use causality test, or correlation measure, then we might get a set words that are strongly correlate with time series, but they may not necessarily mean anything. It might not be cementric connected. So, that would be at the other extreme, on the top.\n10:21\nNow, the ideal is to get the causal topic that's scored high, both in topic coherence and also causal relation. In this approach, it can be regarded as an alternate way to maximize both sine engines. So when we apply the topic models we're maximizing the coherence. But when we decompose the topic model words into sets of words that are very strong correlated with the time series. We select the most strongly correlated words with the time series. We are pushing the model back to the causal dimension to make it better in causal scoring. And then, when we apply the selected words as a prior to guide a topic modeling, we again go back to optimize the coherence. Because topic models, we ensure the next generation of topics to be coherent and we can iterate when they're optimized in this way as shown on this picture.\n11:20\nSo the only I think a component that you haven't seen such a framework is how to measure the causality. Because the rest is just talking more on. So let's have a little bit of discussion of that. So here we show that. And let's say we have a topic about government response here. And then we just talking more of we can get coverage of the topic over time. So, we have a time series, X sub t.\n11:43\nNow, we also have, are give a time series that represents external information. It's a non text time series, Y sub t. It's the stock prices. Now the the question here is does Xt cause Yt?\n11:58\nWell in other words, we want to match the causality relation between the two. Or maybe just measure the correlation of the two.\n12:08\nThere are many measures that we can use in this framework. For example, pairs in correlation is a common use measure. And we got to consider time lag here so that we can try to capture causal relation. Using somewhat past data and using the data in the past\n12:26\nto try to correlate with the data on points of y that represents the future, for example. And by introducing such lag, we can hopefully capture some causal relation by even using correlation measures like person correlation.\n12:45\nBut a common use, the measure for causality here is Granger Causality Test.\n12:52\nAnd the idea of this test is actually quite simple. Basically you're going to have all the regressive model to use the history information of Y to predict itself. And this is the best we could without any other information. So we're going to build such a model. And then we're going to add some history information of X into such model. To see if we can improve the prediction of Y. If we can do that with a statistically significant difference. Then we just say X has some causal inference on Y, or otherwise it wouldn't have causal improvement of prediction of Y.\n13:32\nIf, on the other hand, the difference is insignificant and that would mean X does not really have a cause or relation why. So that's the basic idea. Now, we don't have time to explain this in detail so you could read, but you would read at this cited reference here to know more about this measure. It's a very convenient used measure. Has many applications.\n13:55\nSo next, let's look at some simple results generated by this approach. And here the data is the New York Times and in the time period of June 2000 through December of 2011. And here the time series we used is stock prices of two companies. American Airlines and Apple and the goal is to see if we inject the sum time series contest, whether we can actually get topics that are wise for the time series. Imagine if we don't use any input, we don't use any context. Then the topics from New York times discovered by PRSA would be just general topics that people talk about in news. All right. Those major topics in the news event.\n14:41\nBut here you see these topics are indeed biased toward each time series. And particularly if you look at the underlined words here in the American Airlines result, and you see airlines, airport, air, united trade, or terrorism, etc. So it clearly has topics that are more correlated with the external time series. On the right side, you see that some of the topics are clearly related to Apple, right. So you can see computer, technology, software, internet, com, web, etc. So that just means the time series has effectively served as a context to bias the discovery of topics. From another perspective, these results help us on what people have talked about in each case. So not just the people, what people have talked about, but what are some topics that might be correlated with their stock prices. And so these topics can serve as a starting point for people to further look into issues and you'll find the true causal relations. Here are some other results from analyzing Presidential Election time series. The time series data here is from Iowa Electronic market. And that's a prediction market. And the data is the same. New York Times from May 2000 to October 2000. That's for 2000 presidential campaign election. Now, what you see here are the top three words in significant topics from New York Times.\n16:21\nAnd if you look at these topics, and they are indeed quite related to the campaign. Actually the issues are very much related to the important issues of this presidential election. Now here I should mention that the text data has been filtered by using only the articles that mention these candidate names.\n16:45\nIt's a subset of these news articles. Very different from the previous experiment.\n16:53\nBut the results here clearly show that the approach can uncover some important issues in that presidential election. So tax cut, oil energy, abortion and gun control are all known to be important issues in that presidential election. And that was supported by some literature in political science.\n17:17\nAnd also I was discussing Wikipedia, right. So basically the results show that the approach can effectively discover possibly causal topics based on the time series data.\n17:35\nSo there are two suggested readings here. One is the paper about this iterative topic modeling with time series feedback. Where you can find more details about how this approach works. And the second one is reading about Granger Casuality text.\n17:55\nSo in the end, let's summarize the discussion of Text-based Prediction. Now, Text-based prediction is generally very useful for big data applications that involve text. Because they can help us inform new knowledge about the world. And the knowledge can go beyond what's discussed in the text.\n18:17\nAs a result can also support optimizing of our decision making. And this has a wider spread application.\n18:28\nText data is often combined with non-text data for prediction. because, for this purpose, the prediction purpose, we generally would like to combine non-text data and text data together, as much cruel as possible for prediction. And so as a result during the analysis of text and non-text is very necessary and it's also very useful. Now when we analyze text data together with non-text data, we can see they can help each other. So non-text data, provide a context for mining text data, and we discussed a number of techniques for contextual text mining. And on the other hand, a text data can also help interpret patterns discovered from non-text data, and this is called a pattern annotation.\n19:14\nIn general, this is a very active research topic, and there are new papers being published. And there are also many open challenges that have to be solved. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/Uufkz/7-7-word-association-mining-and-analysisdict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.7 Word Association Mining and Analysis\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.7 Word Association Mining and Analysis\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nChinese (Simplified)\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the word association mining and analysis. In this lecture, we're going to talk about how to mine associations of words from text. Now this is an example of knowledge about the natural language that we can mine from text data.\n0:33\nHere's the outline. We're going to first talk about what is word association and then explain why discovering such relations is useful and finally we're going to talk about some general ideas about how to mine word associations. In general there are two word relations and these are quite basic.\n0:56\nOne is called a paradigmatic relation. The other is syntagmatic relation. A and B have paradigmatic relation if they can be substituted for each other. That means the two words that have paradigmatic relation would be in the same semantic class, or syntactic class. And we can in general replace one by the other without affecting the understanding of the sentence. That means we would still have a valid sentence. For example, cat and dog, these two words have a paradigmatic relation because they are in the same class of animal. And in general, if you replace cat with dog in a sentence, the sentence would still be a valid sentence that you can make sense of.\n1:58\nSimilarly Monday and Tuesday have paradigmatical relation.\n2:04\nThe second kind of relation is called syntagmatical relation.\n2:10\nIn this case, the two words that have this relation, can be combined with each other. So A and B have syntagmatic relation if they can be combined with each other in a sentence, that means these two words are semantically related.\n2:30\nSo for example, cat and sit are related because a cat can sit somewhere.\n2:38\nSimilarly, car and drive are related semantically and they can be combined with each other to convey meaning. However, in general, we can not replace cat with sit in a sentence or car with drive in the sentence to still get a valid sentence, meaning that if we do that, the sentence will become somewhat meaningless. So this is different from paradigmatic relation. And these two relations are in fact so fundamental that they can be\n3:17\ngeneralized to capture basic relations between units in arbitrary sequences. And definitely they can be generalized to describe relations of any items in a language. So, A and B don't have to be words and they can be phrases, for example.\n3:37\nAnd they can even be more complex phrases than just a non-phrase. If you think about the general problem of the sequence mining then we can think about the units being and the sequence data. Then we think of paradigmatic relation as relations that are applied to units that tend to occur in a singular locations in a sentence, or in a sequence of data elements in general. So they occur in similar locations relative to the neighbors in the sequence. Syntagmatical relation on the other hand is related to co-occurrent elements that tend to show up in the same sequence.\n4:33\nSo these two are complimentary and are basic relations of words. And we're interested in discovering them automatically from text data. Discovering such worded relations has many applications.\n4:47\nFirst, such relations can be directly useful for improving accuracy of many NLP tasks, and this is because this is part of our knowledge about a language. So if you know these two words are synonyms, for example, and then you can help a lot of tasks.\n5:05\nAnd grammar learning can be also done by using such techniques. Because if we can learn paradigmatic relations, then we form classes of words, syntactic classes for example. And if we learn syntagmatic relations, then we would be able to know the rules for putting together a larger expression based on component expressions. So we learn the structure and what can go with what else.\n5:39\nWord relations can be also very useful for many applications in text retrieval and mining. For example, in search and text retrieval, we can use word associations to modify a query, and this can be used to introduce additional related words into a query and make the query more effective.\n6:01\nIt's often called a query expansion.\n6:05\nOr you can use related words to suggest related queries to the user to explore the information space.\n6:12\nAnother application is to use word associations to automatically construct the top of the map for browsing. We can have words as nodes and associations as edges. A user could navigate from one word to another to\n6:28\nfind information in the information space.\n6:33\nFinally, such word associations can also be used to compare and summarize opinions. For example, we might be interested in understanding positive and negative opinions about the iPhone 6. In order to do that, we can look at what words are most strongly associated with a feature word like battery in positive versus negative reviews. Such a syntagmatical relations would help us show the detailed opinions about the product.\n7:16\nSo, how can we discover such associations automatically? Now, here are some intuitions about how to do that. Now let's first look at the paradigmatic relation.\n7:29\nHere we essentially can take advantage of similar context.\n7:34\nSo here you see some simple sentences about cat and dog. You can see they generally occur in similar context, and that after all is the definition of paradigmatic relation.\n7:49\nOn the right side you can kind of see I extracted expressly the context of cat and dog from this small sample of text data.\n8:00\nI've taken away cat and dog from these sentences, so that you can see just the context.\n8:08\nNow, of course we can have different perspectives to look at the context.\n8:13\nFor example, we can look at what words occur in the left part of this context. So we can call this left context. What words occur before we see cat or dog? So, you can see in this case, clearly dog and cat have similar left context.\n8:41\nYou generally say his cat or my cat and you say also, my dog and his dog. So that makes them similar in the left context.\n8:53\nSimilarly, if you look at the words that occur after cat and dog, which we can call right context, they are also very similar in this case. Of course, it's an extreme case, where you only see eats.\n9:08\nAnd in general, you'll see many other words, of course, that can't follow cat and dog.\n9:17\nYou can also even look at the general context. And that might include all the words in the sentence or in sentences around this word.\n9:27\nAnd even in the general context, you also see similarity between the two words.\n9:35\nSo this was just a suggestion that we can discover paradigmatic relation by looking at the similarity of context of words. So, for example, if we think about the following questions. How similar are context of cat and context of dog?\n9:56\nIn contrast how similar are context of cat and context of computer?\n10:02\nNow, intuitively, we're to imagine the context of cat and the context of dog would be more similar than the context of cat and context of the computer. That means, in the first case the similarity value would be high,\n10:21\nbetween the context of cat and dog, where as in the second, the similarity between context of cat and computer would be low because they all not having a paradigmatic relationship and imagine what words occur after computer in general. It would be very different from what words occur after cat.\n10:46\nSo this is the basic idea of what this covering, paradigmatic relation.\n10:52\nWhat about the syntagmatic relation? Well, here we're going to explore the correlated occurrences, again based on the definition of syntagmatic relation.\n11:03\nHere you see the same sample of text.\n11:06\nBut here we're interested in knowing what other words are correlated with the verb eats and what words can go with eats.\n11:16\nAnd if you look at the right side of this slide and you see, I've taken away the two words around eats.\n11:27\nI've taken away the word to its left and also the word to its right in each sentence.\n11:35\nAnd then we ask the question, what words tend to occur to the left of eats?\n11:43\nAnd what words tend to occur to the right of eats?\n11:49\nNow thinking about this question would help us discover syntagmatic relations because syntagmatic relations essentially captures such correlations.\n12:03\nSo the important question to ask for syntagmatical relation is, whenever eats occurs, what other words also tend to occur?\n12:16\nSo the question here has to do with whether there are some other words that tend to co-occur together with each. Meaning that whenever you see eats you tend to see the other words.\n12:29\nAnd if you don't see eats, probably, you don't see other words often either.\n12:36\nSo this intuition can help discover syntagmatic relations.\n12:41\nNow again, consider example.\n12:44\nHow helpful is occurrence of eats for predicting occurrence of meat?\n12:49\nRight. All right, so knowing whether eats occurs in a sentence would generally help us predict whether meat also occurs indeed. And if we see eats occur in the sentence, and that should increase the chance that meat would also occur.\n13:08\nIn contrast, if you look at the question in the bottom, how helpful is the occurrence of eats for predicting of occurrence of text?\n13:17\nBecause eats and text are not really related, so knowing whether eats occurred in the sentence doesn't really help us predict the weather, text also occurs in the sentence. So this is in contrast to the question about eats and meat.\n13:35\nThis also helps explain that intuition behind the methods of what discovering syntagmatic relations. Mainly we need to capture the correlation between the occurrences of two words.\n13:50\nSo to summarize the general ideas for discovering word associations are the following.\n13:56\nFor paradigmatic relation, we present each word by its context. And then compute its context similarity. We're going to assume the words that have high context similarity to have paradigmatic relation.\n14:14\nFor syntagmatic relation, we will count how many times two words occur together in a context, which can be a sentence, a paragraph, or a document even. And we're going to compare their co-occurrences with their individual occurrences.\n14:33\nWe're going to assume words with high co-occurrences but relatively low individual occurrences to have syntagmatic relations because they attempt to occur together and they don't usually occur alone. Note that the paradigmatic relation and the syntagmatic relation are actually closely related in that paradigmatically related words tend to have syntagmatic relation with the same word. They tend to be associated with the same word, and that suggests that we can also do join the discovery of the two relations. So these general ideas can be implemented in many different ways. And the course won't cover all of them, but we will cover at least some of the methods that are effective for discovering these relations. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/o93Yl/11-5-opinion-mining-and-sentiment-analysis-motivationdict_values(["List\nCS 410: Text Information Systems\nWeek 11\n11.5 Opinion Mining and Sentiment Analysis: Motivation\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nVideo:\nVideo\n11.1 Text Categorization: Discriminative Classifier Part 1\n. Duration: 20 minutes\n20 min\nVideo:\nVideo\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\n. Duration: 31 minutes\n31 min\nVideo:\nVideo\n11.3 Text Categorization: Evaluation Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n11.4 Text Categorization: Evaluation Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n11.5 Opinion Mining and Sentiment Analysis: Motivation\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\n11.5 Opinion Mining and Sentiment Analysis: Motivation\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about, Opinion Mining and Sentiment Analysis, covering, Motivation. In this lecture, we're going to start, talking about, mining a different kind of knowledge. Namely, knowledge about the observer or humans that have generated the text data. In particular, we're going to talk about the opinion mining and sentiment analysis.\n0:32\nAs we discussed earlier, text data can be regarded as data generated from humans as subjective sensors.\n0:43\nIn contrast, we have other devices such as video recorder that can report what's happening in the real world objective to generate the viewer data for example.\n0:58\nNow the main difference between test data and other data, like video data, is that it has rich opinions, and the content tends to be subjective because it's generated from humans.\n1:16\nNow, this is actually a unique advantaged of text data, as compared with other data, because the office is a great opportunity to understand the observers. We can mine text data to understand their opinions. Understand people's preferences, how people think about something.\n1:37\nSo this lecture and the following lectures will be mainly about how we can mine and analyze opinions buried in a lot of text data.\n1:49\nSo let's start with the concept of opinion. It's not that easy to formally define opinion, but mostly we would define opinion as a subjective statement describing what a person believes or thinks about something.\n2:08\nNow, I highlighted quite a few words here. And that's because it's worth thinking a little bit more about these words. And that will help us better understand what's in an opinion. And this further helps us to define opinion more formally. Which is always needed to computation to resolve the problem of opinion mining. So let's first look at the key word of subjective here. This is in contrast with objective statement or factual statement.\n2:40\nThose statements can be proved right or wrong.\n2:45\nAnd this is a key differentiating factor from opinions which tends to be not easy to prove wrong or right, because it reflects what the person thinks about something.\n2:59\nSo in contrast, objective statement can usually be proved wrong or correct.\n3:07\nFor example, you might say this computer has a screen and a battery.\n3:16\nNow that's something you can check. It's either having a battery or not.\n3:23\nBut in contrast with this, think about the sentence such as, this laptop has the best battery or this laptop has a nice screen. Now these statements are more subjective and it's very hard to prove whether it's wrong or correct.\n3:45\nSo opinion, is a subjective statement.\n3:50\nAnd next lets look at the keyword person here. And that indicates that is an opinion holder. Because when we talk about opinion, it's about an opinion held by someone. And then we notice that there is something here. So that is the target of the opinion. The opinion is expressed on this something.\n4:11\nAnd now, of course, believes or thinks implies that an opinion will depend on the culture or background and the context in general. Because a person might think different in a different context. People from different background may also think in different ways. So this analysis shows that there are multiple elements that we need to include in order to characterize opinion.\n4:38\nSo, what's a basic opinion representation like? Well, it should include at least three elements, right? Firstly, it has to specify what's the opinion holder. So whose opinion is this? Second, it must also specify the target, what's this opinion about?\n4:57\nAnd third, of course, we want opinion content. And so what exactly is opinion? If you can identify these, we get a basic understanding of opinion and can already be useful sometimes. You want to understand further, we want enriched opinion representation.\n5:15\nAnd that means we also want to understand that, for example, the context of the opinion and what situation was the opinion expressed. For example, what time was it expressed? We, also, would like to, people understand the opinion sentiment, and this is to understand that what the opinion tells us about the opinion holder's feeling. For example, is this opinion positive, or negative? Or perhaps the opinion holder was happy or was sad, and so such understanding obvious to those beyond just Extracting the opinion content, it needs some analysis.\n6:00\nSo let's take a simple example of a product review. In this case, this actually expressed the opinion holder, and expressed the target. So its obviously whats opinion holder and that's just reviewer and its also often very clear whats the opinion target and that's the product review for example iPhone 6. When the review is posted usually you can't such information easier.\n6:27\nNow the content, of course, is a review text that's, in general, also easy to obtain. So you can see product reviews are fairly easy to analyze in terms of obtaining a basic opinion of representation. But of course, if you want to get more information, you might know the Context, for example. The review was written in 2015. Or, we want to know that the sentiment of this review is positive. So, this additional understanding of course adds value to mining the opinions.\n7:04\nNow, you can see in this case the task is relatively easy and that's because the opinion holder and the opinion target have already been identified.\n7:14\nNow let's take a look at the sentence in the news. In this case, we have a implicit holder and a implicit target. And the tasker is in general harder. So, we can identify opinion holder here, and that's the governor of Connecticut.\n7:32\nWe can also identify the target. So one target is Hurricane Sandy, but there is also another target mentioned which is hurricane of 1938. So what's the opinion? Well, there's a negative sentiment here that's indicated by words like bad and worst.\n7:53\nAnd we can also, then, identify context, New England in this case.\n8:00\nNow, unlike in the playoff review, all these elements must be extracted by using natural RAM processing techniques. So, the task Is much harder. And we need a deeper natural language processing.\n8:14\nAnd these examples also\n8:17\nsuggest that a lot of work can be easy to done for product reviews. That's indeed what has happened. Analyzing and assembling news is still quite difficult, it's more difficult than the analysis of opinions in product reviews.\n8:36\nNow there are also some other interesting variations. In fact, here we're going to examine the variations of opinions, more systematically. First, let's think about the opinion holder.\n8:47\nThe holder could be an individual or it could be group of people. Sometimes, the opinion was from a committee. Or from a whole country of people.\n8:56\nOpinion target accounts will vary a lot. It can be about one entity, a particular person, a particular product, a particular policy, ect. But it could be about a group of products. Could be about the products from a company in general.\n9:11\nCould also be very specific about one attribute, though. An attribute of the entity. For example, it's just about the battery of iPhone. It could be someone else's opinion. And one person might comment on another person's Opinion, etc. So, you can see there is a lot of variation here that will cause the problem to vary a lot. Now, opinion content, of course, can also vary a lot on the surface, you can identify one-sentence opinion or one-phrase opinion. But you can also have longer text to express an opinion, like the whole article.\n9:48\nAnd furthermore we identify the variation in the sentiment or emotion damage that's above the feeding of the opinion holder. So, we can distinguish a positive versus negative or mutual or happy versus sad, separate.\n10:03\nFinally, the opinion context can also vary. We can have a simple context, like different time or different locations. But there could be also complex contexts, such as some background of topic being discussed. So when opinion is expressed in particular discourse context, it has to be interpreted in different ways than when it's expressed in another context. So the context can be very [INAUDIBLE] to entire discourse context of the opinion. From computational perspective, we're mostly interested in what opinions can be extracted from text data. So, it turns out that we can also differentiate, distinguish, different kinds of opinions in text data from computation perspective. First, the observer might make a comment about opinion targeting, observe the word So in case we have the author's opinion. For example, I don't like this phone at all. And that's an opinion of this author.\n10:59\nIn contrast, the text might also report opinions about others. So the person could also Make observation about another person's opinion and reported this opinion. So for example, I believe he loves the painting. And that opinion is really about the It is really expressed by another person here. So, it doesn't mean this author loves that painting.\n11:33\nSo clearly, the two kinds of opinions need to be analyzed in different ways, and sometimes in product reviews, you can see, although mostly the opinions are false from this reviewer. Sometimes, a reviewer might mention opinions of his friend or her friend.\n11:51\nAnother complication is that there may be indirect opinions or inferred opinions that can be obtained. By making inferences on what's expressed in the text that might not necessarily look like opinion. For example, one statement that might be, this phone ran out of battery in just one hour. Now, this is in a way a factual statement because It's either true or false, right? You can even verify that, but from this statement, one can also infer some negative opinions about the quality of the battery of this phone, or the feeling of the opinion holder about the battery. The opinion holder clearly wished that the battery do last longer.\n12:42\nSo these are interesting variations that we need to pay attention to when we extract opinions. Also, for this reason about indirect opinions,\n12:53\nit's often also very useful to extract whatever the person has said about the product, and sometimes factual sentences like these are also very useful. So, from a practical viewpoint, sometimes we don't necessarily extract the subject of sentences. Instead, again, all the sentences that are about the opinions are useful for understanding the person or understanding the product that we commend.\n13:19\nSo the task of opinion mining can be defined as taking textualized input to generate a set of opinion representations. Each representation we should identify opinion holder, target, content, and the context. Ideally we can also infer opinion sentiment from the comment and the context to better understand.\n13:43\nThe opinion.\n13:44\nNow often, some elements of the representation are already known. I just gave a good example in the case of product we'd use where the opinion holder and the opinion target are often expressly identified. And that's not why this turns out to be one of the simplest opinion mining tasks. Now, it's interesting to think about the other tasks that might be also simple. Because those are the cases where you can easily build applications by using opinion mining techniques.\n14:17\nSo now that we have talked about what is opinion mining, we have defined the task. Let's also just talk a little bit about why opinion mining is very important and why it's very useful. So here, I identify three major reasons, three broad reasons. The first is it can help decision support. It can help us optimize our decisions. We often look at other people's opinions, look at read the reviews in order to make a decisions like buying a product or using a service.\n14:52\nWe also would be interested in others opinions when we decide whom to vote for example.\n15:00\nAnd policy makers, may also want to know people's opinions when designing a new policy. So that's one general, kind of, applications. And it's very broad, of course. The second application is to understand people, and this is also very important. For example, it could help understand people's preferences. And this could help us better serve people. For example, we optimize a product search engine or optimize a recommender system if we know what people are interested in, what people think about product.\n15:35\nIt can also help with advertising, of course, and we can have targeted advertising if we know what kind of people tend to like what kind of plot.\n15:48\nNow the third kind of application can be called voluntary survey. Now this is most important research that used to be done by doing surveys, doing manual surveys. Question, answer it. People need to feel informs to answer their questions. Now this is directly related to humans as sensors, and we can usually aggregate opinions from a lot of humans through kind of assess the general opinion. Now this would be very useful for business intelligence where manufacturers want to know where their products have advantages over others.\n16:31\nWhat are the winning features of their products, winning features of competitive products.\n16:37\nMarket research has to do with understanding consumers oppinions. And this create very useful directive for that. Data-driven social science research can benefit from this because they can do text mining to understand the people's opinions. And if you can aggregate a lot of opinions from social media, from a lot of, popular\n16:58\ninformation then you can actually do some study of some questions. For example, we can study the behavior of people on social media on social networks. And these can be regarded as voluntary survey done by those people.\n17:19\nIn general, we can gain a lot of advantage in any prediction task because we can leverage the text data as extra data above any problem. And so we can use text based prediction techniques to help you make predictions or improve the accuracy of prediction. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/GUQ1Q/lesson-5-7-link-analysis-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.7: Link Analysis - Part 2\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.7: Link Analysis - Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC]\n0:09\nSo let's take a look at this in detail. So in this random surfing model at any page would assume random surfer would choose the next page to visit. So this is a small graph here. That's of course, over simplification of the complicated web. But let's say there are four documents here, d1, d2, d3 and d4. And let's assume that a random surfer or random walker can be any of these pages. And then the random surfer could decide to, just randomly jumping to any page or follow a link and then visit the next page.\n0:56\nSo if the random surfer is at d1,\n1:01\nthen there is some probability that random surfer will follow the links. Now there are two outlinks here, one is pointing to d3, the other is pointing to d4. So the random surfer could pick any of these two to reach d3 and d4. But it also assumes that the random so far might get bore sometimes. So the random surfing which decide to ignore the actual links and simply randomly jump into any page in the web. So if it does that, it would be able to reach any of the other pages even though there's no link you actually, you want from that page.\n1:46\nSo this is to assume that random surfing model. Imagine a random surfer is really doing surfing like this, then we can ask the question how likely on average the surfer would actually reach a particular page like a d1, a d2, or a d3. That's the average probability of visiting a particular page and this probability is precisely what a page ranker computes. So the page rank score of the document is the average probability that the surfer visits a particular page. Now intuitively, this would basically capture the inlink account, why? Because if a page has a lot of inlinks, then it would have a higher chance of being visited. Because there will be more opportunities of having the server to follow a link to come to this page.\n2:41\nAnd this is why the random surfing model actually captures the ID of counting the inlinks. Note that it also considers the interacting links, why? Because if the page is that point then you have themselves a lot of inlinks. That would mean the random surfer would very likely reach one of them and therefore, it increase the chance of visiting you. So this is just a nice way to capture both indirect and a direct links. So mathematically, how can we compute this problem in a day in order to see that, we need to take a look at how this problem there is a computing. So first of all let's take a look at the transition metrics here. And this is just metrics with values indicating how likely the random surfer would go from one page to another. So each rule stands for a starting page. For example, rule one would indicate the probability of going to any of the other four pages from d1. And here we see there are only 2 non 0 entries which is 1/2. So this is because if you look at the graph d1 is pointing to d3 and d4. There is no link from d1 or d2. So we've got 0s for the first 2 columns and 0.5 for d3 and d4. In general, the M in this matrix, M sub ij is the probability of going from di to dj. And obviously for each rule, the values should sum to 1, because the surfer would have to go to precisely one of these other pages. So this is a transition metric. Now how can we compute the probability of a surfer visiting a page?\n4:44\nWell if you look at the surf model then basically,\n4:50\nwe can compute the probability of reaching a page as follows. So here on the left hand side, you see it's the probability\n5:02\nvisiting page dj at time plus 1, so it's the next time point. On the right hand side, you can see the equation involves the probability of at page di at time t.\n5:21\nSo you can see the subscript in that t here, and that indicates that's the probability that the server was at a document at time t. So the equation basically, captures the two possibilities of reaching at dj at the time t plus 1. What are these two possibilities? Well one is through random surfing and one is through following a link, as we just explained.\n5:53\nSo the first part captures the probability that the random surfer would reach this page by following a link. And you can see the random surfer chooses this strategy with probability 1 minus alpha as we assume. And so there is a factor of 1 minus alpha here. But the main party is realist sum over all the possible pages that the surfer could have been at time t.\n6:23\nThere are n pages so it's a sum over all possible n pages. Inside the sum is a product of two probabilities. One is the probability that the surfer was at di at time t, that's p sub t of di. The other is the transition probability from di to dj. And so in order to reach this dj page, the surfer must first be at di at time t. And then also, would also have to follow the link to go from di to dj. So the probability is the probability of being at di at time t multiplied by the probability of going from that page to the target page, dj here. The second part is a similar sum, the only difference is that now the transition probability is a uniform transition probability. 1 over n and this part of captures is the probability of reaching this page through random jumping.\n7:32\nSo the form is exactly the same and this also allows us to see on why PageRank is essentially assumed a smoothing of the transition matrix. If you think about this 1 over n as coming from another transition matrix that has all the elements being 1 over n in uniform matrix. Then you can see very clearly essentially we can merge the two parts, because they are of the same form. We can imagine there's a different metrics that's combination of this m and that uniform metrics where every m is 1 over n. And in this sense PageRank uses this idea of smoothing and ensuring that there's no zero entry in such as transition matrix. Now of course this is the time dependent the calculation of the probabilities. Now we can imagine, if we'll compute the average of the probabilities, the average of probabilities probably with the sets of file this equation without considering the time index. So let's drop the time index and just assume that they will be equal.\n8:42\nNow this would give us any equations, because for each page we have such equation. And if you look at the what variables we have in these equations there are also precisely n variables.\n8:58\nSo this basically means, we now have a system of\n9:04\nn equations with n variables and these are linear equations. So basically, now the problem boils down to solve this system of equations. And here, I also show the equations in the metric form. It's the vector p here equals a matrix or the transpose of the matrix here and multiplied by the vector again.\n9:32\nNow, if you still remember some knowledge that you've learned from linear algebra and then you will realize, this is precisely the equation for eigenvector. When multiply the metrics by this vector, you get the same value as this matter and this can be solved by using iterative algorithm.\n9:54\nSo because the equations here on the back are basically taken from the previous slide. So you'll see the relation between the page that ran sports on different pages. And this iterative approach or power approach, we simply start with s randomly initialized vector p. And then we repeatedly just update this p by multiplying the metrics here by this p factor.\n10:31\nI also show a concrete example here. So you can see this now. If we assume alpha is 0.2, then with the example that we show here on the slide, we have the original transition matrix is here. That includes the graph, the actual links and we have this smoothing transition metrics, uniform transition metrics representing random jumping. And we can combine them together with a liner interpolation to form another metric that would be like this. So essentially, we can imagine now the web looks like this and can be captured like that. They're all virtual links between all the pages now. The page we're on now would just initialize the p vector first and then just computed the updating of this p vector by using this metrics multiplication.\n11:36\nNow if you rewrite this metric multiplication in\n11:42\nterms of individual equations, you'll see this.\n11:46\nAnd this is basically, the updating formula for this particular pages and page score. So you can also see if you want to compute the value of this updated score for d1. You basically multiply this rule by this column, and we'll take the third product of the two. And that will give us the value for this value.\n12:16\nSo this is how we updated the vector we started with an initial values for these guys for this. And then we just revise the scores which generate a new set of scores and the updating formula is this one.\n12:33\nSo we just repeatedly apply this and here it converges. And when the matrix is like this, where there's no 0 values and it can be guaranteed to converge.\n12:44\nAnd at that point the we will just have the PageRank scores for all the pages. We typically go to sets of initial values just to 1 over n.\n12:55\nSo interestingly, this updating formula can be also interpreted as propagating scores on the graph, can you see why? Or if you look at this formula and then compare that with this graph and can you imagine, how we might be able to interpret this as essentially propagating scores over the graph. I hope you will see that indeed, we can imagine we have values initialized on each of these pages. So we can have values here and say, that's a 1 over 4 for each. And then we're going to use these metrics to update this the scores. And if you look at the equation here this one, basically we're going to combine the scores of the pages that possibly would lead to reaching this page. So we'll look at all the pages that are pointing to this page and then combine this score and propagate the sum of the scores to this document, d1. To look at the scores that we present the probability that the random surfer would be visiting the other pages before it reached d1. And then just do the propagation to simulate the probability of reaching this page, d1. So there are two interpretations here. One is just the matrix multiplication. We repeat the multiplying that by these metrics. The other is to just think of it as a propagating these scores repeatedly on the web. So in practice, the combination of PageRank score is actually efficient. Because the matrices is fast and there are some, ways we transform the equation. So that you avoid actually literally computing the values for all those elements.\n14:56\nSometimes you may also normalize the equation and that will give you a somewhat different form of the equation, but then the ranking of pages will not change.\n15:06\nThe results of this potential problem of zero-outlink problem.\n15:10\nIn that case, if a page does not have any outlink then the probability of these pages would not sum to 1. Basically, the probability of reaching the next page from this page would not sum to 1, mainly because we have lost some probability to mass. One would assume there's some probability that the surfer would try to follow the links, but then there is no link to follow. And one possible solution is simply to use a page that is specific damping factor, and that could easily fix this.\n15:46\nBasically, that's to say alpha would be 1.0 for a page with no outlink. In that case, the surfer would just have to randomly jump to another page instead of trying to follow a link.\n15:59\nThere are many extensions of PageRank, one extension is to topic-specific PageRank. Note that PageRank doesn't merely use the query information. So we can make PageRank specific however. So for example, at the top of a specific page you rank, we can simply assume when the surfer is bored. The surfer is not randomly jumping to any page on the web. Instead, he's going to jump to only those pages that are relevant to our query. For example, if the query is not sports then we can assume that when it's doing random jumping, it's going to randomly jump to a sports page. By doing this, then we can buy a PageRank through topic and sports. And then if you know the current theory is about sports, and then you can use this specialized PageRank score to rank documents. That would be better than if you use the generic PageRank score. PageRank is also a channel that can be used in many other applications for network analysis particularly for example, social networks. You can imagine if you compute the PageRank scores for social network, where a link might indicate a friendship or a relation, you would get some meaningful scores for people [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/KaYeS/8-8-probabilistic-topic-models-overview-of-statistical-language-models-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] >> This lecture is about the Overview of Statistical Language Models, which cover proper models as special cases. In this lecture we're going to give a overview of Statical Language Models. These models are general models that cover probabilistic topic models as a special cases. So first off, what is a Statistical Language Model?\n0:31\nA Statistical Language Model is basically a probability distribution over word sequences. So, for example, we might have a distribution that gives, today is Wednesday a probability of .001. It might give today Wednesday is, which is a non-grammatical sentence, a very, very small probability as shown here.\n0:54\nAnd similarly another sentence, the eigenvalue is positive might get the probability of .00001. So as you can see such a distribution clearly is Context Dependent. It depends on the Context of Discussion. Some Word Sequences might have higher probabilities than others but the same Sequence of Words might have different probability in different context.\n1:20\nAnd so this suggests that such a distribution can actually categorize topic\n1:26\nsuch a model can also be regarded as Probabilistic Mechanism for generating text.\n1:33\nAnd that just means we can view text data as data observed from such a model. For this reason, we call such a model as Generating Model. So, now given a model we can then assemble sequences of words. So, for example, based on the distribution that I have shown here on this slide, when matter it say assemble a sequence like today is Wednesday because it has a relative high probability. We might often get such a sequence. We might also get the item value as positive sometimes with a smaller probability and very, very occasionally we might get today is Wednesday because it's probability is so small.\n2:24\nSo in general, in order to categorize such a distribution we must specify probability values for all these different sequences of words. Obviously, it's impossible to specify that because it's impossible to enumerate all of the possible sequences of words. So in practice, we will have to simplify the model in some way. So, the simplest language model is called the Unigram Language Model. In such a case, it was simply a the text is generated by generating each word independently.\n3:02\nBut in general, the words may not be generated independently. But after we make this assumption, we can significantly simplify the language more.\n3:12\nBasically, now the probability of a sequence of words, w1 through wn, will be just the product of the probability of each word.\n3:24\nSo for such a model, we have as many parameters as the number of words in our vocabulary. So here we assume we have n words, so we have n probabilities. One for each word. And then some to 1. So, now we assume that our text is a sample drawn according to this word distribution. That just means, we're going to draw a word each time and then eventually we'll get a text.\n3:53\nSo for example, now again, we can try to assemble words according to a distribution. We might get Wednesday often or today often.\n4:06\nAnd some other words like eigenvalue might have a small probability, etcetera. But with this, we actually can also compute the probability of every sequence, even though our model only specify the probabilities of words. And this is because of the independence. So specifically, we can compute the probability of today is Wednesday.\n4:34\nBecause it's just a product of the probability of today, the probability of is, and probability of Wednesday. For example, I show some fake numbers here and when you multiply these numbers together you get the probability that today's Wednesday. So as you can see, with N probabilities, one for each word, we actually can characterize the probability situation over all kinds of sequences of words. And so, this is a very simple model. Ignore the word order. So it may not be, in fact, in some problems, such as for speech recognition, where you may care about the order of words. But it turns out to be quite sufficient for many tasks that involve topic analysis. And that's also what we're interested in here. So when we have a model, we generally have two problems that we can think about. One is, given a model, how likely are we to observe a certain kind of data points? That is, we are interested in the Sampling Process. The other is the Estimation Process. And that, is to think of the parameters of a model given, some observe the data and we're going to talk about that in a moment. Let's first talk about the sampling. So, here I show two examples of Water Distributions or Unigram Language Models. The first one has higher probabilities for words like a text mining association, it's separate.\n6:10\nNow this signals a topic about text mining because when we assemble words from such a distribution, we tend to see words that often occur in text mining contest.\n6:23\nSo in this case, if we ask the question about what is the probability of generating a particular document. Then, we likely will see text that looks like a text mining paper. Of course, the text that we generate by drawing words. This distribution is unlikely coherent. Although, the probability of generating attacks mine [INAUDIBLE] publishing in the top conference is non-zero assuming that no word has a zero probability in the distribution. And that just means, we can essentially generate all kinds of text documents including very meaningful text documents.\n7:07\nNow, the second distribution show, on the bottom, has different than what was high probabilities. So food [INAUDIBLE] healthy [INAUDIBLE], etcetera. So this clearly indicates a different topic. In this case it's probably about health. So if we sample a word from such a distribution, then the probability of observing a text mining paper would be very, very small.\n7:32\nOn the other hand, the probability of observing a text that looks like a food nutrition paper would be high, relatively higher.\n7:41\nSo that just means, given a particular distribution, different than the text. Now let's look at the estimation problem now. In this case, we're going to assume that we have observed the data. I will know exactly what the text data looks like. In this case, let's assume we have a text mining paper. In fact, it's abstract of the paper, so the total number of words is 100. And I've shown some counts of individual words here.\n8:12\nNow, if we ask the question, what is the most likely\n8:17\nLanguage Model that has been used to generate this text data? Assuming that the text is observed from some Language Model, what's our best guess of this Language Model?\n8:30\nOkay, so the problem now is just to estimate the probabilities of these words. As I've shown here.\n8:37\nSo what do you think? What would be your guess?\n8:40\nWould you guess text has a very small probability, or a relatively large probability?\n8:48\nWhat about query? Well, your guess probably would be dependent on how many times we have observed this word in the text data, right? And if you think about it for a moment. And if you are like many others, you would have guessed that, well, text has a probability of 10 out of 100 because I've observed the text 10 times in the text that has a total of 100 words. And similarly, mining has 5 out of 100. And query has a relatively small probability, just observed for once. So it's 1 out of 100. Right, so that, intuitively, is a reasonable guess. But the question is, is this our best guess or best estimate of the parameters?\n9:37\nOf course, in order to answer this question, we have to define what do we mean by best, in this case, it turns out that our guesses are indeed the best. In some sense and this is called Maximum Likelihood Estimate. And it's the best thing that, it will give the observer data our maximum probability.\n10:01\nMeaning that, if you change the estimate somehow, even slightly, then the probability of the observed text data will be somewhat smaller. And this is called a Maximum Likelihood Estimate. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/13dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 13\nWeek 13 Project Work week\n10 min of readings left\n1 graded assignment left\nthis is a week for working on the project\nThis week is for working on the project...\nReading•\n. Duration: 10 minutes\n10 min\nCourse Project\n1 graded assignment left\nProject Progress Report Submission for Grading\nDue, Nov 15, 12:59 AM EST\nGraded Assignment•\n. Duration: 3 hours\n3h\n•Grade: --'])
https://www.coursera.org/learn/cs-410/lecture/VMh3Z/lesson-3-2-evaluation-of-tr-systems-basic-measuresdict_values(["List\nCS 410: Text Information Systems\nWeek 3\nLesson 3.2: Evaluation of TR Systems - Basic Measures\nPrevious\nNext\nWeek 3 Information\nWeek 3 Lessons\nVideo:\nVideo\nLesson 3.1: Evaluation of TR Systems\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.2: Evaluation of TR Systems - Basic Measures\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 3.3: Evaluation of TR Systems - Evaluating Ranked Lists - Part 1\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\nLesson 3.4: Evaluation of TR Systems - Evaluating Ranked Lists - Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.5: Evaluation of TR Systems - Multi-Level Judgements\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\nLesson 3.6: Evaluation of TR Systems - Practical Issues\n. Duration: 15 minutes\n15 min\nWeek 3 Activities\nProgramming Assignment 2.1\nLesson 3.2: Evaluation of TR Systems - Basic Measures\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the basic measures for evaluation of text retrieval systems. In this lecture, we're going to discuss how we design basic measures to quantitatively compare two retrieval systems. This is a slide that you have seen earlier in the lecture where we talked about the Granville evaluation methodology. We can have a test faction that consists of queries, documents, and [INAUDIBLE]. We can then run two systems on these data sets to contradict the evaluator. Their performance. And we raise the question, about which set of results is better. Is system A better or is system B better? So let's now talk about how to accurately quantify their performance. Suppose we have a total of 10 relevant documents in the collection for this query. Now, the relevant judgments show on the right in [INAUDIBLE] obviously. And we have only seen 3 [INAUDIBLE] there, [INAUDIBLE] documents there. But, we can imagine there are other Random documents in judging for this query. So now, intuitively, we thought that system A is better because it did not have much noise. And in particular we have seen that among the three results, two of them are relevant but in system B, we have five results and only three of them are relevant. So intuitively it looks like system A is more accurate. And this infusion can be captured by a matching holder position, where we simply compute to what extent all the retrieval results are relevant. If you have  position, that would mean that all the retrieval documents are relevant. So in this case system A has a position of two out of three System B has some sweet hold of 5 and this shows that system A is better frequency. But we also talked about System B might be prefered by some other units would like to retrieve as many random documents as possible. So in that case we'll have to compare the number of relevant documents that they retrieve and there's another method called recall. This method uses the completeness of coverage of random documents In your retrieval result. So we just assume that there are ten relevant documents in the collection. And here we've got two of them, in system A. So the recall is 2 out of 10. Whereas System B has called a 3, so it's a 3 out of 10. Now we can see by recall system B is better. And these two measures turn out to be the very basic of measures for evaluating search engine. And they are very important because they are also widely used in many other test evaluation problems. For example, if you look at the applications of machine learning, you tend to see precision recall numbers being reported and for all kinds of tasks.\n3:35\nOkay so, now let's define these two measures more precisely. And these measures are to evaluate a set of retrieved documents, so that means we are considering that approximation of the set of relevant documents.\n3:50\nWe can distinguish 4 cases depending on the situation of the documents. A document can be retrieved or not retrieved, right? Because we are talking about a set of results.\n4:02\nA document can be also relevant or not relevant depending on whether the user thinks this is a useful document.\n4:11\nSo we can now have counts of documents in. Each of the four categories again have a represent the number of documents that have been retrieved and relevant. B for documents that are not retrieved but rather etc.\n4:31\nNo with this table then we can define precision.\n4:36\nAs the ratio of the relevant retrieved documents A to the total of relevant retrieved documents.\n4:48\nSo, this is just A divided by The sum of a and c. The sum of this column.\n4:56\nSingularly recall is defined by dividing a by the sum of a and b. So that's again to divide a by. The sum of the row instead of the column. All right, so we can see precision and recall is all focused on looking at the a,\n5:16\nthat's the number of retrieved relevant documents. But we're going to use different denominators.\n5:23\nOkay, so what would be an ideal result. Well, you can easily see being the ideal case would have precision and recall oil to be 1.0. That means We have got  of all the Relevant documents in our results, and all of the results that we returned all Relevant. At least there's no single Not Relevant document returned.\n5:48\nIn reality, however, high recall tends to be associated with low precision. And you can imagine why that's the case. As you go down the to try to get as many random documents as possible, you tend to encounter a lot of documents, so the precision has to go down. Note that this set can also be defined by a cut off. In the rest of this, that's why although these two measures are defined for retrieve the documents, they are actually very useful for evaluating a rank list. They are the fundamental measures in task retrieval and many other tasks. We often are interested in The precision at ten documents for web search. This means we look at how many documents among the top ten results are actually relevant. Now, this is a very meaningful measure, because it tells us how many relevant documents a user can expect to see On the first page of where they typically show ten results.\n6:50\nSo precision and recall are the basic matches and we need to use them to further evaluate a search engine, but they are the Building blocks.\n7:03\nWe just said that there tends to be a trailoff between precision and recall, so naturally it would be interesting to combine them. And here's one method that's often used, called F-measure And it's a [INAUDIBLE] mean of precision and recall as defined on this slide.\n7:22\nSo, you can see at first, compute the.\n7:29\nInverse of R and P here, and then it would interpret the 2 by using coefficients depending on parameter beta.\n7:42\nAnd after some transformation you can easily see it would be of this form.\n7:49\nAnd in any case it just becomes an agent of precision and recall, and beta is a parameter, that's often set to 1. It can control the emphasis on precision or recall always set beta to 1 We end up having a special case of F-Measure, often called F1. This is a popular measure that's often used as a combined precision and recall. And the formula looks very simple.\n8:16\nIt's just this, here.\n8:20\nNow it's easy to see that if you have a Larger precision, or larger recall than f measure would be high. But, what's interesting is that the trade off between precision and recall is captured an interesting way in f1. So, in order to understand that, we\n8:42\ncan first look at the natural Why not just the combining and using the symbol arithmetically as efficient here? That would be likely the most natural way of combining them So what do you think?\n9:01\nIf you want to think more, you can pause the video.\n9:07\nSo why is this not as good as F1?\n9:13\nOr what's the problem with this?\n9:18\nNow, if you think about the arithmetic mean, you can see this is the sum of multiple terms. In this case, it's the sum of precision and recall. In the case of a sum, the total value tends to be dominated by the large values. that means if you have a very high P or very high R then you really don't care about whether the other value is low so the whole sum would be high. Now this is not desirable because one can easily have a perfect recall. We have perfect recall easily. Can we imagine how?\n9:59\nIt's probably very easy to imagine that we simply retrieve all the documents in the collection and then we have a perfect recall.\n10:07\nAnd this will give us 0.5 as the average. But such results are clearly not very useful for the users even though the average using this formula would be relevantly high.\n10:21\nIn contrast you can see F 1 would reward a case where precision and recall are roughly That seminar, so it would a case where you had extremely high value for one of them.\n10:35\nSo this means f one encodes a different trade off between that. Now this example shows actually a very important. Methodology here. But when you try to solve a problem you might naturally think of one solution, let's say in this it's this error mechanism.\n10:53\nBut it's important not to settle on this source. It's important to think whether you have other ways to combine that.\n11:02\nAnd once you think about the multiple variance It's important to analyze their difference, and then think about which one makes more sense. In this case, if you think more carefully, you will think that F1 probably makes more sense. Than the simple. Although in other cases there may be different results. But in this case the seems not reasonable. But if you don't pay attention to these subtle differences you might just take a easy way to combine them and then go ahead with it. And here later, you will find that, the measure doesn't seem to work well. All right. So this methodology is actually very important in general, in solving problems. Try to think about the best solution. Try to understand the problem very well, and then know why you needed this measure, and why you need to combine precision and recall. And then use that to guide you in finding a good way to solve the problem.\n12:03\nTo summarize, we talked about precision which addresses the question are there retrievable results all relevant? We also talk about the Recall. Which addresses the question, have all of the relevant documents been retrieved. These two, are the two, basic matches in text and retrieval in. They are used for many other tasks, as well. We talk about F measure as a way to combine Precision Precision and recall.\n12:29\nWe also talked about the tradeoff between precision and recall. And this turns out to depend on the user's search tasks and we'll discuss this point more in a later lecture. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/rlksh/11-7-opinion-mining-and-sentiment-analysis-ordinal-logistic-regression-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 11\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nVideo:\nVideo\n11.1 Text Categorization: Discriminative Classifier Part 1\n. Duration: 20 minutes\n20 min\nVideo:\nVideo\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\n. Duration: 31 minutes\n31 min\nVideo:\nVideo\n11.3 Text Categorization: Evaluation Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n11.4 Text Categorization: Evaluation Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n11.5 Opinion Mining and Sentiment Analysis: Motivation\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[NOISE] This lecture is about the ordinal logistic regression for sentiment analysis. So, this is our problem set up for a typical sentiment classification problem. Or more specifically a rating prediction. We have an opinionated text document d as input, and we want to generate as output, a rating in the range of 1 through k so it's a discrete rating, and this is a categorization problem. We have k categories here. Now we could use a regular text for categorization technique to solve this problem. But such a solution would not consider the order and dependency of the categories. Intuitively, the features that can distinguish category 2 from 1, or rather rating 2 from 1, may be similar to those that can distinguish k from k-1. For example, positive words generally suggest a higher rating. When we train categorization problem by treating these categories as independent we would not capture this.\n1:17\nSo what's the solution? Well in general we can order to classify and there are many different approaches. And here we're going to talk about one of them that called ordinal logistic regression. Now, let's first think about how we use logistical regression for a binary sentiment. A categorization problem. So suppose we just wanted to distinguish a positive from a negative and that is just a two category categorization problem. So the predictors are represented as X and these are the features. And there are M features all together. The feature value is a real number. And this can be representation of a text document.\n1:56\nAnd why it has two values, binary response variable 0 or 1. 1 means X is positive, 0 means X is negative. And then of course this is a standard two category categorization problem. We can apply logistical regression. You may recall that in logistical regression, we assume the log of probability that the Y is equal to one, is assumed to be a linear function of these features, as shown here. So this would allow us to also write the probability of Y equals one, given X\n2:36\nin this equation that you are seeing on the bottom.\n2:43\nSo that's a logistical function and you can see it relates this probability to, probability that y=1 to the feature values. And of course beta i's are parameters here, so this is just a direct application of logistical regression for binary categorization.\n3:08\nWhat if we have multiple categories, multiple levels? Well we have to use such a binary logistical regression problem to solve this multi level rating prediction.\n3:21\nAnd the idea is we can introduce multiple binary class files. In each case we asked the class file to predict the, whether the rating is j or above, or the rating's lower than j. So when Yj is equal to 1, it means rating is j or above. When it's 0, that means the rating is Lower than j.\n3:45\nSo basically if we want to predict a rating in the range of 1-k, we first have one classifier to distinguish a k versus others. And that's our classifier one. And then we're going to have another classifier to distinguish it. At k-1 from the rest. That's Classifier 2. And in the end, we need a Classifier to distinguish between 2 and 1. So altogether we'll have k-1 classifiers.\n4:17\nNow if we do that of course then we can also solve this problem and the logistical regression program will be also very straight forward as you have just seen on the previous slide. Only that here we have more parameters. Because for each classifier, we need a different set of parameters. So now the logistical regression classifies index by J, which corresponds to a rating level.\n4:46\nAnd I have also used of J to replace beta 0. And this is to. Make the notation more consistent, than was what we can show in the ordinal logistical regression. So here we now have basically k minus one regular logistic regression classifiers. Each has it's own set of parameters. So now with this approach, we can now do ratings as follows.\n5:19\nAfter we have trained these k-1 logistic regression classifiers, separately of course, then we can take a new instance and then invoke a classifier sequentially to make the decision. So first let look at the classifier that corresponds to level of rating K. So this classifier will tell us whether this object should have a rating of K or about. If probability according to this logistical regression classifier is larger than point five, we're going to say yes. The rating is K.\n6:02\nNow, what if it's not as large as twenty-five? Well, that means the rating's below K, right?\n6:11\nSo now, we need to invoke the next classifier, which tells us whether it's above K minus one.\n6:18\nIt's at least K minus one. And if the probability is larger than twenty-five, then we'll say, well, then it's k-1. What if it says no? Well, that means the rating would be even below k-1. And so we're going to just keep invoking these classifiers. And here we hit the end when we need to decide whether it's two or one. So this would help us solve the problem. Right? So we can have a classifier that would actually give us a prediction of a rating in the range of 1 through k. Now unfortunately such a strategy is not an optimal way of solving this problem. And specifically there are two problems with this approach. So these equations are the same as. You have seen before.\n7:06\nNow the first problem is that there are just too many parameters. There are many parameters. Now, can you count how many parameters do we have exactly here? Now this may be a interesting exercise. To do. So you might want to just pause the video and try to figure out the solution. How many parameters do I have for each classifier?\n7:28\nAnd how many classifiers do we have?\n7:31\nWell you can see the, and so it is that for each classifier we have n plus one parameters, and we have k minus one classifiers all together, so the total number of parameters is k minus one multiplied by n plus one. That's a lot. A lot of parameters, so when the classifier has a lot of parameters, we would in general need a lot of data out to actually help us, training data, to help us decide the optimal parameters of such a complex model.\n8:04\nSo that's not ideal.\n8:07\nNow the second problems is that these problems, these k minus 1 plus fives, are not really independent. These problems are actually dependent.\n8:18\nIn general, words that are positive would make the rating higher\n8:25\nfor any of these classifiers. For all these classifiers. So we should be able to take advantage of this fact.\n8:33\nNow the idea of ordinal logistical regression is precisely that. The key idea is just the improvement over the k-1 independent logistical regression classifiers. And that idea is to tie these beta parameters. And that means we are going to assume the beta parameters. These are the parameters that indicated the inference of those weights. And we're going to assume these beta values are the same for all the K- 1 parameters. And this just encodes our intuition that, positive words in general would make a higher rating more likely.\n9:19\nSo this is intuitively assumptions, so reasonable for our problem setup. And we have this order in these categories.\n9:28\nNow in fact, this would allow us to have two positive benefits. One is it's going to reduce the number of families significantly.\n9:38\nAnd the other is to allow us to share the training data. Because all these parameters are similar to be equal. So these training data, for different classifiers can then be shared to help us set the optimal value for beta.\n9:56\nSo we have more data to help us choose a good beta value.\n10:01\nSo what's the consequence, well the formula would look very similar to what you have seen before only that, now the beta parameter has just one index that corresponds to the feature. It no longer has the other index that corresponds to the level of rating.\n10:19\nSo that means we tie them together. And there's only one set of better values for all the classifiers. However, each classifier still has the distinct R for value. The R for parameter. Except it's different. And this is of course needed to predict the different levels of ratings. So R for sub j is different it depends on j, different than j, has a different R value. But the rest of the parameters, the beta i's are the same. So now you can also ask the question, how many parameters do we have now? Again, that's an interesting question to think about. So if you think about it for a moment, and you will see now, the param, we have far fewer parameters. Specifically we have M plus K minus one. Because we have M, beta values, and plus K minus one of our values.\n11:15\nSo let's just look basically, that's basically the main idea of ordinal logistical regression.\n11:24\nSo, now, let's see how we can use such a method to actually assign ratings. It turns out that with this, this idea of tying all the parameters, the beta values. We also end up by having a similar way to make decisions. And more specifically now, the criteria whether the predictor probabilities are at least 0.5 above, and now is equivalent to whether the score of the object is larger than or equal to negative authors of j, as shown here. Now, the scoring function is just taking the linear combination of all the features with the divided beta values.\n12:15\nSo, this means now we can simply make a decision of rating, by looking at the value of this scoring function, and see which bracket it falls into. Now you can see the general decision rule is thus, when the score is in the particular range of all of our values, then we will assign the corresponding rating to that text object.\n12:49\nSo in this approach, we're going to score the object\n12:55\nby using the features and trained parameter values.\n13:00\nThis score will then be compared with a set of trained alpha values to see which range the score is in. And then, using the range, we can then decide which rating the object should be getting. Because, these ranges of alpha values correspond to the different levels of ratings, and that's from the way we train these alpha values. Each is tied to some level of rating. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/PyTkW/lesson-5-2-feedback-in-vector-space-model-rocchiodict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.2: Feedback in Vector Space Model - Rocchio\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.2: Feedback in Vector Space Model - Rocchio\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the feedback in the vector space model.\n0:12\nIn this lecture, we continue talking about the feedback in text retrieval. Particularly, we're going to talk about feedback in the vector space model.\n0:23\nAs we have discussed before, in the case of feedback the task of text retrieval system is removed from examples in improved retrieval accuracy. We will have positive examples. Those are the documents that assume would be relevant or be charged with being relevant. All the documents that are viewed by users. We also have negative examples. Those are documents known to be non-relevant. They can also be the documents that are skipped by users.\n0:55\nThe general method in the vector space model for feedback is to modify our query vector.\n1:04\nWe want to place the query vector in a better position to make it accurate.\n1:10\nAnd what does that mean exactly? Well, if we think about the query vector that would mean we would have to do something to the vector elements. And in general, that would mean we might add new terms. Or we might just weight of old terms or assign weights to new terms.\n1:29\nAs a result, in general, the query will have more terms. We often call this query expansion.\n1:37\nThe most effective method in the vector space model for feedback is called the Rocchio Feedback, which was actually proposed several decades ago.\n1:47\nSo the idea is quite simple. We illustrate this idea by using a two dimensional display of all the documents in the collection and also the query vector. So now we can see the query vector is here in the center, and these are all the documents. So when we use the query back there and use the same narrative function to find the most similar documents, we are basically doing a circle here and that these documents would be basically the top-ranked documents. And these process are relevant documents, and these are relevant documents, for example, it's relevant, etc. And then these minuses are negative documents, like these.\n2:34\nSo our goal here is trying to move this query back to some position, to improve the retrieval accuracy. By looking at this diagram, what do you think? Where should we move the query vector so that we can improve the retrieval accuracy? Intuitively, where do you want to move query vector?\n2:58\nIf you want to think more, you can pause the video.\n3:02\nIf you think about this picture, you can realize that in order to work well in this case you want the query vector to be as close to the positive vectors as possible. That means ideally, you want to place the query vectors somewhere here. Or we want to move the query vector closer to this point.\n3:26\nNow so what exactly is this point? Well, if you want these relevant documents to rank on the top, you want this to be in the center of all these relevant documents, right? Because then if you draw a circle around this one, you'll get all these relevant documents. So that means we can move the query vector towards the centroid of all the relevant document vectors.\n3:55\nAnd this is basically the idea of Rocchio. Of course, you can consider the centroid of negative documents and we want to move away from the negative documents. Now your match that we're talking about moving vector closer to some other vec and away from other vectors. It just means that we have this formula. Here you can see this is original query vector and this average basically is the centroid vector of relevant documents. When we take the average of these vectors, then were computing the centroid of these vectors. Similarly, this is the average of non-relevant document like this. So it's essentially of non-relevant documents. And we have these three parameters here, alpha, beta, and gamma. They are controlling the amount of movement. When we add these two vectors together, we're moving the query vector closer to the centroid.\n5:03\nThis is when we add them together. When we subtracted this part, we kind of move the query vector away from that centroid. So this is the main idea of Rocchio feedback. And after we have done this, we will get a new query vector which can be used to score documents. This new query vector, will then reflect the move of this original query vector toward this relevant centroid vector and away from the non-relevant value.\n5:45\nOkay, so let's take a look at the example. This is the example that we've seen earlier. Only that I deemed that display of the actual documents. I only showed the vector representation of these documents. We have five documents here and we have\n6:04\nto read in the documents here, right. And they're displayed in red. And these are the term vectors. Now I have just assumed some of weights. A lot of terms, we have zero weights of course. Now these are negative arguments. There are two here. There is another one here. Now in this Rocchio method, we first compute the centroid of each category. And so let's see, look at the centroid vector of the positive documents, we simply just, so it's very easy to see. We just add this with this one the corresponding element. And then that's down here and take the average. And then we're going to add the corresponding elements and then just take the average. And so we do this for all this. In the end, what we have is this one. This is the average vector of these two, so it's a centroid of these two.\n7:10\nLet's also look at the centroid of the negative documents. This is basically the same. We're going to take the average of the three elements. And these are the corresponding elements in the three vectors, and so on and so forth. So in the end, we have this one.\n7:26\nNow in the Rocchio feedback method we're going to combine all these with the original query vector which is this. So now let's see how we combine them together. Well, that's basically this.\n7:38\nSo we have a parameter alpha controlling the original query times weight that's one. And now we have beta to control the inference of the positive centroid of the weight, that's 1.5. That comes from here. All right, so this goes here. And we also have this negative weight here gamma here. And this way, it has come from, of course, the negative centroid here. And we do exactly the same for other terms, each is for one term.\n8:22\nAnd this is our new vector.\n8:25\nAnd we're going to use this new query vector, this one to rank the documents. You can imagine what would happen, right? Because of the movement that this one would matches these red documents much better because we moved this vector closer to them. And it's going to penalize these black documents, these non relevent documents. So this is precisely what we wanted from feedback.\n8:50\nNow of course if we apply this method in practice we will see one potential problem\n8:58\nand that is the original query has only four terms that are now zero.\n9:06\nBut after we do query explaining and merging, we'll have many times that would have non zero weights. So the calculation will have to involve more terms.\n9:18\nIn practice, we often truncate this matter and only retain the terms with highest weights.\n9:27\nSo let's talk about how we use this method in practice.\n9:30\nI just mentioned that they're often truncated vector. Consider only a small number of words that have highest weights in the centroid vector. This is for efficiency concern.\n9:41\nI also said here that negative examples, or non-relevant examples tend not to be very useful, especially compared with positive examples.\n9:50\nNow you can think about why.\n9:55\nOne reason is because negative documents tend to distract the query in all directions. So, when you take the average, it doesn't really tell you where exactly it should be moving to. Whereas positive documents tend to be clustered together. And they will point you to a consistent direction. So that also means that sometimes we don't have to use those negative examples. But note that in some cases, in difficult queries where most results are negative, negative feedback after is very useful.\n10:27\nAnother thing is to avoid over-fitting. That means we have to keep relatively high weight on the original query terms. Why? Because the sample that we see in feedback Is a relatively small sample. We don't want to overly trust the small sample. And the original query terms are still very important. Those terms are heightened by the user and the user has decided that those terms are most important. So in order to prevent the us from over-fitting or drifting, prevent topic drifting due to the bias toward the feed backing symbols. We generally would have to keep a pretty high weight on the original terms so it was safe to do that.\n11:15\nAnd this is especially true for pseudo relevance feedback. Now, this method can be used for both relevance feedback and pseudo-relevance feedback. In the case of pseudo-feedback, the prime and the beta should be set to a smaller value because the relevant examples are assumed not to be relevant. They're not as reliable as the relevance feedback. In the case of relevance feedback, we obviously could use a larger value. So those parameters, they have to be set empirically.\n11:45\nAnd the Rocchio Method is usually robust and effective. It's still a very popular method for feedback. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/dySAJ/week-11-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 11\nWeek 11 Overview\nPrevious\nNext\nWeek 11 Information\nReading:\nReading\nWeek 11 Overview\n. Duration: 10 minutes\n10 min\nWeek 11 Lessons\nWeek 11 Activities\nWeek 11 Overview\nDuring this week's lessons, you will continue learning about various methods for text categorization, particularly discriminative qualifiers, and you will also learn sentiment analysis and opinion mining, including a detailed introduction to a particular technique for sentiment classification (i.e., ordinal regression). \nTime\nThis module should take approximately 4 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with required assignments in bold):\nActivity\nEstimated Time Required\nWeek 11 Video Lectures\n2 hours\nWeek 11 Graded Quiz\n1 hour\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain the basic ideas of Logistic Regression, K-Nearest Neighbors (k-NN), and how K-NN works.\nExplain how to evaluate categorization results.\nExplain the tasks of opinion mining and sentiment analysis and why they are important tasks from an application perspective.\nExplain how sentiment analysis can be done using text categorization techniques and why a straightforward application of regular text categorization techniques may not be adequate.\nGive examples of both simple and complex features that are used for characterizing text data and explain how NLP can enable complex features to be generated from text.\nGuiding Questions\nDevelop your answers to the following guiding questions while watching the video lectures throughout the week.\nWhat’s the general idea of the logistic regression classifier? How is it related to Naïve Bayes? Under what conditions would logistic regression cover Naïve Bayes as a special case for two-category categorization? \nWhat’s the general idea of the k-Nearest Neighbor classifier? How does it work?\nHow do we evaluate categorization results? \nHow do we compute classification accuracy, precision, recall, and F score? \nWhy is harmonic mean as used in F better than the arithmetic mean of precision and recall? \nWhat’s the difference between macro and micro averaging? \nWhy is it sometimes interesting to frame a categorization problem as a ranking problem? \nWhat is an opinion? How is it different from a factual statement? \nWhat’s an opinion holder? What’s an opinion target? \nWhat’s the goal of opinion mining? \nWhat is sentiment analysis? How is it similar to and different from a text categorization task such as topic categorization?\nWhy are unigram features generally insufficient for accurate sentiment classification? \nWhat’s the concern of using too many complex features such as frequent substructures of parse trees? \nWhat are some commonly used features to represent text data?\nAdditional Readings and Resources\nThe following readings are optional:\nC. Zhai and S. Massung, Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining. ACM and Morgan & Claypool Publishers, 2016. Chapters 15 & 18.\nYang, Yiming. An Evaluation of Statistical Approaches to Text Categorization. Inf. Retr. 1, 1-2 (May 1999), 69-90. doi: 10.1023/A:1009982220290\nBing Liu, Sentiment analysis and opinion mining. Morgan & Claypool Publishers, 2012.\nBo Pang and Lillian Lee, Opinion mining and sentiment analysis, Foundations and Trends in Information Retrieval 2(1-2), pp. 1–135, 2008.\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nGenerative classifier vs. discriminative classifier\nTraining data\nLogistic regression\nK-Nearest Neighbor classifier\nClassification accuracy, precision, recall, F measure, macro-averaging, and micro-averaging\nOpinion holder, opinion target, sentiment, and opinion representation\nSentiment classification\nFeatures, n-grams, frequent patterns, and overfitting\nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issuesforum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/d6INf/lesson-5-8-link-analysis-part-3-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 5\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\nPrevious\nNext\nWeek 5 Information\nWeek 5 Lessons\nVideo:\nVideo\nLesson 5.1: Feedback in Text Retrieval\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\nLesson 5.2: Feedback in Vector Space Model - Rocchio\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 5.3: Feedback in Text Retrieval - Feedback in LM\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\nLesson 5.4: Web Search: Introduction & Web Crawler\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\nLesson 5.5: Web Indexing\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.6: Link Analysis - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 5.7: Link Analysis - Part 2\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\n. Duration: 5 minutes\n5 min\nWeek 5 Activities\nProgramming Assignment 2.3\nLesson 5.8: Link Analysis - Part 3 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] So we talked about PageRank as a way to capture the assault.\n0:14\nNow, we also looked at some other examples where a hub might be interesting. So there is another algorithm called HITS, and that going to compute the scores for authorities and hubs. The intuitions are pages that are widely cited are good authorities and whereas pages that cite many other pages are good hubs. I think that the most interesting idea of this algorithm HITS, is it's going to use a reinforcement mechanism to kind of help improve the scoring for hubs and the authorities. And so here's the idea, it was assumed that good authorities are cited by good hubs.\n0:58\nThat means if you are cited by many pages with good hub scores then that inquiry says, you're an authority. And similarly, good hubs are those that point at good authorities. So if you pointed to a lot of good authority pages, then your hubs score would be increased. So then you will have literally reinforced each other, because you have pointed so some good hubs. And so you have pointed to some good authorities to get a good hubs score, whereas those authority scores would be also improved because they are pointing to by a good hub. And this is algorithms is also general it can have many applications in graph and network analysis. So just briefly, here's how it works. We first also construct a matrix, but this time we're going to construct an adjacent matrix and we're not going to normalize the values. So if there's a link there's a 1, if there's no link that's 0. Again, it's the same graph. And then we're going to define the hubs score of page as the sum of the authority scores of all the pages that it appoints to.\n2:08\nSo whether you are hub, really depends on whether you are pointing to a lot of good authority pages. That's what it says in the first equation. In the second equation, we define the authorities of a page as a sum of the hub scores of all those pages that appoint to you. So whether you are good authority would depend on whether those pages that are pointing to you are good hubs. So you can see this forms iterative reinforcement mechanism.\n2:38\nNow, these three questions can be also written in the metrics format. So what we get here is then the hub vector is equal to the product of the adjacency matrix and the authority vector, and this is basically the first equation. And similarly, the second equation can be returned as the authority vector is equal to the product of a transpose multiplied by the hub vector. Now, these are just different ways of expressing these equations. But what's interesting is that if you look at the matrix form, you can also plug in the authority equation into the first one. So if you do that, you have actually eliminated the authority vector completely and you get the equations of only hubs scores.\n3:34\nThe hubs score vector is equal to a multiplied by a transpose multiplied by the hub score again. Similarly, we can do a transformation to have equation for just the authorities also. So although we frame the problem as computing hubs and authorities, we can actually eliminate one of them to obtain equation just for one of them.\n3:59\nNow, the difference between this and page random is that now the matrix is actually a multiplication of the adjacency matrix and it's transpose. So this is different from page rank.\n4:11\nBut mathematically, then we will be computing the same problem. So in HITS, we typically would initialize the values. Let's say, 1 for all these values, and then we would iteratively apply these equations, essentially. And this is equivalent to multiply that by the metrics a and a transpose.\n4:34\nSo the arrows of these is exactly the same in the PageRank. But here because the adjacency matrix is not normalized. So what we have to do is after each iteration we're going to normalize, and this would allow us to control the growth of value. Otherwise they would grow larger and larger. And if we do that, and that will basically get HITS.\n4:58\nThat was the computer, the hubs scores, and authority scores for all the pages. And these scores can then be used in branching just like the PageRank scores.\n5:09\nSo to summarize in this lecture, we have seen that link information's very useful. In particular, the anchor text is very useful to increase the text representation of a page. And we also talk about the PageRank and page anchor as two major link analysis algorithms. Both can generate scores for web pages that can be used in the ranking function. Note that PageRank and the HITS are also very general algorithms. So they have many applications in analyzing other graphs or networks. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/supplement/N6Wif/week-5-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 5\nWeek 5 Overview\nPrevious\nNext\nWeek 5 Information\nReading:\nReading\nWeek 5 Overview\n. Duration: 10 minutes\n10 min\nWeek 5 Lessons\nWeek 5 Activities\nProgramming Assignment 2.3\nWeek 5 Overview\nDuring this week's lessons, you will learn feedback techniques in information retrieval, including the Rocchio feedback method for the vector space model, and a mixture model for feedback with language models. You will also learn how web search engines work, including web crawling, web indexing, and how links between web pages can be leveraged to score web pages. \nTime\nThis module should take approximately 4 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with assignments in bold):\nActivity\nEstimated Time Required\nWeek 5 Video Lectures\n2 hours\nWeek 5 Graded Quiz\n1 hour\nProgramming Assignment 2.3\n30 mins\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain the similarity and differences in the three different kinds of feedback, i.e., relevance feedback, pseudo-relevance feedback, and implicit feedback. \nExplain how the Rocchio feedback algorithm works. \nExplain how the Kullback-Leibler (KL) divergence retrieval function generalizes the query likelihood retrieval function. \nExplain the basic idea of using a mixture model for feedback. \nExplain some of the main general challenges in creating a web search engine. \nExplain what a web crawler is and what factors have to be considered when designing a web crawler.\nExplain the basic idea of Google File System (GFS).\nExplain the basic idea of MapReduce and how we can use it to build an inverted index in parallel.\nExplain how links on the web can be leveraged to improve search results.\nExplain how PageRank algorithm works.\nGuiding Questions\nDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\nWhat is relevance feedback? What is pseudo-relevance feedback? What is implicit feedback? \nHow does Rocchio work? Why do we need to ensure that the original query terms have sufficiently large weights in feedback? \nWhat is the KL-divergence retrieval function? How is it related to the query likelihood retrieval function? \nWhat is the basic idea of the two-component mixture model for feedback? \nWhat are some of the general challenges in building a web search engine? \nWhat is a crawler? How can we implement a simple crawler? \nWhat is focused crawling? What is incremental crawling? \nWhat kind of pages should have a higher priority for recrawling in incremental crawling? \nWhat can we do if the inverted index doesn’t fit in any single machine? \nWhat’s the basic idea of the Google File System (GFS)? \nHow does MapReduce work? What are the two key functions that a programmer needs to implement when programming with a MapReduce framework? \nHow can we use MapReduce to build an inverted index in parallel? \nWhat is anchor text? Why is it useful for improving search accuracy? \nWhat is a hub page? What is an authority page? \nWhat kind of web pages tend to receive high scores from PageRank? \nHow can we interpret PageRank from the perspective of a random surfer “walking” on the Web? \nHow exactly do you compute PageRank scores? \nHow does the HITS algorithm work?  \nAdditional Readings and Resources\nC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapters 7 & 10\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nRelevance feedback \nPseudo-relevance feedback \nImplicit feedback \nRocchio feedback \nKullback-Leiber divergence (KL-divergence) retrieval function \nMixture language model \nScalability and efficiency \nSpams \nCrawler, focused crawling, and incremental crawling \nGoogle File System (GFS) \nMapReduce \nLink analysis and anchor text \nPageRank\nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/irt/pReV0/project-progress-report-submission-for-gradingdict_values(['List\nCS 410: Text Information Systems\nWeek 13\nProject Progress Report Submission for Grading\nPrevious\nNext\nthis is a week for working on the project\nCourse Project\nGraded Assignment: Project Progress Report Submission for Grading\n. Duration: 3 hours\n3h\nProject Progress Report Submission for Grading\nGraded Assignment\n3 hours\n • 3h\nSubmit your assignment\nDue \nNovember 15, 12:59 AM EST\nNov 15, 12:59 AM EST\nStart assignment\nReceive feedback\nYour grade\n-\nNot available\nLike\nDislike\nReport an issue\nClose\nCoursera Honor Code\nWe’re dedicated to protecting the integrity of your work on Coursera.\nAs part of this effort, we’ve created an honor code that we ask everyone to follow. Learn more\nAll learners should:\nSubmit their own original work\nAvoid sharing answers with others\nReport suspected violations\nContinue'])
https://www.coursera.org/learn/cs-410/lecture/nkg5n/lesson-4-1-probabilistic-retrieval-model-basic-ideadict_values(["List\nCS 410: Text Information Systems\nWeek 4\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\nPrevious\nNext\nWeek 4 Information\nWeek 4 Lessons\nVideo:\nVideo\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.2: Statistical Language Model\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 4.3: Query Likelihood Retrieval Function\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.4: Statistical Language Model - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.5: Statistical Language Model - Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.6: Smoothing Methods - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.7: Smoothing Methods - Part 2\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nProgramming Assignment 2.2\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about the Probabilistic Retrieval Model. In this lecture, we're going to continue the discussion of the Text Retrieval Methods. We're going to look at another kind of very different way to design ranking functions than the Vector Space Model that we discussed before.\n0:32\nIn probabilistic models, we define the ranking function, based on the probability that this document is relevant to this query. In other words, we introduce a binary random variable here. This is the variable R here. And we also assume that the query and the documents are all observations from random variables.\n1:00\nNote that in the vector-based models, we assume they are vectors, but here we assume they are the data observed from random variables. And so, the problem of retrieval becomes to estimate the probability of relevance.\n1:19\nIn this category of models, there are different variants. The classic probabilistic model has led to the BM25 retrieval function, which we discussed in in the vectors-based model because its a form is actually similar to a backwards space model.\n1:35\nIn this lecture, we will discuss another sub class in this\n1:41\nP class called a language modeling approaches to retrieval. In particular, we're going to discuss the query likelihood retrieval model,\n1:51\nwhich is one of the most effective models in probabilistic models.\n1:57\nThere was also another line called the divergence from randomness model which has led to the PL2 function,\n2:06\nit's also one of the most effective state of the art retrieval functions. In query likelihood, our assumption is that this probability of relevance can be approximated by the probability of query given a document and relevance. So intuitively, this probability just captures the following probability. And that is if a user likes document d, how likely would the user enter query q ,in order to retrieve document d? So we assume that the user likes d, because we have a relevance value here. And then we ask the question about how likely we'll see this particular query from this user?\n2:54\nSo this is the basic idea. Now, to understand this idea, let's take a look at the general idea or the basic idea of Probabilistic Retrieval Models. So here, I listed some imagined relevance status values or relevance judgments of queries and documents. For example, in this line, it shows that q1 is a query that the user typed in. And d1 is a document that the user has seen. And 1 means the user thinks d1 is relevant to q1. So this R here can be also approximated by the click-through data that a search engine can collect by watching how you interacted with the search results. So in this case, let's say the user clicked on this document. So there's a 1 here.\n3:50\nSimilarly, the user clicked on d2 also, so there is a 1 here. In other words, d2 is assumed to be relevant to q1.\n4:00\nOn the other hand, d3 is non-relevant, there's a 0 here.\n4:07\nAnd d4 is non-relevant and then d5 is again, relevant, and so on and so forth. And this part, maybe, data collected from a different user. So this user typed in q1 and then found that the d1 is actually not useful, so d1 is actually non-relevant. In contrast, here we see it's relevant. Or this could be the same query typed in by the same user at different times. But d2 is also relevant, etc. And then here, we can see more data about other queries.\n4:48\nNow, we can imagine we have a lot of such data.\n4:52\nNow we can ask the question, how can we then estimate the probability of relevance?\n5:00\nSo how can we compute this probability of relevance? Well, intuitively that just means if we look at all the entries where we see this particular d and this particular q, how likely we'll see a one on this other column. So basically that just means that we can just collect the counts.\n5:19\nWe can first count how many times we have seen q and d as a pair in this table and then count how many times we actually have also seen 1 in the third column. And then, we just compute the ratio.\n5:39\nSo let's take a look at some specific examples. Suppose we are trying to compute this probability for d1, d2 and d3 for q1. What is the estimated probability? Now, think about that. You can pause the video if needed. Try to take a look at the table. And try to give your estimate of the probability.\n6:07\nHave you seen that, if we are interested in q1 and d1, we'll be looking at these two pairs? And in both cases, well, actually, in one of the cases, the user has said this is 1, this is relevant. So R = 1 in only one of the two cases. In the other case, it's 0. So that's one out of two. What about the d1 and the d2? Well, they are here, d1 and d2, d1 and d2, in both cases, in this case, R = 1. So it's a two out of two and so on and so forth. So you can see with this approach, we can actually score these documents for the query, right? We now have a score for d1, d2 and d3 for this query. And we can simply rank them based on these probabilities and so that's the basic idea probabilistic retrieval model. And you can see it makes a lot of sense, in this case, it's going to rank d2 above all the other documents. Because in all the cases, when you have c and q1 and d2, R = 1. The user clicked on this document. So this also should show that with a lot of click-through data, a search engine can learn a lot from the data to improve their search engine. This is a simple example that shows that with even with small amount of entries here we can already estimate some probabilities. These probabilities would give us some sense about which document might be more relevant or more useful to a user for typing this query.\n7:47\nNow, of course, the problems that we don't observe all the queries and all the documents and all the relevance values, right?\n7:55\nThere would be a lot of unseen documents, in general, we have only collected the data from the documents that we have shown to the users. And there are even more unseen queries because you cannot predict what queries will be typed in by users. So obviously, this approach won't work if we apply it to unseen queries or unseen documents.\n8:18\nNevertheless, this shows the basic idea of probabilistic retrieval model and it makes sense intuitively. So what do we do in such a case when we have a lot of unseen documents and unseen queries? Well, the solutions that we have to approximate in some way. So in this particular case called a query likelihood retrieval model, we just approximate this by another conditional probability. p(q given d, R=1). So in the condition part, we assume that the user likes the document because we have seen that the user clicked on this document.\n8:56\nAnd this part shows that we're interested in how likely the user would actually enter this query. How likely we will see this query in the same row. So note that here, we have made an interesting assumption here. Basically, we're going to do, assume that whether the user types in this query has something to do with whether user likes the document. In other words, we actually make the following assumption.\n9:22\nAnd that is a user formulates a query based on an imaginary relevant document. Where if you just look at this as conditional probability, it's not obvious we are making this assumption. So what I really meant is that to use this new conditional probability to help us score, then this new conditional probability will have to somehow be able to estimate this conditional probability without relying on this big table. Otherwise we would be having similar problems as before, and by making this assumption, we have some way to bypass this big table, and try to just model how the user formulates the query, okay? So this is how you can simplify the general model so that we can derive a specific relevant function later. So let's look at how this model work for our example. And basically, what we are going to do in this case is to ask the following question. Which of these documents is most likely the imaginary relevant document in the user's mind when the user formulates this query? So we ask this question and we quantify the probability and this probability is a conditional probability of observing this query if a particular document is in fact the imaginary relevant document in the user's mind. Here you can see we've computed all these query likelihood probabilities. The likelihood of queries given each document. Once we have these values, we can then rank these documents based on these values. So to summarize, the general idea of modern relevance in the proper risk model is to assume the we introduce a binary random variable R, here. And then, let the scoring function be defined based on this conditional probability. We also talked about approximating this by using the query likelihood.\n11:22\nAnd in this case we have a ranking function that's basically based on the probability of a query given the document. And this probability should be interpreted as the probability that a user who likes document d, would pose query q.\n11:40\nNow, the question of course is, how do we compute this conditional probability? At this in general has to do with how you compute the probability of text, because q is a text. And this has to do with a model called a Language Model. And these kind of models are proposed to model text.\n12:02\nSo more specifically, we will be very interested in the following conditional probability as is shown in this here. If the user liked this document, how likely the user would pose this query. And in the next lecture we're going to do, giving introduction to language models that we can see how we can model text that was a probable risk model, in general. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/07UZq/7-4-natural-language-content-analysis-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 7\n7.4 Natural Language Content Analysis: Part 2\nPrevious\nNext\nWeek 7 Information\nWeek 7 Lessons\nVideo:\nVideo\n7.1 Overview Text Mining and Analytics: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.2 Overview Text Mining and Analytics: Part 2\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n7.3 Natural Language Content Analysis: Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n7.4 Natural Language Content Analysis: Part 2\n. Duration: 4 minutes\n4 min\nVideo:\nVideo\n7.5 Text Representation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n7.6 Text Representation: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n7.7 Word Association Mining and Analysis\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n7.8 Paradigmatic Relation Discovery Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n7.9 Paradigmatic Relation Discovery Part 2\n. Duration: 17 minutes\n17 min\nExam 1\nWeek 7 Activities\nProgramming Assignment 3\n7.4 Natural Language Content Analysis: Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:10\nSo here are some specific examples of what we can't do today and part of speech tagging is still not easy to do  correctly. So in the example, he turned off the highway verses he turned off the fan and the two offs actually have somewhat a differentness in their active categories and also its very difficult to get a complete the parsing correct. Again, the example, a man saw a boy with a telescope can actually be very difficult to parse depending on the context. Precise deep semantic analysis is also very hard. For example, to define the meaning of own, precisely is very difficult in the sentence, like John owns a restaurant. So the state of the off can be summarized as follows. Robust and general NLP tends to be shallow while a deep understanding does not scale up.\n1:12\nFor this reason in this course, the techniques that we cover are in general, shallow techniques for analyzing text data and mining text data and they are generally based on statistical analysis. So there are robust and general and they are in\n1:36\nthe in category of shallow analysis. So such techniques have the advantage of being able to be applied to any text data in any natural about any topic. But the downside is that, they don't give use a deeper understanding of text. For that, we have to rely on deeper natural language analysis.\n2:00\nThat typically would require a human effort to annotate a lot of examples of analysis that would like to do and then computers can use machine learning techniques and learn from these training examples to do the task. So in practical applications, we generally combine the two kinds of techniques with the general statistical and methods as a backbone as the basis. These can be applied to any text data. And on top of that, we're going to use humans to, and you take more data and to use supervised machine learning to do some tasks as well as we can, especially for those important tasks to bring humans into the loop to analyze text data more precisely. But this course will cover the general statistical approaches that generally, don't require much human effort. So they're practically, more useful that some of the deeper analysis techniques that require a lot of human effort to annotate the text today. So to summarize, the main points we take are first NLP is the foundation for text mining. So obviously, the better we can understand the text data, the better we can do text mining.\n3:30\nComputers today are far from being able to understand the natural language. Deep NLP requires common sense knowledge and inferences. Thus, only working for very limited domains not feasible for large scale text mining. Shallow NLP based on statistical methods can be done in large scale and is the main topic of this course and they are generally applicable to a lot of applications. They are in some sense also, more useful techniques. In practice, we use statistical NLP as the basis and we'll have humans for help as needed in various ways. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/CMCyH/12-6-contextual-text-mining-mining-topics-with-social-network-contextdict_values(["List\nCS 410: Text Information Systems\nWeek 12\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\nPrevious\nNext\nWeek 12 Information\nWeek 12 Lessons\nVideo:\nVideo\n12.1 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 1 (OPTIONAL)\n. Duration: 15 minutes\n15 min\nVideo:\nVideo\n12.2 Opinion Mining and Sentiment Analysis: Latent Aspect Rating Analysis Part 2 (OPTIONAL)\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.3 Text-Based Prediction\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n12.4 Contextual Text Mining: Motivation\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n12.5 Contextual Text Mining: Contextual Probabilistic Latent Semantic Analysis\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n12.7 Contextual Text Mining: Mining Causal Topics with Time Series Supervision\n. Duration: 19 minutes\n19 min\nVideo:\nVideo\n12.8 Summary for Exam 2\n. Duration: 18 minutes\n18 min\nWeek 12 Activities\n12.6 Contextual Text Mining: Mining Topics with Social Network Context\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nAlbanian\nArabic\nEnglish\nFrench\nGerman\nItalian\nJapanese\nKorean\nLithuanian\nMongolian\nPortuguese (European)\nRussian\nSpanish\nThai\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about how to mine text data with social network as context. In this lecture we're going to continue discussing contextual text mining. In particular, we're going to look at the social network of others as context.\n0:26\nSo first, what's our motivation for using network context for analysis of text?\n0:32\nThe context of a text article can form a network.\n0:37\nFor example the authors of research articles might form collaboration networks.\n0:44\nBut authors of social media content might form social networks. For example, in Twitter people might follow each other. Or in Facebook as people might claim friends of others, etc. So such context connects the content of the others. Similarly, locations associated with text can also be connected to form geographical network. But in general you can can imagine the metadata of the text data can form some kind of network if they have some relations.\n1:24\nNow there is some benefit in jointly analyzing text and its social network context or network context in general. And that's because we can use network to impose some constraints on topics of text.\n1:41\nSo for example it's reasonable to assume that authors connected in collaboration networks tend to write about the similar topics.\n1:53\nSo such heuristics can be used to guide us in analyzing topics. Text also can help characterize the content associated with each subnetwork. And this is to say that both\n2:11\nkinds of data, the network and text, can help each other.\n2:16\nSo for example the difference in opinions expressed that are in two subnetworks can be reviewed by doing this type of joint analysis.\n2:30\nSo here briefly you could use a model called a network supervised topic model.\n2:40\nIn this slide we're going to give some general ideas. And then in the next slide we're going to give some more details.\n2:48\nBut in general in this part of the course we don't have enough time to cover these frontier topics in detail. But we provide references that would allow you to read more about the topic to know the details.\n3:05\nBut it should still be useful to know the general ideas. And to know what they can do to know when you might be able to use them. So the general idea of network supervised topic model is the following. Let's start with viewing the regular topic models. Like if you had an LDA as sorting optimization problem. Of course, in this case, the optimization objective function is a likelihood function. So we often use maximum likelihood estimator to obtain the parameters. And these parameters will give us useful information that we want to obtain from text data. For example, topics. So we want to maximize the probability of tests that are given the parameters generally denoted by number. The main idea of incorporating network is to think about the constraints that can be imposed based on the network. In general, the idea is to use the network to impose some constraints on the model parameters, lambda here. For example, the text at adjacent nodes of the network can be similar to cover similar topics. Indeed, in many cases, they tend to cover similar topics.\n4:34\nSo we may be able to smooth the topic distributions\n4:39\non the graph on the network so that adjacent nodes will have very similar topic distributions. So they will share a common distribution on the topics. Or have just a slight variations of the topic of distributions, of the coverage.\n5:02\nSo, technically, what we can do is simply to add a network and use the regularizers to the likelihood of objective function as shown here. So instead of just optimize the probability of test data given parameters lambda, we're going to optimize another function F.\n5:19\nThis function combines the likelihood with a regularizer function called R here. And the regularizer defines the the parameters lambda and the Network. It tells us basically what kind of parameters are preferred from a network constraint perspective. So you can easily see this is in effect implementing the idea of imposing some prior on the model parameters. Only that we're not necessary having a probabilistic model, but the idea is the same. We're going to combine the two in one single objective function.\n5:57\nSo, the advantage of this idea is that it's quite general. Here the top model can be any generative model for text.\n6:07\nIt doesn't have to be PLSA or LEA, or the current topic models.\n6:12\nAnd similarly, the network can be also in a network. Any graph that connects these text objects.\n6:22\nThis regularizer can also be any regularizer. We can be flexible in capturing different heuristics that we want to capture.\n6:32\nAnd finally, the function F can also vary, so there can be many different ways to combine them. So, this general idea is actually quite, quite powerful. It offers a general approach to combining these different types of data in single optimization framework. And this general idea can really be applied for any problem.\n6:56\nBut here in this paper reference here, a particular instantiation called a NetPLSA was started. In this case, it's just for instantiating of PLSA to incorporate this simple constraint imposed by network. And the prior here is the neighbors on the network must have similar topic distribution. They must cover similar topics in similar ways. And that's basically what it says in English.\n7:24\nSo technically we just have a modified objective function here. Let's define both the texts you can actually see in the network graph G here.\n7:34\nAnd if you look at this formula, you can actually recognize some part fairly familiarly.\n7:40\nBecause they are, they should be fairly familiar to you by now. So can you recognize which part is the likelihood for the test given the topic model?\n7:52\nWell if you look at it, you will see this part is precisely the PLSA log-likelihood that we want to maximize when we estimate parameters for PLSA alone. But the second equation shows some additional constraints on the parameters. And in particular, we'll see here it's to measure the difference between the topic coverage at node u and node v. The two adjacent nodes on the network. We want their distributions to be similar. So here we are computing the square of their differences and we want to minimize this difference. And note that there's a negative sign in front of this sum, this whole sum here. So this makes it possible to find the parameters that are both to maximize the PLSA log-likelihood. That means the parameters will fit the data well and, also to respect that this constraint from the network.\n9:06\nAnd this is the negative sign that I just mentioned. Because this is an negative sign, when we maximize this object in function we'll actually minimize this statement term here.\n9:19\nSo if we look further in this picture we'll see the results will weight of edge between u and v here. And that space from out network. If you have a weight that says well, these two nodes are strong collaborators of researchers. These two are strong connections between two people in a social network. And they would have weight. Then that means it would be more important that they're topic coverages are similar. And that's basically what it says here.\n9:55\nAnd finally you see a parameter lambda here. This is a new parameter to control the influence of network constraint. We can see easily, if lambda is set to 0, we just go back to the standard PLSA. But when lambda is set to a larger value, then we will let the network influence the estimated models more. So as you can see, the effect here is that we're going to do basically PLSA. But we're going to also try to make the topic coverages on the two nodes that are strongly connected to be similar. And we ensure their coverages are similar.\n10:33\nSo here are some of the several results, from that paper. This is slide shows the record results of using PLSA. And the data here is DBLP data, bibliographic data, about research articles. And the experiments have to do with using four communities of applications. IR information retrieval. DM stands for data mining. ML for machinery and web. There are four communities of articles, and we were hoping\n11:06\nto see that the topic mining can help us uncover these four communities. But from these assembled topics that you have seen here that are generated by PLSA. And PLSA is unable to generate the four communities that correspond to our intuition. The reason was because they are all mixed together and there are many words that are shared by these communities. So it's not that easy to use four topics to separate them. If we use more topics, perhaps we will have more coherent topics.\n11:42\nBut what's interesting is that if we use the NetPLSA where the network, the collaboration network in this case of authors is used to impose constraints. And in this case we also use four topics. But Ned Pierre said we gave much more meaningful topics. So here we'll see that these topics correspond well to the four communities. The first is information retrieval. The second is data mining. Third is machine learning. And the fourth is web. So that separation was mostly because of the influence of network where with leverage is a collaboration network information. Essentially the people that form a collaborating network would then be kind of assumed to write about similar topics. And that's why we're going to have more coherent topics. And if you just listen to text data alone based on the occurrences, you won't get such coherent topics. Even though a topic model, like PLSA or LDA also should be able to pick up co-occurring words. So in general the topics that they generate represent words that co-occur each other. But still they cannot generate such a coherent results as NetPLSA, showing that the network contest is very useful here.\n13:08\nNow a similar model could have been also useful to to characterize the content associated with each subnetwork of collaborations.\n13:19\nSo a more general view of text mining in context of network is you treat text as living in a rich information network environment. And that means we can connect all the related data together as a big network. And text data can be associated with a lot of structures in the network. For example, text data can be associated with the nodes of the network, and that's basically what we just discussed in the NetPLSA. But text data can be associated with age as well, or paths or even subnetworks. And such a way to represent texts that are in the big environment of all the context information is very powerful. Because it allows to analyze all the data, all the information together. And so in general, analysis of text should be using the entire network information that's related to the text data. So here's one suggested reading. And this is the paper about NetPLSA where you can find more details about the model and how to make such a model. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/gradedLti/NROf1/technology-review-submissiondict_values(['List\nCS 410: Text Information Systems\nWeek 11\nTechnology Review Submission\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nWeek 11 Activities\nPractice Quiz: Week 11 Practice Quiz\n11 questions\nQuiz: Week 11 Quiz\n8 questions\nGraded External Tool:\nGraded External Tool\nTechnology Review Submission\nSubmitted\nTechnology Review Submission\nOverview\nPlease review the general instructions in Week 1 Readings Orientation information: Technology Review. \nPlease follow the submission process mentioned in the pdf below:\ntech review submission.pdf\nPDF File\nDue: Nov 6, 2022 11:59 pm CST\nThis course uses a third-party tool, Technology Review Submission, to enhance your learning experience. The tool will reference basic information like your name, email, and Coursera ID.\nCoursera Honor Code  Learn more\nI, Caleb Thomas, understand that submitting work that isn’t my own may result in permanent failure of this course or deactivation of my Coursera account.\nOpen Tool\nLike\nDislike\nReport an issue\nClose\nCoursera Honor Code\nWe’re dedicated to protecting the integrity of your work on Coursera.\nAs part of this effort, we’ve created an honor code that we ask everyone to follow. Learn more\nAll learners should:\nSubmit their own original work\nAvoid sharing answers with others\nReport suspected violations\nContinue'])
https://www.coursera.org/learn/cs-410/supplement/eXpUf/welcome-to-cs-410-text-information-systemsdict_values(['List\nCS 410: Text Information Systems\nWeek 1\nWelcome to CS 410: Text Information Systems!\nPrevious\nNext\nOrientation Information\nVideo:\nVideo\nCourse Introduction Video\n. Duration: 38 minutes\n38 min\nReading:\nReading\nWelcome to CS 410: Text Information Systems!\n. Duration: 10 minutes\n10 min\nReading:\nReading\nSyllabus\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Deadlines, Late Policies, and Academic Calendar\n. Duration: 15 minutes\n15 min\nReading:\nReading\nCourse Communication\n. Duration: 15 minutes\n15 min\nReading:\nReading\nOffice Hours\n. Duration: 10 minutes\n10 min\nReading:\nReading\nProgramming Assignments Overview\n. Duration: 10 minutes\n10 min\nReading:\nReading\nTechnology Review Information\n. Duration: 10 minutes\n10 min\nReading:\nReading\nHow to Use ProctorU for exams\n. Duration: 10 minutes\n10 min\nReading:\nReading\nCourse Project Overview\n. Duration: 10 minutes\n10 min\nOrientation Activities\nProctorU Exams\nWeek 1 Information\nModule 1 Lessons\nWeek 1 Activities\nWelcome to CS 410: Text Information Systems!\nOverview\nIn this module, you will become familiar with the course, your instructor, your classmates, and our learning environment.\nTime\nThis orientation should take approximately 3 hours to complete.\nGoals and Objectives\nThe goal of the orientation module is to familiarize you with the course structure and the online learning environment. The orientation also helps you obtain the technical skills required for the course.\nAfter this module, you should be able to:\nRecall important information about this course.\nGet to know your classmates.\nBe familiar with how discussion forums operate in the course.\nInstructional Activities\nBelow is a list of the activities and assignments you must complete in this module. Click on the name of each activity for more detailed instructions.\nActivity\nEstimated Time Required\nWatch the Course Introduction Video\n25 minutes\nRead and review the Syllabus, Course Deadlines, and Course Communication pages\n45 minutes\nComplete the Orientation Quiz\n15 minutes\nComplete the course Pre-Quiz\n30 minutes\nMark as completed\nLike\nDislike\nReport an issue'])
https://www.coursera.org/learn/cs-410/home/week/4dict_values(["Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 4\nWeek 4\nAll videos completed\n10 min of readings left\nAll graded assignments completed\nIn this week's lessons, you will learn probabilistic retrieval models and statistical language models, particularly the detail of the query likelihood retrieval function with two specific smoothing methods, and how the query likelihood retrieval function is connected with the retrieval heuristics used in the vector space model.\nWeek 4 Information\nWeek 4 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 4 Lessons\nComplete\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\nVideo•\n. Duration: 12 minutes\n12 min\nLesson 4.2: Statistical Language Model\nVideo•\n. Duration: 17 minutes\n17 min\nLesson 4.3: Query Likelihood Retrieval Function\nVideo•\n. Duration: 12 minutes\n12 min\nLesson 4.4: Statistical Language Model - Part 1\nVideo•\n. Duration: 12 minutes\n12 min\nLesson 4.5: Statistical Language Model - Part 2\nVideo•\n. Duration: 9 minutes\n9 min\nLesson 4.6: Smoothing Methods - Part 1\nVideo•\n. Duration: 9 minutes\n9 min\nLesson 4.7: Smoothing Methods - Part 2\nVideo•\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nComplete\nWeek 4 Practice Quiz\nPractice Quiz•7 questions\nWeek 4 Quiz\nGraded\nQuiz•6 questions\n•Grade: \nProgramming Assignment 2.2\nComplete\nMP2.2\nDue, Sep 19, 12:59 AM EDT\nGraded\nGraded External Tool•Your grade has been overridden\n•Grade: "])
https://www.coursera.org/learn/cs-410/lecture/f82s5/9-4-probabilistic-topic-models-expectation-maximization-algorithm-part-1dict_values(['List\nCS 410: Text Information Systems\nWeek 9\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:06\nThis lecture is about the expectation-maximization algorithm, also called the EM algorithm. In this lecture, we\'re going to continue the discussion of probabilistic topic models. In particular, we\'re going to introduce the EM algorithm, which is a family of useful algorithms for computing the maximum likelihood estimate of mixture models. So this is now familiar scenario of using a two component, the mixture model, to try to factor out the background words from one topic word of distribution here. So we\'re interested in computing this estimate, and we\'re going to try to adjust these probability values to maximize the probability of the observed document. Note that we assume that all the other parameters are known. So the only thing unknown is the word probabilities given by theta sub. In this lecture, we\'re going to look into how to compute this maximum likelihood estimate. Now, let\'s start with the idea of separating the words in the text data into two groups. One group would be explained by the background model. The other group would be explained by the unknown topic word distribution. After all, this is the basic idea of mixture model. But suppose we actually know which word is from which distribution? So that would mean, for example, these words the, is, and we are known to be from this background word distribution. On the other hand, the other words text, mining, clustering etc are known to be from the topic word distribution. If you can see the color, then these are shown in blue. These blue words are then assumed that to be from the topic word distribution. If we already know how to separate these words, then the problem of estimating the word distribution would be extremely simple. If you think about this for a moment, you\'ll realize that, well, we can simply take all these words that are known to be from this word distribution theta sub d and normalize them. So indeed this problem would be very easy to solve if we had known which words are from which a distribution precisely, and this is in fact making this model no longer a mixture model because we can already observe which distribution has been used to generate which part of the data. So we actually go back to the single word distribution problem. In this case let\'s call these words that are known to be from theta d, a pseudo document of d prime, and now all we need to do is just normalize these words counts for each word w_i. That\'s fairly straightforward. It\'s just dictated by the maximum likelihood estimator. Now, this idea however doesn\'t work because we in practice don\'t really know which word is from which distribution, but this gives us the idea of perhaps we can guess which word is from which it is written. Specifically given all the parameters, can we infer the distribution a word is from. So let\'s assume that we actually know tentative probabilities for these words in theta sub d. So now all the parameters are known for this mixture model, and now let\'s consider a word like a "text". So the question is, do you think "text" is more likely having been generated from theta sub d or from theta sub of b? So in other words, we want to infer which distribution has been used to generate this text. Now, this inference process is a typical Bayesian inference situation where we have some prior about these two distributions. So can you see what is our prior here? Well, the prior here is the probability of each distribution. So the prior is given by these two probabilities. In this case, the prior is saying that each model is equally likely, but we can imagine perhaps a different prior is possible. So this is called a prior because this is our guess of which distribution has been used to generate a word before we even off reserve the word. So that\'s why we call it the prior. So if we don\'t observe the word, we don\'t know what word has been observed. Our best guess is to say well, they\'re equally likely. All right. So it\'s just flipping a coin. Now in Bayesian inference we typically learn with update our belief after we have observed the evidence. So what is the evidence here? Well, the evidence here is the word text. Now that we know we\'re interested in the word text. So text that can be regarded as evidence, and if we use Bayes rule to combine the prior and the data likelihood, what we will end up with is to combine the prior with the likelihood that you see here, which is basically the probability of the word text from each distribution. We see that in both cases the text is possible. Note that even in the background it is still possible, it just has a very small probability. So intuitively what would be your guess in this case. Now if you\'re like many others, you are guess text is probably from theta sub d. It\'s more likely from theta sub d. Why? You will probably see that it\'s because text that has a much higher probability here by the theta sub d, then by the background model which has a very small probability. By this we\'re going to say, well, text is more likely from theta sub d. So you see our guess of which distribution has been used to generate the text would depend on how high the probability of the text is in each word distribution. We can do, tend to guess the distribution that gives us a word a higher probability, and this is likely to maximize the likelihood. So we\'re going to choose a word that has a higher likelihood. So in other words, we\'re going to compare these two probabilities of the word given by each distributions. But our guess must also be affected by the prior. So we also need to compare these two priors. Why? Because imagine if we adjust these probabilities, we\'re going to say the probability of choosing a background model is almost 100 percent. Now, if you have that kind of strong prior, then that would affect your guess. You might think, well, wait a moment, maybe text could have been from the background as well. Although the probability is very small here, the prior is very high. So in the end, we have to combine the two, and the base formula provides us a solid and principled way of making this kind of guess to quantify that. So more specifically, let\'s think about the probability that this word has been generated in fact from from theta sub d. Well, in order for texts to be generated from theta sub d two things must happen. First, the theta sub d must have been selected, so we have the selection probability here. Secondly, we also have to actually have observed text from the distribution. So when we multiply the two together, we get the probability that text has in fact been generated from theta sub d. Similarly, for the background model, the probability of generating text is another product of a similar form. Now, we also introduced the latent variable z here to denote whether the word is from the background or the topic. When z is zero, it means it\'s from the topic theta sub d. When it\'s one, it means it\'s from the background theta sub b. So now we have the probability that text is generated from each. Then we can simply normalize them to have an estimate of the probability that the word text is from theta sub d or from theta sub b. Then equivalently, the probability that z is equal to zero given that the observed evidence is text. So this is application of Bayes rule. But this step is very crucial for understanding the EM algorithm because if we can do this, then we would be able to first initialize the parameter values somewhat randomly, and then we\'re going to take a guess of these z values. Which distributing has been used to generate which word, and the initialized the parameter values would allow us to have a complete specification of the mixture model which further allows us to apply Bayes rule to infer which distribution is more likely to generate each word. This prediction essentially helped us to separate the words from the two distributions. Although we can\'t separate them for sure, but we can separate them probabilistically as shown here.\nLike\nDislike\nReport an issue\nShare'])
https://www.coursera.org/learn/cs-410/supplement/GWpls/week-6-overviewdict_values(["List\nCS 410: Text Information Systems\nWeek 6\nWeek 6 Overview\nPrevious\nNext\nWeek 6 Information\nReading:\nReading\nWeek 6 Overview\n. Duration: 10 minutes\n10 min\nWeek 6 Lessons\nWeek 6 Activities\nProject Information\nProgramming Assignment 2.4\nWeek 6 Overview\nDuring this week's lessons, you will learn how machine learning can be used to combine multiple scoring factors to optimize ranking of documents in web search (i.e., learning to rank), and learn techniques used in recommender systems (also called filtering systems), including content-based recommendation/filtering and collaborative filtering. You will also have a chance to review all the text retrieval content.\nTime\nThis module should take approximately 7 hours of dedicated time to complete, with its videos and assignments.\nActivities\nThe activities for this module are listed below (with assignments in bold):\nActivity\nEstimated Time Required\nWeek 6 Video Lectures\n2 hours\nWeek 6 Graded Quiz\n1 hour\nProgramming Assignment 2.4\n4 hours\nGoals and Objectives\nAfter you actively engage in the learning experiences in this module, you should be able to:\nExplain how we can extend a retrieval system to perform content-based information filtering (recommendation).\nExplain how we can use a linear utility function to evaluate an information filtering system.\nExplain the basic idea of collaborative filtering.\nExplain how the memory-based collaborative filtering algorithm works.\nGuiding Questions\nDevelop your answers to the following guiding questions while completing the readings and working on assignments throughout the week.\nWhat is content-based information filtering? \nHow can we use a linear utility function to evaluate a filtering system? How should we set the coefficients in such a linear utility function? \nHow can we extend a retrieval system to perform content-based information filtering? \nWhat is the exploration-exploitation tradeoff? \nHow does the beta-gamma threshold learning algorithm work? \nWhat is the basic idea of collaborative filtering? \nHow does the memory-based collaborative filtering algorithm work? \nWhat is the “cold start” problem in collaborative filtering?  \nAdditional Readings and Resources\nC. Zhai and S. Massung. Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining, ACM Book Series, Morgan & Claypool Publishers, 2016. Chapters 10 - Section 10.4,Chapters 11\nKey Phrases and Concepts\nKeep your eyes open for the following key terms or phrases as you complete the readings and interact with the lectures. These topics will help you better understand the content in this module.\nContent-based filtering \nCollaborative filtering \nBeta-gamma threshold learning \nLinear utility \nUser profile \nExploration-exploitation tradeoff \nMemory-based collaborative filtering \nCold start  \nTips for Success\nTo do well this week, I recommend that you do the following:\nReview the video lectures a number of times to gain a solid understanding of the key questions and concepts introduced this week.\nWhen possible, provide tips and suggestions to your peers in this class. As a learning community, we can help each other learn and grow. One way of doing this is by helping to address the questions that your peers pose. By engaging with each other, we’ll all learn better.\nIt’s always a good idea to refer to the video lectures and chapter readings we've read during this week and reference them in your responses. When appropriate, critique the information presented.\nTake notes while you read the materials and watch the lectures for this week. By taking notes, you are interacting with the material and will find that it is easier to remember and to understand. With your notes, you’ll also find that it’s easier to complete your assignments. So, go ahead, do yourself a favor; take some notes!\nGetting and Giving Help\nYou can get/give help via the following means:\nUse the Learner Help Center to find information regarding specific technical problems. For example, technical problems would include error messages, difficulty submitting assignments, or problems with video playback. If you cannot find an answer in the documentation, you can also report your problem to the Coursera staff by clicking on the Contact Us! link available on each topic's page within the Learner Help Center.\nUse the Content Issues forum to report errors in lecture video content, assignment questions and answers, assignment grading, text and links on course pages, or the content of other course materials. University of Illinois staff and community TAs will monitor this forum and respond to issues\nMark as completed\nLike\nDislike\nReport an issue"])
https://www.coursera.org/learn/cs-410/lecture/dmpQ0/8-5-topic-mining-and-analysis-motivation-and-task-definitiondict_values(["List\nCS 410: Text Information Systems\nWeek 8\n8.5 Topic Mining and Analysis: Motivation and Task Definition\nPrevious\nNext\nWeek 8 Information\nWeek 8 Lessons\nVideo:\nVideo\n8.1 Syntagmatic Relation Discovery: Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\n8.5 Topic Mining and Analysis: Motivation and Task Definition\n. Duration: 7 minutes\n7 min\nVideo:\nVideo\n8.6 Topic Mining and Analysis: Term as Topic\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\n. Duration: 13 minutes\n13 min\nVideo:\nVideo\n8.10 Probabilistic Topic Models: Mining One Topic\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nTechnology Review (4-credit students only)\n8.5 Topic Mining and Analysis: Motivation and Task Definition\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nEstonian\nFrench\nGerman\nItalian\nKorean\nNepali\nPortuguese (European)\nRomanian\nRussian\nSlovak\nSpanish\nTamil\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] >> This lecture is about topic mining and analysis. We're going to talk about its motivation and task definition.\n0:17\nIn this lecture we're going to talk about different kind of mining task.\n0:23\nAs you see on this road map, we have just covered mining knowledge about language, namely discovery of word associations such as paradigmatic and relations and syntagmatic relations.\n0:39\nNow, starting from this lecture, we're going to talk about mining another kind of knowledge, which is content mining, and trying to discover knowledge about the main topics in the text.\n0:56\nAnd we call that topic mining and analysis.\n0:59\nIn this lecture, we're going to talk about its motivation and the task definition. So first of all, let's look at the concept of topic. So topic is something that we all understand, I think, but it's actually not that easy to formally define. Roughly speaking, topic is the main idea discussed in text data. And you can think of this as a theme or subject of a discussion or conversation. It can also have different granularities. For example, we can talk about the topic of a sentence. A topic of article, aa topic of paragraph or the topic of all the research articles in the research library, right, so different grand narratives of topics obviously have different applications.\n1:46\nIndeed, there are many applications that require discovery of topics in text, and they're analyzed then. Here are some examples. For example, we might be interested in knowing about what are Twitter users are talking about today? Are they talking about NBA sports, or are they talking about some international events, etc.? Or we are interested in knowing about research topics. For example, one might be interested in knowing what are the current research topics in data mining, and how are they different from those five years ago? Now this involves discovery of topics in data mining literatures and also we want to discover topics in today's literature and those in the past. And then we can make a comparison. We might also be also interested in knowing what do people like about some products like the iPhone 6, and what do they dislike? And this involves discovering topics in positive opinions about iPhone 6 and also negative reviews about it. Or perhaps we're interested in knowing what are the major topics debated in 2012 presidential election?\n2:59\nAnd all these have to do with discovering topics in text and analyzing them, and we're going to talk about a lot of techniques for doing this. In general we can view a topic as some knowledge about the world. So from text data we expect to discover a number of topics, and then these topics generally provide a description about the world. And it tells us something about the world. About a product, about a person etc.\n3:29\nNow when we have some non-text data, then we can have more context for analyzing the topics. For example, we might know the time associated with the text data, or locations where the text data were produced, or the authors of the text, or the sources of the text, etc. All such meta data, or context variables can be associated with the topics that we discover, and then we can use these context variables help us analyze patterns of topics. For example, looking at topics over time, we would be able to discover whether there's a trending topic, or some topics might be fading away.\n4:15\nSoon you are looking at topics in different locations. We might know some insights about people's opinions in different locations.\n4:26\nSo that's why mining topics is very important. Now, let's look at the tasks of topic mining and analysis. In general, it would involve first discovering a lot of topics, in this case, k topics. And then we also would like to know, which topics are covered in which documents, to what extent. So for example, in document one, we might see that Topic 1 is covered a lot, Topic 2 and Topic k are covered with a small portion.\n4:58\nAnd other topics, perhaps, are not covered. Document two, on the other hand, covered Topic 2 very well, but it did not cover Topic 1 at all, and it also covers Topic k to some extent, etc., right? So now you can see there are generally two different tasks, or sub-tasks, the first is to discover k topics from a collection of text laid out. What are these k topics? Okay, major topics in the text they are. The second task is to figure out which documents cover which topics to what extent. So more formally, we can define the problem as follows. First, we have, as input, a collection of N text documents. Here we can denote the text collection as C, and denote text article as d i. And, we generally also need to have as input the number of topics, k. But there may be techniques that can automatically suggest a number of topics. But in the techniques that we will discuss, which are also the most useful techniques, we often need to specify a number of topics.\n6:14\nNow the output would then be the k topics that we would like to discover, in order as theta sub one through theta sub k.\n6:24\nAlso we want to generate the coverage of topics in each document of d sub i And this is denoted by pi sub i j.\n6:33\nAnd pi sub ij is the probability of document d sub i covering topic theta sub j. So obviously for each document, we have a set of such values to indicate to what extent the document covers, each topic.\n6:48\nAnd we can assume that these probabilities sum to one. Because a document won't be able to cover other topics outside of the topics that we discussed, that we discovered. So now, the question is, how do we define theta sub i, how do we define the topic? Now this problem has not been completely defined until we define what is exactly theta.\n7:16\nSo in the next few lectures, we're going to talk about different ways to define theta. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/2Cbq9/lesson-2-4-implementation-of-tr-systemsdict_values(["List\nCS 410: Text Information Systems\nWeek 2\nLesson 2.4: Implementation of TR Systems\nPrevious\nNext\nWeek 2 Information\nWeek 2 Lessons\nVideo:\nVideo\nLesson 2.1: Vector Space Model - Improved Instantiation\n. Duration: 16 minutes\n16 min\nVideo:\nVideo\nLesson 2.2: TF Transformation\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 2.3: Doc Length Normalization\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.4: Implementation of TR Systems\n. Duration: 21 minutes\n21 min\nVideo:\nVideo\nLesson 2.5: System Implementation - Inverted Index Construction\n. Duration: 18 minutes\n18 min\nVideo:\nVideo\nLesson 2.6: System Implementation - Fast Search\n. Duration: 17 minutes\n17 min\nWeek 2 Activities\nProgramming Assignment 1\nLesson 2.4: Implementation of TR Systems\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[MUSIC] This lecture is about the implementation of text retrieval systems.\n0:12\nIn this lecture we will discuss how we can implement a text retrieval method to build a search engine. The main challenge is to manage a lot of text data and to enable a query to be answered very quickly and to respond to many queries. This is a typical text retrieval system architecture. We can see the documents are first processed by a tokenizer to get tokenized units, for example, words. And then, these words, or tokens, will be processed by a indexer that will create a index, which is a data structure for the search engine to use to quickly answer a query. And the query would be going through a similar processing step. So the Tokenizer would be apprised of the query as well, so that the text can be processed in the same way. The same units would be matched with each other. The query's representation would then be given to the Scorer, which would use the index to quickly answer user's query by scoring the documents and then ranking them. The results will be given to the user. And then the user can look at the results and provided us some feedback that can be explicit judgements of both which documents are good, which documents are bad. Or implicit feedback such as so that user didn't have to do anything extra. End user will just look at the results, and skip some, and click on some result to view. So these interacting signals can be used by the system to improve the ranking accuracy by assuming that viewed documents are better than the skipped ones. So a search engine system then can be divided into three parts. The first part is the indexer, and the second part is a Scorer that responds to the users query, and the third part is a Feedback mechanism. Now typically, the Indexer is done in the offline manner, so you can pre-process the correct data and to build the inventory index, which we will introduce in moment. And this data structure can then be used by the online module which is a scorer to process a user's query dynamically and quickly generate search results. The feedback mechanism can be done online or offline, depending on the method. The implementation of the indexer and the scorer is very standard, and this is the main topic of this lecture and the next few lectures. The feedback mechanism, on the other hand, has variations, it depends on which method is used. So that is usually done in algorithms specific way. Let's first talk about the tokenizer. Tokernization is a normalized lexical units in through the same form, so that semantically similar words can be matched with each other. Now, in the language like English, stemming is often used and this will map all the inflectional forms of words into the same root form. So for example, computer, computation, and computing can all be matched to the root form compute. This way all these different forms of computing can be matched with each other. Now normally, this is a good idea, to increase the coverage of documents that are matched up with this query. But it's also not always beneficial, because sometimes the subtlest difference between computer and computation might still suggest the difference in the coverage of the content. But in most cases, stemming seems to be beneficial. When we tokenize the text in some other languages, for example Chinese, we might face some special challenges in segmenting the text to find the word boundaries. Because it's not obvious where the boundary is as there's no space to separate them. So here of course, we have to use some language specific processing techniques. Once we do tokenization, then we would index the text documents and than it'll convert the documents and do some data structure that can enable faster search. The basic idea is to precompute as much as we can basically. So the most commonly used index is call an Inverted index. And this has been used in many search engines to support basic search algorithms. Sometimes the other indices, for example, document index might be needed in order to support feedback, like I said. And these kind of techniques are not really standard in that they vary a lot according to the feedback methods. To understand why we want to use inverted index it will be useful for you to think about how you would respond to a single term query quickly. So if you want to use more time to think about that, pause the video. So think about how you can pre process the text data so that you can quickly respond to a query with just one word. Where if you have thought about that question, you might realize that where the best is to simply create the list of documents that match every term in the vocabulary. In this way, you can basically pre-construct the answers. So when you see a term you can simply just to fetch the random list of documents for that term and return the list to the user. So that's the fastest way to respond to a single term here. Now the idea of the invert index is actually, basically, like that. We're going to do pre-constructed search an index, that will allows us to quickly find all the documents that match a particular term. So let's take a look at this example. We have three documents here, and these are the documents that you have seen in some previous lectures. Suppose that we want to create an inverted index for these documents. Then we want to maintain a dictionary, in the dictionary we will have one entry for each term and we're going to store some basic statistics about the term. For example, the number of documents that match the term, or the total number of code or frequency of the term, which means we would kind of duplicate the occurrences of the term. And so, for example, news, this term occur in all the three documents, so the count of documents is three. And you might also realize we needed this count of documents, or document frequency, for computing some statistics to be used in the vector space model. Can you think of that? So what weighting heuristic would need this count. Well, that's the idea, right, inverse document frequency. So, IDF is the property of a term, and we can compute it right here. So, with the document that count here, it's easy to compute the idea of, either at this time, or with the old index, or. At random time when we see a query. Now in addition to these basic statistics, we'll also store all the documents that matched the news, and these entries are stored in the file called Postings.\n8:24\nSo in this case it matched three documents and we store information about these three documents here. This is the document id, document 1 and the frequency is 1. The tf is one for news, in the second document it's also 1, et cetera. So from this list, we can get all the documents that match the term news and we can also know the frequency of news in these documents. So, if the query has just one word, news, and we have easily look up to this table to find the entry and go quicker into the postings to fetch all the documents that matching yours. So, let's take a look at another term.\n9:09\nThis time, let's take a look at the word presidential.\n9:14\nThis would occur in only one document, document 3. So the document frequency is 1 but it occurred twice in this document. So the frequency count is two, and the frequency count is used for some other reachable method where we might use the frequency to\n9:34\nassess the popularity of a term in the collection. Similarly we'll have a pointer to the postings here, and in this case, there is only one entry here because\n9:48\nthe term occurred in just one document and that's here. The document id is 3 and it occurred twice.\n9:59\nSo this is the basic idea of inverted index. It's actually pretty simple, right?\n10:06\nWith this structure we can easily fetch all the documents that match a term. And this will be the basis for scoring documents for a query. Now sometimes we also want to store the positions of these terms.\n10:25\nSo in many of these cases the term occurred just once in the document. So there's only one position for example in this case.\n10:35\nBut in this case, the term occurred twice so there's two positions. Now the position information is very useful for the checking whether the matching of query terms is actually within a small window of, let's say, five words or ten words.\n10:52\nOr, whether the matching of the two query terms is, in fact, a phrase of two words. That this can all be checked quickly by using the position from each.\n11:05\nSo, why is inverted index good for fast search? Well, we just talked about the possibility of using the two answer single-term query. And that's very easy. What about the multiple term queries? Well let's first look at the some special cases of the Boolean query. A Boolean query is basically a Boolean expression like this. So I want the value in the document to match both term A and term B. So that's one conjunctive query. Or I want the web documents to match term A or term B. That's a disjunctive query. But how can we answer such a query by using inverted index?\n11:52\nWell if you think a bit about it, it would be obvious because we have simply fetch all the documents that match term A and also fetch all the documents that match term B. And then just take the intersection to answer a query like A and B. Or to take the union to answer the query A or B. So this is all very easy to answer. It's going to be very quick. Now what about the multi-term keyword query? We talked about the vector space model for example and we will do a match such query with document and generate the score. And the score is based on aggregated term weights. So in this case it's not the Boolean query but the scoring can be actually done in similar way. Basically it's similar to disjunctive Boolean query. Basically, it's like A or B. We take the union of all the documents that match at least one query term and then we would aggregate the term weights. So this is a basic idea of using inverted index for scoring documents in general. And we're going to talk about this in more detail later. But for now, let's just look at the question why is in both index, a good idea? Basically why is more efficient than sequentially just scanning documents. This is the obvious approach. You can just compute a score for each document and then you can then sort them. And this is a straightforward method but this is going to be very slow imagine the wealth, there's a lot of documents. If you do this then it will take a long time to answer your query. So the question now is why would the invert index be much faster? Well it has to do is the word distribution in text. So, here's some common phenomena of word distribution in the text. There are some languages independent of patterns that seem to be stable.\n14:00\nAnd these patterns are basically characterized by the following pattern. A few words like the common words like the, a, or we occur very, very frequently in text. So they account for a large percent of occurrences of words.\n14:19\nBut most words would occur just rarely. There are many words that occur just once, let's say, in a document or once in the collection. And there are many such. It's also true that the most frequent the words in one corpus they have to be rare in another. That means although the general phenomenon is applicable, was observed in many cases that exact words that are common may vary from context to context. So this phenomena is characterized by what's called a Zipf's Law. This law says that the rank of a word multiplied by the frequency of the word is roughly constant.\n15:07\nSo formally if we use F(w) to denote the frequency, r(w) to denote the rank of a word. Then this is the formula. It basically says the same thing, just mathematical term. Where C is basically a constant and so, and there is also a parameter, alpha, that might be adjusted to better fit any empirical observations. So if I plot the word frequencies in sorted order, then you can see this more easily. The x axis is basically the word rank. This is r(w) and the y axis is word frequency or F(w). Now this curve shows that the product of the two is roughly the constant. Now if you look at these words, we can see They can be separated into three groups. In the middle, it's the intermediary frequency words. These words tend to occur quite in a few documents, but they are not like those most frequent words. And they are also not very rare.\n16:18\nSo they tend to be often used in\n16:22\nqueries and they also tend to have high TF-IDF weights. These intermediate frequency words. But if you look at the left part of the curve,\n16:35\nthese are the highest frequency words. They are covered very frequently. They are usually words, like the, we, of Etc. Those words are very, very frequent and they are in fact the two frequent to be discriminated, and they are generally not very useful for retrieval. So they are often removed and this is called the stop words removal. So you can use pretty much just the kind of words in the collection to kind of infer what words might be stop words. Those are basically the highest frequency words.\n17:13\nAnd they also occupy a lot of space in the inverted index. You can imagine the posting entries for such a word would be very long. And then therefore, if you can remove such words you can save a lot of space in the inverted index.\n17:29\nWe also show the tail part, which has a lot of rare words. Those words don't occur very frequently, and there are many such words.\n17:39\nThose words are actually very useful for search also, if a user happens to be interested in such a topic. But because they're rare, it's often true that users aren't necessarily interested in those words. But retain them would allow us to match such a document accurately. They generally have very high IDF.\n18:05\nSo what kind of data structures should we use to store inverted index? Well, it has two parts, right. If you recall, we have a dictionary and we also have postings. The dictionary has modest size, although for the web it's still going to be very large but compare it with postings it's more distinct.\n18:26\nAnd we also need to have fast random access to the entries because we're going to look up on the query term very quickly. So therefore, we'd prefer to keep such a dictionary in memory if it's possible. If the collection is not very large, this is feasible, but if the collection is very large then it's in general not possible. If the vocabulary size is very large, obviously we can't do that. So, in general that's how it goes. So the data structures that we often use for storing dictionary, it would be direct access. There are structures like hash table, or b-tree if we can't store everything in memory or use disk. And then try to build a structure that would allow it to quickly look up entries.\n19:14\nFor postings they are huge.\n19:18\nAnd in general, we don't have to have direct access to a specific entry. We generally would just look up a sequence of document IDs and frequencies for all the documents that matches the query term.\n19:33\nSo would read those entries sequentially.\n19:37\nAnd therefore because it's large and we generally have store postings on disc, they have to stay on disc and they would contain information such as document IDs, term frequency or term positions, etcetera. Now because they are very large, compression is often desirable.\n19:59\nNow this is not only to save disc space, and this is of course one benefit of compression, it It's not going to occupy that much space. But it's also to help improving speed.\n20:13\nCan you see why? Well, we know that input and output would cost a lot of time. In comparison with the time taken by CPU. So, CPU is much faster but IO takes time and so by compressing the inverter index, opposing files will become smaller, and the entries, that we have the readings, and memory to process a query term, would be smaller, and then, so we can reduce the amount of tracking IO and that can save a lot of time. Of course, we have to then do more processing of the data when we uncompress the data in the memory. But as I said CPU is fast. So over all we can still save time.\n21:08\nSo compression here is both to save disc space and to speed up the loading of the index. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/hI1vE/lesson-4-5-statistical-language-model-part-2dict_values(["List\nCS 410: Text Information Systems\nWeek 4\nLesson 4.5: Statistical Language Model - Part 2\nPrevious\nNext\nWeek 4 Information\nWeek 4 Lessons\nVideo:\nVideo\nLesson 4.1: Probabilistic Retrieval Model - Basic Idea\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.2: Statistical Language Model\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\nLesson 4.3: Query Likelihood Retrieval Function\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.4: Statistical Language Model - Part 1\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\nLesson 4.5: Statistical Language Model - Part 2\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.6: Smoothing Methods - Part 1\n. Duration: 9 minutes\n9 min\nVideo:\nVideo\nLesson 4.7: Smoothing Methods - Part 2\n. Duration: 13 minutes\n13 min\nWeek 4 Activities\nProgramming Assignment 2.2\nLesson 4.5: Statistical Language Model - Part 2\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND]\n0:12\nSo I showed you how we rewrite the query like holder which is a function into a form that looks like the formula of this slide after if we make the assumption about the smoothing, the language model based on the collection language model. Now if you look at this rewriting, it will actually give us two benefits. The first benefit is it helps us better understand this ranking function. In particular, we're going to show that from this formula we can see smoothing with the collection language model would give us something like a TF-IDF weighting and length normalization. The second benefit is that it also allows us to compute the query like holder more efficiently. In particular we see that the main part of the formula is a sum over the match of the query terms.\n1:09\nSo this is much better than if we take a sum over all the words. After we smooth the document the damage model we essentially have non zero problem for all the words. So this new form of the formula is much easier to score or to compute.\n1:27\nIt's also interesting to note that the last term here is actually independent of the document. Since our goal is to rank the documents for the same query we can ignore this term for ranking. Because it's going to be the same for all the documents. Ignoring it wouldn't affect the order of the documents.\n1:49\nInside the sum, we\n1:52\nalso see that each matched query term would contribute a weight.\n1:58\nAnd this weight actually is very interesting because it looks like a TF-IDF weighting. First we can already see it has a frequency of the word in a query just like in the vector space model. When we take a thought product, we see the word frequency in the query to show up in such a sum.\n2:22\nAnd so naturally this part would correspond between the vector element from the documented vector. And here indeed we can see it actually\n2:35\nencodes a weight that has similar in factor to TF-IDF weight.\n2:41\nI'll let you examine it, can you see it? Can you see which part is capturing TF? And which part is a capturing IDF weighting?\n2:51\nSo if want you can pause the video to think more about it.\n2:55\nSo have you noticed that this P sub seen is related to the term frequency in the sense that if a word occurs very frequently in the document, then the s made through probability here will tend to be larger. So this means this term is really doing something like a TF weight. Now have you also noticed that this term in the denominator is actually achieving the factor of IDF? Why, because this is the popularity of the term in a collection.\n3:31\nBut it's in the denominator, so if the probability in the collection is larger then the weight is actually smaller. And this means a popular term. We actually have a smaller weight and this is precisely what IDF weighting is doing.\n3:47\nOnly that we now have a different form of TF and IDF.\n3:51\nRemember IDF has a logarithm of documented frequency. But here we have something different.\n3:58\nBut intuitively it achieves a similar effect. Interestingly, we also have something related to the length of libation.\n4:07\nAgain, can you see which factor is related to the document length in this formula?\n4:14\nWhat I just say is that this term is related to IDF weighting.\n4:19\nThis collection probability, but it turns out that this term here is actually related to document length normalization. In particular, F of sub d might be related to document length. So it encodes how much probability mass we want to give to unseen worlds.\n4:41\nHow much smoothing do we want to do? Intuitively, if a document is long, then we need to do less smoothing because we can assume that data is large enough. We probably have observed all the words that the author could have written. But if the document is short then r of sub t could be expected to be large. We need to do more smoothing. It's likey there are words that have not been written yet by the author. So this term appears to paralyze the non document in that other sub D would tend to be longer than or larger than for a long document. But note that alpha sub d also occurs here and so this may not actually be necessary paralyzing long documents. The effect is not so clear yet.\n5:31\nBut as we will see later, when we consider some specific smoothing methods, it turns out that they do paralyze long documents. Just like in TF-IDF weighting and document length normalization formula in the vector space model.\n5:47\nSo, that's a very interesting observation because it means we don't even have to think about the specific way of doing smoothing. We just need to assume that if we smooth with this collection memory model, then we would have a formula that looks like TF-IDF weighting and documents length violation.\n6:08\nWhat's also interesting that we have very fixed form of the ranking function.\n6:14\nAnd see we have not heuristically put a logarithm here.\n6:19\nIn fact, you can think about why we would have a logarithm here. You look at the assumptions that we have made, it would be clear it's because we have used a logarithm of query like for scoring. And we turned the product into a sum of logarithm of probability, and that's why we have this logarithm.\n6:40\nNote that if only want to heuristically implement a TF weighting and IDF weighting, we don't necessary have to have a logarithm here. Imagine if we drop this logarithm, we would still have TF and IDF weighting.\n6:55\nBut what's nice with problem risk modeling is that we are automatically given the logarithm function here. And that's basically a fixed form of the formula that we did not really have to heuristically design, and in this case if you try to drop the logarithm the model probably won't work as well as if you keep the logarithm.\n7:19\nSo a nice property of problem risk modeling is that by following some assumptions and the probability rules we'll get a formula automatically. And the formula would have a particular form like in this case.\n7:34\nAnd if we heuristically design the formula we may not necessarily end up having such a specific formula.\n7:41\nSo to summarize, we talked about the need for smoothing the document imaging model. Otherwise it would give zero probability for unseen words in the document, and that's not good for storing a query with such an unseen word.\n7:59\nIt's also necessary, in general, to improve the accuracy of estimating the model represent the topic of this document. The general idea of smoothing in retrieval is to use the connecting memory model to,\n8:17\nto give us some clue about which unseen words should have a higher probability. That is, the probability of an unseen word is assumed to be proportional to its probability in the collection.\n8:29\nWith this assumption, we've shown that we can derive a general ranking formula for query likelihood that has effect of TF-IDF weighting and document length normalization. We also see that, through some rewriting, the scoring of such a ranking function is primarily based on sum of weights on matched query terms, just like in the vector space model. But, the actual ranking function is given us automatically by the probability rules and assumptions that we have made. And like in the vector space model where we have to heuristically think about the form of the function. However, we still need to address the question how exactly we should smooth the document and the model. How exactly we should use the reference and model based on the connection to adjust the probability of the maximum micro is made of and this is the topic of the next batch. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/1dict_values(["Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 1\nWeek 1\nAll videos completed\n2h 15m of readings left\nAll graded assignments completed\nDuring this week's lessons, you will learn of natural language processing techniques, which are the foundation for all kinds of text-processing applications, the concept of a retrieval model, and the basic idea of the vector space model.\nOrientation Information\nCourse Introduction Video\nVideo•\n. Duration: 38 minutes\n38 min\nWelcome to CS 410: Text Information Systems!\nReading•\n. Duration: 10 minutes\n10 min\nSyllabus\nReading•\n. Duration: 15 minutes\n15 min\nCourse Deadlines, Late Policies, and Academic Calendar\nReading•\n. Duration: 15 minutes\n15 min\nCourse Communication\nReading•\n. Duration: 15 minutes\n15 min\nOffice Hours\nReading•\n. Duration: 10 minutes\n10 min\nProgramming Assignments Overview\nReading•\n. Duration: 10 minutes\n10 min\nTechnology Review Information\nReading•\n. Duration: 10 minutes\n10 min\nHow to Use ProctorU for exams\nReading•\n. Duration: 10 minutes\n10 min\nCourse Project Overview\nReading•\n. Duration: 10 minutes\n10 min\nOrientation Activities\nPre-Quiz\nPractice Quiz•13 questions\nOrientation Quiz\nGraded\nQuiz•5 questions\n•Grade: \nProctorU Exams\nProctorU Exams\nReading•\n. Duration: 10 minutes\n10 min\nHow to Schedule and Take the ProctorU Exams\nReading•\n. Duration: 10 minutes\n10 min\nProctorU Readiness Quiz\nPractice Quiz•6 questions\nWeek 1 Information\nWeek 1 Overview\nReading•\n. Duration: 10 minutes\n10 min\nModule 1 Lessons\nComplete\nWeek 1 Activities\nComplete"])
https://www.coursera.org/learn/cs-410/lecture/HKe8K/9-7-probabilistic-latent-semantic-analysis-plsa-part-1dict_values(["List\nCS 410: Text Information Systems\nWeek 9\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\nPrevious\nNext\nWeek 9 Information\nWeek 9 Lessons\nVideo:\nVideo\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\n. Duration: 12 minutes\n12 min\nVideo:\nVideo\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\n. Duration: 8 minutes\n8 min\nVideo:\nVideo\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\n. Duration: 6 minutes\n6 min\nVideo:\nVideo\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.9 Latent Dirichlet Allocation (LDA): Part 1\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n9.10 Latent Dirichlet Allocation (LDA): Part 2\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:00\n[SOUND] This lecture is about probabilistic and latent Semantic Analysis or PLSA.\n0:12\nIn this lecture we're going to introduce probabilistic latent semantic analysis, often called PLSA. This is the most basic topic model, also one of the most useful topic models. Now this kind of models can in general be used to mine multiple topics from text documents. And PRSA is one of the most basic topic models for doing this. So let's first examine this power in the e-mail for more detail. Here I show a sample article which is a blog article about Hurricane Katrina.\n0:48\nAnd I show some simple topics. For example government response, flood of the city of New Orleans. Donation and the background.\n0:59\nYou can see in the article we use words from all these distributions.\n1:05\nSo we first for example see there's a criticism of government response and this is followed by discussion of flooding of the city and donation et cetera. We also see background words mixed with them.\n1:18\nSo the overall of topic analysis here is to try to decode these topics behind the text, to segment the topics, to figure out which words are from which distribution and to figure out first, what are these topics? How do we know there's a topic about government response. There's a topic about a flood in the city. So these are the tasks at the top of the model.\n1:42\nIf we had discovered these topics can color these words, as you see here, to separate the different topics. Then you can do a lot of things, such as summarization, or segmentation, of the topics, clustering of the sentences etc. So the formal definition of problem of mining multiple topics from text is shown here. And this is after a slide that you have seen in an earlier lecture. So the input is a collection, the number of topics, and a vocabulary set, and of course the text data.\n2:16\nAnd then the output is of two kinds. One is the topic category, characterization. Theta i's. Each theta i is a word distribution. And second, it's the topic coverage for each document. These are pi sub i j's. And they tell us which document it covers. Which topic to what extent. So we hope to generate these as output. Because there are many useful applications if we can do that.\n2:42\nSo the idea of PLSA is actually very similar to the two component mixture model that we have already introduced. The only difference is that we are going to have more than two topics. Otherwise, it is essentially the same. So here I illustrate how we can generate the text that has multiple topics and naturally in all cases of Probabilistic modelling would want to figure out the likelihood function. So we would also ask the question, what's the probability of observing a word from such a mixture model? Now if you look at this picture and compare this with the picture that we have seen earlier, you will see the only difference is that we have added more topics here.\n3:26\nSo, before we have just one topic, besides the background topic. But now we have more topics. Specifically, we have k topics now. All these are topics that we assume that exist in the text data. So the consequence is that our switch for choosing a topic is now a multiway switch. Before it's just a two way switch. We can think of it as flipping a coin. But now we have multiple ways. First we can flip a coin to decide whether we're talk about the background. So it's the background lambda sub B versus non-background. 1 minus lambda sub B gives us the probability of actually choosing a non-background topic. After we have made this decision, we have to make another decision to choose one of these K distributions. So there are K way switch here. And this is characterized by pi, and this sum to one.\n4:31\nThis is just the difference of designs. Which is a little bit more complicated. But once we decide which distribution to use the rest is the same we are going to just generate a word by using one of these distributions as shown here.\n4:46\nSo now lets look at the question about the likelihood. So what's the probability of observing a word from such a distribution? What do you think? Now we've seen this problem many times now and if you can recall, it's generally a sum. Of all the different possibilities of generating a word. So let's first look at how the word can be generated from the background mode. Well, the probability that the word is generated from the background model is lambda multiplied by the probability of the word from the background mode. Model, right. Two things must happen. First, we have to have chosen the background model, and that's the probability of lambda, of sub b. Then second, we must have actually obtained the word w from the background, and that's probability of w given theta sub b.\n5:40\nOkay, so similarly, we can figure out the probability of observing the word from another topic. Like the topic theta sub k. Now notice that here's the product of three terms. And that's because of the choice of topic theta sub k, only happens if two things happen. One is we decide not to talk about background. So, that's a probability of 1 minus lambda sub B. Second, we also have to actually choose theta sub K among these K topics. So that's probability of theta sub K, or pi.\n6:17\nAnd similarly, the probability of generating a word from the second. The topic and the first topic are like what you are seeing here. And so in the end the probability of observing the word is just a sum of all these cases. And I have to stress again this is a very important formula to know because this is really key to understanding all the topic models and indeed a lot of mixture models. So make sure that you really understand the probability\n6:49\nof w is indeed the sum of these terms.\n6:56\nSo, next, once we have the likelihood function, we would be interested in knowing the parameters. All right, so to estimate the parameters. But firstly, let's put all these together to have the complete likelihood of function for PLSA. The first line shows the probability of a word as illustrated on the previous slide. And this is an important formula as I said.\n7:22\nSo let's take a closer look at this. This actually commands all the important parameters. So first of all we see lambda sub b here. This represents a percentage of background words\n7:32\nthat we believe exist in the text data. And this can be a known value that we set empirically.\n7:41\nSecond, we see the background language model, and typically we also assume this is known. We can use a large collection of text, or use all the text that we have available to estimate the world of distribution.\n7:52\nNow next in the next stop this formula. [COUGH] Excuse me. You see two interesting kind of parameters, those are the most important parameters. That we are. So one is pi's. And these are the coverage of a topic in the document.\n8:11\nAnd the other is word distributions that characterize all the topics.\n8:18\nSo the next line, then is simply to plug this in to calculate the probability of document. This is, again, of the familiar form where you have a sum and you have a count of a word in the document. And then log of a probability. Now it's a little bit more complicated than the two component. Because now we have more components, so the sum involves more terms. And then this line is just the likelihood for the whole collection. And it's very similar, just accounting for more documents in the collection.\n8:52\nSo what are the unknown parameters? I already said that there are two kinds. One is coverage, one is word distributions. Again, it's a useful exercise for you to think about. Exactly how many parameters there are here.\n9:05\nHow many unknown parameters are there? Now, try and think out that question will help you understand the model in more detail. And will also allow you to understand what would be the output that we generate when use PLSA to analyze text data? And these are precisely the unknown parameters.\n9:24\nSo after we have obtained the likelihood function shown here, the next is to worry about the parameter estimation.\n9:32\nAnd we can do the usual think, maximum likelihood estimator. So again, it's a constrained optimization problem, like what we have seen before. Only that we have a collection of text and we have more parameters to estimate. And we still have two constraints, two kinds of constraints. One is the word distributions.\n9:51\nAll the words must have probabilities that's sum to one for one distribution. The other is the topic coverage distribution and a document will have to cover precisely these k topics so the probability of covering each topic that would have to sum to 1. So at this point though it's basically a well defined applied math problem, you just need to figure out the solutions to optimization problem. There's a function with many variables. and we need to just figure out the patterns of these variables to make the function reach its maximum. >> [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/lecture/L7dem/11-2-text-categorization-discriminative-classifier-part-2-optionaldict_values(["List\nCS 410: Text Information Systems\nWeek 11\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\nPrevious\nNext\nWeek 11 Information\nWeek 11 Lessons\nVideo:\nVideo\n11.1 Text Categorization: Discriminative Classifier Part 1\n. Duration: 20 minutes\n20 min\nVideo:\nVideo\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\n. Duration: 31 minutes\n31 min\nVideo:\nVideo\n11.3 Text Categorization: Evaluation Part 1\n. Duration: 14 minutes\n14 min\nVideo:\nVideo\n11.4 Text Categorization: Evaluation Part 2\n. Duration: 10 minutes\n10 min\nVideo:\nVideo\n11.5 Opinion Mining and Sentiment Analysis: Motivation\n. Duration: 17 minutes\n17 min\nVideo:\nVideo\n11.6 Opinion Mining and Sentiment Analysis: Sentiment Classification\n. Duration: 11 minutes\n11 min\nVideo:\nVideo\n11.7 Opinion Mining and Sentiment Analysis: Ordinal Logistic Regression (OPTIONAL)\n. Duration: 13 minutes\n13 min\nWeek 11 Activities\n11.2 Text Categorization: Discriminative Classifier Part 2 (OPTIONAL)\nNotes\nPlay Video\nSave note\nDownload\nTranscript\nSelect a language\nArabic\nEnglish\nFrench\nGerman\nItalian\nKorean\nPortuguese (European)\nRussian\nSpanish\nVietnamese\nHelp Us Translate\nYou may navigate through the transcript using tab. To save a note for a section of text press CTRL + S. To expand your selection you may use CTRL + arrow key. You may contract your selection using shift + CTRL + arrow key. For screen readers that are incompatible with using arrow keys for shortcuts, you can replace them with the H J K L keys. Some screen readers may require using CTRL in conjunction with the alt key\n0:07\n[SOUND] This lecture is a continued discussion of Discriminative Classifiers for Text Categorization. So, in this lecture, we're going to introduce, yet another Discriminative Classifier called the Support Vector Machine or SVM. Which is a very popular classification method and it has been also shown to be effective for text categorization.\n0:31\nSo to introduce this classifier, let's also think about the simple case of two categories. We have two topic categories, 01 and 02 here. And we want to classify documents into these two categories and we're going to represent again a document by a feature factor x here.\n0:53\nNow, the idea of this classifier is to design also a linear separator\n0:59\nhere that you'll see and it's very similar to what you have seen not just for regression, right? And we're going to do also say that if the sign of this function value is positive then we're going to say the objective is in category one. Otherwise, we're going to say it's in category 2. So that makes 0 that is the decision boundary between the few categories.\n1:28\nSo, in generally hiding marginal space such as, 0. corresponds to a hyper plain.\n1:38\nNow I've shown you a simple case of two dimensional space it was just X1 and X2 and this case this corresponds to a line that you can see here.\n1:51\nSo, this is a line defined by just three parameters here, beta zero, beta one, and beta two.\n2:02\nNow, this line is heading in this direction so it shows that as we increase X1, X2 will also increase. So we know that beta one and beta two have different assigns, one is negative and the other is positive.\n2:20\nSo let's just assume that beta one is negative and beta two Is positive.\n2:28\nNow, it's interesting to examine, then, the data instances on the two sides of the slide. So, here, the data instance are visualized as circles for one class and diamonds for the other class.\n2:43\nNow, one question is to take a point like this one and to ask the question what's the value of this expression, or this classifier, for this data point?\n2:55\nSo what do you think? Basically, we're going to evaluate its value by using this function.\n3:01\nAnd as we said, if this value's positive we're going to say this is in category one, and if it's negative, it's going to be in category two. Intuitively, this line separates these two categories, so we expect the points on one side would be positive and the points on the other side would be negative. Our question is under the assumption that I just mentioned, let's examine a particular point like this one.\n3:27\nSo what do you think is the sine of this expression?\n3:31\nWell, to examine the sine we can simply look at this expression here. And we can compare this with let's say,\n3:42\nvalue on the line, let's see, compare this with this point.\n3:48\nWhile they have identical X1, but then one has a higher value for X2.\n3:54\nNow, let's look at the sin of the coefficient for X2. Well, we know this is a positive.\n4:02\nSo, what that means is that the f value for this point should be higher than the f value for this point on the line that means this will be positive, right?\n4:16\nSo we know in general of all points on this side,\n4:20\nthe function's value will be positive and you can also verify all the points on this side will be negative. And so this is how this kind of linear classifier or linear separator can then separate the points in the two categories.\n4:37\nSo, now the natural question is, which linear separator is the best? Now, I've get you one line here that can separate the two classes. And this line, of course, is determined by the vector beta, the coefficients. Different coefficients will give us different lines. So, we could imagine there are other lines that can do the same job. Gamma, for example, could give us another line that counts a separator to these instances.\n5:06\nOf course, there are also lines that won't separate to them and those are bad lines. But, the question is, when we have multiple lines that can separate both clauses, which align the best? In fact, you can imagine, there are many different ways of choosing the line. So, the logistical regression classifier that you have seen earlier actually uses some criteria to determine where this line should be and so linear separate as well. And uses a conditional likelihood on the training that it determines which line is the best. But in SVM we're going to look at another criteria for determining which line is the best. And this time, the criteria is more tied to the classification arrow as you will see.\n5:49\nSo, the basic idea is to choose the separator to maximize the margin. So what is a margin? So, I choose some dotted lines here to indicate the boundaries of those data points in each class. And the margin is simply the distance between the line, the separator, and the closest point from each class.\n6:18\nSo you can see the margin of this side is as I've shown here and you can also define the margin on the other side.\n6:27\nIn order for the separator to maximize the margin, it has to be kind of in the middle of the two boundaries and you don't want this separator to be very close to one side, and that in intuition makes a lot of sense.\n6:44\nSo this is basic idea of SVM. We're going to choose a linear separator to maximize the margin.\n6:52\nNow on this slide, I've also changed the notation so that I'm not going to use beta to denote the parameters. But instead, I'm going to use w although w was used to denote the words before so don't be confused here. W here is actually a width, a certain width.\n7:12\nSo I'm also using lowercase b to denote the beta 0, a biased constant.\n7:20\nAnd there are instances do represent that as x and I also use the vector form of multiplication here. So we see a transpose of w vector multiply by the future vector.\n7:35\nSo b is a bias constant and w is a set of weights with one way for each feature. We have m features and so we have m weights and that will represent as a vector.\n7:47\nAnd similarly, the data instance here, the text object, is represented by also a feature vector of the same number of elements. Xi is a feature value. For example, word count and you can verify, when we. Multiply these two vectors together, take the dot product, we get the same form of the linear separator as you have seen before. It's just a different way of representing this. Now I use this way so that it's more consistent with what notations people usually use when they talk about SVM. This way you can better connect the slides with some other readings you might do.\n8:31\nOkay, so when we maximize the margins of a separator, it just means the boundary of the separator is only determined by a few data points, and these are the data points that we call support vectors. So here illustrated are two support vectors for one class and two for the other class. And these quotas define the margin basically, and you can imagine once we know which are supportive vectors then this\n9:06\ncenter separator line will be determined by them. So the other data points actually don't really matter that much. And you can see if you change the other data points it won't really affect the margin, so the separator will stay the same. Mainly affected by the the support vector machines. Sorry, it's mainly affected by the support vectors and that's why it's called a support vector machine. Okay, so now the next question is, of course, how can we set it up to optimize the line? How can we actually find the line or the separator? Now this is equivalent to finding values for w and b, because they will determine where exactly the separator is.\n9:58\nSo in the simplest case, the linear SVM is just a simple optimization problem. So again, let's recall that our classifier is such a linear separator, where we have weights for all the features, and the main goal is remove these weights w and b. And the classifier will say X is in category theta 1 if it's positive. Otherwise, it's going to say it's in the other category. So this is our assumption, our setup. So in the linear SVM, we are going to then seek these parameter values to optimize the margins and then the training error.\n10:38\nThe training data would be basically like in other classifiers. We have a set of training points where we know the x vector, and then we also know the corresponding label, y i. And here we define y i as two values, but these values are not 0, 1 as you have seen before, but rather -1 and positive 1, and they're corresponding to these two categories, as I've shown here. Now you might wonder why we don't define them as 0 and 1 instead of having -1, 1. And this is purely for mathematical convenience, as you will see in a moment.\n11:16\nSo the goal of optimization first is to make sure the labeling of training data is all correct. So that just means if y i, the norm label for instance x i, is 1, we would like this classified value to be large. And here we just choose a threshold of 1 here. But if you use another threshold, you can easily fit that constant into the parameter values b and w to make the right-hand side just 1.\n11:48\nNow if, on the other hand, y i is -1, that means it's in a different class, then we want this classifier to give us a very small value, in fact a negative value, and we want this value to be less than or equal to -1. Now these are the two different instances, different kinds of cases. How can we combine them together? Now this is where it's convenient when we have chosen y i as -1 for the other category, because it turns out that we can either combine the two into one constraint.\n12:26\ny i multiplied by the classifier value must be larger than or equal to 1.\n12:33\nAnd obviously when y i is just 1, you see this is the same as the constraint on the left-hand side. But when y i is -1, you also see that this is equivalent to the other inequality. So this one actually captures both constraints in a unified way, and that's a convenient way of capturing these constraints. What's our second goal? Well, that's to maximize margin, so we want to ensure that separator can do well on the training data. But then, among all the cases where we can separate the data, we also would like to choose the separator that has the largest margin. Now the margin can be assumed to be related to the magnitude of the weight. And so w transform multiplied by w would give us basically the sum of squares of all those weights. So to have a small value for this expression, it means all the w i's must be small.\n13:42\nSo we've just assumed that we have a constraint for\n13:46\ngetting the data on the training set to be classified correctly. Now we also have the objective that's tied into a maximization of margin, and this is simply to minimize w transpose multiplied by w, and we often denote this by phi of w. So now you can see this is basically a optimization problem. We have some variables to optimize, and these are the weights and b and we have some constraints. These are linear constraints and the objective function is a quadratic function of the weights. So this a quadratic program with linear constraints, and there are standard algorithm that are variable for solving this problem. And once we solve the problem we obtain the weights w and b. And then this would give us a well-defined classifier. So we can then use this classifier to classify any new text objects. Now the previous formulation did not allow any error in the classification, but sometimes the data may not be linear to the separator. That means that they may not look as nice as you have seen on the previous slide where a line can separate all of them. And what would happen if we allowed some errors? Well, the principle can stay. We want to minimize the training error but try to also maximize the margin. But in this case we have a soft margin, because the data points may not be completely separable.\n15:17\nSo it turns out that we can easily modify SVM to accommodate this. So what you see here is very similar to what you have seen before, but we have introduced the extra variable xi i. And we in fact will have one for each data instance, and this is going to model the error that we allow for each instance. But the optimization problem would be very similar. So specifically, you will see we have added something to the optimization problem. First we have added some error to the constraint so that now we allow a Allow the classifier to make some mistakes here. So, this Xi i is allowed an error. If we set Xi i to 0, then we go back to the original constraint. We want every instance to be classified accurately. But, if we allow this to be non-zero, then we allow some errors here. In fact, if the length of the Xi i is very large, the error can be very, very large. So naturally, we don't want this to happen. So we want to then also minimize this Xi i. So, because Xi i needs to be minimized in order to control the error.\n16:42\nAnd so, as a result, in the objective function, we also add more to the original one, which is only W, by basically ensuring that we not only minimize the weights, but also minimize the errors, as you see here. Here we simply take a sum over all the instances. Each one has a Xi i to model the error allowed for that instance. And when we combine them together, we basically want to minimize the errors on all of them.\n17:16\nNow you see there's a parameter C here, and that's a constant to control the trade-off between minimizing the errors and maximizing the margin. If C is set to zero, you can see, we go back to the original object function where we only maximize the margin.\n17:34\nWe don't really optimize the training errors and then Xi i can be set to a very large value to make the constraints easy to satisfy. That's not very good of course, so C should be set to a non-zero value, a positive value. But when C is set to a very, very large value, we'll see the object of the function will be dominated mostly by the training errors and so the optimization of margin will then play a secondary role. So if that happens, what would happen is\n18:07\nthen we will try to do our best to minimize the training errors, but then we're not going to take care of the margin and that affects the generalization factors of the classify for future data. So it's also not good. So in particular, this parameter C has to be actually set carefully. And this is just like in the case of k-nearest neighbor where you need to optimize a number of neighbors. Here you need to optimize the C. And this is, in general, also achievable by doing cross-validation. Basically, you look at the empirical data and see what value C should be set to in order to optimize the performance.\n18:49\nNow with this modification, the problem is still quadratic programming with linear constraints so the optimizing algorithm can be actually applied to solve this different version of the program.\n19:02\nAgain, once we have obtained the weights and the bias, then we can have classifier that's ready for classifying new objects. So that's the basic idea of SVM.\n19:16\nSo to summarize the text categorization methods, where we introduce the many methods, and some are generative models. Some are discriminative methods. And these tend to perform similarly when optimized. So there's still no clear winner, although each one has its pros and cons. And the performance might also vary on different data sets for different problems. And one reason is also because the feature representation is very critical\n19:52\nand these methods all require effective feature representation. And to design an effective feature set, we need domain knowledge and humans definitely play an important role here, although there are new machine learning methods and algorithm representation learning that can help with learning features.\n20:12\nAnd another common thing is that they might be performing similarly on the data set, but with different mistakes. And so, their performance might be similar, but then the mistakes they make might be different. So that means it's useful to compare different methods for a particular problem and then maybe combine multiple methods because this can improve the robustness and they won't make the same mistakes. So assemble approaches that would combine different methods tend to be more robust and can be useful in practice. Most techniques that we introduce use the supervised machine learning, which is a very general method. So that means that these methods can be actually applied to any text or categorization problem. As long as we have humans to help annotate some training data sets and design features, then supervising machine learning and all these classifiers can be easily applied to those problems to solve the categorization problem to allow us to characterize content of text concisely with categories. Or to predict the sum properties of real world variables that are associated with text data. The computers, of course, here are trying to optimize the combinations of the features provided by human. And as I said, there are many different ways of combining them and they also optimize different object or functions.\n21:58\nBut in order to achieve good performance, they all require effective features and also plenty of training data.\n22:04\nSo as a general rule, and if you can improve the feature representation, and then provide more training data, then you can generally do better. Performance is often much more affected by the effectiveness of features than by the choice of specific classifiers. So feature design tends to be more important than the choice of specific classifier.\n22:30\nSo, how do we design effective features? Well, unfortunately, this is very application-specific. So there's no really much general thing to say here. But we can do some analysis of the categorization problem and try to understand what kind of features might help us distinguish categories. And in general, we can use a lot of domain knowledge to help us design features.\n23:01\nAnd another way to figure out the effective features is to do error analysis on the categorization results. You could, for example, look at which category tends to be confused with which other categories. And you can use a confusion matrix to examine the errors systematically across categories. And then, you can look into specific instances to see why the mistake has been made and what features can prevent the mistake. And this can allow you to obtain insights for design new features. So error analysis is very important in general, and that's where you can get the insights about your specific problem.\n23:42\nAnd finally, we can leverage this on machine learning techniques. So, for example, feature selection is a technique that we haven't really talked about, but is very important. And it has to do with trying to select the most useful features before you actually train a full classifier. Sometimes training a classifier will also help you identify which features have high values. There are also other ways to ensure this sparsity. Of the model, meaning to recognize the widths. For example, the SVM actually tries to minimize the weights on features. But you can further force some features, force to use only a small number of features.\n24:21\nThere are also techniques for dimension reduction. And that's to reduce a high dimensional feature space into a low dimensional space typically by clustering of features in various ways. So metrics factorization has been used to do such a job, and this is some of the techniques are actually very similar to the talking models that we'll discuss. So talking morals like psa or lda can actually help us reduce the dimension of features. Like imagine the words our original feature. But the can be matched to the topic space .Let's say we have k topics. So a document can now be represented as a vector of just k values corresponding to the topics. So we can let each topic define one dimension, so we have a k dimensional space instead of the original high dimensional space corresponding to words. And this is often another way to learn effective features. Especially, we could also use the categories to supervise the learning of such low dimensional structures.\n25:29\nAnd so, the original worth features can be also combined with such amazing dimension features or lower dimensional space features to provide a multi resolution which is often very useful. Deep learning is a new technique that has been developed the machine learning.\n25:51\nIt's particularly useful for learning representations. So deep learning refers to deep neural network, it's another kind of classifier, where you can have intermediate features embedded in the models. That it's highly non-linear transpire, and some recent events that's allowed us to train such a complex network effectively. And the technique has been shown to be quite effective for speech recognition, computer reasoning, and recently has been applied to text as well. It has shown some promise. And one important advantage of this approach in\n26:34\nrelationship with the featured design, is that they can learn intermediate replantations or compound the features automatically. And this is very valuable for learning effective replantation, for text recalibration. Although in text domain, because words are exemplary representation of text content, because these are human's imaging for communication. And they are generally sufficient for For representing content for many tasks. If there's a need for some new representation, people would have invented a new word. So because of this we think of value of deep learning for text processing tends to be lower than for [INAUDIBLE]. And the speech revenue where they are anchored corresponding where the design that worked as features.\n27:31\nBut people only still very promising for learning effective features especially for complicated tasks. Like a analysis it has been shown to be effective\n27:41\nbecause it can provide that goes beyond that of words.\n27:47\nNow regarding the training examples. It's generally hard to get a lot of training examples because it involves human labor.\n27:56\nBut there are also some ways to help with this. So one is to assume in some low quality training examples can also be used. So, those can be called pseudo training examples. For example, if you take reviews from the internet, they might have overall ratings. So, to train a of categorizer, meaning we want to positive or negative. And categorize these reviews into these two categories. Then we could assume five star reviews are all positive training samples. One star are negative. But of course, sometimes even five star reviews will also mention negative opinions so the training sample is not all of that high quality, but they can still be useful.\n28:45\nAnother idea is to exploit the unlabeled data and there are techniques called the semi-supervised machine learning techniques that can allow you to combine labeled data with unlabeled data. So, in other case it's easy to see the next model can be used For both text plus read and the categorization. So you can imagine, if you have a lot of unlabeled text data for categorization, then you can actually do clustering on these text data, learn categories. And then try to somehow align these categories. With the categories defined by the training data, where we already know which documents are in which category. So you can in fact use the Algorithm to actually combine both. That would allow you essentially also pick up useful words and label the data. You can think of this in another way. Basically, we can use let's say a to classify all of the unlabeled text documents, and then we're going to assume the high confidence Classification results are actually liable. Then you suddenly have more training data because from the enabler that we now know some are labeled as category one, some are labeled as category two. All though the label is not completely reliable But then they can still be useful. So let's assume they are actually training label examples, and then we combine them with true training examples through improved categorization method. And so this idea is very powerful.\n30:23\nWhen the enabled data and the training data are very different, and we might need to use other advanced machine learning techniques called domain adaptation or transfer learning. This is when we can Borrow some training examples from a related problem that may be different. Or, from a categorization password\n30:46\nthat follow very different distribution from what we are working on. But basically, when the two domains are very different, then we need to be careful and not overfit the training domain. But yet, we can still want to use some signals from the related training data. So for example, training categorization on news might not give you Effective plus y for class vine topics and tweets. But you can still learn something from news to help look at writing tweets. So there are mission learning techniques that can help you do that effectively. Here's a suggested reading where you can find more details about some more of the methods is that we have covered. [MUSIC]\nLike\nDislike\nReport an issue\nShare"])
https://www.coursera.org/learn/cs-410/home/week/9dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 9\nWeek 9\n1h 42m of videos left\n10 min of readings left\nAll graded assignments completed\nDuring this module, you will learn topic analysis in depth, including mixture models and how they work, Expectation-Maximization (EM) algorithm and how it can be used to estimate parameters of a mixture model, the basic topic model, Probabilistic Latent Semantic Analysis (PLSA), and how Latent Dirichlet Allocation (LDA) extends PLSA.\nWeek 9 Information\nWeek 9 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 9 Lessons\n9.1 Probabilistic Topic Models: Mixture of Unigram Language Models\nVideo•\n. Duration: 12 minutes\n12 min\n9.2 Probabilistic Topic Models: Mixture Model Estimation: Part 1\nVideo•\n. Duration: 10 minutes\n10 min\n9.3 Probabilistic Topic Models: Mixture Model Estimation: Part 2\nVideo•\n. Duration: 8 minutes\n8 min\n9.4 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 1\nVideo•\n. Duration: 11 minutes\n11 min\n9.5 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 2\nVideo•\n. Duration: 10 minutes\n10 min\n9.6 Probabilistic Topic Models: Expectation-Maximization Algorithm: Part 3\nVideo•\n. Duration: 6 minutes\n6 min\n9.7 Probabilistic Latent Semantic Analysis (PLSA): Part 1\nVideo•\n. Duration: 10 minutes\n10 min\n9.8 Probabilistic Latent Semantic Analysis (PLSA): Part 2\nVideo•\n. Duration: 10 minutes\n10 min\n9.9 Latent Dirichlet Allocation (LDA): Part 1\nVideo•\n. Duration: 10 minutes\n10 min\n9.10 Latent Dirichlet Allocation (LDA): Part 2\nVideo•\n. Duration: 12 minutes\n12 min\nWeek 9 Activities\nComplete\nWeek 9 Practice Quiz\nPractice Quiz•11 questions\nWeek 9 Quiz\nGraded\nQuiz•9 questions\n•Grade: \nProject Proposal Submission for Grading\nGraded\nGraded Assignment•Submitted\n•Grade: '])
https://www.coursera.org/learn/cs-410/home/week/8dict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\nWeek 7\nWeek 8\nWeek 9\nWeek 10\nWeek 11\nWeek 12\nWeek 13\nWeek 14\nWeek 15\nWeek 16\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nWeek 8\nWeek 8\n1h 55m of videos left\n10 min of readings left\nAll graded assignments completed\nDuring this module, you will learn more about word association mining with a particular focus on mining the other basic form of word association (i.e., syntagmatic relations), and start learning topic analysis with a focus on techniques for mining one topic from text.\nWeek 8 Information\nWeek 8 Overview\nReading•\n. Duration: 10 minutes\n10 min\nWeek 8 Lessons\n8.1 Syntagmatic Relation Discovery: Entropy\nVideo•\n. Duration: 11 minutes\n11 min\n8.2 Syntagmatic Relation Discovery: Conditional Entropy\nVideo•\n. Duration: 11 minutes\n11 min\n8.3 Syntagmatic Relation Discovery: Mutual Information: Part 1\nVideo•\n. Duration: 13 minutes\n13 min\n8.4 Syntagmatic Relation Discovery: Mutual Information: Part 2\nVideo•\n. Duration: 9 minutes\n9 min\n8.5 Topic Mining and Analysis: Motivation and Task Definition\nVideo•\n. Duration: 7 minutes\n7 min\n8.6 Topic Mining and Analysis: Term as Topic\nVideo•\n. Duration: 11 minutes\n11 min\n8.7 Topic Mining and Analysis: Probabilistic Topic Models\nVideo•\n. Duration: 14 minutes\n14 min\n8.8 Probabilistic Topic Models: Overview of Statistical Language Models: Part 1\nVideo•\n. Duration: 10 minutes\n10 min\n8.9 Probabilistic Topic Models: Overview of Statistical Language Models: Part 2\nVideo•\n. Duration: 13 minutes\n13 min\n8.10 Probabilistic Topic Models: Mining One Topic\nVideo•\n. Duration: 12 minutes\n12 min\nWeek 8 Activities\nComplete\nWeek 8 Practice Quiz\nPractice Quiz•6 questions\nWeek 8 Quiz\nGraded\nQuiz•7 questions\n•Grade: 85.\nTechnology Review (4-credit students only)\nComplete\nTechnology Review Information\nReading•\n. Duration: 10 minutes\n10 min'])
https://www.coursera.org/learn/cs-410/resources/nOxcBdict_values(['Open Coursera membership menu\nSEARCH IN COURSE\nSearch\nCaleb Thomas\nCourse Navigation\nCS 410: Text Information Systems\nUniversity of Illinois at Urbana-Champaign\nCourse Material\nGrades\nNotes\nLive Events\nMessages\nClassmates\nResources\nOffice Hours Recordings\nOffice Hours Recordings\nOffice Hours Recordings:\nWeek\nProf. Zhai\nAssma\nYuxiang\nDaniel\nKevin\n1\n8/22   8/25\n2\n8/29   9/1\n3\n            9/8\n4\n9/12   9/15\n5\n9/19   9/22\n6\n9/26   9/29\n7\n10/3   10/6\n8\n10/10 10/13\n9\n             10/20\n10\n11\n12\n13\n14 Fall Break\n15\n16\n17 Finals'])
